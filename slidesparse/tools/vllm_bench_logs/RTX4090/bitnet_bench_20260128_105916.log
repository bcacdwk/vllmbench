======================================================================
BitNet Benchmark Log
Started: 2026-01-28 10:59:16
======================================================================

Hardware:
  GPU: NVIDIA GeForce RTX 4090 (cc89)
  Python: py312
  CUDA: cu129
  Arch: x86_64

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260128_105916.log

======================================================================
TASK 1: 基础模型准备 (下载 + 量化)
Started: 2026-01-28 10:59:16
======================================================================


------------------------------------------------------------
  Step 1: 下载 BitNet BF16 模型
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/model_download.py --model bitnet1.58-2b-bf16


Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]Downloading 'model.safetensors' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/xGOKKLRSlIhH692hSVvI1-gpoa8=.529637ff6dab1f5890767356928693f69ffe61d3b6040a43de9306b37bfd5ae1.incomplete'
Downloading 'special_tokens_map.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/ahkChHUJFxEmOdq5GDFEmerRzCY=.d8cd5076496dbe4be2320312abc10adc43097b81.incomplete'
Downloading 'README.md' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.cfe176334831acea2ecdaef02b5ae25416b14941.incomplete'
Downloading 'generation_config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/3EVKVggOldJcKSsGjSdoUCN1AyQ=.650ab2390d65b8182f2599fe7a0f2014eec3f38b.incomplete'
Downloading 'config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.8cb95b54570aababf062b4b8d95e78dc74a7ba51.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/special_tokens_map.json
Downloading 'LICENSE' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/DhCjcNQuMpl4FL346qr3tvNUCgY=.48ea6616b5b8581df3401872996cecf1f8b08a0d.incomplete'
Downloading 'data_summary_card.md' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/rO2cXQjMJqsORRCCpgXS1A8CgMk=.156c5705d7cf1e2f11a27e62f673c4576af7aa19.incomplete'
Downloading '.gitattributes' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/generation_config.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/README.md
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/config.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/LICENSE
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/data_summary_card.md
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/.gitattributes

Fetching 10 files:  10%|█         | 1/10 [00:00<00:06,  1.50it/s]Downloading 'tokenizer_config.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.d54415c89774937967b9baac420d12455ff6e267.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/tokenizer_config.json
Downloading 'tokenizer.json' to '/root/vllmbench/checkpoints/BitNet-2B-BF16/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.b197f72effb9d5ed16ee0f5663e11e4cfac2ba62.incomplete'
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/tokenizer.json
Download complete. Moving file to /root/vllmbench/checkpoints/BitNet-2B-BF16/model.safetensors

Fetching 10 files:  70%|███████   | 7/10 [00:38<00:17,  5.79s/it]
Fetching 10 files: 100%|██████████| 10/10 [00:38<00:00,  3.90s/it]
/root/vllmbench/checkpoints/BitNet-2B-BF16

============================================================
  准备下载 1 个模型 (~4.8 GB)
============================================================

  - BitNet-2B-BF16 (4.8 GB)


============================================================
  下载: BitNet-2B-BF16
============================================================

[INFO] HuggingFace: microsoft/bitnet-b1.58-2B-4T-bf16
[INFO] 本地目录: /root/vllmbench/checkpoints/BitNet-2B-BF16

[INFO] 下载命令: hf download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir /root/vllmbench/checkpoints/BitNet-2B-BF16
[SUCCESS] 下载成功: /root/vllmbench/checkpoints/BitNet-2B-BF16


============================================================
  下载完成
============================================================

成功: 1/1

============================================================
  模型下载状态
============================================================


INT8 模型:
----------------------------------------
  ✗ Qwen2.5-0.5B-INT8 - not downloaded
  ✗ Llama3.2-1B-INT8 - not downloaded
  ✗ Qwen2.5-1.5B-INT8 - not downloaded
  ✗ BitNet-2B-INT8 - not downloaded
  ✗ Qwen2.5-3B-INT8 - not downloaded
  ✗ Llama3.2-3B-INT8 - not downloaded
  ✗ Qwen2.5-7B-INT8 - not downloaded
  ✗ Qwen2.5-14B-INT8 - not downloaded

FP8 模型:
----------------------------------------
  ✗ Qwen2.5-0.5B-FP8 - not downloaded
  ✗ Llama3.2-1B-FP8 - not downloaded
  ✗ Qwen2.5-1.5B-FP8 - not downloaded
  ✗ BitNet-2B-FP8 - not downloaded
  ✗ Qwen2.5-3B-FP8 - not downloaded
  ✗ Llama3.2-3B-FP8 - not downloaded
  ✗ Qwen2.5-7B-FP8 - not downloaded
  ✗ Qwen2.5-14B-FP8 - not downloaded

BF16 模型:
----------------------------------------
  ✓ BitNet-2B-BF16 - 4.5 GB

----------------------------------------
总计: 1 已下载, 16 缺失
[INFO] Checkpoints 目录大小: 4.6G
[SUCCESS] 下载完成 (46.4s)

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-INT8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-BF16 --bitnet --output-dtype int8 --Z 2 --L 2 --skip-slide
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/root/vllmbench/slidesparse/utils.py:2928: UserWarning: L=2 < 4，这是纯量化模式（无稀疏），slide 操作将被跳过
  warnings.warn(
======================================================================
Processing: BitNet-2B-BF16
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=2, N=1, expand=0.000)
[INFO] Mode: magnitude
[INFO] BitNet Mode: enabled (output_dtype=int8)
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2

[INFO] Copying non-weight files...
[INFO]   Copied: generation_config.json, config.json, data_summary_card.md, LICENSE, tokenizer.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.20%)
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.11%)
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.86%)
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.95%)
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (30.11%)
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.73%)
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (34.84%)
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (48.35%)
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (48.39%)
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.94%)
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.03%)
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.16%)
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.94%)
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.50%)
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.07%)
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (30.23%)
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.13%)
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.83%)
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.66%)
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.92%)
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.62%)
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.26%)
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.88%)
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.42%)
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.01%)
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.84%)
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.80%)
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (28.71%)
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.19%)
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.88%)
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.23%)
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.39%)
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.59%)
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.83%)
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.56%)
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.50%)
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.43%)
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.34%)
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.94%)
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (32.82%)
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.05%)
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.77%)
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.51%)
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.69%)
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.65%)
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.66%)
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.11%)
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (9.15%)
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.99%)
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.84%)
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.18%)
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.87%)
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.09%)
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (10.66%)
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.45%)
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.72%)
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.49%)
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.51%)
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.87%)
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.63%)
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.37%)
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.68%)
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.87%)
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.39%)
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.39%)
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.62%)
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (9.69%)
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.53%)
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.64%)
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.42%)
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.76%)
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.46%)
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.77%)
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.25%)
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.50%)
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (35.59%)
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (35.75%)
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.00%)
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.57%)
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.80%)
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.10%)
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.53%)
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.29%)
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.71%)
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.97%)
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.26%)
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.80%)
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.48%)
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.43%)
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.31%)
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.73%)
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.93%)
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.33%)
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.75%)
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.56%)
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.30%)
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.90%)
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.96%)
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.52%)
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.68%)
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (10.90%)
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.93%)
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.02%)
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.00%)
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (26.12%)
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.67%)
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.05%)
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.28%)
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.51%)
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (27.09%)
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.74%)
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (26.75%)
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.67%)
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.17%)
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.72%)
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.73%)
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.89%)
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.86%)
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.68%)
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.43%)
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.59%)
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.87%)
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.93%)
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.90%)
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.77%)
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.64%)
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.03%)
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.46%)
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.20%)
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.79%)
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (11.23%)
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.35%)
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.77%)
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.66%)
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.80%)
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.11%)
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.23%)
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.79%)
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.63%)
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.30%)
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.55%)
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (31.95%)
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (24.72%)
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.33%)
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.01%)
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.91%)
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.39%)
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.90%)
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.76%)
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.22%)
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.48%)
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.07%)
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.66%)
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.86%)
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.77%)
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (21.19%)
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.58%)
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.42%)
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.40%)
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.35%)
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.08%)
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.80%)
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.22%)
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (18.52%)
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.81%)
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.55%)
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.32%)
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.18%)
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.32%)
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.95%)
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.81%)
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (20.26%)
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (22.84%)
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (13.07%)
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.65%)
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.41%)
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.82%)
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (16.87%)
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.07%)
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (14.12%)
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (15.52%)
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (23.18%)
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.47%)
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (19.99%)
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: int8
[INFO]     2:2 validation: ✗ (12.45%)

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 122 layers
[INFO] Time: 15.25s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/conversion_report.json
[SUCCESS] BitNet-2B-INT8 量化完成并移动到 /root/vllmbench/checkpoints/BitNet-2B-INT8 (21.6s)
[SUCCESS]   ✓ 已修正 BitNet-2B-INT8/config.json (vLLM 兼容)
[SUCCESS]   ✓ 已删除 60 个不兼容权重 (ffn_sub_norm, attn_sub_norm)

------------------------------------------------------------
  Step 2: 量化为 BitNet-2B-FP8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/weight_convert/weight_convert_entry.py --model BitNet-2B-BF16 --bitnet --output-dtype fp8_e4m3 --Z 2 --L 2 --skip-slide
[INFO] 工作目录: /root/vllmbench/slidesparse/weight_convert

/root/vllmbench/slidesparse/utils.py:2928: UserWarning: L=2 < 4，这是纯量化模式（无稀疏），slide 操作将被跳过
  warnings.warn(
======================================================================
Processing: BitNet-2B-BF16
======================================================================
[INFO] Config: SlideSparseConfig(Z=2, L=2, N=1, expand=0.000)
[INFO] Mode: magnitude
[INFO] BitNet Mode: enabled (output_dtype=fp8_e4m3)
[INFO] Output: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2

[INFO] Copying non-weight files...
[INFO]   Copied: generation_config.json, config.json, data_summary_card.md, LICENSE, tokenizer.json...

[INFO] Processing file: model.safetensors
[INFO] 
Layer: model.layers.0.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.20%)
[INFO] 
Layer: model.layers.0.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.11%)
[INFO] 
Layer: model.layers.0.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.86%)
[INFO] 
Layer: model.layers.0.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.0.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.95%)
[INFO] 
Layer: model.layers.0.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (30.11%)
[INFO] 
Layer: model.layers.0.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.73%)
[INFO] 
Layer: model.layers.1.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (34.84%)
[INFO] 
Layer: model.layers.1.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (48.35%)
[INFO] 
Layer: model.layers.1.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (48.39%)
[INFO] 
Layer: model.layers.1.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.94%)
[INFO] 
Layer: model.layers.1.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.03%)
[INFO] 
Layer: model.layers.1.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.16%)
[INFO] 
Layer: model.layers.1.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.10.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.94%)
[INFO] 
Layer: model.layers.10.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.50%)
[INFO] 
Layer: model.layers.10.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.07%)
[INFO] 
Layer: model.layers.10.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (30.23%)
[INFO] 
Layer: model.layers.10.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.13%)
[INFO] 
Layer: model.layers.10.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.83%)
[INFO] 
Layer: model.layers.10.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.66%)
[INFO] 
Layer: model.layers.11.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.92%)
[INFO] 
Layer: model.layers.11.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.62%)
[INFO] 
Layer: model.layers.11.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.11.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.26%)
[INFO] 
Layer: model.layers.11.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.88%)
[INFO] 
Layer: model.layers.11.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.42%)
[INFO] 
Layer: model.layers.11.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.01%)
[INFO] 
Layer: model.layers.12.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.84%)
[INFO] 
Layer: model.layers.12.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.12.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.80%)
[INFO] 
Layer: model.layers.12.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (28.71%)
[INFO] 
Layer: model.layers.12.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.19%)
[INFO] 
Layer: model.layers.12.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.88%)
[INFO] 
Layer: model.layers.12.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.13.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.23%)
[INFO] 
Layer: model.layers.13.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.39%)
[INFO] 
Layer: model.layers.13.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.59%)
[INFO] 
Layer: model.layers.13.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.83%)
[INFO] 
Layer: model.layers.13.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.56%)
[INFO] 
Layer: model.layers.13.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.50%)
[INFO] 
Layer: model.layers.13.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.14.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.43%)
[INFO] 
Layer: model.layers.14.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.34%)
[INFO] 
Layer: model.layers.14.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.94%)
[INFO] 
Layer: model.layers.14.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (32.82%)
[INFO] 
Layer: model.layers.14.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.05%)
[INFO] 
Layer: model.layers.14.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.77%)
[INFO] 
Layer: model.layers.14.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.51%)
[INFO] 
Layer: model.layers.15.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.15.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.69%)
[INFO] 
Layer: model.layers.15.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.65%)
[INFO] 
Layer: model.layers.15.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.15.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.66%)
[INFO] 
Layer: model.layers.15.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.11%)
[INFO] 
Layer: model.layers.15.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (9.15%)
[INFO] 
Layer: model.layers.16.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.99%)
[INFO] 
Layer: model.layers.16.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.84%)
[INFO] 
Layer: model.layers.16.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.18%)
[INFO] 
Layer: model.layers.16.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.87%)
[INFO] 
Layer: model.layers.16.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.16.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.09%)
[INFO] 
Layer: model.layers.16.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (10.66%)
[INFO] 
Layer: model.layers.17.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.45%)
[INFO] 
Layer: model.layers.17.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.72%)
[INFO] 
Layer: model.layers.17.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.49%)
[INFO] 
Layer: model.layers.17.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.51%)
[INFO] 
Layer: model.layers.17.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.87%)
[INFO] 
Layer: model.layers.17.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.63%)
[INFO] 
Layer: model.layers.17.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.37%)
[INFO] 
Layer: model.layers.18.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.68%)
[INFO] 
Layer: model.layers.18.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.87%)
[INFO] 
Layer: model.layers.18.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.18.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.39%)
[INFO] 
Layer: model.layers.18.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.39%)
[INFO] 
Layer: model.layers.18.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.62%)
[INFO] 
Layer: model.layers.18.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (9.69%)
[INFO] 
Layer: model.layers.19.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.53%)
[INFO] 
Layer: model.layers.19.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.64%)
[INFO] 
Layer: model.layers.19.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.42%)
[INFO] 
Layer: model.layers.19.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.76%)
[INFO] 
Layer: model.layers.19.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.46%)
[INFO] 
Layer: model.layers.19.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.77%)
[INFO] 
Layer: model.layers.19.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.25%)
[INFO] 
Layer: model.layers.2.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.50%)
[INFO] 
Layer: model.layers.2.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (35.59%)
[INFO] 
Layer: model.layers.2.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (35.75%)
[INFO] 
Layer: model.layers.2.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.00%)
[INFO] 
Layer: model.layers.2.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.57%)
[INFO] 
Layer: model.layers.2.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.80%)
[INFO] 
Layer: model.layers.2.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.10%)
[INFO] 
Layer: model.layers.20.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.53%)
[INFO] 
Layer: model.layers.20.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.29%)
[INFO] 
Layer: model.layers.20.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.71%)
[INFO] 
Layer: model.layers.20.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.97%)
[INFO] 
Layer: model.layers.20.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.26%)
[INFO] 
Layer: model.layers.20.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.80%)
[INFO] 
Layer: model.layers.20.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.48%)
[INFO] 
Layer: model.layers.21.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.43%)
[INFO] 
Layer: model.layers.21.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.31%)
[INFO] 
Layer: model.layers.21.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.73%)
[INFO] 
Layer: model.layers.21.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.93%)
[INFO] 
Layer: model.layers.21.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.33%)
[INFO] 
Layer: model.layers.21.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.75%)
[INFO] 
Layer: model.layers.21.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.57%)
[INFO] 
Layer: model.layers.22.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.56%)
[INFO] 
Layer: model.layers.22.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.30%)
[INFO] 
Layer: model.layers.22.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.90%)
[INFO] 
Layer: model.layers.22.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.96%)
[INFO] 
Layer: model.layers.22.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.52%)
[INFO] 
Layer: model.layers.22.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.68%)
[INFO] 
Layer: model.layers.22.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (10.90%)
[INFO] 
Layer: model.layers.23.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.93%)
[INFO] 
Layer: model.layers.23.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.02%)
[INFO] 
Layer: model.layers.23.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.00%)
[INFO] 
Layer: model.layers.23.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (26.12%)
[INFO] 
Layer: model.layers.23.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.42%)
[INFO] 
Layer: model.layers.23.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.67%)
[INFO] 
Layer: model.layers.23.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.05%)
[INFO] 
Layer: model.layers.24.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.28%)
[INFO] 
Layer: model.layers.24.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.27%)
[INFO] 
Layer: model.layers.24.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.51%)
[INFO] 
Layer: model.layers.24.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (27.09%)
[INFO] 
Layer: model.layers.24.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.74%)
[INFO] 
Layer: model.layers.24.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (26.75%)
[INFO] 
Layer: model.layers.24.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.67%)
[INFO] 
Layer: model.layers.25.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.08%)
[INFO] 
Layer: model.layers.25.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.17%)
[INFO] 
Layer: model.layers.25.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.72%)
[INFO] 
Layer: model.layers.25.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.73%)
[INFO] 
Layer: model.layers.25.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.89%)
[INFO] 
Layer: model.layers.25.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.86%)
[INFO] 
Layer: model.layers.25.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.68%)
[INFO] 
Layer: model.layers.26.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.43%)
[INFO] 
Layer: model.layers.26.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.59%)
[INFO] 
Layer: model.layers.26.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.71%)
[INFO] 
Layer: model.layers.26.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.87%)
[INFO] 
Layer: model.layers.26.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.93%)
[INFO] 
Layer: model.layers.26.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.90%)
[INFO] 
Layer: model.layers.26.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.77%)
[INFO] 
Layer: model.layers.27.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.64%)
[INFO] 
Layer: model.layers.27.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.03%)
[INFO] 
Layer: model.layers.27.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.46%)
[INFO] 
Layer: model.layers.27.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.20%)
[INFO] 
Layer: model.layers.27.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.27.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.79%)
[INFO] 
Layer: model.layers.27.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (11.23%)
[INFO] 
Layer: model.layers.28.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.35%)
[INFO] 
Layer: model.layers.28.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.77%)
[INFO] 
Layer: model.layers.28.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.66%)
[INFO] 
Layer: model.layers.28.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.28.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.80%)
[INFO] 
Layer: model.layers.28.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.11%)
[INFO] 
Layer: model.layers.28.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.23%)
[INFO] 
Layer: model.layers.29.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.79%)
[INFO] 
Layer: model.layers.29.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.63%)
[INFO] 
Layer: model.layers.29.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.30%)
[INFO] 
Layer: model.layers.29.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.55%)
[INFO] 
Layer: model.layers.29.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (31.95%)
[INFO] 
Layer: model.layers.29.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.35%)
[INFO] 
Layer: model.layers.29.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (24.72%)
[INFO] 
Layer: model.layers.3.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.33%)
[INFO] 
Layer: model.layers.3.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.01%)
[INFO] 
Layer: model.layers.3.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.91%)
[INFO] 
Layer: model.layers.3.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.39%)
[INFO] 
Layer: model.layers.3.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.90%)
[INFO] 
Layer: model.layers.3.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.76%)
[INFO] 
Layer: model.layers.3.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.22%)
[INFO] 
Layer: model.layers.4.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.48%)
[INFO] 
Layer: model.layers.4.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.07%)
[INFO] 
Layer: model.layers.4.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.66%)
[INFO] 
Layer: model.layers.4.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.86%)
[INFO] 
Layer: model.layers.4.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.77%)
[INFO] 
Layer: model.layers.4.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (21.19%)
[INFO] 
Layer: model.layers.4.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.5.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.58%)
[INFO] 
Layer: model.layers.5.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.12%)
[INFO] 
Layer: model.layers.5.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.42%)
[INFO] 
Layer: model.layers.5.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.40%)
[INFO] 
Layer: model.layers.5.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.35%)
[INFO] 
Layer: model.layers.5.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.08%)
[INFO] 
Layer: model.layers.5.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.80%)
[INFO] 
Layer: model.layers.6.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.22%)
[INFO] 
Layer: model.layers.6.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.38%)
[INFO] 
Layer: model.layers.6.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (18.52%)
[INFO] 
Layer: model.layers.6.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.81%)
[INFO] 
Layer: model.layers.6.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.55%)
[INFO] 
Layer: model.layers.6.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.32%)
[INFO] 
Layer: model.layers.6.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.18%)
[INFO] 
Layer: model.layers.7.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.32%)
[INFO] 
Layer: model.layers.7.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.95%)
[INFO] 
Layer: model.layers.7.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.81%)
[INFO] 
Layer: model.layers.7.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (20.26%)
[INFO] 
Layer: model.layers.7.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.76%)
[INFO] 
Layer: model.layers.7.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (22.84%)
[INFO] 
Layer: model.layers.7.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (13.07%)
[INFO] 
Layer: model.layers.8.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.28%)
[INFO] 
Layer: model.layers.8.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.65%)
[INFO] 
Layer: model.layers.8.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.41%)
[INFO] 
Layer: model.layers.8.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.82%)
[INFO] 
Layer: model.layers.8.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (16.87%)
[INFO] 
Layer: model.layers.8.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (25.70%)
[INFO] 
Layer: model.layers.8.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.07%)
[INFO] 
Layer: model.layers.9.mlp.down_proj.weight
[INFO]   Input: shape=[2560, 6912], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (14.12%)
[INFO] 
Layer: model.layers.9.mlp.gate_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (17.97%)
[INFO] 
Layer: model.layers.9.mlp.up_proj.weight
[INFO]   Input: shape=[6912, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (15.52%)
[INFO] 
Layer: model.layers.9.self_attn.k_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (23.18%)
[INFO] 
Layer: model.layers.9.self_attn.o_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.47%)
[INFO] 
Layer: model.layers.9.self_attn.q_proj.weight
[INFO]   Input: shape=[2560, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (19.99%)
[INFO] 
Layer: model.layers.9.self_attn.v_proj.weight
[INFO]   Input: shape=[640, 2560], dtype=bf16
[INFO]   Stage 1: Quant+Pruning (BitNet 2:2, mode=magnitude)
[INFO]     Output dtype: fp8_e4m3
[INFO]     2:2 validation: ✗ (12.45%)

[INFO] Saving: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/model.safetensors

======================================================================
Summary
======================================================================
[✓] Processed: 210 layers
[INFO] Skipped: 122 layers
[INFO] Time: 17.56s
[INFO] Report: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-BF16-SlideSparse-2_2/conversion_report.json
[SUCCESS] BitNet-2B-FP8 量化完成并移动到 /root/vllmbench/checkpoints/BitNet-2B-FP8 (23.8s)
[SUCCESS]   ✓ 已修正 BitNet-2B-FP8/config.json (vLLM 兼容)
[SUCCESS]   ✓ 已删除 60 个不兼容权重 (ffn_sub_norm, attn_sub_norm)

[INFO] 基础模型准备统计: 成功 2, 失败 0

------------------------------------------------------------
  验证基础模型
------------------------------------------------------------
[SUCCESS]   ✓ BitNet-2B-INT8
[SUCCESS]   ✓ BitNet-2B-FP8

----------------------------------------------------------------------
TASK 1: 基础模型准备 (下载 + 量化) - SUCCESS
Duration: 99.9 seconds (1.7 minutes)
----------------------------------------------------------------------

[INFO] 跳过 Task 2: SlideSparse 转换 (prune + slide)
[INFO] 跳过 Task 3: 离线调优 (粗调优 + 细调优)

======================================================================
TASK 4: 完整 Prefill Benchmark
Started: 2026-01-28 11:00:56
======================================================================


------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 4090                   ││
│ GPU (short):      RTX4090                                   │
│ Memory:           24.0 GB                                    │
│ CC:               cc89 (Ampere)                              ││
│ SM Code:          sm_89                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           581.80                                    │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_110101.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:01:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:01:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=3490) WARNING 01-28 11:01:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=3490) WARNING 01-28 11:01:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.19 requests/s, 7789.92 total tokens/s, 15.19 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:01:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:01:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:01:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:01:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:01:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:01:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:01:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:01:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:01:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:01:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:01:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:01:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:01:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:01:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:01:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:01:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:01:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:01:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:01:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 11:01:26.071170209 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=3490) [2026-01-28 11:01:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3490) [2026-01-28 11:01:27] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3490) [2026-01-28 11:01:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3490) [2026-01-28 11:01:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3490) [2026-01-28 11:01:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3490) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=3490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=3490) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3490) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.29it/s]
(EngineCore_DP0 pid=3490) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.79it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 67/128 [00:00<00:00, 663.20it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 677.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:04, 27.02it/s, est. speed input: 13833.96 toks/s, output: 27.02 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:06, 20.02it/s, est. speed input: 10725.83 toks/s, output: 20.95 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:06, 18.12it/s, est. speed input: 9826.74 toks/s, output: 19.19 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:06, 17.13it/s, est. speed input: 9402.99 toks/s, output: 18.36 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:07, 16.26it/s, est. speed input: 9053.16 toks/s, output: 17.68 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:07, 15.83it/s, est. speed input: 8846.20 toks/s, output: 17.28 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:01<00:07, 15.52it/s, est. speed input: 8686.66 toks/s, output: 16.97 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:07, 15.31it/s, est. speed input: 8563.62 toks/s, output: 16.73 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:07, 15.12it/s, est. speed input: 8458.06 toks/s, output: 16.52 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 15.06it/s, est. speed input: 8382.79 toks/s, output: 16.37 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 15.03it/s, est. speed input: 8322.57 toks/s, output: 16.25 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:06, 14.98it/s, est. speed input: 8266.64 toks/s, output: 16.15 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:06, 14.94it/s, est. speed input: 8219.63 toks/s, output: 16.05 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:02<00:06, 14.90it/s, est. speed input: 8175.98 toks/s, output: 15.97 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:02<00:06, 14.74it/s, est. speed input: 8122.91 toks/s, output: 15.86 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:06, 14.85it/s, est. speed input: 8101.31 toks/s, output: 15.82 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 15.05it/s, est. speed input: 8093.31 toks/s, output: 15.81 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 15.32it/s, est. speed input: 8097.65 toks/s, output: 15.82 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 15.50it/s, est. speed input: 8101.11 toks/s, output: 15.82 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 15.68it/s, est. speed input: 8107.60 toks/s, output: 15.84 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:05, 15.65it/s, est. speed input: 8101.99 toks/s, output: 15.82 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:03<00:05, 15.67it/s, est. speed input: 8099.26 toks/s, output: 15.82 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:03<00:04, 15.71it/s, est. speed input: 8098.99 toks/s, output: 15.82 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 15.78it/s, est. speed input: 8101.56 toks/s, output: 15.82 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 15.80it/s, est. speed input: 8102.16 toks/s, output: 15.82 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 15.75it/s, est. speed input: 8098.56 toks/s, output: 15.82 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 15.83it/s, est. speed input: 8102.30 toks/s, output: 15.82 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 15.87it/s, est. speed input: 8104.38 toks/s, output: 15.83 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:04, 15.89it/s, est. speed input: 8106.18 toks/s, output: 15.83 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:04<00:04, 15.94it/s, est. speed input: 8109.84 toks/s, output: 15.84 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:04<00:03, 15.87it/s, est. speed input: 8107.51 toks/s, output: 15.83 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 15.86it/s, est. speed input: 8107.80 toks/s, output: 15.84 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 15.23it/s, est. speed input: 8076.24 toks/s, output: 15.77 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 15.21it/s, est. speed input: 8067.50 toks/s, output: 15.76 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 15.34it/s, est. speed input: 8065.77 toks/s, output: 15.75 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 15.34it/s, est. speed input: 8060.16 toks/s, output: 15.74 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 15.13it/s, est. speed input: 8045.15 toks/s, output: 15.71 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:05<00:03, 15.25it/s, est. speed input: 8042.63 toks/s, output: 15.71 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:05<00:03, 14.98it/s, est. speed input: 8024.64 toks/s, output: 15.67 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 15.10it/s, est. speed input: 8021.16 toks/s, output: 15.67 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 15.18it/s, est. speed input: 8017.60 toks/s, output: 15.66 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 15.29it/s, est. speed input: 8016.49 toks/s, output: 15.66 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 15.42it/s, est. speed input: 8017.37 toks/s, output: 15.66 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 15.57it/s, est. speed input: 8020.14 toks/s, output: 15.66 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 15.67it/s, est. speed input: 8022.99 toks/s, output: 15.67 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:06<00:02, 15.69it/s, est. speed input: 8023.54 toks/s, output: 15.67 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:06<00:01, 15.68it/s, est. speed input: 8023.37 toks/s, output: 15.67 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 15.73it/s, est. speed input: 8025.13 toks/s, output: 15.67 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 15.83it/s, est. speed input: 8029.11 toks/s, output: 15.68 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 15.86it/s, est. speed input: 8031.60 toks/s, output: 15.69 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 15.87it/s, est. speed input: 8033.50 toks/s, output: 15.69 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 15.82it/s, est. speed input: 8033.68 toks/s, output: 15.69 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:09<00:06,  2.65it/s, est. speed input: 6184.42 toks/s, output: 12.08 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:09<00:04,  3.52it/s, est. speed input: 6207.84 toks/s, output: 12.12 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:09<00:03,  4.55it/s, est. speed input: 6225.18 toks/s, output: 12.16 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:09<00:02,  5.75it/s, est. speed input: 6244.35 toks/s, output: 12.20 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:09<00:01,  7.03it/s, est. speed input: 6262.50 toks/s, output: 12.23 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:09<00:00,  8.33it/s, est. speed input: 6279.75 toks/s, output: 12.27 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:09<00:00,  9.56it/s, est. speed input: 6296.26 toks/s, output: 12.30 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:10<00:00, 10.62it/s, est. speed input: 6310.66 toks/s, output: 12.33 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:10<00:00, 11.52it/s, est. speed input: 6324.68 toks/s, output: 12.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 12.28it/s, est. speed input: 6339.61 toks/s, output: 12.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 12.28it/s, est. speed input: 6339.61 toks/s, output: 12.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 12.38it/s, est. speed input: 6339.61 toks/s, output: 12.38 toks/s]
[rank0]:[W128 11:01:59.879997651 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 61.0s

测试结果:
  Requests/s:   15.19
  Tokens/s:     7789.92
  Total Reqs:   128
  Elapsed:      8.43s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     7774.73

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:02:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:02:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=4075) WARNING 01-28 11:02:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=4075) WARNING 01-28 11:02:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.64 requests/s, 16034.12 total tokens/s, 15.64 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:02:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:02:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:02:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:02:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:02:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:02:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:02:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:02:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:02:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:02:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:02:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:02:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:02:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:02:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:02:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:02:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:02:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:02:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:02:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4075) [2026-01-28 11:02:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4075) [2026-01-28 11:02:18] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4075) [2026-01-28 11:02:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4075) [2026-01-28 11:02:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=4075) [2026-01-28 11:02:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=4075) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.73it/s]
(EngineCore_DP0 pid=4075) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.73it/s]
(EngineCore_DP0 pid=4075) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=4075) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.31it/s]
(EngineCore_DP0 pid=4075) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 35/128 [00:00<00:00, 347.15it/s]
Adding requests:  56%|█████▋    | 72/128 [00:00<00:00, 358.61it/s]
Adding requests:  84%|████████▍ | 108/128 [00:00<00:00, 347.17it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 349.82it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 44.60it/s, est. speed input: 45680.37 toks/s, output: 44.60 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 22.96it/s, est. speed input: 25537.48 toks/s, output: 24.94 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:05, 20.30it/s, est. speed input: 22916.81 toks/s, output: 22.38 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:05, 18.77it/s, est. speed input: 21439.75 toks/s, output: 20.94 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:06, 17.91it/s, est. speed input: 20550.36 toks/s, output: 20.07 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.44it/s, est. speed input: 20085.12 toks/s, output: 19.61 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 17.08it/s, est. speed input: 19721.50 toks/s, output: 19.26 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.82it/s, est. speed input: 19432.05 toks/s, output: 18.98 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 16.69it/s, est. speed input: 19209.79 toks/s, output: 18.76 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.59it/s, est. speed input: 19021.73 toks/s, output: 18.58 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 16.50it/s, est. speed input: 18856.97 toks/s, output: 18.41 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 16.43it/s, est. speed input: 18710.04 toks/s, output: 18.27 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:05, 16.31it/s, est. speed input: 18566.18 toks/s, output: 18.13 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 16.14it/s, est. speed input: 18419.52 toks/s, output: 17.99 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 16.10it/s, est. speed input: 18306.68 toks/s, output: 17.88 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 15.84it/s, est. speed input: 18157.62 toks/s, output: 17.73 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 15.96it/s, est. speed input: 18082.90 toks/s, output: 17.66 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:05, 15.85it/s, est. speed input: 17978.62 toks/s, output: 17.56 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:05, 15.78it/s, est. speed input: 17887.51 toks/s, output: 17.47 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 15.94it/s, est. speed input: 17837.56 toks/s, output: 17.42 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:04, 15.94it/s, est. speed input: 17774.41 toks/s, output: 17.36 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 15.94it/s, est. speed input: 17715.63 toks/s, output: 17.30 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 16.02it/s, est. speed input: 17672.60 toks/s, output: 17.26 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 16.09it/s, est. speed input: 17635.58 toks/s, output: 17.22 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 16.15it/s, est. speed input: 17602.27 toks/s, output: 17.19 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:04, 16.14it/s, est. speed input: 17564.30 toks/s, output: 17.15 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:04, 15.94it/s, est. speed input: 17505.77 toks/s, output: 17.10 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 15.78it/s, est. speed input: 17447.91 toks/s, output: 17.04 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 15.51it/s, est. speed input: 17375.68 toks/s, output: 16.97 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 15.45it/s, est. speed input: 17321.52 toks/s, output: 16.92 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 15.40it/s, est. speed input: 17270.48 toks/s, output: 16.87 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 15.67it/s, est. speed input: 17255.61 toks/s, output: 16.85 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 15.59it/s, est. speed input: 17213.53 toks/s, output: 16.81 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 15.59it/s, est. speed input: 17179.03 toks/s, output: 16.78 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:03, 15.53it/s, est. speed input: 17140.54 toks/s, output: 16.74 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 15.51it/s, est. speed input: 17105.90 toks/s, output: 16.70 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 15.69it/s, est. speed input: 17091.67 toks/s, output: 16.69 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 15.78it/s, est. speed input: 17074.49 toks/s, output: 16.67 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 15.92it/s, est. speed input: 17064.28 toks/s, output: 16.66 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 15.97it/s, est. speed input: 17050.83 toks/s, output: 16.65 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 16.08it/s, est. speed input: 17043.48 toks/s, output: 16.64 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 16.13it/s, est. speed input: 17034.67 toks/s, output: 16.64 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 16.18it/s, est. speed input: 17027.33 toks/s, output: 16.63 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 16.18it/s, est. speed input: 17017.74 toks/s, output: 16.62 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 16.18it/s, est. speed input: 17008.19 toks/s, output: 16.61 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 15.95it/s, est. speed input: 16983.50 toks/s, output: 16.59 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 15.72it/s, est. speed input: 16953.46 toks/s, output: 16.56 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 15.67it/s, est. speed input: 16932.98 toks/s, output: 16.54 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 15.59it/s, est. speed input: 16910.29 toks/s, output: 16.51 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 15.44it/s, est. speed input: 16881.43 toks/s, output: 16.49 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:01, 15.31it/s, est. speed input: 16852.03 toks/s, output: 16.46 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 15.28it/s, est. speed input: 16827.92 toks/s, output: 16.43 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:07<00:00, 15.38it/s, est. speed input: 16812.90 toks/s, output: 16.42 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 15.37it/s, est. speed input: 16792.67 toks/s, output: 16.40 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 15.58it/s, est. speed input: 16787.26 toks/s, output: 16.39 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 15.77it/s, est. speed input: 16784.70 toks/s, output: 16.39 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 15.83it/s, est. speed input: 16777.35 toks/s, output: 16.38 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 15.90it/s, est. speed input: 16772.14 toks/s, output: 16.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.03it/s, est. speed input: 16771.67 toks/s, output: 16.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.03it/s, est. speed input: 16771.67 toks/s, output: 16.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.38it/s, est. speed input: 16771.67 toks/s, output: 16.38 toks/s]
[rank0]:[W128 11:02:49.235694367 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.0s

测试结果:
  Requests/s:   15.64
  Tokens/s:     16034.12
  Total Reqs:   128
  Elapsed:      8.18s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     16018.48

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:03:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:03:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=4636) WARNING 01-28 11:03:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=4636) WARNING 01-28 11:03:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.73 requests/s, 31494.16 total tokens/s, 30.73 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 11:03:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:03:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:03:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:03:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:03:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:03:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:03:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:03:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:03:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:03:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:03:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:03:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:03:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:03:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:03:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:03:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:03:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:03:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4636) [2026-01-28 11:03:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4636) [2026-01-28 11:03:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4636) [2026-01-28 11:03:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4636) [2026-01-28 11:03:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=4636) [2026-01-28 11:03:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=4636) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4636) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=4636) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=4636) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=4636) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.39it/s]
(EngineCore_DP0 pid=4636) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.63it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▎        | 35/256 [00:00<00:00, 349.42it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 371.04it/s]
Adding requests:  44%|████▍     | 112/256 [00:00<00:00, 373.95it/s]
Adding requests:  59%|█████▊    | 150/256 [00:00<00:00, 370.00it/s]
Adding requests:  73%|███████▎  | 188/256 [00:00<00:00, 367.54it/s]
Adding requests:  88%|████████▊ | 225/256 [00:00<00:00, 367.84it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 367.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:02, 100.14it/s, est. speed input: 102562.18 toks/s, output: 100.15 toks/s]
Processed prompts:  10%|▉         | 25/256 [00:00<00:04, 50.22it/s, est. speed input: 56127.34 toks/s, output: 54.81 toks/s]   
Processed prompts:  12%|█▎        | 32/256 [00:00<00:05, 40.33it/s, est. speed input: 46764.71 toks/s, output: 45.66 toks/s]
Processed prompts:  14%|█▍        | 37/256 [00:00<00:05, 40.47it/s, est. speed input: 46033.33 toks/s, output: 44.95 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:01<00:05, 35.91it/s, est. speed input: 42735.27 toks/s, output: 41.73 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:01<00:05, 35.18it/s, est. speed input: 41768.19 toks/s, output: 40.79 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:01<00:05, 34.58it/s, est. speed input: 40976.24 toks/s, output: 40.02 toks/s]
Processed prompts:  21%|██        | 54/256 [00:01<00:05, 34.05it/s, est. speed input: 40301.83 toks/s, output: 39.36 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:05, 33.54it/s, est. speed input: 39697.81 toks/s, output: 38.77 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 33.17it/s, est. speed input: 39186.91 toks/s, output: 38.27 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:05, 33.04it/s, est. speed input: 38789.86 toks/s, output: 37.88 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 32.81it/s, est. speed input: 38405.95 toks/s, output: 37.51 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:05, 32.78it/s, est. speed input: 38107.36 toks/s, output: 37.21 toks/s]
Processed prompts:  30%|███       | 78/256 [00:02<00:05, 32.84it/s, est. speed input: 37856.25 toks/s, output: 36.97 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:02<00:05, 32.74it/s, est. speed input: 37604.78 toks/s, output: 36.72 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:05, 32.22it/s, est. speed input: 37287.46 toks/s, output: 36.41 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:05, 32.25it/s, est. speed input: 37079.18 toks/s, output: 36.21 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:05, 32.24it/s, est. speed input: 36885.23 toks/s, output: 36.02 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 32.27it/s, est. speed input: 36713.37 toks/s, output: 35.85 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 31.94it/s, est. speed input: 36499.89 toks/s, output: 35.64 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 32.22it/s, est. speed input: 36385.98 toks/s, output: 35.53 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:03<00:04, 32.29it/s, est. speed input: 36260.66 toks/s, output: 35.41 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:03<00:04, 32.50it/s, est. speed input: 36167.42 toks/s, output: 35.32 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:04, 32.64it/s, est. speed input: 36080.08 toks/s, output: 35.23 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:03<00:04, 32.70it/s, est. speed input: 35994.86 toks/s, output: 35.15 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 32.53it/s, est. speed input: 35887.37 toks/s, output: 35.05 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 32.63it/s, est. speed input: 35814.39 toks/s, output: 34.97 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 32.55it/s, est. speed input: 35727.96 toks/s, output: 34.89 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 32.63it/s, est. speed input: 35663.38 toks/s, output: 34.83 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:04<00:03, 32.66it/s, est. speed input: 35599.19 toks/s, output: 34.76 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:04<00:03, 32.68it/s, est. speed input: 35538.96 toks/s, output: 34.71 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:04<00:03, 32.55it/s, est. speed input: 35466.27 toks/s, output: 34.63 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:04<00:03, 32.59it/s, est. speed input: 35411.37 toks/s, output: 34.58 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:04<00:03, 32.38it/s, est. speed input: 35335.82 toks/s, output: 34.51 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 32.49it/s, est. speed input: 35289.78 toks/s, output: 34.46 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 32.61it/s, est. speed input: 35248.73 toks/s, output: 34.42 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 32.67it/s, est. speed input: 35207.97 toks/s, output: 34.38 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:05<00:02, 32.71it/s, est. speed input: 35169.14 toks/s, output: 34.34 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:05<00:02, 32.73it/s, est. speed input: 35131.34 toks/s, output: 34.31 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:05<00:02, 32.68it/s, est. speed input: 35090.16 toks/s, output: 34.27 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:05<00:02, 32.61it/s, est. speed input: 35047.66 toks/s, output: 34.23 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:05<00:02, 32.50it/s, est. speed input: 35002.58 toks/s, output: 34.18 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:05<00:01, 32.48it/s, est. speed input: 34963.75 toks/s, output: 34.14 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 32.52it/s, est. speed input: 34930.91 toks/s, output: 34.11 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 32.61it/s, est. speed input: 34903.20 toks/s, output: 34.09 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:06<00:01, 32.68it/s, est. speed input: 34878.04 toks/s, output: 34.06 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:06<00:01, 32.73it/s, est. speed input: 34853.31 toks/s, output: 34.04 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:06<00:01, 32.37it/s, est. speed input: 34802.51 toks/s, output: 33.99 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:08<00:07,  5.17it/s, est. speed input: 26000.94 toks/s, output: 25.39 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:08<00:04,  6.92it/s, est. speed input: 26107.23 toks/s, output: 25.50 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:08<00:03,  9.07it/s, est. speed input: 26213.09 toks/s, output: 25.60 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:08<00:02, 11.58it/s, est. speed input: 26310.69 toks/s, output: 25.69 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:09<00:01, 14.27it/s, est. speed input: 26389.86 toks/s, output: 25.77 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:09<00:01, 17.00it/s, est. speed input: 26461.32 toks/s, output: 25.84 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:09<00:00, 19.59it/s, est. speed input: 26526.86 toks/s, output: 25.91 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:09<00:00, 21.97it/s, est. speed input: 26594.46 toks/s, output: 25.97 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:09<00:00, 24.00it/s, est. speed input: 26658.72 toks/s, output: 26.03 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:09<00:00, 25.71it/s, est. speed input: 26723.97 toks/s, output: 26.10 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 25.71it/s, est. speed input: 26754.02 toks/s, output: 26.13 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 26.13it/s, est. speed input: 26754.02 toks/s, output: 26.13 toks/s]
[rank0]:[W128 11:03:43.349192728 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.3s

测试结果:
  Requests/s:   30.73
  Tokens/s:     31494.16
  Total Reqs:   256
  Elapsed:      8.33s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     31463.43

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:03:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:03:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=5192) WARNING 01-28 11:04:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=5192) WARNING 01-28 11:04:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.00 requests/s, 47153.60 total tokens/s, 46.00 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 11:03:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:03:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:03:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:03:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:03:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:03:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:03:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:03:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:03:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:03:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:04:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:04:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:04:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:04:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:04:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:04:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:04:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:04:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:04:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=5192) [2026-01-28 11:04:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=5192) [2026-01-28 11:04:03] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=5192) [2026-01-28 11:04:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=5192) [2026-01-28 11:04:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=5192) [2026-01-28 11:04:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=5192) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=5192) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=5192) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=5192) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=5192) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.74it/s]
(EngineCore_DP0 pid=5192) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  6.76it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  7.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.81it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 38/512 [00:00<00:01, 376.59it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 387.86it/s]
Adding requests:  23%|██▎       | 117/512 [00:00<00:01, 385.33it/s]
Adding requests:  30%|███       | 156/512 [00:00<00:00, 382.47it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:00, 380.49it/s]
Adding requests:  46%|████▌     | 236/512 [00:00<00:00, 387.50it/s]
Adding requests:  54%|█████▎    | 275/512 [00:00<00:00, 385.30it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 382.23it/s]
Adding requests:  69%|██████▉   | 354/512 [00:00<00:00, 385.65it/s]
Adding requests:  77%|███████▋  | 393/512 [00:01<00:00, 386.92it/s]
Adding requests:  85%|████████▍ | 433/512 [00:01<00:00, 390.76it/s]
Adding requests:  92%|█████████▏| 473/512 [00:01<00:00, 382.86it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 379.17it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 383.39it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:00<00:01, 353.51it/s, est. speed input: 362065.23 toks/s, output: 353.53 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:00<00:05, 81.70it/s, est. speed input: 96100.95 toks/s, output: 93.85 toks/s]   
Processed prompts:  20%|█▉        | 100/512 [00:01<00:05, 71.92it/s, est. speed input: 85085.28 toks/s, output: 83.09 toks/s]
Processed prompts:  22%|██▏       | 112/512 [00:01<00:06, 65.40it/s, est. speed input: 79051.79 toks/s, output: 77.20 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:06, 58.25it/s, est. speed input: 73542.91 toks/s, output: 71.82 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:01<00:06, 56.12it/s, est. speed input: 71406.10 toks/s, output: 69.73 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:06, 54.29it/s, est. speed input: 69616.43 toks/s, output: 67.98 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:06, 52.83it/s, est. speed input: 68112.14 toks/s, output: 66.52 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:06, 51.70it/s, est. speed input: 66823.60 toks/s, output: 65.26 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:06, 50.78it/s, est. speed input: 65689.17 toks/s, output: 64.15 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:06, 50.05it/s, est. speed input: 64681.74 toks/s, output: 63.17 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:06, 49.55it/s, est. speed input: 63799.01 toks/s, output: 62.30 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:06, 49.17it/s, est. speed input: 63010.26 toks/s, output: 61.53 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:06, 48.95it/s, est. speed input: 62314.17 toks/s, output: 60.85 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:06, 48.82it/s, est. speed input: 61692.52 toks/s, output: 60.25 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:06, 48.71it/s, est. speed input: 61125.86 toks/s, output: 59.69 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:06, 48.62it/s, est. speed input: 60606.27 toks/s, output: 59.19 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 48.59it/s, est. speed input: 60139.12 toks/s, output: 58.73 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:05, 48.45it/s, est. speed input: 59688.62 toks/s, output: 58.29 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:05, 48.43it/s, est. speed input: 59287.44 toks/s, output: 57.90 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:04<00:05, 48.42it/s, est. speed input: 58917.51 toks/s, output: 57.54 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 48.42it/s, est. speed input: 58575.97 toks/s, output: 57.20 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:05, 48.43it/s, est. speed input: 58259.77 toks/s, output: 56.89 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 48.36it/s, est. speed input: 57953.88 toks/s, output: 56.60 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:05<00:04, 48.39it/s, est. speed input: 57679.03 toks/s, output: 56.33 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:05<00:04, 48.37it/s, est. speed input: 57416.95 toks/s, output: 56.07 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:04, 48.38it/s, est. speed input: 57173.96 toks/s, output: 55.83 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:04, 48.32it/s, est. speed input: 56937.39 toks/s, output: 55.60 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:04, 48.29it/s, est. speed input: 56716.23 toks/s, output: 55.39 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 48.30it/s, est. speed input: 56510.99 toks/s, output: 55.19 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:06<00:03, 48.06it/s, est. speed input: 56290.68 toks/s, output: 54.97 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:03, 47.77it/s, est. speed input: 56069.43 toks/s, output: 54.76 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:03, 48.60it/s, est. speed input: 55964.16 toks/s, output: 54.65 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:03, 48.11it/s, est. speed input: 55759.31 toks/s, output: 54.45 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:03, 47.82it/s, est. speed input: 55569.52 toks/s, output: 54.27 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 47.61it/s, est. speed input: 55387.42 toks/s, output: 54.09 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:07<00:02, 47.45it/s, est. speed input: 55213.65 toks/s, output: 53.92 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:07<00:02, 47.33it/s, est. speed input: 55046.88 toks/s, output: 53.76 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:02, 47.23it/s, est. speed input: 54888.80 toks/s, output: 53.60 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:02, 47.22it/s, est. speed input: 54739.19 toks/s, output: 53.46 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 47.19it/s, est. speed input: 54595.90 toks/s, output: 53.32 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 47.16it/s, est. speed input: 54458.18 toks/s, output: 53.18 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:08<00:01, 47.19it/s, est. speed input: 54330.23 toks/s, output: 53.06 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:08<00:01, 47.12it/s, est. speed input: 54200.82 toks/s, output: 52.93 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:08<00:01, 47.09it/s, est. speed input: 54077.71 toks/s, output: 52.81 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:08<00:01, 47.03it/s, est. speed input: 53956.29 toks/s, output: 52.69 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 47.01it/s, est. speed input: 53841.91 toks/s, output: 52.58 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 47.01it/s, est. speed input: 53732.56 toks/s, output: 52.47 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:09<00:00, 47.01it/s, est. speed input: 53627.14 toks/s, output: 52.37 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 47.07it/s, est. speed input: 53530.26 toks/s, output: 52.28 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 47.12it/s, est. speed input: 53437.12 toks/s, output: 52.18 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 47.14it/s, est. speed input: 53346.09 toks/s, output: 52.10 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 47.13it/s, est. speed input: 53256.81 toks/s, output: 52.01 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 47.13it/s, est. speed input: 53539.33 toks/s, output: 52.28 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 52.28it/s, est. speed input: 53539.33 toks/s, output: 52.28 toks/s]
[rank0]:[W128 11:04:37.022095405 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.9s

测试结果:
  Requests/s:   46.00
  Tokens/s:     47153.60
  Total Reqs:   512
  Elapsed:      11.13s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     47107.60

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:04:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:04:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=5783) WARNING 01-28 11:05:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=5783) WARNING 01-28 11:05:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.27 requests/s, 53580.12 total tokens/s, 52.27 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 11:04:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:04:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:04:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:04:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:04:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:04:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:04:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:04:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:04:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:04:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:05:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:05:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:05:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:05:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:05:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:05:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:05:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:05:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:05:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:05:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:05:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:05:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:05:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:05:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=5783) [2026-01-28 11:05:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=5783) [2026-01-28 11:05:03] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=5783) [2026-01-28 11:05:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=5783) [2026-01-28 11:05:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=5783) [2026-01-28 11:05:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=5783) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=5783) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=5783) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=5783) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=5783) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  6.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.49it/s]
(EngineCore_DP0 pid=5783) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  6.75it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  7.60it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  7.85it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 355.42it/s]
Adding requests:   7%|▋         | 74/1024 [00:00<00:02, 368.78it/s]
Adding requests:  11%|█         | 112/1024 [00:00<00:02, 372.56it/s]
Adding requests:  15%|█▍        | 150/1024 [00:00<00:02, 372.08it/s]
Adding requests:  18%|█▊        | 188/1024 [00:00<00:02, 368.74it/s]
Adding requests:  22%|██▏       | 227/1024 [00:00<00:02, 372.54it/s]
Adding requests:  26%|██▌       | 265/1024 [00:00<00:02, 374.89it/s]
Adding requests:  30%|██▉       | 303/1024 [00:00<00:01, 368.08it/s]
Adding requests:  33%|███▎      | 341/1024 [00:00<00:01, 369.70it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 373.53it/s]
Adding requests:  41%|████      | 418/1024 [00:01<00:01, 374.34it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 374.27it/s]
Adding requests:  48%|████▊     | 495/1024 [00:01<00:01, 376.30it/s]
Adding requests:  52%|█████▏    | 533/1024 [00:01<00:01, 368.63it/s]
Adding requests:  56%|█████▌    | 573/1024 [00:01<00:01, 376.27it/s]
Adding requests:  60%|█████▉    | 612/1024 [00:01<00:01, 378.20it/s]
Adding requests:  63%|██████▎   | 650/1024 [00:01<00:00, 376.24it/s]
Adding requests:  67%|██████▋   | 688/1024 [00:01<00:00, 377.28it/s]
Adding requests:  71%|███████   | 728/1024 [00:01<00:00, 382.28it/s]
Adding requests:  75%|███████▍  | 767/1024 [00:02<00:00, 379.16it/s]
Adding requests:  79%|███████▊  | 805/1024 [00:02<00:00, 373.71it/s]
Adding requests:  82%|████████▏ | 843/1024 [00:02<00:00, 370.22it/s]
Adding requests:  86%|████████▌ | 882/1024 [00:02<00:00, 375.51it/s]
Adding requests:  90%|█████████ | 922/1024 [00:02<00:00, 381.03it/s]
Adding requests:  94%|█████████▍| 961/1024 [00:02<00:00, 380.80it/s]
Adding requests:  98%|█████████▊| 1000/1024 [00:02<00:00, 382.62it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 375.97it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:00<00:00, 1048.56it/s, est. speed input: 1074022.72 toks/s, output: 1048.71 toks/s]
Processed prompts:  23%|██▎       | 235/1024 [00:02<00:08, 92.99it/s, est. speed input: 112184.67 toks/s, output: 109.56 toks/s]    
Processed prompts:  28%|██▊       | 282/1024 [00:03<00:09, 76.45it/s, est. speed input: 93792.50 toks/s, output: 91.59 toks/s]  
Processed prompts:  30%|███       | 310/1024 [00:03<00:09, 72.87it/s, est. speed input: 89520.80 toks/s, output: 87.42 toks/s]
Processed prompts:  32%|███▏      | 329/1024 [00:03<00:09, 70.84it/s, est. speed input: 87334.02 toks/s, output: 85.29 toks/s]
Processed prompts:  34%|███▎      | 344/1024 [00:04<00:10, 66.38it/s, est. speed input: 84494.41 toks/s, output: 82.51 toks/s]
Processed prompts:  35%|███▍      | 356/1024 [00:04<00:11, 60.26it/s, est. speed input: 81358.62 toks/s, output: 79.45 toks/s]
Processed prompts:  36%|███▌      | 365/1024 [00:04<00:10, 59.93it/s, est. speed input: 80614.72 toks/s, output: 78.73 toks/s]
Processed prompts:  36%|███▋      | 373/1024 [00:04<00:11, 58.55it/s, est. speed input: 79706.67 toks/s, output: 77.84 toks/s]
Processed prompts:  37%|███▋      | 380/1024 [00:04<00:11, 55.98it/s, est. speed input: 78643.76 toks/s, output: 76.80 toks/s]
Processed prompts:  38%|███▊      | 387/1024 [00:05<00:11, 53.65it/s, est. speed input: 77648.74 toks/s, output: 75.83 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:05<00:12, 52.10it/s, est. speed input: 76802.12 toks/s, output: 75.00 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:05<00:11, 52.48it/s, est. speed input: 76198.79 toks/s, output: 74.41 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:11, 52.77it/s, est. speed input: 75626.15 toks/s, output: 73.85 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:11, 53.02it/s, est. speed input: 75086.22 toks/s, output: 73.33 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:11, 53.20it/s, est. speed input: 74573.04 toks/s, output: 72.82 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:05<00:11, 53.31it/s, est. speed input: 74083.18 toks/s, output: 72.35 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:10, 53.39it/s, est. speed input: 73616.13 toks/s, output: 71.89 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:10, 53.47it/s, est. speed input: 73175.09 toks/s, output: 71.46 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:10, 53.54it/s, est. speed input: 72755.25 toks/s, output: 71.05 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:10, 53.59it/s, est. speed input: 72353.25 toks/s, output: 70.66 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:10, 53.62it/s, est. speed input: 71969.35 toks/s, output: 70.28 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:10, 53.63it/s, est. speed input: 71601.63 toks/s, output: 69.92 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:09, 53.62it/s, est. speed input: 71246.92 toks/s, output: 69.58 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:09, 53.61it/s, est. speed input: 70907.42 toks/s, output: 69.25 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:09, 53.63it/s, est. speed input: 70583.35 toks/s, output: 68.93 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:09, 53.65it/s, est. speed input: 70273.39 toks/s, output: 68.63 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:09, 53.67it/s, est. speed input: 69975.78 toks/s, output: 68.34 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:09, 53.68it/s, est. speed input: 69688.91 toks/s, output: 68.06 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:07<00:09, 53.64it/s, est. speed input: 69409.19 toks/s, output: 67.78 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:08, 53.61it/s, est. speed input: 69140.07 toks/s, output: 67.52 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:08, 53.61it/s, est. speed input: 68882.07 toks/s, output: 67.27 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:08, 53.63it/s, est. speed input: 68634.59 toks/s, output: 67.03 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 53.62it/s, est. speed input: 68394.08 toks/s, output: 66.79 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:08, 53.62it/s, est. speed input: 68161.95 toks/s, output: 66.56 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:08, 53.63it/s, est. speed input: 67939.07 toks/s, output: 66.35 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:08<00:08, 53.65it/s, est. speed input: 67723.88 toks/s, output: 66.14 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:07, 53.63it/s, est. speed input: 67513.99 toks/s, output: 65.93 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:07, 53.62it/s, est. speed input: 67310.91 toks/s, output: 65.73 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:07, 53.62it/s, est. speed input: 67114.44 toks/s, output: 65.54 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 53.61it/s, est. speed input: 66923.48 toks/s, output: 65.35 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:07, 53.61it/s, est. speed input: 66739.28 toks/s, output: 65.17 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:07, 53.61it/s, est. speed input: 66560.01 toks/s, output: 65.00 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:06, 53.59it/s, est. speed input: 66385.55 toks/s, output: 64.83 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:06, 53.59it/s, est. speed input: 66216.77 toks/s, output: 64.66 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:06, 53.59it/s, est. speed input: 66052.81 toks/s, output: 64.50 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 53.60it/s, est. speed input: 65894.10 toks/s, output: 64.35 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 53.61it/s, est. speed input: 65739.74 toks/s, output: 64.20 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:06, 53.59it/s, est. speed input: 65588.43 toks/s, output: 64.05 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:10<00:06, 53.58it/s, est. speed input: 65441.51 toks/s, output: 63.91 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:05, 53.59it/s, est. speed input: 65299.53 toks/s, output: 63.77 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:05, 53.60it/s, est. speed input: 65161.45 toks/s, output: 63.63 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 53.59it/s, est. speed input: 65026.20 toks/s, output: 63.50 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 53.60it/s, est. speed input: 64895.05 toks/s, output: 63.37 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 53.60it/s, est. speed input: 64767.19 toks/s, output: 63.25 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:11<00:05, 53.59it/s, est. speed input: 64641.97 toks/s, output: 63.13 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:11<00:05, 53.60it/s, est. speed input: 64520.63 toks/s, output: 63.01 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:04, 53.59it/s, est. speed input: 64401.53 toks/s, output: 62.89 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:04, 53.58it/s, est. speed input: 64284.74 toks/s, output: 62.78 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 53.57it/s, est. speed input: 64171.31 toks/s, output: 62.67 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:12<00:04, 53.57it/s, est. speed input: 64060.60 toks/s, output: 62.56 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 53.59it/s, est. speed input: 63953.18 toks/s, output: 62.45 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:12<00:04, 53.55it/s, est. speed input: 63846.06 toks/s, output: 62.35 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:03, 53.56it/s, est. speed input: 63743.29 toks/s, output: 62.25 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 53.59it/s, est. speed input: 63643.42 toks/s, output: 62.15 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 53.56it/s, est. speed input: 63544.14 toks/s, output: 62.05 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 53.56it/s, est. speed input: 63447.29 toks/s, output: 61.96 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:13<00:03, 53.54it/s, est. speed input: 63352.22 toks/s, output: 61.87 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:03, 53.52it/s, est. speed input: 63258.79 toks/s, output: 61.78 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:13<00:03, 53.53it/s, est. speed input: 63168.13 toks/s, output: 61.69 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:02, 53.55it/s, est. speed input: 63080.13 toks/s, output: 61.60 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 53.54it/s, est. speed input: 62993.28 toks/s, output: 61.52 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 53.53it/s, est. speed input: 62907.63 toks/s, output: 61.43 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 53.55it/s, est. speed input: 62825.13 toks/s, output: 61.35 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:14<00:02, 53.53it/s, est. speed input: 62742.89 toks/s, output: 61.27 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:14<00:02, 53.54it/s, est. speed input: 62663.09 toks/s, output: 61.19 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:14<00:02, 53.55it/s, est. speed input: 62585.15 toks/s, output: 61.12 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:01, 53.56it/s, est. speed input: 62508.79 toks/s, output: 61.04 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 53.56it/s, est. speed input: 62433.70 toks/s, output: 60.97 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 53.55it/s, est. speed input: 62359.94 toks/s, output: 60.90 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:15<00:01, 53.55it/s, est. speed input: 62287.47 toks/s, output: 60.83 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:15<00:01, 53.54it/s, est. speed input: 62216.19 toks/s, output: 60.76 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:15<00:01, 53.53it/s, est. speed input: 62146.31 toks/s, output: 60.69 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:01, 53.51it/s, est. speed input: 62076.87 toks/s, output: 60.62 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 53.52it/s, est. speed input: 62009.81 toks/s, output: 60.56 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 55.34it/s, est. speed input: 62006.16 toks/s, output: 60.55 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:16<00:00, 54.72it/s, est. speed input: 61938.70 toks/s, output: 60.49 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:16<00:00, 54.35it/s, est. speed input: 61874.02 toks/s, output: 60.42 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:16<00:00, 54.09it/s, est. speed input: 61810.58 toks/s, output: 60.36 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:16<00:00, 55.97it/s, est. speed input: 61814.91 toks/s, output: 60.37 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:16<00:00, 55.97it/s, est. speed input: 62178.36 toks/s, output: 60.72 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:16<00:00, 60.72it/s, est. speed input: 62178.36 toks/s, output: 60.72 toks/s]
[rank0]:[W128 11:05:46.918207478 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.2s

测试结果:
  Requests/s:   52.27
  Tokens/s:     53580.12
  Total Reqs:   1024
  Elapsed:      19.59s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     53527.85

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:06:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:06:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=6469) WARNING 01-28 11:06:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=6469) WARNING 01-28 11:06:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.93 requests/s, 53233.32 total tokens/s, 51.93 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 11:06:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:06:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:06:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:06:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:06:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:06:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:06:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:06:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:06:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:06:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:06:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:06:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:06:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:06:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:06:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:06:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:06:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:06:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:06:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=6469) [2026-01-28 11:06:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=6469) [2026-01-28 11:06:16] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=6469) [2026-01-28 11:06:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=6469) [2026-01-28 11:06:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=6469) [2026-01-28 11:06:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=6469) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=6469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=6469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.77it/s]
(EngineCore_DP0 pid=6469) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=6469) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.85it/s]
(EngineCore_DP0 pid=6469) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.71it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  7.62it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  7.83it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 32/2048 [00:00<00:06, 318.37it/s]
Adding requests:   3%|▎         | 68/2048 [00:00<00:05, 337.48it/s]
Adding requests:   5%|▌         | 105/2048 [00:00<00:05, 349.02it/s]
Adding requests:   7%|▋         | 141/2048 [00:00<00:05, 349.27it/s]
Adding requests:   9%|▊         | 176/2048 [00:00<00:05, 348.68it/s]
Adding requests:  10%|█         | 212/2048 [00:00<00:05, 350.03it/s]
Adding requests:  12%|█▏        | 249/2048 [00:00<00:05, 355.98it/s]
Adding requests:  14%|█▍        | 286/2048 [00:00<00:04, 358.54it/s]
Adding requests:  16%|█▌        | 322/2048 [00:00<00:04, 356.23it/s]
Adding requests:  17%|█▋        | 358/2048 [00:01<00:04, 356.74it/s]
Adding requests:  19%|█▉        | 394/2048 [00:01<00:04, 337.81it/s]
Adding requests:  21%|██        | 428/2048 [00:01<00:05, 309.01it/s]
Adding requests:  23%|██▎       | 462/2048 [00:01<00:05, 315.16it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:04, 329.06it/s]
Adding requests:  26%|██▌       | 534/2048 [00:01<00:04, 333.09it/s]
Adding requests:  28%|██▊       | 571/2048 [00:01<00:04, 343.71it/s]
Adding requests:  30%|██▉       | 608/2048 [00:01<00:04, 348.65it/s]
Adding requests:  31%|███▏      | 644/2048 [00:01<00:04, 337.11it/s]
Adding requests:  33%|███▎      | 678/2048 [00:01<00:04, 336.68it/s]
Adding requests:  35%|███▍      | 716/2048 [00:02<00:03, 348.16it/s]
Adding requests:  37%|███▋      | 751/2048 [00:02<00:03, 343.76it/s]
Adding requests:  38%|███▊      | 787/2048 [00:02<00:03, 348.36it/s]
Adding requests:  40%|████      | 822/2048 [00:02<00:03, 343.64it/s]
Adding requests:  42%|████▏     | 859/2048 [00:02<00:03, 350.37it/s]
Adding requests:  44%|████▍     | 902/2048 [00:02<00:03, 370.97it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:02, 377.71it/s]
Adding requests:  48%|████▊     | 984/2048 [00:02<00:02, 387.75it/s]
Adding requests:  50%|█████     | 1026/2048 [00:02<00:02, 394.73it/s]
Adding requests:  52%|█████▏    | 1067/2048 [00:03<00:02, 397.15it/s]
Adding requests:  54%|█████▍    | 1107/2048 [00:03<00:02, 394.71it/s]
Adding requests:  56%|█████▌    | 1147/2048 [00:03<00:02, 392.30it/s]
Adding requests:  58%|█████▊    | 1187/2048 [00:03<00:02, 389.61it/s]
Adding requests:  60%|█████▉    | 1226/2048 [00:03<00:02, 351.42it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:03<00:02, 361.12it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:03<00:01, 374.66it/s]
Adding requests:  66%|██████▌   | 1348/2048 [00:03<00:01, 387.58it/s]
Adding requests:  68%|██████▊   | 1389/2048 [00:03<00:01, 391.87it/s]
Adding requests:  70%|██████▉   | 1431/2048 [00:03<00:01, 397.52it/s]
Adding requests:  72%|███████▏  | 1474/2048 [00:04<00:01, 405.81it/s]
Adding requests:  74%|███████▍  | 1516/2048 [00:04<00:01, 409.28it/s]
Adding requests:  76%|███████▌  | 1558/2048 [00:04<00:01, 411.27it/s]
Adding requests:  78%|███████▊  | 1602/2048 [00:04<00:01, 417.43it/s]
Adding requests:  80%|████████  | 1644/2048 [00:04<00:00, 415.22it/s]
Adding requests:  82%|████████▏ | 1686/2048 [00:04<00:00, 413.47it/s]
Adding requests:  84%|████████▍ | 1728/2048 [00:04<00:00, 415.12it/s]
Adding requests:  86%|████████▋ | 1770/2048 [00:04<00:00, 412.02it/s]
Adding requests:  88%|████████▊ | 1812/2048 [00:04<00:00, 413.31it/s]
Adding requests:  91%|█████████ | 1854/2048 [00:04<00:00, 413.06it/s]
Adding requests:  93%|█████████▎| 1896/2048 [00:05<00:00, 411.81it/s]
Adding requests:  95%|█████████▍| 1938/2048 [00:05<00:00, 401.52it/s]
Adding requests:  97%|█████████▋| 1980/2048 [00:05<00:00, 404.90it/s]
Adding requests:  99%|█████████▊| 2021/2048 [00:05<00:00, 404.27it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 374.44it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:02, 715.29it/s, est. speed input: 732503.02 toks/s, output: 715.31 toks/s]
Processed prompts:  17%|█▋        | 346/2048 [00:01<00:09, 181.00it/s, est. speed input: 225326.64 toks/s, output: 220.04 toks/s]
Processed prompts:  19%|█▊        | 380/2048 [00:02<00:12, 135.42it/s, est. speed input: 179583.13 toks/s, output: 175.37 toks/s]
Processed prompts:  20%|█▉        | 401/2048 [00:02<00:13, 121.92it/s, est. speed input: 166630.41 toks/s, output: 162.72 toks/s]
Processed prompts:  20%|██        | 417/2048 [00:02<00:15, 106.27it/s, est. speed input: 154601.22 toks/s, output: 150.98 toks/s]
Processed prompts:  21%|██        | 429/2048 [00:03<00:17, 89.99it/s, est. speed input: 143578.59 toks/s, output: 140.21 toks/s] 
Processed prompts:  21%|██▏       | 439/2048 [00:03<00:21, 75.31it/s, est. speed input: 133912.89 toks/s, output: 130.77 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:03<00:24, 64.88it/s, est. speed input: 126080.41 toks/s, output: 123.12 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:03<00:25, 61.78it/s, est. speed input: 120732.55 toks/s, output: 117.90 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:04<00:26, 59.48it/s, est. speed input: 116130.04 toks/s, output: 113.41 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:04<00:26, 57.82it/s, est. speed input: 112128.98 toks/s, output: 109.50 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:04<00:27, 56.64it/s, est. speed input: 108625.60 toks/s, output: 106.08 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:05<00:27, 55.78it/s, est. speed input: 105523.55 toks/s, output: 103.05 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:05<00:27, 55.18it/s, est. speed input: 102760.62 toks/s, output: 100.35 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:05<00:27, 54.75it/s, est. speed input: 100283.48 toks/s, output: 97.93 toks/s] 
Processed prompts:  28%|██▊       | 578/2048 [00:06<00:27, 54.44it/s, est. speed input: 98050.36 toks/s, output: 95.75 toks/s] 
Processed prompts:  29%|██▉       | 594/2048 [00:06<00:26, 54.22it/s, est. speed input: 96024.66 toks/s, output: 93.77 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:06<00:26, 54.07it/s, est. speed input: 94183.47 toks/s, output: 91.98 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:06<00:26, 53.96it/s, est. speed input: 92499.00 toks/s, output: 90.33 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:07<00:26, 53.87it/s, est. speed input: 90949.14 toks/s, output: 88.82 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:07<00:25, 53.83it/s, est. speed input: 89528.30 toks/s, output: 87.43 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:07<00:25, 53.77it/s, est. speed input: 88208.72 toks/s, output: 86.14 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:08<00:25, 53.73it/s, est. speed input: 86985.17 toks/s, output: 84.95 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:08<00:24, 53.69it/s, est. speed input: 85848.34 toks/s, output: 83.84 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:08<00:24, 53.67it/s, est. speed input: 84790.07 toks/s, output: 82.80 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:09<00:24, 53.67it/s, est. speed input: 83802.64 toks/s, output: 81.84 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:09<00:24, 53.64it/s, est. speed input: 82874.82 toks/s, output: 80.93 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:09<00:23, 53.62it/s, est. speed input: 82004.43 toks/s, output: 80.08 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:09<00:23, 53.61it/s, est. speed input: 81187.17 toks/s, output: 79.28 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:10<00:23, 53.61it/s, est. speed input: 80418.46 toks/s, output: 78.53 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:10<00:22, 53.62it/s, est. speed input: 79695.21 toks/s, output: 77.83 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:10<00:22, 53.58it/s, est. speed input: 79006.29 toks/s, output: 77.15 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:11<00:22, 53.57it/s, est. speed input: 78356.15 toks/s, output: 76.52 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:11<00:22, 53.57it/s, est. speed input: 77740.68 toks/s, output: 75.92 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:11<00:21, 53.57it/s, est. speed input: 77156.57 toks/s, output: 75.35 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:12<00:21, 53.56it/s, est. speed input: 76600.62 toks/s, output: 74.81 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:12<00:21, 53.55it/s, est. speed input: 76071.12 toks/s, output: 74.29 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:12<00:20, 53.54it/s, est. speed input: 75566.68 toks/s, output: 73.80 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:12<00:20, 53.54it/s, est. speed input: 75086.34 toks/s, output: 73.33 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:13<00:20, 53.52it/s, est. speed input: 74625.46 toks/s, output: 72.88 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:13<00:19, 54.37it/s, est. speed input: 74272.04 toks/s, output: 72.53 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:13<00:19, 54.11it/s, est. speed input: 73849.67 toks/s, output: 72.12 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:14<00:19, 53.92it/s, est. speed input: 73443.74 toks/s, output: 71.72 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:14<00:19, 53.78it/s, est. speed input: 73055.10 toks/s, output: 71.34 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:14<00:18, 53.68it/s, est. speed input: 72681.39 toks/s, output: 70.98 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:14<00:18, 53.60it/s, est. speed input: 72321.55 toks/s, output: 70.63 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:15<00:18, 53.56it/s, est. speed input: 71977.28 toks/s, output: 70.29 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:15<00:17, 53.52it/s, est. speed input: 71645.39 toks/s, output: 69.97 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:15<00:17, 53.50it/s, est. speed input: 71326.70 toks/s, output: 69.65 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:16<00:17, 53.47it/s, est. speed input: 71018.50 toks/s, output: 69.35 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:16<00:17, 53.44it/s, est. speed input: 70721.14 toks/s, output: 69.06 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:16<00:16, 54.31it/s, est. speed input: 70502.19 toks/s, output: 68.85 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:17<00:16, 54.02it/s, est. speed input: 70223.79 toks/s, output: 68.58 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:17<00:16, 53.83it/s, est. speed input: 69955.14 toks/s, output: 68.32 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:17<00:15, 53.72it/s, est. speed input: 69697.71 toks/s, output: 68.06 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:17<00:15, 53.59it/s, est. speed input: 69444.56 toks/s, output: 67.82 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:18<00:15, 52.57it/s, est. speed input: 69133.30 toks/s, output: 67.51 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:18<00:15, 51.42it/s, est. speed input: 68798.56 toks/s, output: 67.19 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:18<00:15, 50.64it/s, est. speed input: 68475.03 toks/s, output: 66.87 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:19<00:15, 50.12it/s, est. speed input: 68163.30 toks/s, output: 66.57 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:19<00:15, 49.79it/s, est. speed input: 67864.27 toks/s, output: 66.27 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:19<00:14, 49.54it/s, est. speed input: 67573.37 toks/s, output: 65.99 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:20<00:14, 49.36it/s, est. speed input: 67291.30 toks/s, output: 65.71 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:20<00:14, 49.23it/s, est. speed input: 67018.15 toks/s, output: 65.45 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:20<00:13, 49.13it/s, est. speed input: 66752.45 toks/s, output: 65.19 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:21<00:13, 49.75it/s, est. speed input: 66542.53 toks/s, output: 64.98 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:21<00:12, 50.76it/s, est. speed input: 66375.19 toks/s, output: 64.82 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:21<00:12, 51.50it/s, est. speed input: 66213.38 toks/s, output: 64.66 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:24<00:37, 16.42it/s, est. speed input: 60018.88 toks/s, output: 58.61 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:24<00:29, 20.72it/s, est. speed input: 59953.06 toks/s, output: 58.55 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:24<00:23, 25.38it/s, est. speed input: 59888.74 toks/s, output: 58.49 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:25<00:19, 30.11it/s, est. speed input: 59826.08 toks/s, output: 58.42 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:25<00:16, 34.52it/s, est. speed input: 59752.57 toks/s, output: 58.35 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:25<00:14, 37.86it/s, est. speed input: 59629.99 toks/s, output: 58.23 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:26<00:12, 40.60it/s, est. speed input: 59509.88 toks/s, output: 58.12 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:26<00:11, 42.77it/s, est. speed input: 59392.90 toks/s, output: 58.00 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:26<00:11, 44.43it/s, est. speed input: 59278.66 toks/s, output: 57.89 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:27<00:10, 45.67it/s, est. speed input: 59167.12 toks/s, output: 57.78 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:27<00:09, 46.58it/s, est. speed input: 59058.48 toks/s, output: 57.67 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:27<00:09, 47.24it/s, est. speed input: 58952.18 toks/s, output: 57.57 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:28<00:09, 47.72it/s, est. speed input: 58848.36 toks/s, output: 57.47 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:28<00:08, 48.05it/s, est. speed input: 58746.99 toks/s, output: 57.37 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:28<00:08, 48.29it/s, est. speed input: 58647.68 toks/s, output: 57.27 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:29<00:07, 48.45it/s, est. speed input: 58550.41 toks/s, output: 57.18 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:29<00:07, 48.60it/s, est. speed input: 58457.25 toks/s, output: 57.09 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:29<00:07, 48.64it/s, est. speed input: 58362.73 toks/s, output: 56.99 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:30<00:06, 48.70it/s, est. speed input: 58271.95 toks/s, output: 56.91 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:30<00:06, 48.75it/s, est. speed input: 58183.27 toks/s, output: 56.82 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:30<00:06, 48.77it/s, est. speed input: 58096.26 toks/s, output: 56.73 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:31<00:05, 48.80it/s, est. speed input: 58011.45 toks/s, output: 56.65 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:31<00:05, 49.42it/s, est. speed input: 57952.69 toks/s, output: 56.59 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:31<00:05, 50.53it/s, est. speed input: 57921.12 toks/s, output: 56.56 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:32<00:04, 51.33it/s, est. speed input: 57889.67 toks/s, output: 56.53 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:32<00:04, 51.90it/s, est. speed input: 57858.84 toks/s, output: 56.50 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:32<00:03, 52.30it/s, est. speed input: 57828.35 toks/s, output: 56.47 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:32<00:03, 52.61it/s, est. speed input: 57798.95 toks/s, output: 56.44 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:33<00:03, 53.65it/s, est. speed input: 57797.22 toks/s, output: 56.44 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:33<00:02, 53.53it/s, est. speed input: 57767.83 toks/s, output: 56.41 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:33<00:02, 53.45it/s, est. speed input: 57739.36 toks/s, output: 56.39 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:34<00:02, 53.41it/s, est. speed input: 57711.59 toks/s, output: 56.36 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:34<00:02, 53.39it/s, est. speed input: 57684.54 toks/s, output: 56.33 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:34<00:01, 53.36it/s, est. speed input: 57657.74 toks/s, output: 56.31 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:35<00:01, 53.34it/s, est. speed input: 57631.27 toks/s, output: 56.28 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:35<00:01, 53.32it/s, est. speed input: 57604.91 toks/s, output: 56.25 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:35<00:00, 53.31it/s, est. speed input: 57579.35 toks/s, output: 56.23 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:35<00:00, 53.31it/s, est. speed input: 57554.47 toks/s, output: 56.21 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:36<00:00, 54.32it/s, est. speed input: 57559.33 toks/s, output: 56.21 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:36<00:00, 54.32it/s, est. speed input: 57954.95 toks/s, output: 56.60 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:36<00:00, 56.60it/s, est. speed input: 57954.95 toks/s, output: 56.60 toks/s]
[rank0]:[W128 11:07:22.205303936 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 95.9s

测试结果:
  Requests/s:   51.93
  Tokens/s:     53233.32
  Total Reqs:   2048
  Elapsed:      39.43s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     53181.39

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:07:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:07:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=7362) WARNING 01-28 11:08:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=7362) WARNING 01-28 11:08:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.20 requests/s, 54528.27 total tokens/s, 53.20 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 11:07:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:07:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:07:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:07:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:07:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:07:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:07:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:07:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:07:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:07:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:07:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:07:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:07:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:07:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:08:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:08:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:08:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:08:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:08:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:08:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:08:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:08:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:08:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:08:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:08:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:08:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:08:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:08:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=7362) [2026-01-28 11:08:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=7362) [2026-01-28 11:08:03] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=7362) [2026-01-28 11:08:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=7362) [2026-01-28 11:08:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=7362) [2026-01-28 11:08:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=7362) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=7362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=7362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=7362) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=7362) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  7.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.61it/s]
(EngineCore_DP0 pid=7362) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.66it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  7.73it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.23it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.42it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 371.52it/s]
Adding requests:   2%|▏         | 78/4096 [00:00<00:10, 385.28it/s]
Adding requests:   3%|▎         | 117/4096 [00:00<00:10, 386.81it/s]
Adding requests:   4%|▍         | 156/4096 [00:00<00:10, 387.91it/s]
Adding requests:   5%|▍         | 195/4096 [00:00<00:10, 386.18it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:09, 389.75it/s]
Adding requests:   7%|▋         | 276/4096 [00:00<00:09, 393.68it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:09, 388.63it/s]
Adding requests:   9%|▊         | 357/4096 [00:00<00:09, 392.13it/s]
Adding requests:  10%|▉         | 398/4096 [00:01<00:09, 396.03it/s]
Adding requests:  11%|█         | 438/4096 [00:01<00:09, 395.36it/s]
Adding requests:  12%|█▏        | 480/4096 [00:01<00:09, 399.97it/s]
Adding requests:  13%|█▎        | 521/4096 [00:01<00:09, 393.41it/s]
Adding requests:  14%|█▎        | 563/4096 [00:01<00:08, 399.26it/s]
Adding requests:  15%|█▍        | 603/4096 [00:01<00:08, 397.23it/s]
Adding requests:  16%|█▌        | 644/4096 [00:01<00:08, 397.77it/s]
Adding requests:  17%|█▋        | 685/4096 [00:01<00:08, 400.37it/s]
Adding requests:  18%|█▊        | 726/4096 [00:01<00:09, 373.73it/s]
Adding requests:  19%|█▊        | 766/4096 [00:01<00:08, 380.24it/s]
Adding requests:  20%|█▉        | 805/4096 [00:02<00:08, 377.91it/s]
Adding requests:  21%|██        | 844/4096 [00:02<00:08, 381.12it/s]
Adding requests:  22%|██▏       | 886/4096 [00:02<00:08, 392.21it/s]
Adding requests:  23%|██▎       | 928/4096 [00:02<00:07, 399.76it/s]
Adding requests:  24%|██▎       | 970/4096 [00:02<00:07, 404.66it/s]
Adding requests:  25%|██▍       | 1011/4096 [00:02<00:07, 405.35it/s]
Adding requests:  26%|██▌       | 1053/4096 [00:02<00:07, 408.57it/s]
Adding requests:  27%|██▋       | 1094/4096 [00:02<00:07, 406.66it/s]
Adding requests:  28%|██▊       | 1135/4096 [00:02<00:07, 404.38it/s]
Adding requests:  29%|██▉       | 1179/4096 [00:02<00:07, 413.26it/s]
Adding requests:  30%|██▉       | 1221/4096 [00:03<00:07, 407.35it/s]
Adding requests:  31%|███       | 1262/4096 [00:03<00:06, 405.53it/s]
Adding requests:  32%|███▏      | 1303/4096 [00:03<00:06, 400.89it/s]
Adding requests:  33%|███▎      | 1344/4096 [00:03<00:06, 403.23it/s]
Adding requests:  34%|███▍      | 1385/4096 [00:03<00:06, 401.38it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:06, 405.55it/s]
Adding requests:  36%|███▌      | 1469/4096 [00:03<00:06, 409.34it/s]
Adding requests:  37%|███▋      | 1511/4096 [00:03<00:06, 410.89it/s]
Adding requests:  38%|███▊      | 1553/4096 [00:03<00:06, 409.90it/s]
Adding requests:  39%|███▉      | 1596/4096 [00:04<00:06, 414.97it/s]
Adding requests:  40%|███▉      | 1638/4096 [00:04<00:05, 414.85it/s]
Adding requests:  41%|████      | 1680/4096 [00:04<00:05, 412.60it/s]
Adding requests:  42%|████▏     | 1723/4096 [00:04<00:05, 416.65it/s]
Adding requests:  43%|████▎     | 1765/4096 [00:04<00:05, 410.94it/s]
Adding requests:  44%|████▍     | 1808/4096 [00:04<00:05, 413.69it/s]
Adding requests:  45%|████▌     | 1850/4096 [00:04<00:05, 415.45it/s]
Adding requests:  46%|████▌     | 1892/4096 [00:04<00:05, 416.53it/s]
Adding requests:  47%|████▋     | 1935/4096 [00:04<00:05, 418.34it/s]
Adding requests:  48%|████▊     | 1977/4096 [00:04<00:05, 416.69it/s]
Adding requests:  49%|████▉     | 2019/4096 [00:05<00:04, 416.60it/s]
Adding requests:  50%|█████     | 2062/4096 [00:05<00:04, 419.61it/s]
Adding requests:  51%|█████▏    | 2104/4096 [00:05<00:04, 413.51it/s]
Adding requests:  52%|█████▏    | 2146/4096 [00:05<00:04, 409.89it/s]
Adding requests:  53%|█████▎    | 2188/4096 [00:05<00:04, 407.29it/s]
Adding requests:  54%|█████▍    | 2231/4096 [00:05<00:04, 412.61it/s]
Adding requests:  55%|█████▌    | 2273/4096 [00:05<00:04, 411.02it/s]
Adding requests:  57%|█████▋    | 2315/4096 [00:05<00:04, 409.94it/s]
Adding requests:  58%|█████▊    | 2357/4096 [00:05<00:04, 405.03it/s]
Adding requests:  59%|█████▊    | 2399/4096 [00:05<00:04, 408.21it/s]
Adding requests:  60%|█████▉    | 2440/4096 [00:06<00:04, 393.87it/s]
Adding requests:  61%|██████    | 2482/4096 [00:06<00:04, 399.67it/s]
Adding requests:  62%|██████▏   | 2523/4096 [00:06<00:03, 393.97it/s]
Adding requests:  63%|██████▎   | 2566/4096 [00:06<00:03, 404.02it/s]
Adding requests:  64%|██████▎   | 2608/4096 [00:06<00:03, 408.16it/s]
Adding requests:  65%|██████▍   | 2650/4096 [00:06<00:03, 409.39it/s]
Adding requests:  66%|██████▌   | 2692/4096 [00:06<00:03, 410.66it/s]
Adding requests:  67%|██████▋   | 2734/4096 [00:06<00:03, 408.61it/s]
Adding requests:  68%|██████▊   | 2776/4096 [00:06<00:03, 409.34it/s]
Adding requests:  69%|██████▉   | 2817/4096 [00:06<00:03, 408.02it/s]
Adding requests:  70%|██████▉   | 2858/4096 [00:07<00:03, 408.55it/s]
Adding requests:  71%|███████   | 2901/4096 [00:07<00:02, 412.30it/s]
Adding requests:  72%|███████▏  | 2943/4096 [00:07<00:02, 409.19it/s]
Adding requests:  73%|███████▎  | 2985/4096 [00:07<00:02, 410.97it/s]
Adding requests:  74%|███████▍  | 3027/4096 [00:07<00:02, 409.79it/s]
Adding requests:  75%|███████▍  | 3068/4096 [00:07<00:02, 409.84it/s]
Adding requests:  76%|███████▌  | 3110/4096 [00:07<00:02, 410.43it/s]
Adding requests:  77%|███████▋  | 3152/4096 [00:07<00:02, 407.57it/s]
Adding requests:  78%|███████▊  | 3193/4096 [00:07<00:02, 407.56it/s]
Adding requests:  79%|███████▉  | 3235/4096 [00:08<00:02, 409.02it/s]
Adding requests:  80%|████████  | 3277/4096 [00:08<00:01, 411.48it/s]
Adding requests:  81%|████████  | 3319/4096 [00:08<00:01, 412.59it/s]
Adding requests:  82%|████████▏ | 3362/4096 [00:08<00:01, 415.40it/s]
Adding requests:  83%|████████▎ | 3404/4096 [00:08<00:01, 412.91it/s]
Adding requests:  84%|████████▍ | 3446/4096 [00:08<00:01, 411.57it/s]
Adding requests:  85%|████████▌ | 3488/4096 [00:08<00:01, 406.44it/s]
Adding requests:  86%|████████▌ | 3529/4096 [00:08<00:01, 405.54it/s]
Adding requests:  87%|████████▋ | 3570/4096 [00:08<00:01, 405.99it/s]
Adding requests:  88%|████████▊ | 3612/4096 [00:08<00:01, 408.06it/s]
Adding requests:  89%|████████▉ | 3653/4096 [00:09<00:01, 401.87it/s]
Adding requests:  90%|█████████ | 3695/4096 [00:09<00:00, 404.86it/s]
Adding requests:  91%|█████████ | 3736/4096 [00:09<00:00, 404.61it/s]
Adding requests:  92%|█████████▏| 3777/4096 [00:09<00:00, 399.10it/s]
Adding requests:  93%|█████████▎| 3819/4096 [00:09<00:00, 404.44it/s]
Adding requests:  94%|█████████▍| 3862/4096 [00:09<00:00, 411.43it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:09<00:00, 408.03it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:09<00:00, 410.29it/s]
Adding requests:  97%|█████████▋| 3988/4096 [00:09<00:00, 401.08it/s]
Adding requests:  98%|█████████▊| 4030/4096 [00:09<00:00, 403.72it/s]
Adding requests:  99%|█████████▉| 4071/4096 [00:10<00:00, 403.52it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 404.65it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:00<00:01, 3119.05it/s, est. speed input: 3194416.30 toks/s, output: 3119.19 toks/s]
Processed prompts:  20%|██        | 826/4096 [00:05<00:26, 123.96it/s, est. speed input: 154654.00 toks/s, output: 151.03 toks/s]   
Processed prompts:  23%|██▎       | 958/4096 [00:07<00:32, 97.82it/s, est. speed input: 125252.37 toks/s, output: 122.32 toks/s] 
Processed prompts:  25%|██▌       | 1033/4096 [00:09<00:37, 81.61it/s, est. speed input: 110105.71 toks/s, output: 107.53 toks/s]
Processed prompts:  26%|██▋       | 1080/4096 [00:10<00:37, 80.81it/s, est. speed input: 108095.70 toks/s, output: 105.56 toks/s]
Processed prompts:  27%|██▋       | 1113/4096 [00:10<00:39, 76.09it/s, est. speed input: 104782.98 toks/s, output: 102.33 toks/s]
Processed prompts:  28%|██▊       | 1137/4096 [00:11<00:42, 68.88it/s, est. speed input: 101040.26 toks/s, output: 98.67 toks/s] 
Processed prompts:  28%|██▊       | 1154/4096 [00:12<00:48, 61.06it/s, est. speed input: 97545.47 toks/s, output: 95.26 toks/s] 
Processed prompts:  29%|██▉       | 1186/4096 [00:12<00:48, 59.54it/s, est. speed input: 95580.41 toks/s, output: 93.34 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:13<00:49, 58.22it/s, est. speed input: 93786.47 toks/s, output: 91.59 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:13<00:49, 57.16it/s, est. speed input: 92145.73 toks/s, output: 89.99 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:16<01:41, 27.66it/s, est. speed input: 78172.90 toks/s, output: 76.34 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:17<01:28, 31.35it/s, est. speed input: 77153.08 toks/s, output: 75.34 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:18<01:18, 34.88it/s, est. speed input: 76207.91 toks/s, output: 74.42 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:18<01:11, 38.05it/s, est. speed input: 75324.64 toks/s, output: 73.56 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:19<01:05, 40.76it/s, est. speed input: 74501.34 toks/s, output: 72.76 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:20<01:01, 42.96it/s, est. speed input: 73730.60 toks/s, output: 72.00 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:20<00:58, 44.69it/s, est. speed input: 73008.38 toks/s, output: 71.30 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:21<00:56, 46.00it/s, est. speed input: 72329.83 toks/s, output: 70.63 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:21<00:54, 46.96it/s, est. speed input: 71689.46 toks/s, output: 70.01 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:22<00:51, 48.78it/s, est. speed input: 71247.58 toks/s, output: 69.58 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:23<00:49, 50.22it/s, est. speed input: 70837.79 toks/s, output: 69.18 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:23<00:48, 51.28it/s, est. speed input: 70447.88 toks/s, output: 68.80 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:24<00:46, 52.05it/s, est. speed input: 70077.46 toks/s, output: 68.43 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:24<00:45, 52.61it/s, est. speed input: 69725.02 toks/s, output: 68.09 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:25<00:44, 53.00it/s, est. speed input: 69388.41 toks/s, output: 67.76 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:26<00:43, 53.29it/s, est. speed input: 69068.14 toks/s, output: 67.45 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:26<00:43, 53.49it/s, est. speed input: 68761.39 toks/s, output: 67.15 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:27<00:42, 53.63it/s, est. speed input: 68467.79 toks/s, output: 66.86 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:27<00:41, 54.16it/s, est. speed input: 68226.17 toks/s, output: 66.63 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:28<00:40, 54.10it/s, est. speed input: 67956.28 toks/s, output: 66.36 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:29<00:40, 54.07it/s, est. speed input: 67698.01 toks/s, output: 66.11 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:29<00:39, 54.05it/s, est. speed input: 67450.42 toks/s, output: 65.87 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:30<00:39, 54.04it/s, est. speed input: 67212.64 toks/s, output: 65.64 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:30<00:38, 54.03it/s, est. speed input: 66983.57 toks/s, output: 65.41 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:31<00:37, 54.02it/s, est. speed input: 66763.15 toks/s, output: 65.20 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:32<00:37, 54.01it/s, est. speed input: 66551.00 toks/s, output: 64.99 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:32<00:36, 54.01it/s, est. speed input: 66346.60 toks/s, output: 64.79 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:33<00:36, 54.01it/s, est. speed input: 66149.53 toks/s, output: 64.60 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:33<00:35, 54.01it/s, est. speed input: 65959.52 toks/s, output: 64.41 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:34<00:34, 54.00it/s, est. speed input: 65775.46 toks/s, output: 64.23 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:34<00:34, 54.00it/s, est. speed input: 65597.93 toks/s, output: 64.06 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:35<00:33, 54.43it/s, est. speed input: 65455.11 toks/s, output: 63.92 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:36<00:32, 54.30it/s, est. speed input: 65288.95 toks/s, output: 63.76 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:36<00:32, 54.22it/s, est. speed input: 65128.28 toks/s, output: 63.60 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:37<00:31, 54.15it/s, est. speed input: 64972.18 toks/s, output: 63.45 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:37<00:31, 54.10it/s, est. speed input: 64820.95 toks/s, output: 63.30 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:38<00:30, 54.07it/s, est. speed input: 64674.59 toks/s, output: 63.16 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:39<00:30, 54.06it/s, est. speed input: 64532.79 toks/s, output: 63.02 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:39<00:29, 54.47it/s, est. speed input: 64420.62 toks/s, output: 62.91 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:40<00:28, 54.34it/s, est. speed input: 64286.91 toks/s, output: 62.78 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:40<00:28, 54.24it/s, est. speed input: 64157.03 toks/s, output: 62.65 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:41<00:27, 54.17it/s, est. speed input: 64030.52 toks/s, output: 62.53 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:42<00:27, 54.12it/s, est. speed input: 63907.61 toks/s, output: 62.41 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:42<00:26, 54.08it/s, est. speed input: 63787.80 toks/s, output: 62.29 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:43<00:26, 54.06it/s, est. speed input: 63671.53 toks/s, output: 62.18 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:43<00:25, 54.04it/s, est. speed input: 63558.13 toks/s, output: 62.07 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:44<00:24, 54.02it/s, est. speed input: 63447.95 toks/s, output: 61.96 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:45<00:24, 53.62it/s, est. speed input: 63319.96 toks/s, output: 61.84 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:45<00:24, 52.31it/s, est. speed input: 63141.26 toks/s, output: 61.66 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:46<00:23, 52.10it/s, est. speed input: 63003.72 toks/s, output: 61.53 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:46<00:23, 52.65it/s, est. speed input: 62906.13 toks/s, output: 61.43 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:47<00:22, 53.05it/s, est. speed input: 62811.10 toks/s, output: 61.34 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:48<00:21, 53.33it/s, est. speed input: 62718.52 toks/s, output: 61.25 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:48<00:20, 53.52it/s, est. speed input: 62627.70 toks/s, output: 61.16 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:51<00:44, 24.54it/s, est. speed input: 59682.08 toks/s, output: 58.28 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:52<00:36, 29.15it/s, est. speed input: 59605.01 toks/s, output: 58.21 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:52<00:30, 33.24it/s, est. speed input: 59494.99 toks/s, output: 58.10 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:53<00:26, 36.86it/s, est. speed input: 59388.04 toks/s, output: 58.00 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:54<00:24, 39.91it/s, est. speed input: 59283.60 toks/s, output: 57.89 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:54<00:21, 42.37it/s, est. speed input: 59182.01 toks/s, output: 57.79 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:55<00:20, 44.27it/s, est. speed input: 59082.27 toks/s, output: 57.70 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:56<00:18, 45.71it/s, est. speed input: 58985.18 toks/s, output: 57.60 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:56<00:17, 47.46it/s, est. speed input: 58924.34 toks/s, output: 57.54 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:57<00:16, 49.23it/s, est. speed input: 58886.17 toks/s, output: 57.51 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:57<00:15, 50.57it/s, est. speed input: 58849.18 toks/s, output: 57.47 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:58<00:14, 51.54it/s, est. speed input: 58812.65 toks/s, output: 57.43 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:59<00:13, 52.24it/s, est. speed input: 58776.84 toks/s, output: 57.40 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:59<00:12, 52.74it/s, est. speed input: 58741.84 toks/s, output: 57.37 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:00<00:12, 53.10it/s, est. speed input: 58707.41 toks/s, output: 57.33 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:00<00:11, 53.35it/s, est. speed input: 58673.83 toks/s, output: 57.30 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:01<00:10, 53.53it/s, est. speed input: 58640.66 toks/s, output: 57.27 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:02<00:10, 53.65it/s, est. speed input: 58608.13 toks/s, output: 57.23 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:02<00:09, 53.74it/s, est. speed input: 58576.13 toks/s, output: 57.20 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:03<00:08, 53.81it/s, est. speed input: 58545.41 toks/s, output: 57.17 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:03<00:08, 53.88it/s, est. speed input: 58515.60 toks/s, output: 57.14 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:04<00:07, 53.92it/s, est. speed input: 58486.17 toks/s, output: 57.12 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:05<00:07, 54.38it/s, est. speed input: 58471.28 toks/s, output: 57.10 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:05<00:06, 54.27it/s, est. speed input: 58442.84 toks/s, output: 57.07 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:06<00:05, 54.20it/s, est. speed input: 58414.85 toks/s, output: 57.05 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:06<00:05, 54.16it/s, est. speed input: 58387.84 toks/s, output: 57.02 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:07<00:04, 54.17it/s, est. speed input: 58362.52 toks/s, output: 56.99 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:08<00:04, 54.08it/s, est. speed input: 58334.64 toks/s, output: 56.97 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:08<00:03, 54.06it/s, est. speed input: 58308.48 toks/s, output: 56.94 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:09<00:02, 54.05it/s, est. speed input: 58282.86 toks/s, output: 56.92 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:09<00:02, 54.03it/s, est. speed input: 58257.44 toks/s, output: 56.89 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:10<00:01, 54.03it/s, est. speed input: 58232.60 toks/s, output: 56.87 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:10<00:01, 54.46it/s, est. speed input: 58221.13 toks/s, output: 56.86 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:11<00:00, 54.92it/s, est. speed input: 58214.26 toks/s, output: 56.85 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:11<00:00, 54.92it/s, est. speed input: 58643.32 toks/s, output: 57.27 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:11<00:00, 57.27it/s, est. speed input: 58643.32 toks/s, output: 57.27 toks/s]
[rank0]:[W128 11:09:49.619448606 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 147.7s

测试结果:
  Requests/s:   53.20
  Tokens/s:     54528.27
  Total Reqs:   4096
  Elapsed:      76.99s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     54475.07


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,15.1850,7789.9184,8.4294
1024,1024,1,128,128,15.6430,16034.1212,8.1826
2048,1024,2,256,128,30.7260,31494.1560,8.3317
4096,1024,4,512,128,46.0035,47153.6012,11.1296
8192,1024,8,1024,128,52.2733,53580.1226,19.5894
16384,1024,16,2048,128,51.9349,53233.3203,39.4339
32768,1024,32,4096,128,53.1983,54528.2674,76.9949

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:09:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:10:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=8401) WARNING 01-28 11:10:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=8401) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=8401) WARNING 01-28 11:10:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.85 requests/s, 8642.90 total tokens/s, 16.85 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:09:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:09:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:09:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:09:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:09:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:09:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:09:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:09:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:09:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:09:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:09:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:09:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:09:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:09:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:10:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:10:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:10:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:10:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:10:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:10:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:10:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:10:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:10:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=8401) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=8401) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.49s/it]
(EngineCore_DP0 pid=8401) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.49s/it]
(EngineCore_DP0 pid=8401) 
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=8401) [2026-01-28 11:10:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=8401) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  6.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.66it/s]
(EngineCore_DP0 pid=8401) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.18it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  44%|████▍     | 56/128 [00:00<00:00, 557.39it/s]
Adding requests:  98%|█████████▊| 125/128 [00:00<00:00, 632.39it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 623.90it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 36.43it/s, est. speed input: 18658.48 toks/s, output: 36.44 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.95it/s, est. speed input: 11952.45 toks/s, output: 23.34 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 19.88it/s, est. speed input: 10912.55 toks/s, output: 21.31 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 18.88it/s, est. speed input: 10393.95 toks/s, output: 20.30 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 18.21it/s, est. speed input: 10101.22 toks/s, output: 19.73 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:06, 17.84it/s, est. speed input: 9918.10 toks/s, output: 19.37 toks/s] 
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.66it/s, est. speed input: 9794.27 toks/s, output: 19.13 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.48it/s, est. speed input: 9686.37 toks/s, output: 18.92 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 17.22it/s, est. speed input: 9574.76 toks/s, output: 18.70 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 17.02it/s, est. speed input: 9480.75 toks/s, output: 18.52 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 16.94it/s, est. speed input: 9409.24 toks/s, output: 18.38 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.86it/s, est. speed input: 9345.41 toks/s, output: 18.25 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:02<00:22,  4.18it/s, est. speed input: 5506.58 toks/s, output: 10.75 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:03<00:17,  5.37it/s, est. speed input: 5621.86 toks/s, output: 10.98 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:03<00:13,  6.73it/s, est. speed input: 5729.20 toks/s, output: 11.19 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:03<00:11,  8.17it/s, est. speed input: 5827.24 toks/s, output: 11.38 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:03<00:09,  9.66it/s, est. speed input: 5923.15 toks/s, output: 11.57 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:03<00:07, 11.07it/s, est. speed input: 6012.43 toks/s, output: 11.74 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:03<00:06, 12.31it/s, est. speed input: 6094.56 toks/s, output: 11.90 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:03<00:06, 13.37it/s, est. speed input: 6171.92 toks/s, output: 12.05 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:03<00:05, 14.24it/s, est. speed input: 6245.33 toks/s, output: 12.20 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:04<00:05, 14.85it/s, est. speed input: 6310.99 toks/s, output: 12.33 toks/s]
Processed prompts:  41%|████      | 52/128 [00:04<00:04, 15.33it/s, est. speed input: 6373.91 toks/s, output: 12.45 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:04<00:04, 15.78it/s, est. speed input: 6437.26 toks/s, output: 12.57 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:04<00:04, 15.97it/s, est. speed input: 6491.81 toks/s, output: 12.68 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:04<00:04, 16.18it/s, est. speed input: 6545.90 toks/s, output: 12.78 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:04<00:04, 16.35it/s, est. speed input: 6598.17 toks/s, output: 12.89 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:04<00:04, 16.46it/s, est. speed input: 6647.17 toks/s, output: 12.98 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:04<00:03, 16.57it/s, est. speed input: 6695.08 toks/s, output: 13.08 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:05<00:03, 16.71it/s, est. speed input: 6742.61 toks/s, output: 13.17 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:05<00:03, 16.79it/s, est. speed input: 6787.50 toks/s, output: 13.26 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:05<00:03, 16.82it/s, est. speed input: 6829.55 toks/s, output: 13.34 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:05<00:03, 16.89it/s, est. speed input: 6871.16 toks/s, output: 13.42 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:05<00:03, 16.96it/s, est. speed input: 6911.62 toks/s, output: 13.50 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:05<00:03, 16.98it/s, est. speed input: 6949.49 toks/s, output: 13.57 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:05<00:02, 17.03it/s, est. speed input: 6986.86 toks/s, output: 13.65 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:05<00:02, 17.08it/s, est. speed input: 7022.93 toks/s, output: 13.72 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:05<00:02, 17.05it/s, est. speed input: 7056.00 toks/s, output: 13.78 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:06<00:02, 16.99it/s, est. speed input: 7086.91 toks/s, output: 13.84 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:06<00:02, 16.96it/s, est. speed input: 7116.78 toks/s, output: 13.90 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:06<00:02, 17.05it/s, est. speed input: 7148.45 toks/s, output: 13.96 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:06<00:02, 17.00it/s, est. speed input: 7175.87 toks/s, output: 14.02 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:06<00:02, 16.99it/s, est. speed input: 7203.31 toks/s, output: 14.07 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:06<00:01, 17.10it/s, est. speed input: 7232.36 toks/s, output: 14.13 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:06<00:01, 17.05it/s, est. speed input: 7257.48 toks/s, output: 14.17 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:06<00:01, 16.93it/s, est. speed input: 7279.72 toks/s, output: 14.22 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:07<00:01, 16.98it/s, est. speed input: 7304.18 toks/s, output: 14.27 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:07<00:01, 16.96it/s, est. speed input: 7326.82 toks/s, output: 14.31 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:07<00:01, 17.07it/s, est. speed input: 7351.36 toks/s, output: 14.36 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:07<00:01, 17.35it/s, est. speed input: 7379.75 toks/s, output: 14.41 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:07<00:01, 17.54it/s, est. speed input: 7407.19 toks/s, output: 14.47 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:07<00:01, 17.69it/s, est. speed input: 7434.02 toks/s, output: 14.52 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:07<00:00, 17.74it/s, est. speed input: 7458.86 toks/s, output: 14.57 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:07<00:00, 17.67it/s, est. speed input: 7480.84 toks/s, output: 14.61 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:07<00:00, 17.69it/s, est. speed input: 7503.70 toks/s, output: 14.66 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:08<00:00, 17.68it/s, est. speed input: 7525.41 toks/s, output: 14.70 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:08<00:00, 17.61it/s, est. speed input: 7545.28 toks/s, output: 14.74 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:08<00:00, 17.62it/s, est. speed input: 7565.61 toks/s, output: 14.78 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:08<00:00, 17.64it/s, est. speed input: 7585.78 toks/s, output: 14.82 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:08<00:00, 17.62it/s, est. speed input: 7604.65 toks/s, output: 14.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 17.45it/s, est. speed input: 7620.07 toks/s, output: 14.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 17.45it/s, est. speed input: 7620.07 toks/s, output: 14.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:08<00:00, 14.88it/s, est. speed input: 7620.07 toks/s, output: 14.88 toks/s]
[rank0]:[W128 11:10:43.842051730 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.7s

测试结果:
  Requests/s:   16.85
  Tokens/s:     8642.90
  Total Reqs:   128
  Elapsed:      7.60s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8626.05

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:10:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:10:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=8954) WARNING 01-28 11:11:00 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=8954) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=8954) WARNING 01-28 11:11:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.33 requests/s, 17760.25 total tokens/s, 17.33 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:10:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:10:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:10:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:10:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:10:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:10:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:10:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:10:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:10:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:10:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:10:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:10:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:10:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:10:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:10:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:10:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:10:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:10:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:10:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=8954) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=8954) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=8954) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=8954) 
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=8954) [2026-01-28 11:11:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=8954) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.35it/s]
(EngineCore_DP0 pid=8954) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.56it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 355.34it/s]
Adding requests:  60%|██████    | 77/128 [00:00<00:00, 383.81it/s]
Adding requests:  91%|█████████ | 116/128 [00:00<00:00, 372.10it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 371.57it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 45.22it/s, est. speed input: 46311.92 toks/s, output: 45.22 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 24.38it/s, est. speed input: 27158.38 toks/s, output: 26.52 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:05, 21.74it/s, est. speed input: 24575.63 toks/s, output: 24.00 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:05, 20.20it/s, est. speed input: 23103.64 toks/s, output: 22.56 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:05, 19.23it/s, est. speed input: 22142.20 toks/s, output: 21.62 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 18.77it/s, est. speed input: 21672.24 toks/s, output: 21.16 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 18.45it/s, est. speed input: 21318.44 toks/s, output: 20.82 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 18.18it/s, est. speed input: 21014.28 toks/s, output: 20.52 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.93it/s, est. speed input: 20748.43 toks/s, output: 20.26 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.82it/s, est. speed input: 20541.37 toks/s, output: 20.06 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.71it/s, est. speed input: 20354.88 toks/s, output: 19.88 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.66it/s, est. speed input: 20201.70 toks/s, output: 19.73 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:05, 17.65it/s, est. speed input: 20071.57 toks/s, output: 19.60 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.66it/s, est. speed input: 19960.12 toks/s, output: 19.49 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 17.62it/s, est. speed input: 19852.94 toks/s, output: 19.39 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.64it/s, est. speed input: 19763.23 toks/s, output: 19.30 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.61it/s, est. speed input: 19676.82 toks/s, output: 19.22 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.65it/s, est. speed input: 19607.39 toks/s, output: 19.15 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.65it/s, est. speed input: 19538.91 toks/s, output: 19.08 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 17.58it/s, est. speed input: 19466.67 toks/s, output: 19.01 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 17.60it/s, est. speed input: 19410.36 toks/s, output: 18.96 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:04, 17.59it/s, est. speed input: 19354.40 toks/s, output: 18.90 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 17.50it/s, est. speed input: 19291.82 toks/s, output: 18.84 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.54it/s, est. speed input: 19247.49 toks/s, output: 18.80 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.53it/s, est. speed input: 19200.62 toks/s, output: 18.75 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.48it/s, est. speed input: 19152.70 toks/s, output: 18.70 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 17.42it/s, est. speed input: 19104.10 toks/s, output: 18.66 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 17.38it/s, est. speed input: 19058.96 toks/s, output: 18.61 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 17.35it/s, est. speed input: 19016.87 toks/s, output: 18.57 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:03, 17.36it/s, est. speed input: 18979.65 toks/s, output: 18.53 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:03<00:03, 17.41it/s, est. speed input: 18950.60 toks/s, output: 18.51 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.47it/s, est. speed input: 18925.02 toks/s, output: 18.48 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.47it/s, est. speed input: 18896.78 toks/s, output: 18.45 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.46it/s, est. speed input: 18868.38 toks/s, output: 18.43 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.51it/s, est. speed input: 18847.28 toks/s, output: 18.41 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.51it/s, est. speed input: 18824.63 toks/s, output: 18.38 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 17.47it/s, est. speed input: 18798.31 toks/s, output: 18.36 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:02, 17.51it/s, est. speed input: 18780.40 toks/s, output: 18.34 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:02, 17.56it/s, est. speed input: 18764.50 toks/s, output: 18.32 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:04<00:02, 17.62it/s, est. speed input: 18751.02 toks/s, output: 18.31 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:01, 17.66it/s, est. speed input: 18738.84 toks/s, output: 18.30 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.79it/s, est. speed input: 18734.40 toks/s, output: 18.30 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.83it/s, est. speed input: 18726.08 toks/s, output: 18.29 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 17.89it/s, est. speed input: 18720.60 toks/s, output: 18.28 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.82it/s, est. speed input: 18707.54 toks/s, output: 18.27 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 17.80it/s, est. speed input: 18696.81 toks/s, output: 18.26 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 17.83it/s, est. speed input: 18690.04 toks/s, output: 18.25 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 17.86it/s, est. speed input: 18683.94 toks/s, output: 18.25 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:05<00:01, 17.87it/s, est. speed input: 18677.23 toks/s, output: 18.24 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.85it/s, est. speed input: 18668.64 toks/s, output: 18.23 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.85it/s, est. speed input: 18661.48 toks/s, output: 18.22 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.83it/s, est. speed input: 18653.48 toks/s, output: 18.22 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 17.84it/s, est. speed input: 18647.16 toks/s, output: 18.21 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.83it/s, est. speed input: 18640.14 toks/s, output: 18.20 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 17.79it/s, est. speed input: 18631.50 toks/s, output: 18.19 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:06<00:00, 17.84it/s, est. speed input: 18627.42 toks/s, output: 18.19 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:06<00:00, 17.88it/s, est. speed input: 18623.73 toks/s, output: 18.19 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:06<00:00, 17.88it/s, est. speed input: 18618.67 toks/s, output: 18.18 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.88it/s, est. speed input: 18614.68 toks/s, output: 18.18 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 18.18it/s, est. speed input: 18614.68 toks/s, output: 18.18 toks/s]
[rank0]:[W128 11:11:31.156636928 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.5s

测试结果:
  Requests/s:   17.33
  Tokens/s:     17760.25
  Total Reqs:   128
  Elapsed:      7.39s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     17742.92

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:11:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:11:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=9470) WARNING 01-28 11:11:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=9470) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=9470) WARNING 01-28 11:12:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.27 requests/s, 35128.95 total tokens/s, 34.27 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 11:11:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:11:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:11:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:11:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:11:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:11:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:11:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:11:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:11:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:11:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:11:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:11:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:11:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:11:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:11:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:11:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:11:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:11:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:11:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=9470) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=9470) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=9470) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=9470) 
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=9470) [2026-01-28 11:11:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=9470) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.34it/s]
(EngineCore_DP0 pid=9470) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.53it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  15%|█▍        | 38/256 [00:00<00:00, 371.28it/s]
Adding requests:  31%|███       | 79/256 [00:00<00:00, 391.81it/s]
Adding requests:  46%|████▋     | 119/256 [00:00<00:00, 392.87it/s]
Adding requests:  62%|██████▏   | 159/256 [00:00<00:00, 393.01it/s]
Adding requests:  78%|███████▊  | 199/256 [00:00<00:00, 385.24it/s]
Adding requests:  94%|█████████▍| 241/256 [00:00<00:00, 393.64it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 391.05it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:01, 142.39it/s, est. speed input: 145839.07 toks/s, output: 142.40 toks/s]
Processed prompts:  14%|█▍        | 37/256 [00:00<00:03, 59.67it/s, est. speed input: 68165.40 toks/s, output: 66.57 toks/s]   
Processed prompts:  18%|█▊        | 46/256 [00:00<00:04, 46.54it/s, est. speed input: 55430.50 toks/s, output: 54.13 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:04, 42.95it/s, est. speed input: 51823.58 toks/s, output: 50.61 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 42.83it/s, est. speed input: 50954.16 toks/s, output: 49.76 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 38.20it/s, est. speed input: 47951.47 toks/s, output: 46.83 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:05, 37.09it/s, est. speed input: 46809.99 toks/s, output: 45.71 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 36.13it/s, est. speed input: 45824.62 toks/s, output: 44.75 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:05, 35.47it/s, est. speed input: 45015.47 toks/s, output: 43.96 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:05, 35.00it/s, est. speed input: 44326.20 toks/s, output: 43.29 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:05, 34.49it/s, est. speed input: 43678.13 toks/s, output: 42.65 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:04, 34.23it/s, est. speed input: 43135.02 toks/s, output: 42.12 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 34.63it/s, est. speed input: 42792.13 toks/s, output: 41.79 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 35.08it/s, est. speed input: 42514.83 toks/s, output: 41.52 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 35.21it/s, est. speed input: 42224.80 toks/s, output: 41.23 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 35.48it/s, est. speed input: 41993.12 toks/s, output: 41.01 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 35.72it/s, est. speed input: 41787.80 toks/s, output: 40.81 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:04, 35.84it/s, est. speed input: 41592.33 toks/s, output: 40.62 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 35.89it/s, est. speed input: 41405.82 toks/s, output: 40.44 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 35.99it/s, est. speed input: 41244.14 toks/s, output: 40.28 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:03<00:03, 36.05it/s, est. speed input: 41092.16 toks/s, output: 40.13 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 36.20it/s, est. speed input: 40965.02 toks/s, output: 40.00 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 35.95it/s, est. speed input: 40801.57 toks/s, output: 39.84 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 35.78it/s, est. speed input: 40647.77 toks/s, output: 39.69 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 35.64it/s, est. speed input: 40502.83 toks/s, output: 39.55 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 35.62it/s, est. speed input: 40375.28 toks/s, output: 39.43 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:03, 35.64it/s, est. speed input: 40259.20 toks/s, output: 39.32 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 35.67it/s, est. speed input: 40151.98 toks/s, output: 39.21 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 35.73it/s, est. speed input: 40055.47 toks/s, output: 39.12 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:04<00:02, 35.77it/s, est. speed input: 39963.70 toks/s, output: 39.03 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 35.64it/s, est. speed input: 39860.66 toks/s, output: 38.93 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 35.58it/s, est. speed input: 39766.95 toks/s, output: 38.83 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 35.60it/s, est. speed input: 39682.74 toks/s, output: 38.75 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 35.58it/s, est. speed input: 39600.81 toks/s, output: 38.67 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 35.63it/s, est. speed input: 39528.20 toks/s, output: 38.60 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 35.52it/s, est. speed input: 39446.46 toks/s, output: 38.52 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 35.38it/s, est. speed input: 39362.74 toks/s, output: 38.44 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 35.12it/s, est. speed input: 39268.63 toks/s, output: 38.35 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:05<00:01, 35.26it/s, est. speed input: 39206.10 toks/s, output: 38.29 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 35.37it/s, est. speed input: 39146.93 toks/s, output: 38.23 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 35.45it/s, est. speed input: 39090.40 toks/s, output: 38.17 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:05<00:01, 35.43it/s, est. speed input: 39030.80 toks/s, output: 38.12 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 35.44it/s, est. speed input: 38975.16 toks/s, output: 38.06 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 35.48it/s, est. speed input: 38924.62 toks/s, output: 38.01 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:01, 35.59it/s, est. speed input: 38881.06 toks/s, output: 37.97 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 35.65it/s, est. speed input: 38838.88 toks/s, output: 37.93 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 35.72it/s, est. speed input: 38799.29 toks/s, output: 37.89 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:06<00:00, 35.73it/s, est. speed input: 38759.47 toks/s, output: 37.85 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:06<00:00, 35.20it/s, est. speed input: 38685.17 toks/s, output: 37.78 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:06<00:00, 35.23it/s, est. speed input: 38639.54 toks/s, output: 37.73 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:06<00:00, 35.33it/s, est. speed input: 38600.82 toks/s, output: 37.70 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 35.36it/s, est. speed input: 38560.68 toks/s, output: 37.66 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 35.45it/s, est. speed input: 38526.02 toks/s, output: 37.62 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 35.57it/s, est. speed input: 38496.26 toks/s, output: 37.59 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 35.57it/s, est. speed input: 38474.90 toks/s, output: 37.57 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 37.57it/s, est. speed input: 38474.90 toks/s, output: 37.57 toks/s]
[rank0]:[W128 11:12:19.273319077 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.1s

测试结果:
  Requests/s:   34.27
  Tokens/s:     35128.95
  Total Reqs:   256
  Elapsed:      7.47s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     35094.68

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:12:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:12:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=10008) WARNING 01-28 11:12:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=10008) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=10008) WARNING 01-28 11:12:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 65.86 requests/s, 67509.83 total tokens/s, 65.86 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 11:12:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:12:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:12:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:12:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:12:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:12:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:12:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:12:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:12:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:12:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:12:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:12:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:12:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:12:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:12:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:12:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:12:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:12:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:12:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=10008) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=10008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=10008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=10008) 
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=10008) [2026-01-28 11:12:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=10008) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.65it/s]
(EngineCore_DP0 pid=10008) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.76it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.95it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 39/512 [00:00<00:01, 383.98it/s]
Adding requests:  15%|█▌        | 79/512 [00:00<00:01, 391.96it/s]
Adding requests:  23%|██▎       | 120/512 [00:00<00:00, 397.47it/s]
Adding requests:  31%|███▏      | 160/512 [00:00<00:00, 396.59it/s]
Adding requests:  39%|███▉      | 200/512 [00:00<00:00, 390.69it/s]
Adding requests:  47%|████▋     | 241/512 [00:00<00:00, 394.20it/s]
Adding requests:  55%|█████▍    | 281/512 [00:00<00:00, 393.38it/s]
Adding requests:  63%|██████▎   | 321/512 [00:00<00:00, 392.85it/s]
Adding requests:  71%|███████   | 363/512 [00:00<00:00, 399.47it/s]
Adding requests:  79%|███████▉  | 404/512 [00:01<00:00, 400.33it/s]
Adding requests:  87%|████████▋ | 445/512 [00:01<00:00, 400.75it/s]
Adding requests:  95%|█████████▍| 486/512 [00:01<00:00, 402.53it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 397.02it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:00, 534.06it/s, est. speed input: 546982.37 toks/s, output: 534.09 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:00<00:03, 121.14it/s, est. speed input: 141605.52 toks/s, output: 138.29 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:01<00:03, 99.79it/s, est. speed input: 119065.89 toks/s, output: 116.27 toks/s] 
Processed prompts:  31%|███▏      | 161/512 [00:01<00:03, 94.63it/s, est. speed input: 113193.25 toks/s, output: 110.54 toks/s]
Processed prompts:  34%|███▍      | 175/512 [00:01<00:03, 86.28it/s, est. speed input: 106710.64 toks/s, output: 104.21 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:01<00:03, 83.14it/s, est. speed input: 103728.12 toks/s, output: 101.30 toks/s]
Processed prompts:  38%|███▊      | 197/512 [00:01<00:03, 84.18it/s, est. speed input: 103011.82 toks/s, output: 100.60 toks/s]
Processed prompts:  40%|████      | 207/512 [00:02<00:03, 77.44it/s, est. speed input: 99667.42 toks/s, output: 97.33 toks/s]  
Processed prompts:  42%|████▏     | 216/512 [00:02<00:03, 77.89it/s, est. speed input: 98753.71 toks/s, output: 96.44 toks/s]
Processed prompts:  44%|████▍     | 225/512 [00:02<00:03, 78.44it/s, est. speed input: 97968.04 toks/s, output: 95.67 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:02<00:03, 70.37it/s, est. speed input: 95108.38 toks/s, output: 92.88 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:02<00:03, 70.80it/s, est. speed input: 94214.51 toks/s, output: 92.01 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:02<00:03, 70.56it/s, est. speed input: 93269.03 toks/s, output: 91.08 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:02<00:03, 70.39it/s, est. speed input: 92402.54 toks/s, output: 90.24 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:02<00:03, 70.29it/s, est. speed input: 91608.02 toks/s, output: 89.46 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:03<00:03, 70.29it/s, est. speed input: 90885.08 toks/s, output: 88.75 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 70.21it/s, est. speed input: 90199.89 toks/s, output: 88.09 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:03<00:03, 70.25it/s, est. speed input: 89576.39 toks/s, output: 87.48 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:03<00:03, 69.90it/s, est. speed input: 88940.81 toks/s, output: 86.86 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:03<00:02, 69.80it/s, est. speed input: 88365.76 toks/s, output: 86.29 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:03<00:02, 69.96it/s, est. speed input: 87858.31 toks/s, output: 85.80 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:03<00:02, 69.96it/s, est. speed input: 87366.96 toks/s, output: 85.32 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:03<00:02, 69.57it/s, est. speed input: 86855.92 toks/s, output: 84.82 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:04<00:02, 69.33it/s, est. speed input: 86379.67 toks/s, output: 84.35 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:04<00:02, 68.79it/s, est. speed input: 85886.22 toks/s, output: 83.87 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:04<00:02, 68.81it/s, est. speed input: 85465.13 toks/s, output: 83.46 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:04<00:02, 69.13it/s, est. speed input: 85099.88 toks/s, output: 83.10 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:04<00:02, 69.37it/s, est. speed input: 84754.47 toks/s, output: 82.77 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:04<00:01, 69.63it/s, est. speed input: 84435.56 toks/s, output: 82.46 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:04<00:01, 69.75it/s, est. speed input: 84126.26 toks/s, output: 82.15 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:04<00:01, 69.84it/s, est. speed input: 83832.17 toks/s, output: 81.87 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:04<00:01, 69.73it/s, est. speed input: 83535.71 toks/s, output: 81.58 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:05<00:01, 69.79it/s, est. speed input: 83264.67 toks/s, output: 81.31 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:01, 69.62it/s, est. speed input: 82987.61 toks/s, output: 81.04 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:05<00:01, 69.64it/s, est. speed input: 82734.26 toks/s, output: 80.79 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:05<00:01, 69.65it/s, est. speed input: 82491.91 toks/s, output: 80.56 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:05<00:01, 69.35it/s, est. speed input: 82234.07 toks/s, output: 80.31 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:05<00:00, 69.47it/s, est. speed input: 82013.00 toks/s, output: 80.09 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:05<00:00, 69.65it/s, est. speed input: 81809.01 toks/s, output: 79.89 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:05<00:00, 69.76it/s, est. speed input: 81611.29 toks/s, output: 79.70 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:05<00:00, 69.77it/s, est. speed input: 81416.12 toks/s, output: 79.51 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:06<00:00, 69.90it/s, est. speed input: 81237.29 toks/s, output: 79.33 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:06<00:00, 69.88it/s, est. speed input: 81057.33 toks/s, output: 79.16 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:06<00:00, 69.54it/s, est. speed input: 80860.88 toks/s, output: 78.97 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:06<00:00, 69.39it/s, est. speed input: 80677.79 toks/s, output: 78.79 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 69.39it/s, est. speed input: 80878.00 toks/s, output: 78.98 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 78.98it/s, est. speed input: 80878.00 toks/s, output: 78.98 toks/s]
[rank0]:[W128 11:13:12.191118281 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.2s

测试结果:
  Requests/s:   65.86
  Tokens/s:     67509.83
  Total Reqs:   512
  Elapsed:      7.77s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     67443.97

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:13:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:13:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=10571) WARNING 01-28 11:13:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=10571) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=10571) WARNING 01-28 11:13:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 68.58 requests/s, 70299.44 total tokens/s, 68.58 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 11:13:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:13:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:13:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:13:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:13:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:13:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:13:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:13:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:13:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:13:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:13:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:13:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:13:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:13:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:13:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:13:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:13:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:13:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:13:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=10571) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=10571) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=10571) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=10571) 
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=10571) [2026-01-28 11:13:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=10571) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.45it/s]
(EngineCore_DP0 pid=10571) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.69it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.76it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 34/1024 [00:00<00:02, 337.01it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 376.25it/s]
Adding requests:  11%|█▏        | 116/1024 [00:00<00:02, 390.34it/s]
Adding requests:  15%|█▌        | 156/1024 [00:00<00:02, 388.57it/s]
Adding requests:  19%|█▉        | 195/1024 [00:00<00:02, 386.35it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:02, 393.84it/s]
Adding requests:  27%|██▋       | 277/1024 [00:00<00:01, 396.50it/s]
Adding requests:  31%|███       | 317/1024 [00:00<00:01, 392.69it/s]
Adding requests:  35%|███▍      | 358/1024 [00:00<00:01, 396.65it/s]
Adding requests:  39%|███▉      | 398/1024 [00:01<00:01, 394.45it/s]
Adding requests:  43%|████▎     | 438/1024 [00:01<00:01, 390.17it/s]
Adding requests:  47%|████▋     | 478/1024 [00:01<00:01, 390.21it/s]
Adding requests:  51%|█████     | 518/1024 [00:01<00:01, 387.57it/s]
Adding requests:  54%|█████▍    | 557/1024 [00:01<00:01, 387.62it/s]
Adding requests:  58%|█████▊    | 599/1024 [00:01<00:01, 394.47it/s]
Adding requests:  63%|██████▎   | 641/1024 [00:01<00:00, 400.79it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:01<00:00, 405.69it/s]
Adding requests:  71%|███████   | 724/1024 [00:01<00:00, 401.60it/s]
Adding requests:  75%|███████▍  | 765/1024 [00:01<00:00, 401.03it/s]
Adding requests:  79%|███████▊  | 806/1024 [00:02<00:00, 391.75it/s]
Adding requests:  83%|████████▎ | 846/1024 [00:02<00:00, 389.35it/s]
Adding requests:  87%|████████▋ | 888/1024 [00:02<00:00, 396.41it/s]
Adding requests:  91%|█████████ | 930/1024 [00:02<00:00, 402.58it/s]
Adding requests:  95%|█████████▍| 972/1024 [00:02<00:00, 405.75it/s]
Adding requests:  99%|█████████▉| 1014/1024 [00:02<00:00, 409.94it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 396.15it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:00<00:00, 1244.18it/s, est. speed input: 1274237.47 toks/s, output: 1244.24 toks/s]
Processed prompts:  29%|██▉       | 295/1024 [00:01<00:05, 135.61it/s, est. speed input: 164145.14 toks/s, output: 160.30 toks/s]   
Processed prompts:  34%|███▍      | 351/1024 [00:02<00:06, 110.87it/s, est. speed input: 136848.11 toks/s, output: 133.64 toks/s]
Processed prompts:  38%|███▊      | 385/1024 [00:03<00:06, 102.15it/s, est. speed input: 127938.29 toks/s, output: 124.94 toks/s]
Processed prompts:  40%|███▉      | 409/1024 [00:03<00:06, 95.31it/s, est. speed input: 122172.80 toks/s, output: 119.31 toks/s] 
Processed prompts:  42%|████▏     | 427/1024 [00:03<00:07, 83.95it/s, est. speed input: 115050.99 toks/s, output: 112.35 toks/s]
Processed prompts:  43%|████▎     | 441/1024 [00:03<00:06, 86.90it/s, est. speed input: 115055.13 toks/s, output: 112.36 toks/s]
Processed prompts:  44%|████▍     | 454/1024 [00:06<00:22, 25.24it/s, est. speed input: 71572.55 toks/s, output: 69.90 toks/s]  
Processed prompts:  45%|████▌     | 463/1024 [00:06<00:20, 27.58it/s, est. speed input: 71625.55 toks/s, output: 69.95 toks/s]
Processed prompts:  46%|████▌     | 472/1024 [00:06<00:18, 30.55it/s, est. speed input: 71675.20 toks/s, output: 70.00 toks/s]
Processed prompts:  47%|████▋     | 480/1024 [00:06<00:16, 33.47it/s, est. speed input: 71576.75 toks/s, output: 69.90 toks/s]
Processed prompts:  48%|████▊     | 488/1024 [00:06<00:14, 36.87it/s, est. speed input: 71478.79 toks/s, output: 69.80 toks/s]
Processed prompts:  48%|████▊     | 496/1024 [00:07<00:13, 40.58it/s, est. speed input: 71378.45 toks/s, output: 69.71 toks/s]
Processed prompts:  49%|████▉     | 503/1024 [00:07<00:12, 43.12it/s, est. speed input: 71141.22 toks/s, output: 69.47 toks/s]
Processed prompts:  50%|████▉     | 510/1024 [00:07<00:11, 45.62it/s, est. speed input: 70915.66 toks/s, output: 69.25 toks/s]
Processed prompts:  50%|█████     | 517/1024 [00:07<00:10, 47.86it/s, est. speed input: 70693.33 toks/s, output: 69.04 toks/s]
Processed prompts:  51%|█████     | 524/1024 [00:07<00:10, 49.83it/s, est. speed input: 70481.26 toks/s, output: 68.83 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:09, 49.48it/s, est. speed input: 70147.12 toks/s, output: 68.50 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:07<00:09, 53.70it/s, est. speed input: 70104.09 toks/s, output: 68.46 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:07<00:08, 57.97it/s, est. speed input: 70131.96 toks/s, output: 68.49 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:07, 61.23it/s, est. speed input: 70154.88 toks/s, output: 68.51 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:07, 63.75it/s, est. speed input: 70180.48 toks/s, output: 68.54 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:06, 65.57it/s, est. speed input: 70203.33 toks/s, output: 68.56 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:06, 66.89it/s, est. speed input: 70225.40 toks/s, output: 68.58 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:06, 67.93it/s, est. speed input: 70251.25 toks/s, output: 68.60 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:08<00:06, 68.64it/s, est. speed input: 70274.48 toks/s, output: 68.63 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:08<00:06, 69.18it/s, est. speed input: 70298.91 toks/s, output: 68.65 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:08<00:05, 69.51it/s, est. speed input: 70320.58 toks/s, output: 68.67 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:08<00:05, 69.75it/s, est. speed input: 70341.86 toks/s, output: 68.69 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:05, 69.91it/s, est. speed input: 70362.23 toks/s, output: 68.71 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:05, 69.97it/s, est. speed input: 70379.72 toks/s, output: 68.73 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:05, 70.06it/s, est. speed input: 70399.10 toks/s, output: 68.75 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:09<00:05, 70.09it/s, est. speed input: 70416.39 toks/s, output: 68.77 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:09<00:05, 70.19it/s, est. speed input: 70436.60 toks/s, output: 68.79 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:09<00:05, 70.27it/s, est. speed input: 70456.50 toks/s, output: 68.81 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:09<00:04, 70.31it/s, est. speed input: 70475.45 toks/s, output: 68.82 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:09<00:04, 70.30it/s, est. speed input: 70492.48 toks/s, output: 68.84 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:04, 70.24it/s, est. speed input: 70507.12 toks/s, output: 68.85 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:10<00:04, 70.18it/s, est. speed input: 70520.91 toks/s, output: 68.87 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:10<00:04, 70.17it/s, est. speed input: 70535.33 toks/s, output: 68.88 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:10<00:04, 70.19it/s, est. speed input: 70550.67 toks/s, output: 68.90 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:10<00:04, 70.25it/s, est. speed input: 70567.38 toks/s, output: 68.91 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:10<00:04, 70.28it/s, est. speed input: 70583.20 toks/s, output: 68.93 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:10<00:04, 70.31it/s, est. speed input: 70598.82 toks/s, output: 68.94 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:10<00:03, 70.28it/s, est. speed input: 70612.50 toks/s, output: 68.96 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:10<00:03, 70.29it/s, est. speed input: 70627.03 toks/s, output: 68.97 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:11<00:03, 70.30it/s, est. speed input: 70641.26 toks/s, output: 68.99 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:11<00:03, 70.22it/s, est. speed input: 70652.29 toks/s, output: 69.00 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:11<00:03, 70.18it/s, est. speed input: 70663.71 toks/s, output: 69.01 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:11<00:03, 70.18it/s, est. speed input: 70675.75 toks/s, output: 69.02 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:11<00:03, 70.26it/s, est. speed input: 70690.22 toks/s, output: 69.03 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:11<00:03, 70.27it/s, est. speed input: 70702.69 toks/s, output: 69.05 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:11<00:03, 70.30it/s, est. speed input: 70716.04 toks/s, output: 69.06 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:11<00:02, 70.34it/s, est. speed input: 70729.57 toks/s, output: 69.07 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:11<00:02, 70.33it/s, est. speed input: 70741.58 toks/s, output: 69.08 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:12<00:02, 70.28it/s, est. speed input: 70751.97 toks/s, output: 69.09 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:12<00:02, 70.27it/s, est. speed input: 70763.15 toks/s, output: 69.10 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:12<00:02, 70.21it/s, est. speed input: 70772.15 toks/s, output: 69.11 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:12<00:02, 70.23it/s, est. speed input: 70783.13 toks/s, output: 69.12 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:12<00:02, 70.28it/s, est. speed input: 70794.86 toks/s, output: 69.14 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:12<00:02, 70.28it/s, est. speed input: 70805.53 toks/s, output: 69.15 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:12<00:02, 70.18it/s, est. speed input: 70812.71 toks/s, output: 69.15 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:12<00:01, 70.19it/s, est. speed input: 70822.31 toks/s, output: 69.16 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:12<00:01, 70.17it/s, est. speed input: 70830.97 toks/s, output: 69.17 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:13<00:01, 70.15it/s, est. speed input: 70839.34 toks/s, output: 69.18 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:13<00:01, 70.19it/s, est. speed input: 70848.99 toks/s, output: 69.19 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:13<00:01, 70.16it/s, est. speed input: 70856.96 toks/s, output: 69.20 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:13<00:01, 70.16it/s, est. speed input: 70865.34 toks/s, output: 69.20 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:13<00:01, 70.13it/s, est. speed input: 70872.83 toks/s, output: 69.21 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:13<00:01, 70.14it/s, est. speed input: 70880.91 toks/s, output: 69.22 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:13<00:00, 70.24it/s, est. speed input: 70891.35 toks/s, output: 69.23 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:13<00:00, 70.29it/s, est. speed input: 70901.21 toks/s, output: 69.24 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:14<00:00, 70.25it/s, est. speed input: 70908.79 toks/s, output: 69.25 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:14<00:00, 70.22it/s, est. speed input: 70916.35 toks/s, output: 69.25 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:14<00:00, 72.41it/s, est. speed input: 70981.63 toks/s, output: 69.32 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:14<00:00, 71.79it/s, est. speed input: 70990.30 toks/s, output: 69.33 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:14<00:00, 71.31it/s, est. speed input: 70997.40 toks/s, output: 69.33 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:14<00:00, 70.99it/s, est. speed input: 71004.74 toks/s, output: 69.34 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:14<00:00, 73.38it/s, est. speed input: 71076.97 toks/s, output: 69.41 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:14<00:00, 73.38it/s, est. speed input: 71494.69 toks/s, output: 69.82 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:14<00:00, 69.82it/s, est. speed input: 71494.69 toks/s, output: 69.82 toks/s]
[rank0]:[W128 11:14:15.432488833 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 62.8s

测试结果:
  Requests/s:   68.58
  Tokens/s:     70299.44
  Total Reqs:   1024
  Elapsed:      14.93s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     70230.85

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:14:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:14:35 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=11362) WARNING 01-28 11:14:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=11362) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=11362) WARNING 01-28 11:14:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 67.29 requests/s, 68968.98 total tokens/s, 67.29 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 11:14:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:14:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:14:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:14:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:14:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:14:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:14:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:14:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:14:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:14:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:14:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:14:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:14:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:14:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:14:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:14:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:14:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:14:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:14:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=11362) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=11362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.90it/s]
(EngineCore_DP0 pid=11362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.90it/s]
(EngineCore_DP0 pid=11362) 
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=11362) [2026-01-28 11:14:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=11362) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.58it/s]
(EngineCore_DP0 pid=11362) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.89it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 33/2048 [00:00<00:06, 322.07it/s]
Adding requests:   3%|▎         | 67/2048 [00:00<00:06, 327.27it/s]
Adding requests:   5%|▌         | 104/2048 [00:00<00:05, 345.11it/s]
Adding requests:   7%|▋         | 143/2048 [00:00<00:05, 360.71it/s]
Adding requests:   9%|▉         | 182/2048 [00:00<00:05, 368.09it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 366.50it/s]
Adding requests:  13%|█▎        | 257/2048 [00:00<00:04, 369.36it/s]
Adding requests:  14%|█▍        | 295/2048 [00:00<00:04, 372.66it/s]
Adding requests:  16%|█▋        | 334/2048 [00:00<00:04, 377.20it/s]
Adding requests:  18%|█▊        | 374/2048 [00:01<00:04, 382.50it/s]
Adding requests:  20%|██        | 413/2048 [00:01<00:04, 382.47it/s]
Adding requests:  22%|██▏       | 453/2048 [00:01<00:04, 383.24it/s]
Adding requests:  24%|██▍       | 492/2048 [00:01<00:04, 378.85it/s]
Adding requests:  26%|██▌       | 530/2048 [00:01<00:04, 362.77it/s]
Adding requests:  28%|██▊       | 568/2048 [00:01<00:04, 366.95it/s]
Adding requests:  30%|██▉       | 605/2048 [00:01<00:03, 367.67it/s]
Adding requests:  31%|███▏      | 644/2048 [00:01<00:03, 372.15it/s]
Adding requests:  33%|███▎      | 686/2048 [00:01<00:03, 384.18it/s]
Adding requests:  36%|███▌      | 728/2048 [00:01<00:03, 392.49it/s]
Adding requests:  38%|███▊      | 768/2048 [00:02<00:03, 384.52it/s]
Adding requests:  39%|███▉      | 807/2048 [00:02<00:03, 382.91it/s]
Adding requests:  41%|████▏     | 846/2048 [00:02<00:03, 374.97it/s]
Adding requests:  43%|████▎     | 886/2048 [00:02<00:03, 380.75it/s]
Adding requests:  45%|████▌     | 927/2048 [00:02<00:02, 387.21it/s]
Adding requests:  47%|████▋     | 966/2048 [00:02<00:02, 387.13it/s]
Adding requests:  49%|████▉     | 1006/2048 [00:02<00:02, 389.69it/s]
Adding requests:  51%|█████     | 1046/2048 [00:02<00:02, 390.49it/s]
Adding requests:  53%|█████▎    | 1086/2048 [00:02<00:02, 391.08it/s]
Adding requests:  55%|█████▍    | 1126/2048 [00:02<00:02, 390.99it/s]
Adding requests:  57%|█████▋    | 1168/2048 [00:03<00:02, 398.92it/s]
Adding requests:  59%|█████▉    | 1210/2048 [00:03<00:02, 403.59it/s]
Adding requests:  61%|██████    | 1251/2048 [00:03<00:02, 393.67it/s]
Adding requests:  63%|██████▎   | 1291/2048 [00:03<00:01, 395.49it/s]
Adding requests:  65%|██████▌   | 1332/2048 [00:03<00:01, 398.43it/s]
Adding requests:  67%|██████▋   | 1373/2048 [00:03<00:01, 401.33it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:03<00:01, 398.45it/s]
Adding requests:  71%|███████   | 1454/2048 [00:03<00:01, 392.47it/s]
Adding requests:  73%|███████▎  | 1495/2048 [00:03<00:01, 395.51it/s]
Adding requests:  75%|███████▌  | 1536/2048 [00:04<00:01, 397.79it/s]
Adding requests:  77%|███████▋  | 1577/2048 [00:04<00:01, 399.99it/s]
Adding requests:  79%|███████▉  | 1618/2048 [00:04<00:01, 399.51it/s]
Adding requests:  81%|████████  | 1658/2048 [00:04<00:00, 398.97it/s]
Adding requests:  83%|████████▎ | 1698/2048 [00:04<00:00, 394.91it/s]
Adding requests:  85%|████████▍ | 1739/2048 [00:04<00:00, 397.56it/s]
Adding requests:  87%|████████▋ | 1779/2048 [00:04<00:00, 393.81it/s]
Adding requests:  89%|████████▉ | 1819/2048 [00:04<00:00, 393.99it/s]
Adding requests:  91%|█████████ | 1860/2048 [00:04<00:00, 394.22it/s]
Adding requests:  93%|█████████▎| 1900/2048 [00:04<00:00, 383.44it/s]
Adding requests:  95%|█████████▍| 1940/2048 [00:05<00:00, 388.06it/s]
Adding requests:  97%|█████████▋| 1980/2048 [00:05<00:00, 389.37it/s]
Adding requests:  99%|█████████▊| 2019/2048 [00:05<00:00, 388.47it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 385.24it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:00<00:01, 1589.13it/s, est. speed input: 1627445.12 toks/s, output: 1589.18 toks/s]
Processed prompts:  24%|██▍       | 497/2048 [00:04<00:17, 89.49it/s, est. speed input: 113493.03 toks/s, output: 110.83 toks/s]    
Processed prompts:  28%|██▊       | 565/2048 [00:05<00:18, 79.82it/s, est. speed input: 101301.69 toks/s, output: 98.93 toks/s] 
Processed prompts:  30%|██▉       | 605/2048 [00:06<00:17, 80.23it/s, est. speed input: 100014.87 toks/s, output: 97.67 toks/s]
Processed prompts:  31%|███       | 633/2048 [00:06<00:18, 76.58it/s, est. speed input: 97076.64 toks/s, output: 94.80 toks/s] 
Processed prompts:  32%|███▏      | 653/2048 [00:06<00:18, 77.23it/s, est. speed input: 96645.77 toks/s, output: 94.38 toks/s]
Processed prompts:  33%|███▎      | 669/2048 [00:07<00:18, 75.79it/s, est. speed input: 95673.87 toks/s, output: 93.43 toks/s]
Processed prompts:  33%|███▎      | 682/2048 [00:07<00:18, 72.35it/s, est. speed input: 94352.77 toks/s, output: 92.14 toks/s]
Processed prompts:  34%|███▍      | 693/2048 [00:07<00:20, 67.45it/s, est. speed input: 92848.06 toks/s, output: 90.67 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:07<00:20, 64.65it/s, est. speed input: 91698.03 toks/s, output: 89.55 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:08<00:20, 64.98it/s, est. speed input: 90983.07 toks/s, output: 88.85 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:08<00:20, 65.29it/s, est. speed input: 90314.78 toks/s, output: 88.20 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:08<00:19, 65.54it/s, est. speed input: 89684.46 toks/s, output: 87.58 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:08<00:19, 65.73it/s, est. speed input: 89088.17 toks/s, output: 87.00 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:09<00:19, 65.86it/s, est. speed input: 88521.41 toks/s, output: 86.45 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:09<00:18, 65.97it/s, est. speed input: 87986.61 toks/s, output: 85.92 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:09<00:18, 66.04it/s, est. speed input: 87477.38 toks/s, output: 85.43 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:09<00:18, 66.11it/s, est. speed input: 86995.45 toks/s, output: 84.96 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:10<00:18, 66.14it/s, est. speed input: 86533.54 toks/s, output: 84.51 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:10<00:17, 66.16it/s, est. speed input: 86093.95 toks/s, output: 84.08 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:10<00:17, 66.18it/s, est. speed input: 85674.79 toks/s, output: 83.67 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:10<00:17, 66.20it/s, est. speed input: 85275.54 toks/s, output: 83.28 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:11<00:17, 66.20it/s, est. speed input: 84892.49 toks/s, output: 82.90 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:11<00:16, 66.28it/s, est. speed input: 84532.95 toks/s, output: 82.55 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:11<00:16, 66.19it/s, est. speed input: 84174.86 toks/s, output: 82.20 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:11<00:16, 66.19it/s, est. speed input: 83837.34 toks/s, output: 81.87 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:11<00:15, 67.30it/s, est. speed input: 83606.75 toks/s, output: 81.65 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:12<00:15, 66.95it/s, est. speed input: 83292.27 toks/s, output: 81.34 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:12<00:15, 66.72it/s, est. speed input: 82991.22 toks/s, output: 81.05 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:12<00:15, 66.55it/s, est. speed input: 82701.19 toks/s, output: 80.76 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:12<00:15, 66.42it/s, est. speed input: 82420.48 toks/s, output: 80.49 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:13<00:14, 67.10it/s, est. speed input: 82207.95 toks/s, output: 80.28 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:13<00:14, 67.76it/s, est. speed input: 82015.46 toks/s, output: 80.09 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:13<00:14, 68.21it/s, est. speed input: 81828.35 toks/s, output: 79.91 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:13<00:13, 68.55it/s, est. speed input: 81648.69 toks/s, output: 79.73 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:14<00:13, 68.77it/s, est. speed input: 81473.72 toks/s, output: 79.56 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:14<00:13, 68.93it/s, est. speed input: 81304.11 toks/s, output: 79.40 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:14<00:12, 70.28it/s, est. speed input: 81216.24 toks/s, output: 79.31 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:14<00:12, 69.99it/s, est. speed input: 81056.66 toks/s, output: 79.16 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:15<00:12, 69.78it/s, est. speed input: 80901.13 toks/s, output: 79.00 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:15<00:12, 69.62it/s, est. speed input: 80750.04 toks/s, output: 78.86 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:15<00:11, 69.53it/s, est. speed input: 80604.25 toks/s, output: 78.71 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:15<00:11, 69.44it/s, est. speed input: 80461.16 toks/s, output: 78.58 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:15<00:11, 69.37it/s, est. speed input: 80322.23 toks/s, output: 78.44 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:16<00:11, 69.34it/s, est. speed input: 80188.19 toks/s, output: 78.31 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:16<00:11, 69.36it/s, est. speed input: 80060.10 toks/s, output: 78.18 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:16<00:10, 69.25it/s, est. speed input: 79928.88 toks/s, output: 78.06 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:16<00:10, 69.20it/s, est. speed input: 79802.76 toks/s, output: 77.93 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:17<00:10, 69.23it/s, est. speed input: 79682.97 toks/s, output: 77.82 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:17<00:10, 69.23it/s, est. speed input: 79566.06 toks/s, output: 77.70 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:17<00:09, 69.25it/s, est. speed input: 79452.50 toks/s, output: 77.59 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:17<00:09, 69.22it/s, est. speed input: 79340.35 toks/s, output: 77.48 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:18<00:09, 69.24it/s, est. speed input: 79232.65 toks/s, output: 77.38 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:18<00:09, 69.23it/s, est. speed input: 79126.68 toks/s, output: 77.27 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:18<00:08, 69.20it/s, est. speed input: 79022.38 toks/s, output: 77.17 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:18<00:08, 69.21it/s, est. speed input: 78921.68 toks/s, output: 77.07 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:18<00:08, 69.20it/s, est. speed input: 78823.19 toks/s, output: 76.98 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:19<00:08, 69.18it/s, est. speed input: 78726.18 toks/s, output: 76.88 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:19<00:08, 69.08it/s, est. speed input: 78627.81 toks/s, output: 76.78 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:19<00:07, 69.14it/s, est. speed input: 78537.25 toks/s, output: 76.70 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:19<00:07, 69.14it/s, est. speed input: 78447.09 toks/s, output: 76.61 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:20<00:07, 69.13it/s, est. speed input: 78358.72 toks/s, output: 76.52 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:20<00:07, 69.14it/s, est. speed input: 78273.00 toks/s, output: 76.44 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:20<00:06, 69.15it/s, est. speed input: 78189.26 toks/s, output: 76.36 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:20<00:06, 69.15it/s, est. speed input: 78107.26 toks/s, output: 76.28 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:21<00:06, 69.15it/s, est. speed input: 78027.03 toks/s, output: 76.20 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:21<00:06, 69.17it/s, est. speed input: 77949.06 toks/s, output: 76.12 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:21<00:05, 69.16it/s, est. speed input: 77872.25 toks/s, output: 76.05 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:21<00:05, 69.17it/s, est. speed input: 77797.47 toks/s, output: 75.97 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:21<00:05, 69.15it/s, est. speed input: 77723.24 toks/s, output: 75.90 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:22<00:05, 69.14it/s, est. speed input: 77650.51 toks/s, output: 75.83 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:22<00:05, 69.14it/s, est. speed input: 77579.76 toks/s, output: 75.76 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:22<00:04, 69.13it/s, est. speed input: 77510.15 toks/s, output: 75.69 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:22<00:04, 69.22it/s, est. speed input: 77445.44 toks/s, output: 75.63 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:23<00:04, 69.14it/s, est. speed input: 77376.75 toks/s, output: 75.56 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:23<00:04, 69.14it/s, est. speed input: 77311.63 toks/s, output: 75.50 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:23<00:03, 69.15it/s, est. speed input: 77248.15 toks/s, output: 75.44 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:23<00:03, 69.18it/s, est. speed input: 77186.51 toks/s, output: 75.38 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:24<00:03, 69.19it/s, est. speed input: 77125.89 toks/s, output: 75.32 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:24<00:03, 69.22it/s, est. speed input: 77066.94 toks/s, output: 75.26 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:24<00:02, 69.19it/s, est. speed input: 77007.63 toks/s, output: 75.20 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:24<00:02, 69.17it/s, est. speed input: 76949.37 toks/s, output: 75.15 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:24<00:02, 70.39it/s, est. speed input: 76933.70 toks/s, output: 75.13 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:25<00:02, 70.02it/s, est. speed input: 76877.83 toks/s, output: 75.08 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:25<00:02, 69.74it/s, est. speed input: 76822.04 toks/s, output: 75.02 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:25<00:01, 69.56it/s, est. speed input: 76767.49 toks/s, output: 74.97 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:25<00:01, 69.42it/s, est. speed input: 76713.66 toks/s, output: 74.92 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:26<00:01, 69.34it/s, est. speed input: 76661.50 toks/s, output: 74.86 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:26<00:01, 69.28it/s, est. speed input: 76609.80 toks/s, output: 74.81 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:26<00:00, 69.24it/s, est. speed input: 76559.39 toks/s, output: 74.76 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:26<00:00, 69.19it/s, est. speed input: 76508.99 toks/s, output: 74.72 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:27<00:00, 69.18it/s, est. speed input: 76460.35 toks/s, output: 74.67 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:27<00:00, 70.58it/s, est. speed input: 76455.35 toks/s, output: 74.66 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:27<00:00, 70.58it/s, est. speed input: 76980.48 toks/s, output: 75.18 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:27<00:00, 75.18it/s, est. speed input: 76980.48 toks/s, output: 75.18 toks/s]
[rank0]:[W128 11:15:39.958736131 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 83.9s

测试结果:
  Requests/s:   67.29
  Tokens/s:     68968.98
  Total Reqs:   2048
  Elapsed:      30.44s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     68901.70

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:16:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:16:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=12723) WARNING 01-28 11:16:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=12723) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=12723) WARNING 01-28 11:16:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 69.74 requests/s, 71486.47 total tokens/s, 69.74 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 11:16:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:16:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:16:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:16:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:16:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:16:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:16:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:16:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:16:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:16:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:16:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:16:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:16:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:16:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:16:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:16:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:16:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:16:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:16:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=12723) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=12723) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=12723) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=12723) 
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=12723) [2026-01-28 11:16:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=12723) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  9.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  9.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.83it/s]
(EngineCore_DP0 pid=12723) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.67it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.23it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.16it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.14it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 372.13it/s]
Adding requests:   2%|▏         | 78/4096 [00:00<00:10, 384.18it/s]
Adding requests:   3%|▎         | 119/4096 [00:00<00:10, 394.03it/s]
Adding requests:   4%|▍         | 159/4096 [00:00<00:09, 395.35it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:09, 392.89it/s]
Adding requests:   6%|▌         | 240/4096 [00:00<00:09, 397.29it/s]
Adding requests:   7%|▋         | 281/4096 [00:00<00:09, 399.73it/s]
Adding requests:   8%|▊         | 321/4096 [00:00<00:09, 397.41it/s]
Adding requests:   9%|▉         | 363/4096 [00:00<00:09, 402.51it/s]
Adding requests:  10%|▉         | 404/4096 [00:01<00:09, 400.03it/s]
Adding requests:  11%|█         | 445/4096 [00:01<00:09, 398.54it/s]
Adding requests:  12%|█▏        | 485/4096 [00:01<00:09, 396.85it/s]
Adding requests:  13%|█▎        | 525/4096 [00:01<00:09, 386.97it/s]
Adding requests:  14%|█▍        | 568/4096 [00:01<00:08, 397.74it/s]
Adding requests:  15%|█▍        | 609/4096 [00:01<00:08, 400.49it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:08, 407.11it/s]
Adding requests:  17%|█▋        | 695/4096 [00:01<00:08, 412.85it/s]
Adding requests:  18%|█▊        | 737/4096 [00:01<00:08, 413.26it/s]
Adding requests:  19%|█▉        | 779/4096 [00:01<00:08, 409.87it/s]
Adding requests:  20%|██        | 821/4096 [00:02<00:08, 403.93it/s]
Adding requests:  21%|██        | 863/4096 [00:02<00:07, 407.56it/s]
Adding requests:  22%|██▏       | 905/4096 [00:02<00:07, 410.35it/s]
Adding requests:  23%|██▎       | 947/4096 [00:02<00:07, 408.72it/s]
Adding requests:  24%|██▍       | 989/4096 [00:02<00:07, 411.87it/s]
Adding requests:  25%|██▌       | 1031/4096 [00:02<00:07, 414.04it/s]
Adding requests:  26%|██▌       | 1073/4096 [00:02<00:07, 397.46it/s]
Adding requests:  27%|██▋       | 1115/4096 [00:02<00:07, 401.58it/s]
Adding requests:  28%|██▊       | 1159/4096 [00:02<00:07, 412.48it/s]
Adding requests:  29%|██▉       | 1201/4096 [00:02<00:07, 404.32it/s]
Adding requests:  30%|███       | 1244/4096 [00:03<00:06, 409.12it/s]
Adding requests:  31%|███▏      | 1286/4096 [00:03<00:06, 410.05it/s]
Adding requests:  32%|███▏      | 1329/4096 [00:03<00:06, 414.68it/s]
Adding requests:  33%|███▎      | 1371/4096 [00:03<00:06, 414.29it/s]
Adding requests:  35%|███▍      | 1414/4096 [00:03<00:06, 418.28it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:06, 420.09it/s]
Adding requests:  37%|███▋      | 1501/4096 [00:03<00:06, 422.89it/s]
Adding requests:  38%|███▊      | 1544/4096 [00:03<00:06, 419.24it/s]
Adding requests:  39%|███▊      | 1586/4096 [00:03<00:06, 415.05it/s]
Adding requests:  40%|███▉      | 1629/4096 [00:04<00:05, 418.82it/s]
Adding requests:  41%|████      | 1671/4096 [00:04<00:05, 415.15it/s]
Adding requests:  42%|████▏     | 1715/4096 [00:04<00:05, 420.07it/s]
Adding requests:  43%|████▎     | 1758/4096 [00:04<00:05, 414.82it/s]
Adding requests:  44%|████▍     | 1801/4096 [00:04<00:05, 417.09it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:04<00:05, 422.14it/s]
Adding requests:  46%|████▌     | 1888/4096 [00:04<00:05, 420.00it/s]
Adding requests:  47%|████▋     | 1931/4096 [00:04<00:05, 410.18it/s]
Adding requests:  48%|████▊     | 1973/4096 [00:04<00:05, 412.44it/s]
Adding requests:  49%|████▉     | 2015/4096 [00:04<00:05, 410.50it/s]
Adding requests:  50%|█████     | 2057/4096 [00:05<00:04, 412.31it/s]
Adding requests:  51%|█████▏    | 2100/4096 [00:05<00:04, 415.24it/s]
Adding requests:  52%|█████▏    | 2142/4096 [00:05<00:04, 408.87it/s]
Adding requests:  53%|█████▎    | 2183/4096 [00:05<00:04, 406.71it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:05<00:04, 413.38it/s]
Adding requests:  55%|█████▌    | 2269/4096 [00:05<00:04, 415.40it/s]
Adding requests:  56%|█████▋    | 2312/4096 [00:05<00:04, 417.82it/s]
Adding requests:  57%|█████▋    | 2354/4096 [00:05<00:04, 414.84it/s]
Adding requests:  58%|█████▊    | 2396/4096 [00:05<00:04, 400.14it/s]
Adding requests:  60%|█████▉    | 2438/4096 [00:05<00:04, 405.14it/s]
Adding requests:  61%|██████    | 2481/4096 [00:06<00:03, 410.57it/s]
Adding requests:  62%|██████▏   | 2523/4096 [00:06<00:03, 407.59it/s]
Adding requests:  63%|██████▎   | 2566/4096 [00:06<00:03, 411.62it/s]
Adding requests:  64%|██████▎   | 2608/4096 [00:06<00:03, 413.26it/s]
Adding requests:  65%|██████▍   | 2650/4096 [00:06<00:03, 411.86it/s]
Adding requests:  66%|██████▌   | 2692/4096 [00:06<00:03, 408.91it/s]
Adding requests:  67%|██████▋   | 2733/4096 [00:06<00:03, 405.35it/s]
Adding requests:  68%|██████▊   | 2775/4096 [00:06<00:03, 407.48it/s]
Adding requests:  69%|██████▉   | 2817/4096 [00:06<00:03, 409.41it/s]
Adding requests:  70%|██████▉   | 2859/4096 [00:06<00:03, 410.48it/s]
Adding requests:  71%|███████   | 2902/4096 [00:07<00:02, 413.48it/s]
Adding requests:  72%|███████▏  | 2944/4096 [00:07<00:02, 412.14it/s]
Adding requests:  73%|███████▎  | 2986/4096 [00:07<00:02, 412.93it/s]
Adding requests:  74%|███████▍  | 3028/4096 [00:07<00:02, 412.15it/s]
Adding requests:  75%|███████▍  | 3070/4096 [00:07<00:02, 410.50it/s]
Adding requests:  76%|███████▌  | 3112/4096 [00:07<00:02, 410.97it/s]
Adding requests:  77%|███████▋  | 3154/4096 [00:07<00:02, 412.06it/s]
Adding requests:  78%|███████▊  | 3196/4096 [00:07<00:02, 410.36it/s]
Adding requests:  79%|███████▉  | 3238/4096 [00:07<00:02, 412.87it/s]
Adding requests:  80%|████████  | 3280/4096 [00:08<00:01, 410.32it/s]
Adding requests:  81%|████████  | 3322/4096 [00:08<00:01, 411.27it/s]
Adding requests:  82%|████████▏ | 3366/4096 [00:08<00:01, 417.80it/s]
Adding requests:  83%|████████▎ | 3408/4096 [00:08<00:01, 412.89it/s]
Adding requests:  84%|████████▍ | 3450/4096 [00:08<00:01, 410.31it/s]
Adding requests:  85%|████████▌ | 3492/4096 [00:08<00:01, 407.21it/s]
Adding requests:  86%|████████▋ | 3534/4096 [00:08<00:01, 408.01it/s]
Adding requests:  87%|████████▋ | 3576/4096 [00:08<00:01, 408.99it/s]
Adding requests:  88%|████████▊ | 3617/4096 [00:08<00:01, 400.60it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:08<00:01, 401.34it/s]
Adding requests:  90%|█████████ | 3701/4096 [00:09<00:00, 407.74it/s]
Adding requests:  91%|█████████▏| 3742/4096 [00:09<00:00, 392.54it/s]
Adding requests:  92%|█████████▏| 3784/4096 [00:09<00:00, 397.66it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:09<00:00, 399.83it/s]
Adding requests:  94%|█████████▍| 3866/4096 [00:09<00:00, 402.36it/s]
Adding requests:  95%|█████████▌| 3908/4096 [00:09<00:00, 405.61it/s]
Adding requests:  96%|█████████▋| 3949/4096 [00:09<00:00, 406.84it/s]
Adding requests:  97%|█████████▋| 3990/4096 [00:09<00:00, 404.64it/s]
Adding requests:  98%|█████████▊| 4032/4096 [00:09<00:00, 407.63it/s]
Adding requests:  99%|█████████▉| 4073/4096 [00:09<00:00, 403.86it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 408.36it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:00<00:02, 1382.03it/s, est. speed input: 1415342.57 toks/s, output: 1382.12 toks/s]
Processed prompts:  21%|██        | 845/4096 [00:02<00:11, 274.98it/s, est. speed input: 352323.94 toks/s, output: 344.07 toks/s]   
Processed prompts:  22%|██▏       | 907/4096 [00:03<00:16, 197.50it/s, est. speed input: 272985.92 toks/s, output: 266.59 toks/s]
Processed prompts:  23%|██▎       | 944/4096 [00:06<00:36, 86.65it/s, est. speed input: 157942.74 toks/s, output: 154.24 toks/s] 
Processed prompts:  24%|██▎       | 967/4096 [00:06<00:38, 82.28it/s, est. speed input: 150779.41 toks/s, output: 147.25 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:07<00:39, 78.65it/s, est. speed input: 144848.76 toks/s, output: 141.45 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:07<00:39, 77.00it/s, est. speed input: 140320.07 toks/s, output: 137.03 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:07<00:40, 75.46it/s, est. speed input: 136314.53 toks/s, output: 133.12 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:08<00:40, 74.16it/s, est. speed input: 132770.09 toks/s, output: 129.66 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:08<00:40, 73.01it/s, est. speed input: 129565.30 toks/s, output: 126.53 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:09<00:40, 72.21it/s, est. speed input: 126718.91 toks/s, output: 123.75 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:09<00:41, 70.29it/s, est. speed input: 123743.64 toks/s, output: 120.84 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:10<00:41, 68.97it/s, est. speed input: 121071.22 toks/s, output: 118.23 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:10<00:41, 68.02it/s, est. speed input: 118642.50 toks/s, output: 115.86 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:11<00:41, 67.22it/s, est. speed input: 116394.03 toks/s, output: 113.67 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:11<00:41, 66.68it/s, est. speed input: 114337.71 toks/s, output: 111.66 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:12<00:41, 66.38it/s, est. speed input: 112466.52 toks/s, output: 109.83 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:12<00:41, 66.17it/s, est. speed input: 110737.82 toks/s, output: 108.14 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:13<00:40, 66.09it/s, est. speed input: 109149.46 toks/s, output: 106.59 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:13<00:39, 67.65it/s, est. speed input: 107979.63 toks/s, output: 105.45 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:14<00:38, 68.79it/s, est. speed input: 106883.79 toks/s, output: 104.38 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:14<00:37, 69.60it/s, est. speed input: 105853.71 toks/s, output: 103.37 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:15<00:36, 70.19it/s, est. speed input: 104886.27 toks/s, output: 102.43 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:15<00:35, 70.61it/s, est. speed input: 103974.63 toks/s, output: 101.54 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:15<00:35, 70.91it/s, est. speed input: 103113.57 toks/s, output: 100.70 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:16<00:34, 71.11it/s, est. speed input: 102298.41 toks/s, output: 99.90 toks/s] 
Processed prompts:  41%|████      | 1666/4096 [00:16<00:34, 71.25it/s, est. speed input: 101526.77 toks/s, output: 99.15 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:17<00:33, 71.34it/s, est. speed input: 100794.30 toks/s, output: 98.43 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:17<00:33, 71.39it/s, est. speed input: 100096.67 toks/s, output: 97.75 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:18<00:32, 71.45it/s, est. speed input: 99437.07 toks/s, output: 97.11 toks/s] 
Processed prompts:  44%|████▍     | 1794/4096 [00:18<00:32, 71.47it/s, est. speed input: 98806.99 toks/s, output: 96.49 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:19<00:31, 71.49it/s, est. speed input: 98205.92 toks/s, output: 95.90 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:19<00:31, 72.12it/s, est. speed input: 97697.32 toks/s, output: 95.41 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:19<00:30, 71.94it/s, est. speed input: 97147.98 toks/s, output: 94.87 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:20<00:30, 71.83it/s, est. speed input: 96624.75 toks/s, output: 94.36 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:20<00:29, 71.82it/s, est. speed input: 96130.12 toks/s, output: 93.88 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:21<00:29, 71.76it/s, est. speed input: 95651.09 toks/s, output: 93.41 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:21<00:29, 71.60it/s, est. speed input: 95181.06 toks/s, output: 92.95 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:22<00:28, 71.59it/s, est. speed input: 94738.90 toks/s, output: 92.52 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:22<00:28, 71.56it/s, est. speed input: 94312.80 toks/s, output: 92.10 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:23<00:27, 71.55it/s, est. speed input: 93903.62 toks/s, output: 91.70 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:23<00:27, 71.55it/s, est. speed input: 93510.79 toks/s, output: 91.32 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:23<00:26, 71.52it/s, est. speed input: 93130.43 toks/s, output: 90.95 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:24<00:26, 71.50it/s, est. speed input: 92763.69 toks/s, output: 90.59 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:24<00:25, 71.51it/s, est. speed input: 92412.28 toks/s, output: 90.25 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:25<00:25, 72.15it/s, est. speed input: 92120.91 toks/s, output: 89.96 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:25<00:24, 71.96it/s, est. speed input: 91792.97 toks/s, output: 89.64 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:26<00:24, 71.83it/s, est. speed input: 91475.93 toks/s, output: 89.33 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:26<00:24, 71.74it/s, est. speed input: 91169.67 toks/s, output: 89.03 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:27<00:23, 71.68it/s, est. speed input: 90873.59 toks/s, output: 88.74 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:27<00:23, 71.62it/s, est. speed input: 90585.81 toks/s, output: 88.46 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:27<00:22, 71.59it/s, est. speed input: 90308.45 toks/s, output: 88.19 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:28<00:22, 72.19it/s, est. speed input: 90080.25 toks/s, output: 87.97 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:28<00:21, 71.99it/s, est. speed input: 89818.85 toks/s, output: 87.71 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:29<00:21, 71.84it/s, est. speed input: 89565.08 toks/s, output: 87.47 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:29<00:20, 71.74it/s, est. speed input: 89319.34 toks/s, output: 87.23 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:30<00:20, 71.67it/s, est. speed input: 89080.56 toks/s, output: 86.99 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:30<00:20, 71.62it/s, est. speed input: 88848.74 toks/s, output: 86.77 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:31<00:19, 71.56it/s, est. speed input: 88622.49 toks/s, output: 86.55 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:31<00:19, 71.54it/s, est. speed input: 88403.81 toks/s, output: 86.33 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:31<00:18, 71.54it/s, est. speed input: 88191.79 toks/s, output: 86.12 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:32<00:18, 71.53it/s, est. speed input: 87984.98 toks/s, output: 85.92 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:32<00:17, 71.53it/s, est. speed input: 87784.66 toks/s, output: 85.73 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:33<00:17, 71.52it/s, est. speed input: 87589.05 toks/s, output: 85.54 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:33<00:16, 71.52it/s, est. speed input: 87398.69 toks/s, output: 85.35 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:34<00:16, 71.52it/s, est. speed input: 87213.47 toks/s, output: 85.17 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:34<00:16, 71.51it/s, est. speed input: 87032.78 toks/s, output: 84.99 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:35<00:15, 70.34it/s, est. speed input: 86795.41 toks/s, output: 84.76 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:35<00:15, 68.82it/s, est. speed input: 86525.14 toks/s, output: 84.50 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:36<00:15, 67.78it/s, est. speed input: 86261.56 toks/s, output: 84.24 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:36<00:15, 67.08it/s, est. speed input: 86005.54 toks/s, output: 83.99 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:37<00:14, 66.60it/s, est. speed input: 85755.82 toks/s, output: 83.75 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:37<00:14, 67.47it/s, est. speed input: 85578.76 toks/s, output: 83.57 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:37<00:13, 68.62it/s, est. speed input: 85432.69 toks/s, output: 83.43 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:38<00:12, 69.47it/s, est. speed input: 85290.90 toks/s, output: 83.29 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:41<00:30, 28.20it/s, est. speed input: 80474.59 toks/s, output: 78.59 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:41<00:24, 34.46it/s, est. speed input: 80396.56 toks/s, output: 78.51 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:42<00:19, 40.52it/s, est. speed input: 80285.80 toks/s, output: 78.40 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:42<00:16, 45.77it/s, est. speed input: 80135.60 toks/s, output: 78.26 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:43<00:14, 50.33it/s, est. speed input: 79987.97 toks/s, output: 78.11 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:43<00:12, 54.10it/s, est. speed input: 79843.79 toks/s, output: 77.97 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:44<00:11, 57.08it/s, est. speed input: 79702.36 toks/s, output: 77.83 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:44<00:10, 59.38it/s, est. speed input: 79564.00 toks/s, output: 77.70 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:44<00:09, 61.10it/s, est. speed input: 79428.82 toks/s, output: 77.57 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:45<00:09, 62.41it/s, est. speed input: 79298.41 toks/s, output: 77.44 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:45<00:08, 63.23it/s, est. speed input: 79165.32 toks/s, output: 77.31 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:46<00:07, 63.90it/s, est. speed input: 79038.42 toks/s, output: 77.19 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:46<00:07, 64.40it/s, est. speed input: 78914.96 toks/s, output: 77.07 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:47<00:06, 64.73it/s, est. speed input: 78793.27 toks/s, output: 76.95 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:47<00:06, 66.40it/s, est. speed input: 78732.41 toks/s, output: 76.89 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:48<00:05, 68.42it/s, est. speed input: 78702.30 toks/s, output: 76.86 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:48<00:05, 69.31it/s, est. speed input: 78651.94 toks/s, output: 76.81 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:49<00:04, 69.97it/s, est. speed input: 78603.19 toks/s, output: 76.76 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:49<00:04, 70.42it/s, est. speed input: 78554.62 toks/s, output: 76.71 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:50<00:03, 70.74it/s, est. speed input: 78506.89 toks/s, output: 76.67 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:50<00:03, 70.97it/s, est. speed input: 78460.14 toks/s, output: 76.62 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:51<00:02, 71.13it/s, est. speed input: 78414.09 toks/s, output: 76.58 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:51<00:02, 71.23it/s, est. speed input: 78368.44 toks/s, output: 76.53 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:51<00:01, 71.30it/s, est. speed input: 78323.91 toks/s, output: 76.49 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:52<00:01, 71.36it/s, est. speed input: 78280.12 toks/s, output: 76.45 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:52<00:00, 72.03it/s, est. speed input: 78256.43 toks/s, output: 76.42 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:53<00:00, 72.73it/s, est. speed input: 78239.91 toks/s, output: 76.41 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:53<00:00, 72.73it/s, est. speed input: 78816.32 toks/s, output: 76.97 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:53<00:00, 76.97it/s, est. speed input: 78816.32 toks/s, output: 76.97 toks/s]
[rank0]:[W128 11:17:47.602806581 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 128.6s

测试结果:
  Requests/s:   69.74
  Tokens/s:     71486.47
  Total Reqs:   4096
  Elapsed:      58.73s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     71416.72


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,16.8477,8642.8955,7.5975
1024,1024,1,128,128,17.3271,17760.2494,7.3873
2048,1024,2,256,128,34.2721,35128.9511,7.4696
4096,1024,4,512,128,65.8633,67509.8330,7.7737
8192,1024,8,1024,128,68.5848,70299.4358,14.9304
16384,1024,16,2048,128,67.2868,68968.9825,30.4369
32768,1024,32,4096,128,69.7429,71486.4664,58.7300

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:17:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:17:57 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=13610) WARNING 01-28 11:18:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=13610) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=13610) WARNING 01-28 11:18:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.37 requests/s, 8910.05 total tokens/s, 17.37 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:17:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:17:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:17:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:17:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:17:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:17:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:17:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:17:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:17:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:17:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:17:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:17:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:17:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:17:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:18:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:18:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:18:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:18:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:18:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:18:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:18:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:18:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:18:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=13610) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=13610) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.86s/it]
(EngineCore_DP0 pid=13610) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.86s/it]
(EngineCore_DP0 pid=13610) 
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=13610) [2026-01-28 11:18:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=13610) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.20it/s]
(EngineCore_DP0 pid=13610) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  55%|█████▍    | 70/128 [00:00<00:00, 698.31it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 701.75it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 37.24it/s, est. speed input: 19074.50 toks/s, output: 37.25 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 22.56it/s, est. speed input: 12279.93 toks/s, output: 23.98 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 20.46it/s, est. speed input: 11220.28 toks/s, output: 21.91 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:05, 19.37it/s, est. speed input: 10672.37 toks/s, output: 20.84 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:05, 18.64it/s, est. speed input: 10304.21 toks/s, output: 20.12 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 18.38it/s, est. speed input: 10152.89 toks/s, output: 19.83 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 18.23it/s, est. speed input: 10044.42 toks/s, output: 19.62 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 18.17it/s, est. speed input: 9967.67 toks/s, output: 19.47 toks/s] 
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 18.10it/s, est. speed input: 9898.40 toks/s, output: 19.33 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.98it/s, est. speed input: 9830.40 toks/s, output: 19.20 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.94it/s, est. speed input: 9779.76 toks/s, output: 19.10 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.93it/s, est. speed input: 9738.14 toks/s, output: 19.02 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.90it/s, est. speed input: 9698.96 toks/s, output: 18.94 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.89it/s, est. speed input: 9665.14 toks/s, output: 18.88 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:05, 17.89it/s, est. speed input: 9636.38 toks/s, output: 18.82 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:04, 17.84it/s, est. speed input: 9606.02 toks/s, output: 18.76 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 17.77it/s, est. speed input: 9575.82 toks/s, output: 18.70 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.83it/s, est. speed input: 9557.16 toks/s, output: 18.67 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.83it/s, est. speed input: 9537.54 toks/s, output: 18.63 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.87it/s, est. speed input: 9522.16 toks/s, output: 18.60 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.90it/s, est. speed input: 9508.73 toks/s, output: 18.57 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 17.87it/s, est. speed input: 9492.44 toks/s, output: 18.54 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 17.77it/s, est. speed input: 9472.59 toks/s, output: 18.50 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:04, 17.77it/s, est. speed input: 9458.09 toks/s, output: 18.47 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:03, 17.75it/s, est. speed input: 9443.80 toks/s, output: 18.44 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.63it/s, est. speed input: 9423.64 toks/s, output: 18.41 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.47it/s, est. speed input: 9400.36 toks/s, output: 18.36 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.43it/s, est. speed input: 9382.88 toks/s, output: 18.33 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 17.37it/s, est. speed input: 9364.64 toks/s, output: 18.29 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 17.41it/s, est. speed input: 9352.18 toks/s, output: 18.27 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 17.44it/s, est. speed input: 9340.26 toks/s, output: 18.24 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:03, 17.46it/s, est. speed input: 9329.51 toks/s, output: 18.22 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.45it/s, est. speed input: 9317.63 toks/s, output: 18.20 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.39it/s, est. speed input: 9303.91 toks/s, output: 18.17 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.31it/s, est. speed input: 9289.51 toks/s, output: 18.14 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.34it/s, est. speed input: 9279.64 toks/s, output: 18.12 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.34it/s, est. speed input: 9269.07 toks/s, output: 18.10 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.36it/s, est. speed input: 9260.02 toks/s, output: 18.09 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 17.40it/s, est. speed input: 9252.79 toks/s, output: 18.07 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:02, 17.37it/s, est. speed input: 9243.43 toks/s, output: 18.05 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:02, 17.36it/s, est. speed input: 9234.66 toks/s, output: 18.04 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.37it/s, est. speed input: 9227.11 toks/s, output: 18.02 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.33it/s, est. speed input: 9218.10 toks/s, output: 18.00 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.28it/s, est. speed input: 9208.85 toks/s, output: 17.99 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.30it/s, est. speed input: 9201.58 toks/s, output: 17.97 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 17.33it/s, est. speed input: 9195.48 toks/s, output: 17.96 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.39it/s, est. speed input: 9191.11 toks/s, output: 17.95 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 17.39it/s, est. speed input: 9185.32 toks/s, output: 17.94 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 17.40it/s, est. speed input: 9180.00 toks/s, output: 17.93 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 17.39it/s, est. speed input: 9174.38 toks/s, output: 17.92 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 17.31it/s, est. speed input: 9166.82 toks/s, output: 17.90 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.28it/s, est. speed input: 9160.24 toks/s, output: 17.89 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.19it/s, est. speed input: 9151.44 toks/s, output: 17.87 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.19it/s, est. speed input: 9145.29 toks/s, output: 17.86 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 17.27it/s, est. speed input: 9141.53 toks/s, output: 17.85 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.32it/s, est. speed input: 9137.80 toks/s, output: 17.85 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 17.31it/s, est. speed input: 9132.96 toks/s, output: 17.84 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:06<00:00, 17.28it/s, est. speed input: 9127.66 toks/s, output: 17.83 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 17.29it/s, est. speed input: 9123.25 toks/s, output: 17.82 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.37it/s, est. speed input: 9121.18 toks/s, output: 17.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.37it/s, est. speed input: 9120.27 toks/s, output: 17.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.81it/s, est. speed input: 9120.27 toks/s, output: 17.81 toks/s]
[rank0]:[W128 11:18:37.565609216 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.2s

测试结果:
  Requests/s:   17.37
  Tokens/s:     8910.05
  Total Reqs:   128
  Elapsed:      7.37s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8892.68

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:18:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:18:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=14147) WARNING 01-28 11:18:56 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=14147) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=14147) WARNING 01-28 11:19:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.61 requests/s, 17024.56 total tokens/s, 16.61 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:18:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:18:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:18:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:18:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:18:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:18:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:18:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:18:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:18:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:18:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:18:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:18:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:18:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:18:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:18:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:18:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:18:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:18:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:18:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:57] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=14147) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=14147) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.61it/s]
(EngineCore_DP0 pid=14147) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.61it/s]
(EngineCore_DP0 pid=14147) 
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=14147) [2026-01-28 11:18:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=14147) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.95it/s]
(EngineCore_DP0 pid=14147) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 354.18it/s]
Adding requests:  59%|█████▉    | 76/128 [00:00<00:00, 379.36it/s]
Adding requests:  91%|█████████ | 116/128 [00:00<00:00, 385.64it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 382.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 46.81it/s, est. speed input: 47940.48 toks/s, output: 46.81 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 25.06it/s, est. speed input: 27937.46 toks/s, output: 27.28 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:05, 21.49it/s, est. speed input: 24564.09 toks/s, output: 23.99 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:05, 19.56it/s, est. speed input: 22735.62 toks/s, output: 22.20 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:05, 18.59it/s, est. speed input: 21707.43 toks/s, output: 21.20 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 18.10it/s, est. speed input: 21193.80 toks/s, output: 20.70 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.73it/s, est. speed input: 20791.83 toks/s, output: 20.30 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.45it/s, est. speed input: 20461.05 toks/s, output: 19.98 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.19it/s, est. speed input: 20168.80 toks/s, output: 19.70 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.99it/s, est. speed input: 19919.14 toks/s, output: 19.45 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.84it/s, est. speed input: 19703.89 toks/s, output: 19.24 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 16.74it/s, est. speed input: 19517.97 toks/s, output: 19.06 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:05, 16.72it/s, est. speed input: 19368.40 toks/s, output: 18.91 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.65it/s, est. speed input: 19222.33 toks/s, output: 18.77 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.55it/s, est. speed input: 19081.80 toks/s, output: 18.63 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.55it/s, est. speed input: 18971.35 toks/s, output: 18.53 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.51it/s, est. speed input: 18863.23 toks/s, output: 18.42 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.45it/s, est. speed input: 18760.16 toks/s, output: 18.32 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 16.37it/s, est. speed input: 18659.83 toks/s, output: 18.22 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 16.38it/s, est. speed input: 18578.97 toks/s, output: 18.14 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 16.44it/s, est. speed input: 18512.79 toks/s, output: 18.08 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.23it/s, est. speed input: 18413.97 toks/s, output: 17.98 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.06it/s, est. speed input: 18320.11 toks/s, output: 17.89 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.10it/s, est. speed input: 18254.49 toks/s, output: 17.83 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.14it/s, est. speed input: 18196.35 toks/s, output: 17.77 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:04, 16.13it/s, est. speed input: 18137.02 toks/s, output: 17.71 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.18it/s, est. speed input: 18088.87 toks/s, output: 17.66 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 16.19it/s, est. speed input: 18041.08 toks/s, output: 17.62 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 16.17it/s, est. speed input: 17992.00 toks/s, output: 17.57 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.19it/s, est. speed input: 17950.93 toks/s, output: 17.53 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.16it/s, est. speed input: 17906.69 toks/s, output: 17.49 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.13it/s, est. speed input: 17864.76 toks/s, output: 17.45 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.11it/s, est. speed input: 17824.76 toks/s, output: 17.41 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:03, 16.10it/s, est. speed input: 17787.60 toks/s, output: 17.37 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.13it/s, est. speed input: 17755.36 toks/s, output: 17.34 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 16.19it/s, est. speed input: 17729.06 toks/s, output: 17.31 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 16.55it/s, est. speed input: 17732.66 toks/s, output: 17.32 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.83it/s, est. speed input: 17737.51 toks/s, output: 17.32 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 17.03it/s, est. speed input: 17742.20 toks/s, output: 17.33 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.20it/s, est. speed input: 17748.48 toks/s, output: 17.33 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.26it/s, est. speed input: 17749.88 toks/s, output: 17.33 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.28it/s, est. speed input: 17750.05 toks/s, output: 17.33 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.29it/s, est. speed input: 17749.68 toks/s, output: 17.33 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 17.29it/s, est. speed input: 17748.80 toks/s, output: 17.33 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.37it/s, est. speed input: 17753.23 toks/s, output: 17.34 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 17.42it/s, est. speed input: 17757.30 toks/s, output: 17.34 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 17.45it/s, est. speed input: 17760.68 toks/s, output: 17.34 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 17.47it/s, est. speed input: 17764.17 toks/s, output: 17.35 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 17.39it/s, est. speed input: 17761.41 toks/s, output: 17.35 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.42it/s, est. speed input: 17763.97 toks/s, output: 17.35 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.42it/s, est. speed input: 17765.44 toks/s, output: 17.35 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.46it/s, est. speed input: 17768.88 toks/s, output: 17.35 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 17.44it/s, est. speed input: 17769.86 toks/s, output: 17.35 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.40it/s, est. speed input: 17769.06 toks/s, output: 17.35 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 17.42it/s, est. speed input: 17770.72 toks/s, output: 17.35 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 17.46it/s, est. speed input: 17773.98 toks/s, output: 17.36 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 17.47it/s, est. speed input: 17776.49 toks/s, output: 17.36 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.51it/s, est. speed input: 17780.30 toks/s, output: 17.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.51it/s, est. speed input: 17783.32 toks/s, output: 17.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.37it/s, est. speed input: 17783.32 toks/s, output: 17.37 toks/s]
[rank0]:[W128 11:19:27.450498808 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.5s

测试结果:
  Requests/s:   16.61
  Tokens/s:     17024.56
  Total Reqs:   128
  Elapsed:      7.71s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     17007.95

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:19:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:19:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=14745) WARNING 01-28 11:19:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=14745) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=14745) WARNING 01-28 11:19:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.81 requests/s, 35684.14 total tokens/s, 34.81 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 11:19:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:19:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:19:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:19:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:19:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:19:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:19:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:19:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:19:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:19:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:19:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:19:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:19:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:19:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:19:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:19:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:19:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:19:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:19:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=14745) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=14745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.69it/s]
(EngineCore_DP0 pid=14745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.69it/s]
(EngineCore_DP0 pid=14745) 
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=14745) [2026-01-28 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=14745) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.07it/s]
(EngineCore_DP0 pid=14745) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.22it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  15%|█▌        | 39/256 [00:00<00:00, 381.90it/s]
Adding requests:  31%|███       | 79/256 [00:00<00:00, 389.30it/s]
Adding requests:  46%|████▌     | 118/256 [00:00<00:00, 389.54it/s]
Adding requests:  62%|██████▏   | 158/256 [00:00<00:00, 391.59it/s]
Adding requests:  77%|███████▋  | 198/256 [00:00<00:00, 379.75it/s]
Adding requests:  94%|█████████▍| 240/256 [00:00<00:00, 390.12it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 389.07it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 168.15it/s, est. speed input: 172218.91 toks/s, output: 168.16 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:03, 56.19it/s, est. speed input: 64123.51 toks/s, output: 62.62 toks/s]   
Processed prompts:  18%|█▊        | 45/256 [00:00<00:04, 47.46it/s, est. speed input: 55021.50 toks/s, output: 53.73 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:04, 42.24it/s, est. speed input: 50277.33 toks/s, output: 49.10 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:04, 40.35it/s, est. speed input: 48290.88 toks/s, output: 47.16 toks/s]
Processed prompts:  25%|██▍       | 63/256 [00:01<00:04, 41.13it/s, est. speed input: 48034.75 toks/s, output: 46.91 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:04, 38.00it/s, est. speed input: 46205.78 toks/s, output: 45.12 toks/s]
Processed prompts:  29%|██▊       | 73/256 [00:01<00:04, 39.58it/s, est. speed input: 46206.50 toks/s, output: 45.12 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:04, 36.62it/s, est. speed input: 44804.13 toks/s, output: 43.75 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 36.63it/s, est. speed input: 44386.02 toks/s, output: 43.35 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:04, 36.64it/s, est. speed input: 44012.64 toks/s, output: 42.98 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 36.53it/s, est. speed input: 43652.19 toks/s, output: 42.63 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 36.39it/s, est. speed input: 43314.41 toks/s, output: 42.30 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 36.08it/s, est. speed input: 42967.03 toks/s, output: 41.96 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 36.08it/s, est. speed input: 42693.62 toks/s, output: 41.69 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 36.22it/s, est. speed input: 42469.30 toks/s, output: 41.47 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:04, 36.37it/s, est. speed input: 42270.49 toks/s, output: 41.28 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 36.43it/s, est. speed input: 42080.70 toks/s, output: 41.09 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 36.49it/s, est. speed input: 41907.20 toks/s, output: 40.92 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 36.54it/s, est. speed input: 41748.93 toks/s, output: 40.77 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 36.51it/s, est. speed input: 41591.29 toks/s, output: 40.62 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 36.35it/s, est. speed input: 41426.78 toks/s, output: 40.46 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 36.15it/s, est. speed input: 41262.56 toks/s, output: 40.30 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 36.12it/s, est. speed input: 41121.84 toks/s, output: 40.16 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 36.05it/s, est. speed input: 40984.95 toks/s, output: 40.02 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:03, 36.02it/s, est. speed input: 40857.84 toks/s, output: 39.90 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 36.10it/s, est. speed input: 40749.26 toks/s, output: 39.79 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 36.15it/s, est. speed input: 40646.92 toks/s, output: 39.69 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 36.23it/s, est. speed input: 40554.36 toks/s, output: 39.60 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 36.13it/s, est. speed input: 40451.08 toks/s, output: 39.50 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 36.23it/s, est. speed input: 40370.18 toks/s, output: 39.42 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 36.27it/s, est. speed input: 40290.66 toks/s, output: 39.35 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 36.20it/s, est. speed input: 40205.65 toks/s, output: 39.26 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 36.28it/s, est. speed input: 40136.70 toks/s, output: 39.20 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 36.35it/s, est. speed input: 40071.55 toks/s, output: 39.13 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 36.43it/s, est. speed input: 40012.33 toks/s, output: 39.07 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 36.49it/s, est. speed input: 39956.28 toks/s, output: 39.02 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 36.57it/s, est. speed input: 39905.70 toks/s, output: 38.97 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 36.46it/s, est. speed input: 39844.68 toks/s, output: 38.91 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 36.37it/s, est. speed input: 39784.24 toks/s, output: 38.85 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:05<00:01, 36.23it/s, est. speed input: 39720.86 toks/s, output: 38.79 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 36.33it/s, est. speed input: 39675.40 toks/s, output: 38.75 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 36.30it/s, est. speed input: 39623.81 toks/s, output: 38.69 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:01, 36.27it/s, est. speed input: 39574.04 toks/s, output: 38.65 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 36.01it/s, est. speed input: 39509.04 toks/s, output: 38.58 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 36.19it/s, est. speed input: 39471.25 toks/s, output: 38.55 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 36.10it/s, est. speed input: 39420.64 toks/s, output: 38.50 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:06<00:00, 36.00it/s, est. speed input: 39369.48 toks/s, output: 38.45 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:06<00:00, 36.06it/s, est. speed input: 39328.25 toks/s, output: 38.41 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:06<00:00, 36.00it/s, est. speed input: 39282.26 toks/s, output: 38.36 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 36.09it/s, est. speed input: 39246.09 toks/s, output: 38.33 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 36.16it/s, est. speed input: 39211.76 toks/s, output: 38.29 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 36.20it/s, est. speed input: 39177.47 toks/s, output: 38.26 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 36.20it/s, est. speed input: 39160.85 toks/s, output: 38.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 38.24it/s, est. speed input: 39160.85 toks/s, output: 38.24 toks/s]
[rank0]:[W128 11:20:15.395376694 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.3s

测试结果:
  Requests/s:   34.81
  Tokens/s:     35684.14
  Total Reqs:   256
  Elapsed:      7.35s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     35649.32

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:20:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:20:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=15271) WARNING 01-28 11:20:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=15271) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=15271) WARNING 01-28 11:20:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.25 requests/s, 54579.34 total tokens/s, 53.25 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 11:20:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:20:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:20:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:20:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:20:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:20:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:20:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:20:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:20:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:20:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:20:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:20:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:20:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:20:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:20:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:20:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:20:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:20:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:20:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=15271) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=15271) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.62it/s]
(EngineCore_DP0 pid=15271) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.62it/s]
(EngineCore_DP0 pid=15271) 
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=15271) [2026-01-28 11:20:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=15271) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.38it/s]
(EngineCore_DP0 pid=15271) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.63it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.83it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 35/512 [00:00<00:01, 344.87it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:01, 360.88it/s]
Adding requests:  21%|██▏       | 110/512 [00:00<00:01, 364.32it/s]
Adding requests:  29%|██▊       | 147/512 [00:00<00:01, 363.87it/s]
Adding requests:  36%|███▌      | 184/512 [00:00<00:00, 361.51it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:00, 358.15it/s]
Adding requests:  51%|█████     | 259/512 [00:00<00:00, 363.50it/s]
Adding requests:  58%|█████▊    | 296/512 [00:00<00:00, 352.47it/s]
Adding requests:  65%|██████▍   | 332/512 [00:00<00:00, 352.22it/s]
Adding requests:  72%|███████▏  | 369/512 [00:01<00:00, 355.22it/s]
Adding requests:  79%|███████▉  | 405/512 [00:01<00:00, 352.78it/s]
Adding requests:  86%|████████▌ | 441/512 [00:01<00:00, 352.53it/s]
Adding requests:  93%|█████████▎| 478/512 [00:01<00:00, 357.09it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 357.14it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 496.58it/s, est. speed input: 508590.21 toks/s, output: 496.60 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:01<00:04, 98.29it/s, est. speed input: 116617.28 toks/s, output: 113.88 toks/s]
Processed prompts:  27%|██▋       | 140/512 [00:01<00:04, 81.07it/s, est. speed input: 98184.87 toks/s, output: 95.88 toks/s]  
Processed prompts:  30%|███       | 156/512 [00:01<00:04, 73.79it/s, est. speed input: 91037.01 toks/s, output: 88.90 toks/s]
Processed prompts:  33%|███▎      | 168/512 [00:04<00:15, 21.91it/s, est. speed input: 41978.66 toks/s, output: 40.99 toks/s]
Processed prompts:  34%|███▍      | 176/512 [00:04<00:14, 23.97it/s, est. speed input: 42449.10 toks/s, output: 41.45 toks/s]
Processed prompts:  36%|███▌      | 183/512 [00:04<00:12, 25.93it/s, est. speed input: 42659.52 toks/s, output: 41.66 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:11, 28.21it/s, est. speed input: 42850.38 toks/s, output: 41.84 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:09, 31.62it/s, est. speed input: 43240.87 toks/s, output: 42.23 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:08, 35.18it/s, est. speed input: 43609.84 toks/s, output: 42.59 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 38.68it/s, est. speed input: 43962.56 toks/s, output: 42.93 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:06, 41.91it/s, est. speed input: 44292.10 toks/s, output: 43.25 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:06, 44.73it/s, est. speed input: 44604.63 toks/s, output: 43.56 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:05, 47.02it/s, est. speed input: 44895.74 toks/s, output: 43.84 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:05, 48.87it/s, est. speed input: 45174.31 toks/s, output: 44.12 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:05, 50.32it/s, est. speed input: 45440.55 toks/s, output: 44.38 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:04, 51.41it/s, est. speed input: 45694.03 toks/s, output: 44.62 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:04, 52.20it/s, est. speed input: 45935.46 toks/s, output: 44.86 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:04, 52.76it/s, est. speed input: 46161.66 toks/s, output: 45.08 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:04, 53.18it/s, est. speed input: 46380.18 toks/s, output: 45.29 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:04, 53.49it/s, est. speed input: 46589.66 toks/s, output: 45.50 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:03, 53.69it/s, est. speed input: 46787.61 toks/s, output: 45.69 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:03, 53.85it/s, est. speed input: 46978.30 toks/s, output: 45.88 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:06<00:03, 53.91it/s, est. speed input: 47157.69 toks/s, output: 46.05 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:03, 53.97it/s, est. speed input: 47330.49 toks/s, output: 46.22 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:03, 54.03it/s, est. speed input: 47497.32 toks/s, output: 46.38 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:03, 54.89it/s, est. speed input: 47705.19 toks/s, output: 46.59 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:02, 54.66it/s, est. speed input: 47857.71 toks/s, output: 46.74 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:02, 54.47it/s, est. speed input: 48002.80 toks/s, output: 46.88 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:07<00:02, 54.36it/s, est. speed input: 48143.07 toks/s, output: 47.01 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:07<00:02, 54.72it/s, est. speed input: 48302.35 toks/s, output: 47.17 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:02, 55.45it/s, est. speed input: 48480.63 toks/s, output: 47.34 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:02, 55.93it/s, est. speed input: 48651.02 toks/s, output: 47.51 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 56.29it/s, est. speed input: 48816.39 toks/s, output: 47.67 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:08<00:01, 56.59it/s, est. speed input: 48978.67 toks/s, output: 47.83 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:08<00:01, 56.75it/s, est. speed input: 49133.18 toks/s, output: 47.98 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:08<00:01, 56.79it/s, est. speed input: 49279.57 toks/s, output: 48.12 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:08<00:01, 56.93it/s, est. speed input: 49425.95 toks/s, output: 48.27 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 56.99it/s, est. speed input: 49566.49 toks/s, output: 48.40 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:09<00:01, 57.02it/s, est. speed input: 49702.28 toks/s, output: 48.54 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:09<00:01, 57.06it/s, est. speed input: 49834.60 toks/s, output: 48.67 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:09<00:00, 57.02it/s, est. speed input: 49959.94 toks/s, output: 48.79 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:09<00:00, 57.07it/s, est. speed input: 50085.17 toks/s, output: 48.91 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:09<00:00, 57.03it/s, est. speed input: 50203.73 toks/s, output: 49.03 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:09<00:00, 57.03it/s, est. speed input: 50320.31 toks/s, output: 49.14 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:10<00:00, 57.05it/s, est. speed input: 50433.58 toks/s, output: 49.25 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:10<00:00, 57.09it/s, est. speed input: 50545.58 toks/s, output: 49.36 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:10<00:00, 58.15it/s, est. speed input: 50695.09 toks/s, output: 49.51 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 58.15it/s, est. speed input: 50892.99 toks/s, output: 49.70 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 49.70it/s, est. speed input: 50892.99 toks/s, output: 49.70 toks/s]
[rank0]:[W128 11:21:10.412359074 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.7s

测试结果:
  Requests/s:   53.25
  Tokens/s:     54579.34
  Total Reqs:   512
  Elapsed:      9.62s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     54526.09

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:21:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:21:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=15835) WARNING 01-28 11:21:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=15835) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=15835) WARNING 01-28 11:21:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.31 requests/s, 56696.32 total tokens/s, 55.31 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 11:21:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:21:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:21:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:21:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:21:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:21:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:21:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:21:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:21:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:21:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:21:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:21:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:21:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:21:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:21:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:21:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:21:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:21:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:21:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=15835) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=15835) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.64it/s]
(EngineCore_DP0 pid=15835) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.64it/s]
(EngineCore_DP0 pid=15835) 
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=15835) [2026-01-28 11:21:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=15835) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.45it/s]
(EngineCore_DP0 pid=15835) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.50it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.80it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 37/1024 [00:00<00:02, 362.20it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 370.30it/s]
Adding requests:  11%|█         | 114/1024 [00:00<00:02, 374.91it/s]
Adding requests:  15%|█▍        | 152/1024 [00:00<00:02, 372.69it/s]
Adding requests:  19%|█▊        | 191/1024 [00:00<00:02, 376.03it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:02, 379.30it/s]
Adding requests:  26%|██▋       | 269/1024 [00:00<00:01, 380.10it/s]
Adding requests:  30%|███       | 308/1024 [00:00<00:01, 376.40it/s]
Adding requests:  34%|███▍      | 347/1024 [00:00<00:01, 377.88it/s]
Adding requests:  38%|███▊      | 386/1024 [00:01<00:01, 380.78it/s]
Adding requests:  42%|████▏     | 426/1024 [00:01<00:01, 382.92it/s]
Adding requests:  45%|████▌     | 465/1024 [00:01<00:01, 380.52it/s]
Adding requests:  49%|████▉     | 504/1024 [00:01<00:01, 380.18it/s]
Adding requests:  53%|█████▎    | 543/1024 [00:01<00:01, 371.12it/s]
Adding requests:  57%|█████▋    | 582/1024 [00:01<00:01, 375.92it/s]
Adding requests:  61%|██████    | 620/1024 [00:01<00:01, 369.61it/s]
Adding requests:  64%|██████▍   | 659/1024 [00:01<00:00, 373.41it/s]
Adding requests:  68%|██████▊   | 697/1024 [00:01<00:00, 374.40it/s]
Adding requests:  72%|███████▏  | 737/1024 [00:01<00:00, 380.33it/s]
Adding requests:  76%|███████▌  | 776/1024 [00:02<00:00, 378.86it/s]
Adding requests:  79%|███████▉  | 814/1024 [00:02<00:00, 375.04it/s]
Adding requests:  83%|████████▎ | 853/1024 [00:02<00:00, 377.48it/s]
Adding requests:  87%|████████▋ | 894/1024 [00:02<00:00, 386.90it/s]
Adding requests:  91%|█████████▏| 935/1024 [00:02<00:00, 391.30it/s]
Adding requests:  95%|█████████▌| 975/1024 [00:02<00:00, 393.48it/s]
Adding requests:  99%|█████████▉| 1015/1024 [00:02<00:00, 394.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 380.60it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:00<00:01, 675.71it/s, est. speed input: 692008.11 toks/s, output: 675.73 toks/s]
Processed prompts:  21%|██        | 214/1024 [00:01<00:06, 131.39it/s, est. speed input: 161101.01 toks/s, output: 157.32 toks/s]
Processed prompts:  24%|██▍       | 246/1024 [00:01<00:07, 102.09it/s, est. speed input: 130257.72 toks/s, output: 127.20 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:02<00:08, 85.78it/s, est. speed input: 115215.05 toks/s, output: 112.51 toks/s] 
Processed prompts:  27%|██▋       | 280/1024 [00:02<00:08, 87.23it/s, est. speed input: 114342.23 toks/s, output: 111.66 toks/s]
Processed prompts:  29%|██▊       | 293/1024 [00:02<00:09, 76.40it/s, est. speed input: 107391.95 toks/s, output: 104.87 toks/s]
Processed prompts:  30%|██▉       | 303/1024 [00:02<00:09, 75.36it/s, est. speed input: 105636.47 toks/s, output: 103.16 toks/s]
Processed prompts:  30%|███       | 312/1024 [00:03<00:09, 73.08it/s, est. speed input: 103718.11 toks/s, output: 101.29 toks/s]
Processed prompts:  31%|███▏      | 320/1024 [00:03<00:10, 69.52it/s, est. speed input: 101652.53 toks/s, output: 99.27 toks/s] 
Processed prompts:  32%|███▏      | 328/1024 [00:03<00:10, 66.39it/s, est. speed input: 99755.43 toks/s, output: 97.42 toks/s] 
Processed prompts:  33%|███▎      | 335/1024 [00:03<00:11, 62.01it/s, est. speed input: 97707.92 toks/s, output: 95.42 toks/s]
Processed prompts:  33%|███▎      | 342/1024 [00:03<00:11, 58.57it/s, est. speed input: 95832.60 toks/s, output: 93.59 toks/s]
Processed prompts:  34%|███▍      | 348/1024 [00:03<00:12, 54.06it/s, est. speed input: 93835.61 toks/s, output: 91.64 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:03<00:13, 50.68it/s, est. speed input: 91987.68 toks/s, output: 89.83 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:04<00:12, 52.17it/s, est. speed input: 90771.00 toks/s, output: 88.64 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:04<00:12, 53.22it/s, est. speed input: 89630.23 toks/s, output: 87.53 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:04<00:11, 53.90it/s, est. speed input: 88549.86 toks/s, output: 86.47 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:04<00:11, 54.46it/s, est. speed input: 87552.91 toks/s, output: 85.50 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:04<00:11, 54.84it/s, est. speed input: 86614.24 toks/s, output: 84.58 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:04<00:11, 55.11it/s, est. speed input: 85731.41 toks/s, output: 83.72 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:04<00:11, 55.29it/s, est. speed input: 84898.37 toks/s, output: 82.91 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:10, 55.43it/s, est. speed input: 84114.54 toks/s, output: 82.14 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:10, 55.53it/s, est. speed input: 83373.57 toks/s, output: 81.42 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:05<00:10, 55.60it/s, est. speed input: 82673.00 toks/s, output: 80.74 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:05<00:10, 55.62it/s, est. speed input: 82003.92 toks/s, output: 80.08 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:05<00:10, 55.64it/s, est. speed input: 81370.40 toks/s, output: 79.46 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:05<00:10, 55.67it/s, est. speed input: 80769.69 toks/s, output: 78.88 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:05<00:10, 55.71it/s, est. speed input: 80200.76 toks/s, output: 78.32 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:09, 55.75it/s, est. speed input: 79659.78 toks/s, output: 77.79 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:09, 55.78it/s, est. speed input: 79143.52 toks/s, output: 77.29 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:06<00:09, 55.79it/s, est. speed input: 78649.03 toks/s, output: 76.81 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:06<00:09, 55.79it/s, est. speed input: 78176.49 toks/s, output: 76.34 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:06<00:09, 55.78it/s, est. speed input: 77722.95 toks/s, output: 75.90 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:06<00:09, 55.78it/s, est. speed input: 77289.14 toks/s, output: 75.48 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:06<00:08, 55.79it/s, est. speed input: 76874.08 toks/s, output: 75.07 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:08, 55.82it/s, est. speed input: 76477.53 toks/s, output: 74.68 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:07<00:08, 55.86it/s, est. speed input: 76099.07 toks/s, output: 74.32 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:07<00:08, 55.87it/s, est. speed input: 75733.33 toks/s, output: 73.96 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:07<00:08, 55.86it/s, est. speed input: 75379.97 toks/s, output: 73.61 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:07<00:08, 55.84it/s, est. speed input: 75038.86 toks/s, output: 73.28 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:07<00:08, 55.83it/s, est. speed input: 74710.10 toks/s, output: 72.96 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:07<00:07, 55.85it/s, est. speed input: 74396.04 toks/s, output: 72.65 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:07, 55.85it/s, est. speed input: 74092.00 toks/s, output: 72.36 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:08<00:07, 55.86it/s, est. speed input: 73798.86 toks/s, output: 72.07 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:08<00:07, 55.85it/s, est. speed input: 73514.39 toks/s, output: 71.79 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:08<00:07, 55.87it/s, est. speed input: 73241.73 toks/s, output: 71.52 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:08<00:07, 55.85it/s, est. speed input: 72975.80 toks/s, output: 71.27 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:08<00:07, 55.84it/s, est. speed input: 72718.39 toks/s, output: 71.01 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:08<00:06, 55.82it/s, est. speed input: 72468.57 toks/s, output: 70.77 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:06, 55.85it/s, est. speed input: 72229.60 toks/s, output: 70.54 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:09<00:06, 55.85it/s, est. speed input: 71996.46 toks/s, output: 70.31 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:09<00:06, 55.88it/s, est. speed input: 71772.35 toks/s, output: 70.09 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:09<00:06, 55.86it/s, est. speed input: 71552.95 toks/s, output: 69.88 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:09<00:06, 55.81it/s, est. speed input: 71337.30 toks/s, output: 69.67 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:09<00:06, 55.83it/s, est. speed input: 71131.52 toks/s, output: 69.46 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:09<00:05, 55.85it/s, est. speed input: 70931.73 toks/s, output: 69.27 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:10<00:05, 55.85it/s, est. speed input: 70736.77 toks/s, output: 69.08 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:10<00:05, 55.84it/s, est. speed input: 70546.72 toks/s, output: 68.89 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:10<00:05, 55.79it/s, est. speed input: 70359.63 toks/s, output: 68.71 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:10<00:05, 55.76it/s, est. speed input: 70178.06 toks/s, output: 68.53 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:10<00:05, 55.73it/s, est. speed input: 70000.53 toks/s, output: 68.36 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:10<00:05, 55.75it/s, est. speed input: 69829.87 toks/s, output: 68.19 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:10<00:04, 55.79it/s, est. speed input: 69665.68 toks/s, output: 68.03 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:11<00:04, 55.81it/s, est. speed input: 69504.55 toks/s, output: 67.88 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:11<00:04, 55.80it/s, est. speed input: 69346.52 toks/s, output: 67.72 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:11<00:04, 55.81it/s, est. speed input: 69193.62 toks/s, output: 67.57 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:11<00:04, 55.83it/s, est. speed input: 69044.93 toks/s, output: 67.43 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:11<00:04, 55.79it/s, est. speed input: 68897.48 toks/s, output: 67.28 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:11<00:04, 55.74it/s, est. speed input: 68751.94 toks/s, output: 67.14 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:11<00:03, 55.75it/s, est. speed input: 68612.77 toks/s, output: 67.00 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:17, 12.11it/s, est. speed input: 59942.39 toks/s, output: 58.54 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:13, 15.82it/s, est. speed input: 59914.00 toks/s, output: 58.51 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:14<00:09, 20.16it/s, est. speed input: 59886.61 toks/s, output: 58.48 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:14<00:07, 24.94it/s, est. speed input: 59858.87 toks/s, output: 58.46 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:14<00:06, 29.89it/s, est. speed input: 59831.16 toks/s, output: 58.43 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:14<00:05, 34.73it/s, est. speed input: 59804.67 toks/s, output: 58.40 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:04, 39.17it/s, est. speed input: 59778.84 toks/s, output: 58.38 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:03, 43.00it/s, est. speed input: 59752.61 toks/s, output: 58.35 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:03, 46.16it/s, est. speed input: 59726.63 toks/s, output: 58.33 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:15<00:02, 48.64it/s, est. speed input: 59700.21 toks/s, output: 58.30 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:15<00:02, 50.56it/s, est. speed input: 59675.16 toks/s, output: 58.28 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:15<00:02, 52.01it/s, est. speed input: 59650.77 toks/s, output: 58.25 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:15<00:02, 53.04it/s, est. speed input: 59625.85 toks/s, output: 58.23 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:15<00:02, 53.81it/s, est. speed input: 59602.13 toks/s, output: 58.21 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:01, 54.40it/s, est. speed input: 59580.12 toks/s, output: 58.18 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 54.77it/s, est. speed input: 59556.69 toks/s, output: 58.16 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:16<00:01, 55.03it/s, est. speed input: 59533.79 toks/s, output: 58.14 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:16<00:01, 55.23it/s, est. speed input: 59511.80 toks/s, output: 58.12 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:16<00:01, 55.41it/s, est. speed input: 59491.26 toks/s, output: 58.10 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:16<00:01, 55.49it/s, est. speed input: 59469.97 toks/s, output: 58.08 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:00, 55.47it/s, est. speed input: 59446.48 toks/s, output: 58.05 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 55.36it/s, est. speed input: 59420.48 toks/s, output: 58.03 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 57.21it/s, est. speed input: 59451.60 toks/s, output: 58.06 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:17<00:00, 56.61it/s, est. speed input: 59427.42 toks/s, output: 58.03 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:17<00:00, 56.17it/s, est. speed input: 59402.90 toks/s, output: 58.01 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:17<00:00, 55.90it/s, est. speed input: 59379.60 toks/s, output: 57.99 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:17<00:00, 57.79it/s, est. speed input: 59414.88 toks/s, output: 58.02 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 57.79it/s, est. speed input: 59764.27 toks/s, output: 58.36 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 58.36it/s, est. speed input: 59764.27 toks/s, output: 58.36 toks/s]
[rank0]:[W128 11:22:16.959794254 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 65.9s

测试结果:
  Requests/s:   55.31
  Tokens/s:     56696.32
  Total Reqs:   1024
  Elapsed:      18.51s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     56641.01

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:22:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:22:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=16518) WARNING 01-28 11:22:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=16518) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=16518) WARNING 01-28 11:22:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.37 requests/s, 56756.52 total tokens/s, 55.37 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 11:22:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:22:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:22:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:22:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:22:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:22:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:22:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:22:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:22:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:22:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:22:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:22:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:22:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:22:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:22:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:22:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:22:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:22:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:22:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=16518) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=16518) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.38s/it]
(EngineCore_DP0 pid=16518) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.38s/it]
(EngineCore_DP0 pid=16518) 
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=16518) [2026-01-28 11:22:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=16518) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.56it/s]
(EngineCore_DP0 pid=16518) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.05it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 36/2048 [00:00<00:05, 357.94it/s]
Adding requests:   4%|▎         | 75/2048 [00:00<00:05, 372.59it/s]
Adding requests:   6%|▌         | 115/2048 [00:00<00:05, 381.67it/s]
Adding requests:   8%|▊         | 154/2048 [00:00<00:04, 382.15it/s]
Adding requests:   9%|▉         | 193/2048 [00:00<00:04, 381.09it/s]
Adding requests:  11%|█▏        | 234/2048 [00:00<00:04, 387.46it/s]
Adding requests:  13%|█▎        | 274/2048 [00:00<00:04, 391.29it/s]
Adding requests:  15%|█▌        | 314/2048 [00:00<00:04, 387.49it/s]
Adding requests:  17%|█▋        | 354/2048 [00:00<00:04, 390.43it/s]
Adding requests:  19%|█▉        | 394/2048 [00:01<00:04, 391.35it/s]
Adding requests:  21%|██        | 434/2048 [00:01<00:04, 388.92it/s]
Adding requests:  23%|██▎       | 474/2048 [00:01<00:04, 391.27it/s]
Adding requests:  25%|██▌       | 514/2048 [00:01<00:03, 390.83it/s]
Adding requests:  27%|██▋       | 554/2048 [00:01<00:03, 386.68it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:03, 392.13it/s]
Adding requests:  31%|███       | 636/2048 [00:01<00:03, 389.21it/s]
Adding requests:  33%|███▎      | 676/2048 [00:01<00:03, 391.07it/s]
Adding requests:  35%|███▍      | 716/2048 [00:01<00:03, 393.02it/s]
Adding requests:  37%|███▋      | 756/2048 [00:01<00:03, 386.17it/s]
Adding requests:  39%|███▉      | 795/2048 [00:02<00:03, 385.69it/s]
Adding requests:  41%|████      | 834/2048 [00:02<00:03, 383.07it/s]
Adding requests:  43%|████▎     | 873/2048 [00:02<00:03, 380.55it/s]
Adding requests:  45%|████▍     | 915/2048 [00:02<00:02, 391.13it/s]
Adding requests:  47%|████▋     | 956/2048 [00:02<00:02, 395.26it/s]
Adding requests:  49%|████▊     | 996/2048 [00:02<00:02, 395.91it/s]
Adding requests:  51%|█████     | 1038/2048 [00:02<00:02, 401.12it/s]
Adding requests:  53%|█████▎    | 1079/2048 [00:02<00:02, 401.18it/s]
Adding requests:  55%|█████▍    | 1120/2048 [00:02<00:02, 398.00it/s]
Adding requests:  57%|█████▋    | 1163/2048 [00:02<00:02, 404.69it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:03<00:02, 411.29it/s]
Adding requests:  61%|██████    | 1248/2048 [00:03<00:01, 405.07it/s]
Adding requests:  63%|██████▎   | 1289/2048 [00:03<00:01, 404.28it/s]
Adding requests:  65%|██████▍   | 1331/2048 [00:03<00:01, 406.29it/s]
Adding requests:  67%|██████▋   | 1373/2048 [00:03<00:01, 408.10it/s]
Adding requests:  69%|██████▉   | 1415/2048 [00:03<00:01, 410.57it/s]
Adding requests:  71%|███████   | 1457/2048 [00:03<00:01, 407.79it/s]
Adding requests:  73%|███████▎  | 1499/2048 [00:03<00:01, 410.68it/s]
Adding requests:  75%|███████▌  | 1541/2048 [00:03<00:01, 409.54it/s]
Adding requests:  77%|███████▋  | 1582/2048 [00:04<00:01, 404.36it/s]
Adding requests:  79%|███████▉  | 1624/2048 [00:04<00:01, 407.38it/s]
Adding requests:  81%|████████▏ | 1665/2048 [00:04<00:00, 407.62it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:04<00:00, 406.50it/s]
Adding requests:  85%|████████▌ | 1747/2048 [00:04<00:00, 406.74it/s]
Adding requests:  87%|████████▋ | 1788/2048 [00:04<00:00, 402.44it/s]
Adding requests:  89%|████████▉ | 1829/2048 [00:04<00:00, 400.94it/s]
Adding requests:  91%|█████████▏| 1870/2048 [00:04<00:00, 399.27it/s]
Adding requests:  93%|█████████▎| 1911/2048 [00:04<00:00, 400.61it/s]
Adding requests:  95%|█████████▌| 1952/2048 [00:04<00:00, 402.93it/s]
Adding requests:  97%|█████████▋| 1993/2048 [00:05<00:00, 403.36it/s]
Adding requests:  99%|█████████▉| 2034/2048 [00:05<00:00, 393.28it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 396.49it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:01, 1418.99it/s, est. speed input: 1453310.44 toks/s, output: 1419.11 toks/s]
Processed prompts:  20%|██        | 416/2048 [00:02<00:11, 137.44it/s, est. speed input: 171305.61 toks/s, output: 167.29 toks/s]   
Processed prompts:  23%|██▎       | 479/2048 [00:03<00:15, 103.91it/s, est. speed input: 134694.18 toks/s, output: 131.54 toks/s]
Processed prompts:  25%|██▌       | 516/2048 [00:04<00:17, 85.58it/s, est. speed input: 117370.42 toks/s, output: 114.62 toks/s] 
Processed prompts:  26%|██▋       | 540/2048 [00:04<00:17, 85.33it/s, est. speed input: 115463.32 toks/s, output: 112.76 toks/s]
Processed prompts:  27%|██▋       | 559/2048 [00:05<00:18, 82.40it/s, est. speed input: 112758.08 toks/s, output: 110.12 toks/s]
Processed prompts:  28%|██▊       | 574/2048 [00:05<00:19, 77.01it/s, est. speed input: 109578.92 toks/s, output: 107.01 toks/s]
Processed prompts:  29%|██▊       | 586/2048 [00:05<00:20, 69.86it/s, est. speed input: 106185.69 toks/s, output: 103.70 toks/s]
Processed prompts:  29%|██▉       | 596/2048 [00:05<00:23, 61.97it/s, est. speed input: 102773.52 toks/s, output: 100.36 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:06<00:24, 58.74it/s, est. speed input: 100329.49 toks/s, output: 97.98 toks/s] 
Processed prompts:  31%|███       | 626/2048 [00:08<01:03, 22.23it/s, est. speed input: 77199.50 toks/s, output: 75.39 toks/s] 
Processed prompts:  31%|███▏      | 642/2048 [00:08<00:53, 26.37it/s, est. speed input: 76522.32 toks/s, output: 74.73 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:08<00:45, 30.77it/s, est. speed input: 75887.89 toks/s, output: 74.11 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:09<00:39, 35.17it/s, est. speed input: 75295.02 toks/s, output: 73.53 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:09<00:34, 39.29it/s, est. speed input: 74735.40 toks/s, output: 72.98 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:09<00:31, 42.94it/s, est. speed input: 74208.73 toks/s, output: 72.47 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:10<00:28, 45.98it/s, est. speed input: 73708.23 toks/s, output: 71.98 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:10<00:27, 48.33it/s, est. speed input: 73220.26 toks/s, output: 71.50 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:10<00:25, 50.14it/s, est. speed input: 72758.79 toks/s, output: 71.05 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:10<00:24, 51.51it/s, est. speed input: 72321.56 toks/s, output: 70.63 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:11<00:24, 52.50it/s, est. speed input: 71905.11 toks/s, output: 70.22 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:11<00:23, 53.25it/s, est. speed input: 71513.89 toks/s, output: 69.84 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:11<00:22, 53.79it/s, est. speed input: 71141.70 toks/s, output: 69.47 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:12<00:22, 54.18it/s, est. speed input: 70787.98 toks/s, output: 69.13 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:12<00:22, 54.44it/s, est. speed input: 70448.93 toks/s, output: 68.80 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:12<00:21, 54.63it/s, est. speed input: 70126.49 toks/s, output: 68.48 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:12<00:21, 54.76it/s, est. speed input: 69817.44 toks/s, output: 68.18 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:13<00:20, 54.86it/s, est. speed input: 69522.97 toks/s, output: 67.89 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:13<00:20, 54.93it/s, est. speed input: 69242.04 toks/s, output: 67.62 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:13<00:20, 54.97it/s, est. speed input: 68971.31 toks/s, output: 67.35 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:14<00:20, 55.01it/s, est. speed input: 68712.72 toks/s, output: 67.10 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:14<00:19, 55.03it/s, est. speed input: 68464.60 toks/s, output: 66.86 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:14<00:19, 55.96it/s, est. speed input: 68299.45 toks/s, output: 66.70 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:14<00:18, 55.68it/s, est. speed input: 68067.13 toks/s, output: 66.47 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:15<00:18, 55.49it/s, est. speed input: 67844.39 toks/s, output: 66.25 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:15<00:18, 55.36it/s, est. speed input: 67629.78 toks/s, output: 66.04 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:15<00:18, 55.25it/s, est. speed input: 67421.52 toks/s, output: 65.84 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:16<00:17, 55.18it/s, est. speed input: 67221.87 toks/s, output: 65.65 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:16<00:17, 55.11it/s, est. speed input: 67027.57 toks/s, output: 65.46 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:16<00:17, 55.07it/s, est. speed input: 66840.68 toks/s, output: 65.27 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:16<00:17, 55.04it/s, est. speed input: 66659.56 toks/s, output: 65.10 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:17<00:16, 55.01it/s, est. speed input: 66484.42 toks/s, output: 64.93 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:17<00:16, 55.00it/s, est. speed input: 66315.77 toks/s, output: 64.76 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:17<00:15, 55.96it/s, est. speed input: 66214.35 toks/s, output: 64.66 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:18<00:15, 55.82it/s, est. speed input: 66065.13 toks/s, output: 64.52 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:18<00:15, 55.88it/s, est. speed input: 65930.37 toks/s, output: 64.39 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:18<00:15, 55.95it/s, est. speed input: 65801.13 toks/s, output: 64.26 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:18<00:14, 55.98it/s, est. speed input: 65674.77 toks/s, output: 64.14 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:19<00:14, 56.00it/s, est. speed input: 65552.39 toks/s, output: 64.02 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:19<00:14, 56.02it/s, est. speed input: 65433.40 toks/s, output: 63.90 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:19<00:13, 56.06it/s, est. speed input: 65319.82 toks/s, output: 63.79 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:20<00:13, 56.02it/s, est. speed input: 65205.47 toks/s, output: 63.68 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:20<00:13, 56.04it/s, est. speed input: 65096.81 toks/s, output: 63.57 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:20<00:13, 56.05it/s, est. speed input: 64990.69 toks/s, output: 63.47 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:20<00:12, 56.03it/s, est. speed input: 64886.48 toks/s, output: 63.37 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:21<00:12, 56.02it/s, est. speed input: 64785.24 toks/s, output: 63.27 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:21<00:12, 56.02it/s, est. speed input: 64687.08 toks/s, output: 63.17 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:21<00:11, 56.01it/s, est. speed input: 64590.57 toks/s, output: 63.08 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:22<00:11, 56.01it/s, est. speed input: 64497.02 toks/s, output: 62.99 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:22<00:11, 56.01it/s, est. speed input: 64406.23 toks/s, output: 62.90 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:22<00:11, 56.00it/s, est. speed input: 64317.20 toks/s, output: 62.81 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:22<00:10, 56.00it/s, est. speed input: 64230.57 toks/s, output: 62.73 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:23<00:10, 56.00it/s, est. speed input: 64145.77 toks/s, output: 62.64 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:23<00:10, 56.00it/s, est. speed input: 64063.51 toks/s, output: 62.56 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:23<00:09, 55.99it/s, est. speed input: 63982.64 toks/s, output: 62.48 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:24<00:09, 55.99it/s, est. speed input: 63903.97 toks/s, output: 62.41 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:24<00:09, 56.01it/s, est. speed input: 63827.62 toks/s, output: 62.33 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:24<00:09, 56.00it/s, est. speed input: 63752.67 toks/s, output: 62.26 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:24<00:08, 55.98it/s, est. speed input: 63678.58 toks/s, output: 62.19 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:25<00:08, 55.99it/s, est. speed input: 63606.94 toks/s, output: 62.12 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:25<00:08, 55.98it/s, est. speed input: 63536.46 toks/s, output: 62.05 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:25<00:07, 55.97it/s, est. speed input: 63467.17 toks/s, output: 61.98 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:26<00:07, 55.89it/s, est. speed input: 63396.51 toks/s, output: 61.91 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:26<00:07, 55.75it/s, est. speed input: 63324.09 toks/s, output: 61.84 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:26<00:07, 55.63it/s, est. speed input: 63252.35 toks/s, output: 61.77 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:27<00:06, 55.57it/s, est. speed input: 63182.72 toks/s, output: 61.70 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:27<00:06, 55.53it/s, est. speed input: 63114.85 toks/s, output: 61.64 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:27<00:06, 55.47it/s, est. speed input: 63047.40 toks/s, output: 61.57 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:27<00:06, 55.45it/s, est. speed input: 62981.75 toks/s, output: 61.51 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:28<00:05, 55.45it/s, est. speed input: 62918.11 toks/s, output: 61.44 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:28<00:05, 55.44it/s, est. speed input: 62855.55 toks/s, output: 61.38 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:28<00:05, 55.44it/s, est. speed input: 62794.29 toks/s, output: 61.32 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:29<00:04, 55.44it/s, est. speed input: 62734.57 toks/s, output: 61.26 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:29<00:04, 55.43it/s, est. speed input: 62675.34 toks/s, output: 61.21 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:29<00:04, 55.40it/s, est. speed input: 62616.66 toks/s, output: 61.15 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:29<00:04, 55.38it/s, est. speed input: 62559.18 toks/s, output: 61.09 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:30<00:03, 55.44it/s, est. speed input: 62505.22 toks/s, output: 61.04 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:30<00:03, 55.37it/s, est. speed input: 62448.44 toks/s, output: 60.98 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:30<00:03, 56.33it/s, est. speed input: 62427.55 toks/s, output: 60.96 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:31<00:02, 56.05it/s, est. speed input: 62374.83 toks/s, output: 60.91 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:31<00:02, 55.85it/s, est. speed input: 62322.53 toks/s, output: 60.86 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:31<00:02, 55.72it/s, est. speed input: 62271.66 toks/s, output: 60.81 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:31<00:01, 55.62it/s, est. speed input: 62221.47 toks/s, output: 60.76 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:32<00:01, 55.57it/s, est. speed input: 62172.54 toks/s, output: 60.72 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:32<00:01, 55.52it/s, est. speed input: 62124.24 toks/s, output: 60.67 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:32<00:01, 55.50it/s, est. speed input: 62077.05 toks/s, output: 60.62 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:33<00:00, 55.46it/s, est. speed input: 62030.02 toks/s, output: 60.58 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:33<00:00, 55.45it/s, est. speed input: 61984.27 toks/s, output: 60.53 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:33<00:00, 56.52it/s, est. speed input: 61973.42 toks/s, output: 60.52 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 56.52it/s, est. speed input: 62399.33 toks/s, output: 60.94 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 60.94it/s, est. speed input: 62399.33 toks/s, output: 60.94 toks/s]
[rank0]:[W128 11:23:46.741363310 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 90.3s

测试结果:
  Requests/s:   55.37
  Tokens/s:     56756.52
  Total Reqs:   2048
  Elapsed:      36.99s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     56701.14

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:24:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:24:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=17383) WARNING 01-28 11:24:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=17383) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=17383) WARNING 01-28 11:24:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 59.23 requests/s, 60713.72 total tokens/s, 59.23 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 11:24:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:24:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:24:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:24:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:24:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:24:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:24:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:24:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:24:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:24:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:24:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:24:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:24:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:24:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:24:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:24:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:24:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:24:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:24:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=17383) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=17383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.46s/it]
(EngineCore_DP0 pid=17383) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.46s/it]
(EngineCore_DP0 pid=17383) 
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=17383) [2026-01-28 11:24:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=17383) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.49it/s]
(EngineCore_DP0 pid=17383) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.47it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.16it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.06it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 35/4096 [00:00<00:11, 342.98it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:11, 356.46it/s]
Adding requests:   3%|▎         | 110/4096 [00:00<00:10, 363.91it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:10, 368.86it/s]
Adding requests:   5%|▍         | 187/4096 [00:00<00:10, 375.12it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:10, 375.90it/s]
Adding requests:   6%|▋         | 265/4096 [00:00<00:10, 380.95it/s]
Adding requests:   7%|▋         | 304/4096 [00:00<00:10, 374.83it/s]
Adding requests:   8%|▊         | 342/4096 [00:00<00:10, 373.95it/s]
Adding requests:   9%|▉         | 382/4096 [00:01<00:09, 380.76it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:09, 384.08it/s]
Adding requests:  11%|█▏        | 461/4096 [00:01<00:09, 385.49it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:09, 384.89it/s]
Adding requests:  13%|█▎        | 539/4096 [00:01<00:09, 375.32it/s]
Adding requests:  14%|█▍        | 580/4096 [00:01<00:09, 384.86it/s]
Adding requests:  15%|█▌        | 620/4096 [00:01<00:08, 389.13it/s]
Adding requests:  16%|█▌        | 661/4096 [00:01<00:08, 393.95it/s]
Adding requests:  17%|█▋        | 702/4096 [00:01<00:08, 398.47it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:08, 391.45it/s]
Adding requests:  19%|█▉        | 782/4096 [00:02<00:08, 393.26it/s]
Adding requests:  20%|██        | 822/4096 [00:02<00:08, 385.81it/s]
Adding requests:  21%|██        | 863/4096 [00:02<00:08, 391.35it/s]
Adding requests:  22%|██▏       | 905/4096 [00:02<00:08, 397.08it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:07, 397.17it/s]
Adding requests:  24%|██▍       | 986/4096 [00:02<00:07, 398.56it/s]
Adding requests:  25%|██▌       | 1026/4096 [00:02<00:07, 396.84it/s]
Adding requests:  26%|██▌       | 1066/4096 [00:02<00:07, 397.57it/s]
Adding requests:  27%|██▋       | 1106/4096 [00:02<00:07, 388.37it/s]
Adding requests:  28%|██▊       | 1145/4096 [00:02<00:07, 387.36it/s]
Adding requests:  29%|██▉       | 1184/4096 [00:03<00:07, 380.18it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:03<00:07, 386.23it/s]
Adding requests:  31%|███       | 1265/4096 [00:03<00:07, 388.29it/s]
Adding requests:  32%|███▏      | 1305/4096 [00:03<00:07, 390.72it/s]
Adding requests:  33%|███▎      | 1346/4096 [00:03<00:06, 395.06it/s]
Adding requests:  34%|███▍      | 1386/4096 [00:03<00:06, 396.28it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:06, 399.66it/s]
Adding requests:  36%|███▌      | 1469/4096 [00:03<00:06, 404.06it/s]
Adding requests:  37%|███▋      | 1510/4096 [00:03<00:06, 405.47it/s]
Adding requests:  38%|███▊      | 1551/4096 [00:03<00:06, 406.30it/s]
Adding requests:  39%|███▉      | 1592/4096 [00:04<00:06, 405.58it/s]
Adding requests:  40%|███▉      | 1634/4096 [00:04<00:06, 408.37it/s]
Adding requests:  41%|████      | 1675/4096 [00:04<00:05, 403.85it/s]
Adding requests:  42%|████▏     | 1717/4096 [00:04<00:05, 406.09it/s]
Adding requests:  43%|████▎     | 1758/4096 [00:04<00:05, 405.64it/s]
Adding requests:  44%|████▍     | 1799/4096 [00:04<00:05, 404.05it/s]
Adding requests:  45%|████▍     | 1840/4096 [00:04<00:05, 398.50it/s]
Adding requests:  46%|████▌     | 1880/4096 [00:04<00:05, 395.74it/s]
Adding requests:  47%|████▋     | 1920/4096 [00:04<00:05, 394.75it/s]
Adding requests:  48%|████▊     | 1960/4096 [00:05<00:05, 396.10it/s]
Adding requests:  49%|████▉     | 2001/4096 [00:05<00:05, 398.57it/s]
Adding requests:  50%|████▉     | 2041/4096 [00:05<00:05, 397.26it/s]
Adding requests:  51%|█████     | 2083/4096 [00:05<00:05, 402.03it/s]
Adding requests:  52%|█████▏    | 2124/4096 [00:05<00:04, 400.29it/s]
Adding requests:  53%|█████▎    | 2165/4096 [00:05<00:04, 395.43it/s]
Adding requests:  54%|█████▍    | 2205/4096 [00:05<00:04, 391.38it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:05<00:04, 390.43it/s]
Adding requests:  56%|█████▌    | 2285/4096 [00:05<00:04, 389.66it/s]
Adding requests:  57%|█████▋    | 2325/4096 [00:05<00:04, 389.90it/s]
Adding requests:  58%|█████▊    | 2365/4096 [00:06<00:04, 389.89it/s]
Adding requests:  59%|█████▊    | 2404/4096 [00:06<00:04, 380.41it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:06<00:04, 387.11it/s]
Adding requests:  61%|██████    | 2486/4096 [00:06<00:04, 391.65it/s]
Adding requests:  62%|██████▏   | 2526/4096 [00:06<00:03, 392.61it/s]
Adding requests:  63%|██████▎   | 2568/4096 [00:06<00:03, 398.18it/s]
Adding requests:  64%|██████▎   | 2608/4096 [00:06<00:03, 397.97it/s]
Adding requests:  65%|██████▍   | 2648/4096 [00:06<00:03, 394.36it/s]
Adding requests:  66%|██████▌   | 2688/4096 [00:06<00:03, 390.77it/s]
Adding requests:  67%|██████▋   | 2728/4096 [00:06<00:03, 385.78it/s]
Adding requests:  68%|██████▊   | 2768/4096 [00:07<00:03, 388.64it/s]
Adding requests:  69%|██████▊   | 2808/4096 [00:07<00:03, 389.37it/s]
Adding requests:  70%|██████▉   | 2848/4096 [00:07<00:03, 391.46it/s]
Adding requests:  71%|███████   | 2889/4096 [00:07<00:03, 394.80it/s]
Adding requests:  72%|███████▏  | 2929/4096 [00:07<00:02, 389.64it/s]
Adding requests:  72%|███████▏  | 2969/4096 [00:07<00:02, 391.70it/s]
Adding requests:  73%|███████▎  | 3010/4096 [00:07<00:02, 395.59it/s]
Adding requests:  74%|███████▍  | 3050/4096 [00:07<00:02, 395.44it/s]
Adding requests:  75%|███████▌  | 3090/4096 [00:07<00:02, 387.51it/s]
Adding requests:  76%|███████▋  | 3131/4096 [00:07<00:02, 391.22it/s]
Adding requests:  77%|███████▋  | 3171/4096 [00:08<00:02, 390.87it/s]
Adding requests:  78%|███████▊  | 3211/4096 [00:08<00:02, 390.99it/s]
Adding requests:  79%|███████▉  | 3252/4096 [00:08<00:02, 395.92it/s]
Adding requests:  80%|████████  | 3292/4096 [00:08<00:02, 394.12it/s]
Adding requests:  81%|████████▏ | 3332/4096 [00:08<00:01, 395.28it/s]
Adding requests:  82%|████████▏ | 3372/4096 [00:08<00:01, 391.44it/s]
Adding requests:  83%|████████▎ | 3412/4096 [00:08<00:01, 392.28it/s]
Adding requests:  84%|████████▍ | 3452/4096 [00:08<00:01, 384.22it/s]
Adding requests:  85%|████████▌ | 3491/4096 [00:08<00:01, 382.20it/s]
Adding requests:  86%|████████▌ | 3530/4096 [00:09<00:01, 381.45it/s]
Adding requests:  87%|████████▋ | 3570/4096 [00:09<00:01, 385.85it/s]
Adding requests:  88%|████████▊ | 3611/4096 [00:09<00:01, 390.80it/s]
Adding requests:  89%|████████▉ | 3651/4096 [00:09<00:01, 391.03it/s]
Adding requests:  90%|█████████ | 3691/4096 [00:09<00:01, 391.99it/s]
Adding requests:  91%|█████████ | 3731/4096 [00:09<00:00, 391.44it/s]
Adding requests:  92%|█████████▏| 3771/4096 [00:09<00:00, 382.29it/s]
Adding requests:  93%|█████████▎| 3812/4096 [00:09<00:00, 389.37it/s]
Adding requests:  94%|█████████▍| 3853/4096 [00:09<00:00, 394.43it/s]
Adding requests:  95%|█████████▌| 3894/4096 [00:09<00:00, 396.57it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:10<00:00, 394.73it/s]
Adding requests:  97%|█████████▋| 3974/4096 [00:10<00:00, 396.17it/s]
Adding requests:  98%|█████████▊| 4015/4096 [00:10<00:00, 397.17it/s]
Adding requests:  99%|█████████▉| 4055/4096 [00:10<00:00, 394.26it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 396.47it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 391.55it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:02<00:13, 255.63it/s, est. speed input: 261764.80 toks/s, output: 255.63 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:02<00:16, 207.48it/s, est. speed input: 224512.53 toks/s, output: 219.25 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:03<00:20, 168.85it/s, est. speed input: 198784.24 toks/s, output: 194.12 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:04<00:24, 138.77it/s, est. speed input: 179835.95 toks/s, output: 175.62 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:04<00:28, 116.37it/s, est. speed input: 165500.95 toks/s, output: 161.62 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:05<00:33, 99.88it/s, est. speed input: 154225.08 toks/s, output: 150.61 toks/s] 
Processed prompts:  20%|█▉        | 802/4096 [00:05<00:37, 87.91it/s, est. speed input: 145122.89 toks/s, output: 141.72 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:06<00:41, 79.30it/s, est. speed input: 137614.06 toks/s, output: 134.39 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:06<00:44, 73.19it/s, est. speed input: 131328.71 toks/s, output: 128.25 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:07<00:46, 68.84it/s, est. speed input: 125981.10 toks/s, output: 123.03 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:07<00:48, 65.76it/s, est. speed input: 121371.08 toks/s, output: 118.53 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:08<00:48, 64.12it/s, est. speed input: 117575.53 toks/s, output: 114.82 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:08<00:49, 62.41it/s, est. speed input: 114035.26 toks/s, output: 111.36 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:09<00:50, 61.22it/s, est. speed input: 110902.44 toks/s, output: 108.30 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:10<00:50, 60.37it/s, est. speed input: 108108.92 toks/s, output: 105.57 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:10<00:50, 59.78it/s, est. speed input: 105603.08 toks/s, output: 103.13 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:11<00:49, 59.88it/s, est. speed input: 103493.51 toks/s, output: 101.07 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:11<00:48, 60.57it/s, est. speed input: 101734.26 toks/s, output: 99.35 toks/s] 
Processed prompts:  29%|██▉       | 1186/4096 [00:12<00:48, 60.53it/s, est. speed input: 99998.58 toks/s, output: 97.65 toks/s] 
Processed prompts:  30%|██▉       | 1218/4096 [00:12<00:47, 60.51it/s, est. speed input: 98406.68 toks/s, output: 96.10 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:13<00:47, 60.50it/s, est. speed input: 96944.67 toks/s, output: 94.67 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:13<00:46, 60.48it/s, est. speed input: 95592.52 toks/s, output: 93.35 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:14<00:46, 60.46it/s, est. speed input: 94341.13 toks/s, output: 92.13 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:14<00:45, 60.45it/s, est. speed input: 93178.00 toks/s, output: 90.99 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:15<00:44, 60.45it/s, est. speed input: 92097.08 toks/s, output: 89.94 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:15<00:44, 60.43it/s, est. speed input: 91085.45 toks/s, output: 88.95 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:16<00:43, 60.42it/s, est. speed input: 90139.49 toks/s, output: 88.03 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:16<00:43, 60.40it/s, est. speed input: 89252.02 toks/s, output: 87.16 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:17<00:42, 60.39it/s, est. speed input: 88418.03 toks/s, output: 86.35 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:17<00:42, 60.37it/s, est. speed input: 87631.25 toks/s, output: 85.58 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:18<00:41, 60.36it/s, est. speed input: 86890.64 toks/s, output: 84.85 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:19<00:41, 60.14it/s, est. speed input: 86161.92 toks/s, output: 84.14 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:19<00:41, 59.81it/s, est. speed input: 85452.00 toks/s, output: 83.45 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:20<00:40, 59.59it/s, est. speed input: 84780.28 toks/s, output: 82.79 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:20<00:40, 59.52it/s, est. speed input: 84154.23 toks/s, output: 82.18 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:21<00:39, 59.47it/s, est. speed input: 83559.23 toks/s, output: 81.60 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:21<00:39, 59.42it/s, est. speed input: 82992.83 toks/s, output: 81.05 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:22<00:38, 59.39it/s, est. speed input: 82454.52 toks/s, output: 80.52 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:22<00:38, 59.38it/s, est. speed input: 81942.20 toks/s, output: 80.02 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:23<00:37, 59.88it/s, est. speed input: 81507.16 toks/s, output: 79.60 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:23<00:36, 59.72it/s, est. speed input: 81039.05 toks/s, output: 79.14 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:24<00:36, 59.61it/s, est. speed input: 80591.23 toks/s, output: 78.70 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:24<00:35, 59.52it/s, est. speed input: 80162.02 toks/s, output: 78.28 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:25<00:35, 59.45it/s, est. speed input: 79750.44 toks/s, output: 77.88 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:26<00:34, 59.40it/s, est. speed input: 79355.83 toks/s, output: 77.50 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:26<00:34, 59.37it/s, est. speed input: 78977.04 toks/s, output: 77.13 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:27<00:34, 59.21it/s, est. speed input: 78601.69 toks/s, output: 76.76 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:27<00:33, 59.07it/s, est. speed input: 78238.57 toks/s, output: 76.40 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:28<00:33, 58.97it/s, est. speed input: 77889.74 toks/s, output: 76.06 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:28<00:32, 58.91it/s, est. speed input: 77554.04 toks/s, output: 75.74 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:29<00:32, 58.87it/s, est. speed input: 77231.44 toks/s, output: 75.42 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:29<00:31, 58.83it/s, est. speed input: 76919.82 toks/s, output: 75.12 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:30<00:30, 59.32it/s, est. speed input: 76659.10 toks/s, output: 74.86 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:30<00:30, 59.16it/s, est. speed input: 76369.19 toks/s, output: 74.58 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:31<00:29, 59.05it/s, est. speed input: 76089.30 toks/s, output: 74.31 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:32<00:29, 58.96it/s, est. speed input: 75818.85 toks/s, output: 74.04 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:32<00:28, 58.90it/s, est. speed input: 75557.07 toks/s, output: 73.79 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:33<00:28, 58.86it/s, est. speed input: 75303.94 toks/s, output: 73.54 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:33<00:27, 58.83it/s, est. speed input: 75059.22 toks/s, output: 73.30 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:34<00:26, 59.33it/s, est. speed input: 74856.45 toks/s, output: 73.10 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:36<00:53, 29.39it/s, est. speed input: 70896.71 toks/s, output: 69.24 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:37<00:44, 34.45it/s, est. speed input: 70717.44 toks/s, output: 69.06 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:37<00:38, 39.19it/s, est. speed input: 70547.45 toks/s, output: 68.89 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:38<00:33, 43.38it/s, est. speed input: 70381.94 toks/s, output: 68.73 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:38<00:30, 46.87it/s, est. speed input: 70220.67 toks/s, output: 68.57 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:39<00:28, 49.73it/s, est. speed input: 70068.49 toks/s, output: 68.43 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:39<00:26, 51.84it/s, est. speed input: 69913.18 toks/s, output: 68.27 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:40<00:25, 53.50it/s, est. speed input: 69766.17 toks/s, output: 68.13 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:40<00:23, 54.77it/s, est. speed input: 69626.04 toks/s, output: 67.99 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:41<00:22, 55.59it/s, est. speed input: 69483.89 toks/s, output: 67.86 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:42<00:22, 56.24it/s, est. speed input: 69348.77 toks/s, output: 67.72 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:42<00:21, 56.69it/s, est. speed input: 69216.49 toks/s, output: 67.59 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:43<00:20, 57.02it/s, est. speed input: 69087.92 toks/s, output: 67.47 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:43<00:20, 57.25it/s, est. speed input: 68962.53 toks/s, output: 67.35 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:44<00:19, 57.41it/s, est. speed input: 68839.90 toks/s, output: 67.23 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:44<00:18, 58.09it/s, est. speed input: 68748.55 toks/s, output: 67.14 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:45<00:17, 58.95it/s, est. speed input: 68676.52 toks/s, output: 67.07 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:45<00:17, 59.56it/s, est. speed input: 68605.82 toks/s, output: 67.00 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:46<00:16, 60.00it/s, est. speed input: 68536.97 toks/s, output: 66.93 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:46<00:15, 60.31it/s, est. speed input: 68469.71 toks/s, output: 66.86 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:47<00:15, 60.53it/s, est. speed input: 68404.14 toks/s, output: 66.80 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:47<00:14, 60.69it/s, est. speed input: 68340.05 toks/s, output: 66.74 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:48<00:14, 60.80it/s, est. speed input: 68277.25 toks/s, output: 66.68 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:49<00:13, 60.88it/s, est. speed input: 68215.62 toks/s, output: 66.62 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:49<00:13, 60.94it/s, est. speed input: 68155.69 toks/s, output: 66.56 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:50<00:12, 60.97it/s, est. speed input: 68096.56 toks/s, output: 66.50 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:50<00:12, 61.00it/s, est. speed input: 68038.82 toks/s, output: 66.44 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:51<00:11, 61.02it/s, est. speed input: 67982.57 toks/s, output: 66.39 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:51<00:10, 61.02it/s, est. speed input: 67926.80 toks/s, output: 66.33 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:52<00:10, 61.04it/s, est. speed input: 67872.81 toks/s, output: 66.28 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:52<00:09, 60.91it/s, est. speed input: 67814.70 toks/s, output: 66.23 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:53<00:09, 60.57it/s, est. speed input: 67748.41 toks/s, output: 66.16 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:53<00:08, 60.33it/s, est. speed input: 67683.26 toks/s, output: 66.10 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:54<00:08, 60.16it/s, est. speed input: 67619.34 toks/s, output: 66.03 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:54<00:07, 60.04it/s, est. speed input: 67556.80 toks/s, output: 65.97 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:55<00:07, 60.01it/s, est. speed input: 67497.28 toks/s, output: 65.92 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:55<00:06, 59.86it/s, est. speed input: 67434.23 toks/s, output: 65.85 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:56<00:06, 60.35it/s, est. speed input: 67393.38 toks/s, output: 65.81 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:56<00:05, 60.17it/s, est. speed input: 67334.95 toks/s, output: 65.76 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:57<00:05, 60.05it/s, est. speed input: 67277.94 toks/s, output: 65.70 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:58<00:04, 59.97it/s, est. speed input: 67222.05 toks/s, output: 65.65 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:58<00:04, 59.91it/s, est. speed input: 67166.91 toks/s, output: 65.59 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:59<00:03, 59.86it/s, est. speed input: 67112.90 toks/s, output: 65.54 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:59<00:03, 59.83it/s, est. speed input: 67059.80 toks/s, output: 65.49 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:00<00:02, 59.81it/s, est. speed input: 67007.69 toks/s, output: 65.44 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:00<00:02, 59.65it/s, est. speed input: 66951.53 toks/s, output: 65.38 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:01<00:01, 59.27it/s, est. speed input: 66887.50 toks/s, output: 65.32 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:01<00:01, 59.52it/s, est. speed input: 66841.57 toks/s, output: 65.27 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:02<00:00, 59.86it/s, est. speed input: 66801.71 toks/s, output: 65.24 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:02<00:00, 59.86it/s, est. speed input: 67293.98 toks/s, output: 65.72 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:02<00:00, 65.72it/s, est. speed input: 67293.98 toks/s, output: 65.72 toks/s]
[rank0]:[W128 11:26:04.142898497 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 137.8s

测试结果:
  Requests/s:   59.23
  Tokens/s:     60713.72
  Total Reqs:   4096
  Elapsed:      69.15s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     60654.49


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,17.3685,8910.0510,7.3697
1024,1024,1,128,128,16.6093,17024.5643,7.7065
2048,1024,2,256,128,34.8138,35684.1369,7.3534
4096,1024,4,512,128,53.2481,54579.3410,9.6154
8192,1024,8,1024,128,55.3135,56696.3207,18.5127
16384,1024,16,2048,128,55.3722,56756.5172,36.9861
32768,1024,32,4096,128,59.2329,60713.7236,69.1508

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:26:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:26:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=18341) WARNING 01-28 11:26:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=18341) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=18341) WARNING 01-28 11:26:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.70 requests/s, 8569.15 total tokens/s, 16.70 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:26:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:26:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:26:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:26:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:26:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:26:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:26:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:26:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:26:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:26:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:26:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:26:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:26:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:26:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:26:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:26:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:26:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:26:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:26:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=18341) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=18341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.04s/it]
(EngineCore_DP0 pid=18341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.04s/it]
(EngineCore_DP0 pid=18341) 
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=18341) [2026-01-28 11:26:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=18341) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:02<00:02,  2.20s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:02<00:00,  1.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]
(EngineCore_DP0 pid=18341) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 638.05it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 657.50it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 38.60it/s, est. speed input: 19768.33 toks/s, output: 38.61 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.90it/s, est. speed input: 11992.43 toks/s, output: 23.42 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 19.58it/s, est. speed input: 10817.85 toks/s, output: 21.13 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 18.22it/s, est. speed input: 10154.72 toks/s, output: 19.83 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 17.87it/s, est. speed input: 9938.59 toks/s, output: 19.41 toks/s] 
Processed prompts:  14%|█▍        | 18/128 [00:00<00:06, 17.62it/s, est. speed input: 9781.36 toks/s, output: 19.10 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.41it/s, est. speed input: 9655.28 toks/s, output: 18.86 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.25it/s, est. speed input: 9551.44 toks/s, output: 18.65 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 17.13it/s, est. speed input: 9466.85 toks/s, output: 18.49 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.98it/s, est. speed input: 9386.38 toks/s, output: 18.33 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 16.90it/s, est. speed input: 9321.77 toks/s, output: 18.21 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.71it/s, est. speed input: 9248.71 toks/s, output: 18.06 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 16.69it/s, est. speed input: 9199.74 toks/s, output: 17.97 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 16.68it/s, est. speed input: 9156.89 toks/s, output: 17.88 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 16.73it/s, est. speed input: 9126.31 toks/s, output: 17.82 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 16.76it/s, est. speed input: 9098.05 toks/s, output: 17.77 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 16.72it/s, est. speed input: 9066.21 toks/s, output: 17.71 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 16.71it/s, est. speed input: 9040.15 toks/s, output: 17.66 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 16.73it/s, est. speed input: 9018.54 toks/s, output: 17.61 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 16.69it/s, est. speed input: 8994.67 toks/s, output: 17.57 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 16.71it/s, est. speed input: 8976.38 toks/s, output: 17.53 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 16.76it/s, est. speed input: 8962.41 toks/s, output: 17.50 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:04, 16.80it/s, est. speed input: 8949.85 toks/s, output: 17.48 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 16.83it/s, est. speed input: 8938.51 toks/s, output: 17.46 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 16.79it/s, est. speed input: 8923.86 toks/s, output: 17.43 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 16.76it/s, est. speed input: 8910.69 toks/s, output: 17.40 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 16.69it/s, est. speed input: 8894.84 toks/s, output: 17.37 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 16.56it/s, est. speed input: 8875.56 toks/s, output: 17.33 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 16.60it/s, est. speed input: 8864.74 toks/s, output: 17.31 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 16.62it/s, est. speed input: 8854.33 toks/s, output: 17.29 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 16.54it/s, est. speed input: 8839.31 toks/s, output: 17.26 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 16.53it/s, est. speed input: 8827.87 toks/s, output: 17.24 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 16.48it/s, est. speed input: 8814.75 toks/s, output: 17.22 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 16.51it/s, est. speed input: 8805.49 toks/s, output: 17.20 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 16.50it/s, est. speed input: 8795.50 toks/s, output: 17.18 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 16.51it/s, est. speed input: 8786.71 toks/s, output: 17.16 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 16.48it/s, est. speed input: 8776.73 toks/s, output: 17.14 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 16.51it/s, est. speed input: 8769.42 toks/s, output: 17.13 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 16.54it/s, est. speed input: 8762.74 toks/s, output: 17.11 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 16.53it/s, est. speed input: 8755.28 toks/s, output: 17.10 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 16.72it/s, est. speed input: 8756.30 toks/s, output: 17.10 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 16.91it/s, est. speed input: 8759.19 toks/s, output: 17.11 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 17.04it/s, est. speed input: 8761.94 toks/s, output: 17.11 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 17.09it/s, est. speed input: 8763.01 toks/s, output: 17.12 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 17.17it/s, est. speed input: 8765.66 toks/s, output: 17.12 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 17.25it/s, est. speed input: 8768.83 toks/s, output: 17.13 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 17.30it/s, est. speed input: 8771.80 toks/s, output: 17.13 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 17.28it/s, est. speed input: 8772.94 toks/s, output: 17.13 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 17.24it/s, est. speed input: 8773.00 toks/s, output: 17.13 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 17.18it/s, est. speed input: 8771.98 toks/s, output: 17.13 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 17.16it/s, est. speed input: 8771.89 toks/s, output: 17.13 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 17.15it/s, est. speed input: 8771.74 toks/s, output: 17.13 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 17.13it/s, est. speed input: 8771.22 toks/s, output: 17.13 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 17.10it/s, est. speed input: 8770.34 toks/s, output: 17.13 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 17.07it/s, est. speed input: 8769.20 toks/s, output: 17.13 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:06<00:00, 17.22it/s, est. speed input: 8773.01 toks/s, output: 17.13 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 17.32it/s, est. speed input: 8776.65 toks/s, output: 17.14 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 17.28it/s, est. speed input: 8776.97 toks/s, output: 17.14 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 17.23it/s, est. speed input: 8776.64 toks/s, output: 17.14 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 17.21it/s, est. speed input: 8776.84 toks/s, output: 17.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.20it/s, est. speed input: 8777.12 toks/s, output: 17.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.20it/s, est. speed input: 8777.12 toks/s, output: 17.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.14it/s, est. speed input: 8777.12 toks/s, output: 17.14 toks/s]
[rank0]:[W128 11:26:56.662011514 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.2s

测试结果:
  Requests/s:   16.70
  Tokens/s:     8569.15
  Total Reqs:   128
  Elapsed:      7.66s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8552.45

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:27:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:27:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=18885) WARNING 01-28 11:27:13 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=18885) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=18885) WARNING 01-28 11:27:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.74 requests/s, 17155.58 total tokens/s, 16.74 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:27:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:27:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:27:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:27:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:27:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:27:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:27:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:27:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:27:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:27:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:27:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:27:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:27:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:27:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:27:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:27:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:27:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:27:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=18885) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=18885) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.46it/s]
(EngineCore_DP0 pid=18885) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.46it/s]
(EngineCore_DP0 pid=18885) 
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=18885) [2026-01-28 11:27:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=18885) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.73it/s]
(EngineCore_DP0 pid=18885) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 377.78it/s]
Adding requests:  62%|██████▏   | 79/128 [00:00<00:00, 393.65it/s]
Adding requests:  93%|█████████▎| 119/128 [00:00<00:00, 388.09it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 388.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:03, 38.33it/s, est. speed input: 39251.70 toks/s, output: 38.33 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:04, 24.37it/s, est. speed input: 26702.64 toks/s, output: 26.08 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 21.38it/s, est. speed input: 23911.65 toks/s, output: 23.35 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 19.90it/s, est. speed input: 22482.00 toks/s, output: 21.95 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 18.92it/s, est. speed input: 21526.54 toks/s, output: 21.02 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:05, 18.45it/s, est. speed input: 21060.81 toks/s, output: 20.57 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.96it/s, est. speed input: 20633.82 toks/s, output: 20.15 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.66it/s, est. speed input: 20319.19 toks/s, output: 19.84 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.47it/s, est. speed input: 20069.34 toks/s, output: 19.60 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.36it/s, est. speed input: 19866.31 toks/s, output: 19.40 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.30it/s, est. speed input: 19700.44 toks/s, output: 19.24 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.24it/s, est. speed input: 19553.32 toks/s, output: 19.09 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.20it/s, est. speed input: 19422.16 toks/s, output: 18.97 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:05, 17.13it/s, est. speed input: 19299.35 toks/s, output: 18.85 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.10it/s, est. speed input: 19195.29 toks/s, output: 18.75 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 17.15it/s, est. speed input: 19114.03 toks/s, output: 18.67 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.17it/s, est. speed input: 19039.81 toks/s, output: 18.59 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.08it/s, est. speed input: 18954.08 toks/s, output: 18.51 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.06it/s, est. speed input: 18883.41 toks/s, output: 18.44 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.11it/s, est. speed input: 18828.57 toks/s, output: 18.39 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 17.14it/s, est. speed input: 18779.33 toks/s, output: 18.34 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 17.19it/s, est. speed input: 18736.18 toks/s, output: 18.30 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 17.17it/s, est. speed input: 18690.21 toks/s, output: 18.25 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 17.17it/s, est. speed input: 18648.67 toks/s, output: 18.21 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 17.11it/s, est. speed input: 18603.30 toks/s, output: 18.17 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.11it/s, est. speed input: 18564.67 toks/s, output: 18.13 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.09it/s, est. speed input: 18528.05 toks/s, output: 18.09 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 17.10it/s, est. speed input: 18495.65 toks/s, output: 18.06 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 17.13it/s, est. speed input: 18467.71 toks/s, output: 18.03 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 17.19it/s, est. speed input: 18445.92 toks/s, output: 18.01 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:03, 17.26it/s, est. speed input: 18428.28 toks/s, output: 18.00 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.26it/s, est. speed input: 18406.58 toks/s, output: 17.98 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.28it/s, est. speed input: 18388.30 toks/s, output: 17.96 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.28it/s, est. speed input: 18370.09 toks/s, output: 17.94 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.27it/s, est. speed input: 18350.75 toks/s, output: 17.92 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.18it/s, est. speed input: 18326.18 toks/s, output: 17.90 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.13it/s, est. speed input: 18303.17 toks/s, output: 17.87 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 17.10it/s, est. speed input: 18281.94 toks/s, output: 17.85 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:02, 17.13it/s, est. speed input: 18265.48 toks/s, output: 17.84 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:02, 17.18it/s, est. speed input: 18252.54 toks/s, output: 17.82 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.19it/s, est. speed input: 18238.87 toks/s, output: 17.81 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.23it/s, est. speed input: 18227.24 toks/s, output: 17.80 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.15it/s, est. speed input: 18208.63 toks/s, output: 17.78 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.17it/s, est. speed input: 18196.22 toks/s, output: 17.77 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 17.18it/s, est. speed input: 18183.97 toks/s, output: 17.76 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.22it/s, est. speed input: 18175.19 toks/s, output: 17.75 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 17.35it/s, est. speed input: 18173.03 toks/s, output: 17.75 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 17.36it/s, est. speed input: 18166.11 toks/s, output: 17.74 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.74it/s, est. speed input: 18115.51 toks/s, output: 17.69 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.44it/s, est. speed input: 18075.77 toks/s, output: 17.65 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.27it/s, est. speed input: 18039.26 toks/s, output: 17.62 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.11it/s, est. speed input: 18001.89 toks/s, output: 17.58 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 16.04it/s, est. speed input: 17968.44 toks/s, output: 17.55 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 16.02it/s, est. speed input: 17937.81 toks/s, output: 17.52 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 16.36it/s, est. speed input: 17932.39 toks/s, output: 17.51 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 16.66it/s, est. speed input: 17930.82 toks/s, output: 17.51 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.84it/s, est. speed input: 17927.06 toks/s, output: 17.51 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.96it/s, est. speed input: 17922.38 toks/s, output: 17.50 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.04it/s, est. speed input: 17917.81 toks/s, output: 17.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.04it/s, est. speed input: 17914.78 toks/s, output: 17.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.49it/s, est. speed input: 17914.78 toks/s, output: 17.49 toks/s]
[rank0]:[W128 11:27:45.897150993 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.3s

测试结果:
  Requests/s:   16.74
  Tokens/s:     17155.58
  Total Reqs:   128
  Elapsed:      7.65s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     17138.84

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:27:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:27:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=19415) WARNING 01-28 11:28:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=19415) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=19415) WARNING 01-28 11:28:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.91 requests/s, 33736.67 total tokens/s, 32.91 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 11:27:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:27:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:27:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:27:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:27:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:27:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:27:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:27:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:27:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:27:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:28:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:28:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:28:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:28:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:28:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:28:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:28:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:28:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:28:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=19415) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=19415) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=19415) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=19415) 
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=19415) [2026-01-28 11:28:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=19415) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.55it/s]
(EngineCore_DP0 pid=19415) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.80it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 34/256 [00:00<00:00, 339.23it/s]
Adding requests:  29%|██▊       | 73/256 [00:00<00:00, 367.05it/s]
Adding requests:  44%|████▍     | 112/256 [00:00<00:00, 374.31it/s]
Adding requests:  59%|█████▊    | 150/256 [00:00<00:00, 374.18it/s]
Adding requests:  74%|███████▍  | 189/256 [00:00<00:00, 379.46it/s]
Adding requests:  89%|████████▉ | 229/256 [00:00<00:00, 386.00it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 379.79it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 156.10it/s, est. speed input: 159872.52 toks/s, output: 156.11 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:03, 56.45it/s, est. speed input: 64687.79 toks/s, output: 63.17 toks/s]   
Processed prompts:  18%|█▊        | 45/256 [00:00<00:04, 50.51it/s, est. speed input: 58192.90 toks/s, output: 56.83 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:04, 44.00it/s, est. speed input: 52594.45 toks/s, output: 51.36 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:04, 41.92it/s, est. speed input: 50433.59 toks/s, output: 49.25 toks/s]
Processed prompts:  25%|██▍       | 63/256 [00:01<00:04, 42.59it/s, est. speed input: 50071.63 toks/s, output: 48.90 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:04, 38.81it/s, est. speed input: 47895.17 toks/s, output: 46.77 toks/s]
Processed prompts:  29%|██▊       | 73/256 [00:01<00:04, 40.10it/s, est. speed input: 47724.46 toks/s, output: 46.61 toks/s]
Processed prompts:  30%|███       | 78/256 [00:03<00:25,  7.09it/s, est. speed input: 20614.48 toks/s, output: 20.13 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:03<00:19,  8.72it/s, est. speed input: 21068.83 toks/s, output: 20.57 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:04<00:15, 10.73it/s, est. speed input: 21472.78 toks/s, output: 20.97 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:04<00:12, 13.03it/s, est. speed input: 21820.21 toks/s, output: 21.31 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:04<00:10, 15.61it/s, est. speed input: 22154.29 toks/s, output: 21.63 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:04<00:08, 18.30it/s, est. speed input: 22466.24 toks/s, output: 21.94 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:04<00:07, 20.98it/s, est. speed input: 22765.97 toks/s, output: 22.23 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:04<00:06, 23.43it/s, est. speed input: 23046.15 toks/s, output: 22.51 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:04<00:05, 25.60it/s, est. speed input: 23315.68 toks/s, output: 22.77 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:04<00:05, 27.32it/s, est. speed input: 23563.43 toks/s, output: 23.01 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:05<00:04, 28.79it/s, est. speed input: 23807.69 toks/s, output: 23.25 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:05<00:04, 29.99it/s, est. speed input: 24045.10 toks/s, output: 23.48 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:05<00:04, 30.86it/s, est. speed input: 24269.42 toks/s, output: 23.70 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:05<00:03, 31.63it/s, est. speed input: 24491.11 toks/s, output: 23.92 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:05<00:03, 32.17it/s, est. speed input: 24702.61 toks/s, output: 24.12 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:05<00:03, 32.49it/s, est. speed input: 24901.02 toks/s, output: 24.32 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:05<00:03, 32.66it/s, est. speed input: 25087.53 toks/s, output: 24.50 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:05<00:03, 32.86it/s, est. speed input: 25271.04 toks/s, output: 24.68 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:06<00:03, 32.94it/s, est. speed input: 25444.34 toks/s, output: 24.85 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:06<00:03, 32.87it/s, est. speed input: 25604.24 toks/s, output: 25.00 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:06<00:02, 32.94it/s, est. speed input: 25763.57 toks/s, output: 25.16 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:06<00:02, 33.02it/s, est. speed input: 25918.77 toks/s, output: 25.31 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:06<00:02, 33.02it/s, est. speed input: 26065.60 toks/s, output: 25.45 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:06<00:02, 33.05it/s, est. speed input: 26208.07 toks/s, output: 25.59 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:06<00:02, 33.09it/s, est. speed input: 26346.51 toks/s, output: 25.73 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:06<00:02, 33.23it/s, est. speed input: 26485.45 toks/s, output: 25.86 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:07<00:02, 33.06it/s, est. speed input: 26607.12 toks/s, output: 25.98 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:07<00:02, 33.10it/s, est. speed input: 26732.22 toks/s, output: 26.11 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:07<00:01, 33.14it/s, est. speed input: 26853.51 toks/s, output: 26.22 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:07<00:01, 33.15it/s, est. speed input: 26969.80 toks/s, output: 26.34 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:07<00:01, 33.01it/s, est. speed input: 27076.19 toks/s, output: 26.44 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:07<00:01, 32.93it/s, est. speed input: 27179.86 toks/s, output: 26.54 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:07<00:01, 32.56it/s, est. speed input: 27266.39 toks/s, output: 26.63 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:07<00:01, 32.62it/s, est. speed input: 27363.70 toks/s, output: 26.72 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:07<00:01, 32.76it/s, est. speed input: 27462.67 toks/s, output: 26.82 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:08<00:01, 32.91it/s, est. speed input: 27560.79 toks/s, output: 26.91 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:08<00:01, 33.02it/s, est. speed input: 27656.11 toks/s, output: 27.01 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:08<00:00, 33.19it/s, est. speed input: 27752.26 toks/s, output: 27.10 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:08<00:00, 33.26it/s, est. speed input: 27843.99 toks/s, output: 27.19 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:08<00:00, 33.30it/s, est. speed input: 27932.50 toks/s, output: 27.28 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:08<00:00, 33.21it/s, est. speed input: 28014.38 toks/s, output: 27.36 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:08<00:00, 33.19it/s, est. speed input: 28095.29 toks/s, output: 27.44 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:08<00:00, 33.17it/s, est. speed input: 28173.92 toks/s, output: 27.51 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:09<00:00, 33.03it/s, est. speed input: 28245.58 toks/s, output: 27.58 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:09<00:00, 33.10it/s, est. speed input: 28321.76 toks/s, output: 27.66 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 33.10it/s, est. speed input: 28359.83 toks/s, output: 27.70 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 27.69it/s, est. speed input: 28359.83 toks/s, output: 27.70 toks/s]
[rank0]:[W128 11:28:36.054795518 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.4s

测试结果:
  Requests/s:   32.91
  Tokens/s:     33736.67
  Total Reqs:   256
  Elapsed:      7.78s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     33703.76

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:28:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:28:48 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=19958) WARNING 01-28 11:28:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=19958) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=19958) WARNING 01-28 11:29:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.66 requests/s, 58071.79 total tokens/s, 56.66 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 11:28:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:28:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:28:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:28:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:28:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:28:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:28:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:28:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:28:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:28:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:28:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:28:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:28:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:28:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:28:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:28:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:28:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:28:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:28:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=19958) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=19958) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.57it/s]
(EngineCore_DP0 pid=19958) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.57it/s]
(EngineCore_DP0 pid=19958) 
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=19958) [2026-01-28 11:28:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=19958) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.61it/s]
(EngineCore_DP0 pid=19958) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.75it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  9.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.96it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 38/512 [00:00<00:01, 374.49it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 387.85it/s]
Adding requests:  23%|██▎       | 118/512 [00:00<00:01, 389.67it/s]
Adding requests:  31%|███       | 157/512 [00:00<00:00, 387.12it/s]
Adding requests:  38%|███▊      | 196/512 [00:00<00:00, 384.05it/s]
Adding requests:  46%|████▌     | 235/512 [00:00<00:00, 382.78it/s]
Adding requests:  54%|█████▎    | 275/512 [00:00<00:00, 385.54it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 385.64it/s]
Adding requests:  69%|██████▉   | 354/512 [00:00<00:00, 387.78it/s]
Adding requests:  77%|███████▋  | 393/512 [00:01<00:00, 387.77it/s]
Adding requests:  85%|████████▍ | 433/512 [00:01<00:00, 389.29it/s]
Adding requests:  92%|█████████▏| 473/512 [00:01<00:00, 390.07it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 387.61it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:00<00:01, 455.64it/s, est. speed input: 466689.88 toks/s, output: 455.68 toks/s]
Processed prompts:  18%|█▊        | 92/512 [00:00<00:04, 98.76it/s, est. speed input: 114599.33 toks/s, output: 111.91 toks/s] 
Processed prompts:  22%|██▏       | 115/512 [00:01<00:04, 82.01it/s, est. speed input: 96863.84 toks/s, output: 94.59 toks/s] 
Processed prompts:  25%|██▌       | 130/512 [00:01<00:05, 74.91it/s, est. speed input: 90017.68 toks/s, output: 87.91 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:01<00:05, 71.69it/s, est. speed input: 86794.69 toks/s, output: 84.76 toks/s]
Processed prompts:  30%|██▉       | 152/512 [00:01<00:04, 72.53it/s, est. speed input: 86166.06 toks/s, output: 84.15 toks/s]
Processed prompts:  31%|███▏      | 161/512 [00:01<00:04, 71.74it/s, est. speed input: 85093.31 toks/s, output: 83.10 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:05, 63.73it/s, est. speed input: 81549.42 toks/s, output: 79.64 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:05, 63.05it/s, est. speed input: 80421.99 toks/s, output: 78.54 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:02<00:05, 62.56it/s, est. speed input: 79443.02 toks/s, output: 77.58 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:02<00:05, 62.16it/s, est. speed input: 78564.27 toks/s, output: 76.72 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:02<00:05, 61.88it/s, est. speed input: 77778.14 toks/s, output: 75.95 toks/s]
Processed prompts:  41%|████      | 210/512 [00:02<00:04, 61.65it/s, est. speed input: 77061.22 toks/s, output: 75.25 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:02<00:04, 61.44it/s, est. speed input: 76401.83 toks/s, output: 74.61 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:04, 61.28it/s, est. speed input: 75795.60 toks/s, output: 74.02 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:03<00:04, 61.19it/s, est. speed input: 75245.06 toks/s, output: 73.48 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:03<00:04, 61.13it/s, est. speed input: 74739.00 toks/s, output: 72.99 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:03<00:04, 61.01it/s, est. speed input: 74258.62 toks/s, output: 72.52 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:03<00:04, 61.01it/s, est. speed input: 73826.94 toks/s, output: 72.10 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:03<00:04, 61.01it/s, est. speed input: 73425.84 toks/s, output: 71.70 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:03<00:03, 60.97it/s, est. speed input: 73046.37 toks/s, output: 71.33 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:03<00:03, 60.91it/s, est. speed input: 72687.96 toks/s, output: 70.98 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:04<00:03, 60.89it/s, est. speed input: 72355.30 toks/s, output: 70.66 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:04<00:03, 60.87it/s, est. speed input: 72042.10 toks/s, output: 70.35 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:04<00:03, 60.87it/s, est. speed input: 71749.50 toks/s, output: 70.07 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:04<00:03, 60.91it/s, est. speed input: 71479.45 toks/s, output: 69.80 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:04<00:03, 60.92it/s, est. speed input: 71221.81 toks/s, output: 69.55 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:04<00:02, 60.88it/s, est. speed input: 70973.98 toks/s, output: 69.31 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:04<00:02, 60.88it/s, est. speed input: 70741.99 toks/s, output: 69.08 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:05<00:02, 60.89it/s, est. speed input: 70523.05 toks/s, output: 68.87 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:05<00:02, 60.90it/s, est. speed input: 70316.25 toks/s, output: 68.67 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:05<00:02, 60.89it/s, est. speed input: 70118.40 toks/s, output: 68.47 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:05<00:02, 60.91it/s, est. speed input: 69930.88 toks/s, output: 68.29 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:05<00:02, 60.87it/s, est. speed input: 69748.74 toks/s, output: 68.11 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:05<00:02, 60.81it/s, est. speed input: 69572.01 toks/s, output: 67.94 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:05<00:01, 60.85it/s, est. speed input: 69409.63 toks/s, output: 67.78 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:05<00:01, 60.87it/s, est. speed input: 69254.37 toks/s, output: 67.63 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:06<00:01, 60.88it/s, est. speed input: 69105.16 toks/s, output: 67.49 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:06<00:01, 60.87it/s, est. speed input: 68961.00 toks/s, output: 67.34 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:06<00:01, 60.87it/s, est. speed input: 68823.44 toks/s, output: 67.21 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:06<00:01, 60.86it/s, est. speed input: 68691.26 toks/s, output: 67.08 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:06<00:01, 60.85it/s, est. speed input: 68563.38 toks/s, output: 66.96 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:06<00:01, 60.81it/s, est. speed input: 68438.62 toks/s, output: 66.83 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:06<00:00, 60.79it/s, est. speed input: 68318.55 toks/s, output: 66.72 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:06<00:00, 60.75it/s, est. speed input: 68202.01 toks/s, output: 66.60 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:07<00:00, 60.74it/s, est. speed input: 68090.34 toks/s, output: 66.49 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:07<00:00, 60.72it/s, est. speed input: 67981.98 toks/s, output: 66.39 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:07<00:00, 60.79it/s, est. speed input: 67883.12 toks/s, output: 66.29 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:07<00:00, 60.78it/s, est. speed input: 67783.98 toks/s, output: 66.20 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:07<00:00, 60.77it/s, est. speed input: 67688.10 toks/s, output: 66.10 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 60.77it/s, est. speed input: 67960.55 toks/s, output: 66.37 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 66.37it/s, est. speed input: 67960.55 toks/s, output: 66.37 toks/s]
[rank0]:[W128 11:29:28.680752613 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.8s

测试结果:
  Requests/s:   56.66
  Tokens/s:     58071.79
  Total Reqs:   512
  Elapsed:      9.04s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     58015.13

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:29:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:29:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=20527) WARNING 01-28 11:29:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=20527) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=20527) WARNING 01-28 11:30:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.76 requests/s, 56132.09 total tokens/s, 54.76 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 11:29:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:29:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:29:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:29:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:29:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:29:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:29:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:29:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:29:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:29:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:29:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:29:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:29:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:29:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:29:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:29:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:29:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:29:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:29:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=20527) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=20527) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.56it/s]
(EngineCore_DP0 pid=20527) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.56it/s]
(EngineCore_DP0 pid=20527) 
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=20527) [2026-01-28 11:29:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=20527) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.29it/s]
(EngineCore_DP0 pid=20527) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:02<00:02,  1.23s/it]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:02<00:00,  1.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:02<00:00,  2.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 33/1024 [00:00<00:03, 326.01it/s]
Adding requests:   7%|▋         | 68/1024 [00:00<00:02, 335.41it/s]
Adding requests:  10%|█         | 103/1024 [00:00<00:02, 341.72it/s]
Adding requests:  14%|█▎        | 139/1024 [00:00<00:02, 346.20it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:02, 342.05it/s]
Adding requests:  21%|██        | 211/1024 [00:00<00:02, 348.85it/s]
Adding requests:  24%|██▍       | 249/1024 [00:00<00:02, 356.53it/s]
Adding requests:  28%|██▊       | 286/1024 [00:00<00:02, 358.52it/s]
Adding requests:  31%|███▏      | 322/1024 [00:00<00:01, 356.73it/s]
Adding requests:  35%|███▌      | 359/1024 [00:01<00:01, 359.01it/s]
Adding requests:  39%|███▊      | 395/1024 [00:01<00:01, 357.25it/s]
Adding requests:  42%|████▏     | 432/1024 [00:01<00:01, 359.61it/s]
Adding requests:  46%|████▌     | 468/1024 [00:01<00:01, 357.76it/s]
Adding requests:  49%|████▉     | 504/1024 [00:01<00:01, 354.05it/s]
Adding requests:  53%|█████▎    | 540/1024 [00:01<00:01, 350.16it/s]
Adding requests:  56%|█████▋    | 578/1024 [00:01<00:01, 358.41it/s]
Adding requests:  60%|██████    | 615/1024 [00:01<00:01, 359.34it/s]
Adding requests:  64%|██████▎   | 651/1024 [00:01<00:01, 357.57it/s]
Adding requests:  67%|██████▋   | 690/1024 [00:01<00:00, 365.87it/s]
Adding requests:  71%|███████   | 727/1024 [00:02<00:00, 366.25it/s]
Adding requests:  75%|███████▍  | 764/1024 [00:02<00:00, 363.13it/s]
Adding requests:  78%|███████▊  | 801/1024 [00:02<00:00, 364.33it/s]
Adding requests:  82%|████████▏ | 838/1024 [00:02<00:00, 358.60it/s]
Adding requests:  86%|████████▌ | 876/1024 [00:02<00:00, 362.21it/s]
Adding requests:  89%|████████▉ | 914/1024 [00:02<00:00, 367.35it/s]
Adding requests:  93%|█████████▎| 953/1024 [00:02<00:00, 372.47it/s]
Adding requests:  97%|█████████▋| 995/1024 [00:02<00:00, 384.65it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 361.95it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:00<00:01, 775.25it/s, est. speed input: 793973.90 toks/s, output: 775.28 toks/s]
Processed prompts:  21%|██        | 216/1024 [00:01<00:06, 125.53it/s, est. speed input: 153140.48 toks/s, output: 149.55 toks/s]
Processed prompts:  25%|██▍       | 252/1024 [00:02<00:08, 94.31it/s, est. speed input: 120113.21 toks/s, output: 117.30 toks/s] 
Processed prompts:  27%|██▋       | 274/1024 [00:02<00:09, 83.15it/s, est. speed input: 109154.36 toks/s, output: 106.59 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:02<00:09, 77.85it/s, est. speed input: 104123.69 toks/s, output: 101.68 toks/s]
Processed prompts:  29%|██▉       | 302/1024 [00:02<00:09, 78.79it/s, est. speed input: 103331.24 toks/s, output: 100.91 toks/s]
Processed prompts:  31%|███       | 313/1024 [00:03<00:09, 78.65it/s, est. speed input: 102268.87 toks/s, output: 99.87 toks/s] 
Processed prompts:  32%|███▏      | 323/1024 [00:03<00:10, 65.53it/s, est. speed input: 96817.57 toks/s, output: 94.55 toks/s] 
Processed prompts:  32%|███▏      | 331/1024 [00:03<00:10, 63.92it/s, est. speed input: 95266.03 toks/s, output: 93.03 toks/s]
Processed prompts:  33%|███▎      | 339/1024 [00:03<00:10, 62.45it/s, est. speed input: 93845.83 toks/s, output: 91.65 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:03<00:11, 59.61it/s, est. speed input: 92275.32 toks/s, output: 90.11 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:03<00:11, 58.91it/s, est. speed input: 91064.01 toks/s, output: 88.93 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:04<00:11, 58.31it/s, est. speed input: 89931.06 toks/s, output: 87.82 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:04<00:11, 57.91it/s, est. speed input: 88879.04 toks/s, output: 86.80 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:04<00:11, 57.57it/s, est. speed input: 87890.97 toks/s, output: 85.83 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:04<00:11, 57.28it/s, est. speed input: 86959.76 toks/s, output: 84.92 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:04<00:11, 57.14it/s, est. speed input: 86092.84 toks/s, output: 84.07 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:04<00:10, 57.05it/s, est. speed input: 85279.76 toks/s, output: 83.28 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:04<00:10, 56.97it/s, est. speed input: 84509.44 toks/s, output: 82.53 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:10, 56.92it/s, est. speed input: 83783.48 toks/s, output: 81.82 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:10, 56.83it/s, est. speed input: 83089.04 toks/s, output: 81.14 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:05<00:10, 56.84it/s, est. speed input: 82439.81 toks/s, output: 80.51 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:05<00:10, 56.77it/s, est. speed input: 81814.11 toks/s, output: 79.90 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:05<00:10, 56.72it/s, est. speed input: 81221.11 toks/s, output: 79.32 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:05<00:09, 56.70it/s, est. speed input: 80658.91 toks/s, output: 78.77 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:05<00:09, 56.68it/s, est. speed input: 80118.65 toks/s, output: 78.24 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:09, 56.64it/s, est. speed input: 79603.90 toks/s, output: 77.74 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:09, 56.63it/s, est. speed input: 79113.94 toks/s, output: 77.26 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:06<00:09, 56.61it/s, est. speed input: 78643.75 toks/s, output: 76.80 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:06<00:09, 56.65it/s, est. speed input: 78200.36 toks/s, output: 76.37 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:06<00:09, 56.69it/s, est. speed input: 77775.64 toks/s, output: 75.95 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:06<00:08, 56.70it/s, est. speed input: 77367.73 toks/s, output: 75.55 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:06<00:08, 56.67it/s, est. speed input: 76973.64 toks/s, output: 75.17 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:08, 56.64it/s, est. speed input: 76592.01 toks/s, output: 74.80 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:07<00:08, 56.67it/s, est. speed input: 76231.66 toks/s, output: 74.44 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:07<00:08, 55.99it/s, est. speed input: 75824.70 toks/s, output: 74.05 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:07<00:08, 54.92it/s, est. speed input: 75379.42 toks/s, output: 73.61 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:07<00:08, 54.15it/s, est. speed input: 74949.05 toks/s, output: 73.19 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:07<00:08, 53.83it/s, est. speed input: 74553.94 toks/s, output: 72.81 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:07<00:08, 54.40it/s, est. speed input: 74239.35 toks/s, output: 72.50 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:07, 54.84it/s, est. speed input: 73938.99 toks/s, output: 72.21 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:08<00:07, 55.09it/s, est. speed input: 73644.34 toks/s, output: 71.92 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:08<00:07, 55.33it/s, est. speed input: 73365.35 toks/s, output: 71.65 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:08<00:07, 55.49it/s, est. speed input: 73094.73 toks/s, output: 71.38 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:08<00:07, 55.54it/s, est. speed input: 72828.45 toks/s, output: 71.12 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:08<00:07, 55.52it/s, est. speed input: 72566.60 toks/s, output: 70.87 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:08<00:07, 55.58it/s, est. speed input: 72318.66 toks/s, output: 70.62 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:06, 55.58it/s, est. speed input: 72076.04 toks/s, output: 70.39 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:09<00:06, 55.63it/s, est. speed input: 71844.68 toks/s, output: 70.16 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:09<00:06, 55.65it/s, est. speed input: 71617.17 toks/s, output: 69.94 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:09<00:06, 55.68it/s, est. speed input: 71398.98 toks/s, output: 69.73 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:09<00:06, 55.66it/s, est. speed input: 71184.85 toks/s, output: 69.52 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:09<00:06, 55.67it/s, est. speed input: 70979.14 toks/s, output: 69.31 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:09<00:06, 55.63it/s, est. speed input: 70774.47 toks/s, output: 69.12 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:10<00:05, 55.65it/s, est. speed input: 70579.62 toks/s, output: 68.93 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:10<00:05, 55.69it/s, est. speed input: 70391.50 toks/s, output: 68.74 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:10<00:05, 55.66it/s, est. speed input: 70205.22 toks/s, output: 68.56 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:10<00:05, 55.67it/s, est. speed input: 70026.37 toks/s, output: 68.38 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:10<00:05, 55.63it/s, est. speed input: 69848.86 toks/s, output: 68.21 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:10<00:05, 55.62it/s, est. speed input: 69677.77 toks/s, output: 68.04 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:10<00:04, 55.66it/s, est. speed input: 69513.11 toks/s, output: 67.88 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:11<00:04, 55.70it/s, est. speed input: 69354.51 toks/s, output: 67.73 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:11<00:04, 55.72it/s, est. speed input: 69198.27 toks/s, output: 67.58 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:11<00:04, 55.66it/s, est. speed input: 69042.58 toks/s, output: 67.42 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:11<00:04, 55.68it/s, est. speed input: 68893.46 toks/s, output: 67.28 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:11<00:04, 55.59it/s, est. speed input: 68744.44 toks/s, output: 67.13 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:11<00:04, 55.54it/s, est. speed input: 68597.43 toks/s, output: 66.99 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:11<00:03, 55.53it/s, est. speed input: 68455.81 toks/s, output: 66.85 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:12<00:03, 55.50it/s, est. speed input: 68316.87 toks/s, output: 66.72 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:12<00:03, 55.46it/s, est. speed input: 68180.16 toks/s, output: 66.58 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:12<00:03, 55.48it/s, est. speed input: 68049.19 toks/s, output: 66.45 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:12<00:03, 55.49it/s, est. speed input: 67920.36 toks/s, output: 66.33 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:12<00:03, 55.47it/s, est. speed input: 67793.73 toks/s, output: 66.20 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:12<00:03, 55.45it/s, est. speed input: 67669.70 toks/s, output: 66.08 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:13<00:02, 55.49it/s, est. speed input: 67550.53 toks/s, output: 65.97 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:13<00:02, 55.53it/s, est. speed input: 67434.77 toks/s, output: 65.85 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:13<00:02, 55.56it/s, est. speed input: 67321.37 toks/s, output: 65.74 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:13<00:02, 55.52it/s, est. speed input: 67208.22 toks/s, output: 65.63 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:13<00:02, 55.52it/s, est. speed input: 67098.15 toks/s, output: 65.53 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:13<00:02, 55.49it/s, est. speed input: 66989.47 toks/s, output: 65.42 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:13<00:02, 55.50it/s, est. speed input: 66883.79 toks/s, output: 65.32 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:14<00:01, 55.53it/s, est. speed input: 66782.11 toks/s, output: 65.22 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:14<00:01, 55.53it/s, est. speed input: 66680.82 toks/s, output: 65.12 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:14<00:01, 55.46it/s, est. speed input: 66579.06 toks/s, output: 65.02 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:14<00:01, 55.46it/s, est. speed input: 66481.59 toks/s, output: 64.92 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:14<00:01, 55.44it/s, est. speed input: 66384.61 toks/s, output: 64.83 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:14<00:01, 55.42it/s, est. speed input: 66289.69 toks/s, output: 64.74 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:14<00:01, 55.45it/s, est. speed input: 66198.35 toks/s, output: 64.65 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:15<00:00, 55.45it/s, est. speed input: 66108.02 toks/s, output: 64.56 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:15<00:00, 55.46it/s, est. speed input: 66019.55 toks/s, output: 64.47 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:15<00:00, 57.28it/s, est. speed input: 65998.65 toks/s, output: 64.45 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:15<00:00, 56.75it/s, est. speed input: 65913.72 toks/s, output: 64.37 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:15<00:00, 56.39it/s, est. speed input: 65830.33 toks/s, output: 64.29 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:15<00:00, 55.63it/s, est. speed input: 65730.55 toks/s, output: 64.19 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:15<00:00, 56.92it/s, est. speed input: 65696.08 toks/s, output: 64.16 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:15<00:00, 56.92it/s, est. speed input: 66082.02 toks/s, output: 64.53 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:15<00:00, 64.53it/s, est. speed input: 66082.02 toks/s, output: 64.53 toks/s]
[rank0]:[W128 11:30:35.528034893 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 67.0s

测试结果:
  Requests/s:   54.76
  Tokens/s:     56132.09
  Total Reqs:   1024
  Elapsed:      18.70s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     56077.32

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:30:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:30:57 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=21204) WARNING 01-28 11:31:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=21204) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=21204) WARNING 01-28 11:31:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 58.97 requests/s, 60445.26 total tokens/s, 58.97 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 11:30:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:30:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:30:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:30:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:30:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:30:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:30:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:30:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:30:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:30:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:30:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:30:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:30:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:30:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:31:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:31:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:31:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:31:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:31:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:31:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:31:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:31:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:31:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:31:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:31:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:31:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:31:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:31:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=21204) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=21204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.54it/s]
(EngineCore_DP0 pid=21204) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.53it/s]
(EngineCore_DP0 pid=21204) 
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=21204) [2026-01-28 11:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=21204) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.32it/s]
(EngineCore_DP0 pid=21204) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.03it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  7.88it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.46it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 35/2048 [00:00<00:05, 342.70it/s]
Adding requests:   3%|▎         | 71/2048 [00:00<00:05, 352.02it/s]
Adding requests:   5%|▌         | 109/2048 [00:00<00:05, 362.02it/s]
Adding requests:   7%|▋         | 147/2048 [00:00<00:05, 366.25it/s]
Adding requests:   9%|▉         | 184/2048 [00:00<00:05, 367.32it/s]
Adding requests:  11%|█         | 223/2048 [00:00<00:04, 374.03it/s]
Adding requests:  13%|█▎        | 263/2048 [00:00<00:04, 380.34it/s]
Adding requests:  15%|█▍        | 302/2048 [00:00<00:04, 378.87it/s]
Adding requests:  17%|█▋        | 341/2048 [00:00<00:04, 381.91it/s]
Adding requests:  19%|█▊        | 380/2048 [00:01<00:04, 384.12it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 381.44it/s]
Adding requests:  22%|██▏       | 458/2048 [00:01<00:04, 378.57it/s]
Adding requests:  24%|██▍       | 497/2048 [00:01<00:04, 380.17it/s]
Adding requests:  26%|██▌       | 536/2048 [00:01<00:04, 375.52it/s]
Adding requests:  28%|██▊       | 577/2048 [00:01<00:03, 383.69it/s]
Adding requests:  30%|███       | 616/2048 [00:01<00:03, 385.13it/s]
Adding requests:  32%|███▏      | 656/2048 [00:01<00:03, 388.52it/s]
Adding requests:  34%|███▍      | 697/2048 [00:01<00:03, 393.54it/s]
Adding requests:  36%|███▌      | 738/2048 [00:01<00:03, 397.00it/s]
Adding requests:  38%|███▊      | 778/2048 [00:02<00:03, 383.92it/s]
Adding requests:  40%|███▉      | 817/2048 [00:02<00:03, 381.04it/s]
Adding requests:  42%|████▏     | 856/2048 [00:02<00:03, 377.34it/s]
Adding requests:  44%|████▍     | 897/2048 [00:02<00:02, 386.39it/s]
Adding requests:  46%|████▌     | 937/2048 [00:02<00:02, 388.28it/s]
Adding requests:  48%|████▊     | 978/2048 [00:02<00:02, 392.21it/s]
Adding requests:  50%|████▉     | 1020/2048 [00:02<00:02, 397.80it/s]
Adding requests:  52%|█████▏    | 1060/2048 [00:02<00:02, 391.28it/s]
Adding requests:  54%|█████▎    | 1100/2048 [00:02<00:02, 391.68it/s]
Adding requests:  56%|█████▌    | 1140/2048 [00:02<00:02, 381.58it/s]
Adding requests:  58%|█████▊    | 1182/2048 [00:03<00:02, 390.83it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:03<00:02, 392.21it/s]
Adding requests:  62%|██████▏   | 1262/2048 [00:03<00:02, 386.56it/s]
Adding requests:  64%|██████▎   | 1303/2048 [00:03<00:01, 392.39it/s]
Adding requests:  66%|██████▌   | 1344/2048 [00:03<00:01, 395.85it/s]
Adding requests:  68%|██████▊   | 1385/2048 [00:03<00:01, 398.79it/s]
Adding requests:  70%|██████▉   | 1426/2048 [00:03<00:01, 401.30it/s]
Adding requests:  72%|███████▏  | 1468/2048 [00:03<00:01, 403.92it/s]
Adding requests:  74%|███████▎  | 1510/2048 [00:03<00:01, 407.03it/s]
Adding requests:  76%|███████▌  | 1551/2048 [00:04<00:01, 402.09it/s]
Adding requests:  78%|███████▊  | 1593/2048 [00:04<00:01, 405.02it/s]
Adding requests:  80%|███████▉  | 1634/2048 [00:04<00:01, 405.96it/s]
Adding requests:  82%|████████▏ | 1675/2048 [00:04<00:00, 396.95it/s]
Adding requests:  84%|████████▍ | 1716/2048 [00:04<00:00, 398.65it/s]
Adding requests:  86%|████████▌ | 1757/2048 [00:04<00:00, 401.08it/s]
Adding requests:  88%|████████▊ | 1798/2048 [00:04<00:00, 401.80it/s]
Adding requests:  90%|████████▉ | 1839/2048 [00:04<00:00, 403.26it/s]
Adding requests:  92%|█████████▏| 1880/2048 [00:04<00:00, 404.31it/s]
Adding requests:  94%|█████████▍| 1921/2048 [00:04<00:00, 388.42it/s]
Adding requests:  96%|█████████▌| 1962/2048 [00:05<00:00, 392.86it/s]
Adding requests:  98%|█████████▊| 2003/2048 [00:05<00:00, 396.54it/s]
Adding requests: 100%|█████████▉| 2043/2048 [00:05<00:00, 396.22it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 389.80it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:00<00:00, 2666.16it/s, est. speed input: 2730670.02 toks/s, output: 2666.33 toks/s]
Processed prompts:  27%|██▋       | 557/2048 [00:04<00:13, 108.42it/s, est. speed input: 130584.59 toks/s, output: 127.52 toks/s]   
Processed prompts:  33%|███▎      | 671/2048 [00:06<00:15, 90.71it/s, est. speed input: 110526.07 toks/s, output: 107.94 toks/s] 
Processed prompts:  36%|███▌      | 736/2048 [00:07<00:15, 83.77it/s, est. speed input: 103580.16 toks/s, output: 101.15 toks/s]
Processed prompts:  38%|███▊      | 778/2048 [00:08<00:16, 77.54it/s, est. speed input: 98728.12 toks/s, output: 96.41 toks/s]  
Processed prompts:  39%|███▉      | 807/2048 [00:08<00:16, 73.86it/s, est. speed input: 96102.09 toks/s, output: 93.85 toks/s]
Processed prompts:  40%|████      | 828/2048 [00:08<00:16, 74.42it/s, est. speed input: 95655.50 toks/s, output: 93.41 toks/s]
Processed prompts:  41%|████▏     | 845/2048 [00:09<00:16, 73.11it/s, est. speed input: 94789.44 toks/s, output: 92.57 toks/s]
Processed prompts:  42%|████▏     | 859/2048 [00:09<00:17, 69.82it/s, est. speed input: 93608.08 toks/s, output: 91.41 toks/s]
Processed prompts:  42%|████▏     | 870/2048 [00:09<00:18, 64.32it/s, est. speed input: 92127.64 toks/s, output: 89.97 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:09<00:19, 60.04it/s, est. speed input: 90832.56 toks/s, output: 88.70 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:10<00:19, 59.69it/s, est. speed input: 90005.29 toks/s, output: 87.90 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:10<00:19, 59.40it/s, est. speed input: 89220.78 toks/s, output: 87.13 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:10<00:18, 59.17it/s, est. speed input: 88477.29 toks/s, output: 86.40 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:11<00:18, 59.01it/s, est. speed input: 87772.14 toks/s, output: 85.71 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:11<00:18, 58.86it/s, est. speed input: 87096.73 toks/s, output: 85.06 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:11<00:17, 59.65it/s, est. speed input: 86562.26 toks/s, output: 84.53 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:11<00:17, 59.30it/s, est. speed input: 85945.26 toks/s, output: 83.93 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:12<00:17, 59.06it/s, est. speed input: 85356.42 toks/s, output: 83.36 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:12<00:17, 58.90it/s, est. speed input: 84795.36 toks/s, output: 82.81 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:12<00:17, 58.75it/s, est. speed input: 84254.66 toks/s, output: 82.28 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:12<00:16, 58.66it/s, est. speed input: 83738.27 toks/s, output: 81.78 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:13<00:16, 58.59it/s, est. speed input: 83242.40 toks/s, output: 81.29 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:13<00:16, 58.54it/s, est. speed input: 82766.71 toks/s, output: 80.83 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:13<00:16, 58.52it/s, est. speed input: 82311.24 toks/s, output: 80.38 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:14<00:15, 58.50it/s, est. speed input: 81873.84 toks/s, output: 79.95 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:14<00:15, 58.48it/s, est. speed input: 81452.22 toks/s, output: 79.54 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:14<00:15, 59.43it/s, est. speed input: 81128.77 toks/s, output: 79.23 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:14<00:14, 59.11it/s, est. speed input: 80733.84 toks/s, output: 78.84 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:15<00:14, 58.90it/s, est. speed input: 80354.64 toks/s, output: 78.47 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:15<00:14, 58.76it/s, est. speed input: 79989.44 toks/s, output: 78.11 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:15<00:14, 58.65it/s, est. speed input: 79636.65 toks/s, output: 77.77 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:15<00:13, 58.58it/s, est. speed input: 79295.93 toks/s, output: 77.44 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:16<00:13, 58.53it/s, est. speed input: 78966.65 toks/s, output: 77.12 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:16<00:13, 58.48it/s, est. speed input: 78647.46 toks/s, output: 76.80 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:16<00:13, 58.48it/s, est. speed input: 78340.73 toks/s, output: 76.50 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:17<00:12, 58.49it/s, est. speed input: 78045.08 toks/s, output: 76.22 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:17<00:12, 58.49it/s, est. speed input: 77758.43 toks/s, output: 75.94 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:17<00:12, 58.82it/s, est. speed input: 77502.49 toks/s, output: 75.69 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:17<00:11, 59.05it/s, est. speed input: 77254.46 toks/s, output: 75.44 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:18<00:11, 59.20it/s, est. speed input: 77013.54 toks/s, output: 75.21 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:18<00:11, 59.31it/s, est. speed input: 76779.21 toks/s, output: 74.98 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:18<00:11, 59.38it/s, est. speed input: 76551.53 toks/s, output: 74.76 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:18<00:10, 59.43it/s, est. speed input: 76330.47 toks/s, output: 74.54 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:19<00:10, 59.49it/s, est. speed input: 76116.60 toks/s, output: 74.33 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:19<00:10, 59.50it/s, est. speed input: 75906.89 toks/s, output: 74.13 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:19<00:09, 59.49it/s, est. speed input: 75702.34 toks/s, output: 73.93 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:19<00:09, 59.51it/s, est. speed input: 75504.54 toks/s, output: 73.73 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:20<00:09, 59.51it/s, est. speed input: 75310.99 toks/s, output: 73.55 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:20<00:09, 59.49it/s, est. speed input: 75121.85 toks/s, output: 73.36 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:20<00:08, 59.49it/s, est. speed input: 74938.10 toks/s, output: 73.18 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:21<00:08, 59.49it/s, est. speed input: 74759.28 toks/s, output: 73.01 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:21<00:08, 59.48it/s, est. speed input: 74584.36 toks/s, output: 72.84 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:21<00:08, 59.47it/s, est. speed input: 74413.37 toks/s, output: 72.67 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:21<00:07, 59.47it/s, est. speed input: 74247.11 toks/s, output: 72.51 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:22<00:07, 59.48it/s, est. speed input: 74085.33 toks/s, output: 72.35 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:24<00:22, 19.01it/s, est. speed input: 68125.76 toks/s, output: 66.53 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:24<00:17, 23.89it/s, est. speed input: 68046.69 toks/s, output: 66.45 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:24<00:13, 29.11it/s, est. speed input: 67969.32 toks/s, output: 66.38 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:25<00:11, 34.37it/s, est. speed input: 67892.59 toks/s, output: 66.30 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:25<00:09, 39.35it/s, est. speed input: 67818.33 toks/s, output: 66.23 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:25<00:07, 43.80it/s, est. speed input: 67746.02 toks/s, output: 66.16 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:25<00:07, 47.56it/s, est. speed input: 67675.36 toks/s, output: 66.09 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:26<00:06, 50.60it/s, est. speed input: 67605.75 toks/s, output: 66.02 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:26<00:05, 52.96it/s, est. speed input: 67537.01 toks/s, output: 65.95 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:26<00:05, 54.75it/s, est. speed input: 67469.96 toks/s, output: 65.89 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:27<00:04, 55.76it/s, est. speed input: 67390.71 toks/s, output: 65.81 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:27<00:04, 56.03it/s, est. speed input: 67293.88 toks/s, output: 65.72 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:27<00:04, 56.72it/s, est. speed input: 67219.25 toks/s, output: 65.64 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:27<00:03, 57.20it/s, est. speed input: 67145.62 toks/s, output: 65.57 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:28<00:03, 57.53it/s, est. speed input: 67073.20 toks/s, output: 65.50 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:28<00:03, 57.76it/s, est. speed input: 67002.15 toks/s, output: 65.43 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:28<00:02, 58.88it/s, est. speed input: 66967.25 toks/s, output: 65.40 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:28<00:02, 58.72it/s, est. speed input: 66898.90 toks/s, output: 65.33 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:29<00:02, 58.59it/s, est. speed input: 66831.22 toks/s, output: 65.26 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:29<00:02, 58.52it/s, est. speed input: 66765.34 toks/s, output: 65.20 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:29<00:01, 58.47it/s, est. speed input: 66700.73 toks/s, output: 65.14 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:30<00:01, 58.43it/s, est. speed input: 66637.07 toks/s, output: 65.08 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:30<00:01, 58.41it/s, est. speed input: 66574.92 toks/s, output: 65.01 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:30<00:01, 58.40it/s, est. speed input: 66513.83 toks/s, output: 64.95 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:30<00:00, 58.37it/s, est. speed input: 66453.19 toks/s, output: 64.90 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:31<00:00, 58.36it/s, est. speed input: 66394.02 toks/s, output: 64.84 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:31<00:00, 59.48it/s, est. speed input: 66372.49 toks/s, output: 64.82 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:31<00:00, 59.48it/s, est. speed input: 66828.52 toks/s, output: 65.26 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:31<00:00, 65.26it/s, est. speed input: 66828.52 toks/s, output: 65.26 toks/s]
[rank0]:[W128 11:32:05.759448831 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 90.0s

测试结果:
  Requests/s:   58.97
  Tokens/s:     60445.26
  Total Reqs:   2048
  Elapsed:      34.73s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     60386.28

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:32:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:32:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=22060) WARNING 01-28 11:32:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=22060) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=22060) WARNING 01-28 11:32:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 58.89 requests/s, 60364.02 total tokens/s, 58.89 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 11:32:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:32:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:32:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:32:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:32:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:32:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:32:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:32:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:32:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:32:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:32:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:32:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:32:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:32:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:32:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:32:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:32:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:32:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:32:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=22060) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=22060) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=22060) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=22060) 
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=22060) [2026-01-28 11:32:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=22060) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:02<00:08,  1.00s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:02<00:04,  1.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:02<00:02,  2.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:02<00:01,  2.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:02<00:01,  3.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:02<00:00,  4.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:02<00:00,  5.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:03<00:00,  6.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:03<00:00,  6.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:03<00:00,  3.41it/s]
(EngineCore_DP0 pid=22060) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.28it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.01it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.01it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:12, 335.65it/s]
Adding requests:   2%|▏         | 70/4096 [00:00<00:11, 348.51it/s]
Adding requests:   3%|▎         | 109/4096 [00:00<00:10, 363.51it/s]
Adding requests:   4%|▎         | 146/4096 [00:00<00:10, 361.31it/s]
Adding requests:   4%|▍         | 183/4096 [00:00<00:10, 362.43it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:10, 363.50it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:10, 370.69it/s]
Adding requests:   7%|▋         | 297/4096 [00:00<00:10, 372.50it/s]
Adding requests:   8%|▊         | 335/4096 [00:00<00:10, 368.71it/s]
Adding requests:   9%|▉         | 374/4096 [00:01<00:09, 375.10it/s]
Adding requests:  10%|█         | 412/4096 [00:01<00:09, 376.22it/s]
Adding requests:  11%|█         | 450/4096 [00:01<00:09, 375.23it/s]
Adding requests:  12%|█▏        | 489/4096 [00:01<00:09, 377.37it/s]
Adding requests:  13%|█▎        | 527/4096 [00:01<00:09, 367.39it/s]
Adding requests:  14%|█▍        | 567/4096 [00:01<00:09, 376.45it/s]
Adding requests:  15%|█▍        | 605/4096 [00:01<00:09, 377.19it/s]
Adding requests:  16%|█▌        | 646/4096 [00:01<00:08, 384.82it/s]
Adding requests:  17%|█▋        | 686/4096 [00:01<00:08, 386.94it/s]
Adding requests:  18%|█▊        | 725/4096 [00:01<00:08, 386.86it/s]
Adding requests:  19%|█▊        | 764/4096 [00:02<00:08, 384.86it/s]
Adding requests:  20%|█▉        | 803/4096 [00:02<00:08, 381.79it/s]
Adding requests:  21%|██        | 842/4096 [00:02<00:08, 377.48it/s]
Adding requests:  22%|██▏       | 883/4096 [00:02<00:08, 385.10it/s]
Adding requests:  23%|██▎       | 923/4096 [00:02<00:08, 388.52it/s]
Adding requests:  23%|██▎       | 962/4096 [00:02<00:08, 388.59it/s]
Adding requests:  24%|██▍       | 1002/4096 [00:02<00:07, 390.24it/s]
Adding requests:  25%|██▌       | 1043/4096 [00:02<00:07, 393.72it/s]
Adding requests:  26%|██▋       | 1083/4096 [00:02<00:07, 392.22it/s]
Adding requests:  27%|██▋       | 1123/4096 [00:02<00:07, 390.25it/s]
Adding requests:  28%|██▊       | 1163/4096 [00:03<00:07, 392.73it/s]
Adding requests:  29%|██▉       | 1203/4096 [00:03<00:07, 384.32it/s]
Adding requests:  30%|███       | 1242/4096 [00:03<00:07, 376.09it/s]
Adding requests:  31%|███▏      | 1280/4096 [00:03<00:07, 376.21it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:03<00:07, 379.89it/s]
Adding requests:  33%|███▎      | 1359/4096 [00:03<00:07, 379.98it/s]
Adding requests:  34%|███▍      | 1400/4096 [00:03<00:06, 387.58it/s]
Adding requests:  35%|███▌      | 1440/4096 [00:03<00:06, 390.67it/s]
Adding requests:  36%|███▌      | 1480/4096 [00:03<00:06, 388.44it/s]
Adding requests:  37%|███▋      | 1521/4096 [00:03<00:06, 392.61it/s]
Adding requests:  38%|███▊      | 1562/4096 [00:04<00:06, 394.63it/s]
Adding requests:  39%|███▉      | 1602/4096 [00:04<00:06, 395.97it/s]
Adding requests:  40%|████      | 1642/4096 [00:04<00:06, 394.19it/s]
Adding requests:  41%|████      | 1682/4096 [00:04<00:06, 389.97it/s]
Adding requests:  42%|████▏     | 1723/4096 [00:04<00:06, 394.06it/s]
Adding requests:  43%|████▎     | 1763/4096 [00:04<00:05, 394.35it/s]
Adding requests:  44%|████▍     | 1803/4096 [00:04<00:05, 392.64it/s]
Adding requests:  45%|████▌     | 1844/4096 [00:04<00:05, 396.45it/s]
Adding requests:  46%|████▌     | 1884/4096 [00:04<00:05, 392.98it/s]
Adding requests:  47%|████▋     | 1924/4096 [00:05<00:05, 394.27it/s]
Adding requests:  48%|████▊     | 1964/4096 [00:05<00:05, 391.84it/s]
Adding requests:  49%|████▉     | 2005/4096 [00:05<00:05, 395.05it/s]
Adding requests:  50%|████▉     | 2045/4096 [00:05<00:05, 391.13it/s]
Adding requests:  51%|█████     | 2085/4096 [00:05<00:05, 389.83it/s]
Adding requests:  52%|█████▏    | 2124/4096 [00:05<00:05, 388.90it/s]
Adding requests:  53%|█████▎    | 2163/4096 [00:05<00:05, 384.80it/s]
Adding requests:  54%|█████▍    | 2202/4096 [00:05<00:04, 384.12it/s]
Adding requests:  55%|█████▍    | 2242/4096 [00:05<00:04, 388.48it/s]
Adding requests:  56%|█████▌    | 2281/4096 [00:05<00:04, 386.08it/s]
Adding requests:  57%|█████▋    | 2321/4096 [00:06<00:04, 387.41it/s]
Adding requests:  58%|█████▊    | 2360/4096 [00:06<00:04, 383.83it/s]
Adding requests:  59%|█████▊    | 2399/4096 [00:06<00:04, 372.77it/s]
Adding requests:  59%|█████▉    | 2437/4096 [00:06<00:04, 372.02it/s]
Adding requests:  60%|██████    | 2476/4096 [00:06<00:04, 376.06it/s]
Adding requests:  61%|██████▏   | 2515/4096 [00:06<00:04, 378.83it/s]
Adding requests:  62%|██████▏   | 2555/4096 [00:06<00:04, 384.80it/s]
Adding requests:  63%|██████▎   | 2596/4096 [00:06<00:03, 390.17it/s]
Adding requests:  64%|██████▍   | 2637/4096 [00:06<00:03, 394.91it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:06<00:03, 397.00it/s]
Adding requests:  66%|██████▋   | 2718/4096 [00:07<00:03, 397.49it/s]
Adding requests:  67%|██████▋   | 2758/4096 [00:07<00:03, 398.12it/s]
Adding requests:  68%|██████▊   | 2798/4096 [00:07<00:03, 397.65it/s]
Adding requests:  69%|██████▉   | 2838/4096 [00:07<00:03, 396.18it/s]
Adding requests:  70%|███████   | 2878/4096 [00:07<00:03, 395.57it/s]
Adding requests:  71%|███████▏  | 2919/4096 [00:07<00:02, 399.09it/s]
Adding requests:  72%|███████▏  | 2959/4096 [00:07<00:02, 396.37it/s]
Adding requests:  73%|███████▎  | 3000/4096 [00:07<00:02, 398.03it/s]
Adding requests:  74%|███████▍  | 3041/4096 [00:07<00:02, 399.87it/s]
Adding requests:  75%|███████▌  | 3081/4096 [00:07<00:02, 391.38it/s]
Adding requests:  76%|███████▌  | 3122/4096 [00:08<00:02, 396.03it/s]
Adding requests:  77%|███████▋  | 3162/4096 [00:08<00:02, 396.46it/s]
Adding requests:  78%|███████▊  | 3202/4096 [00:08<00:02, 395.28it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:08<00:02, 398.47it/s]
Adding requests:  80%|████████  | 3284/4096 [00:08<00:02, 399.17it/s]
Adding requests:  81%|████████  | 3324/4096 [00:08<00:01, 396.49it/s]
Adding requests:  82%|████████▏ | 3366/4096 [00:08<00:01, 402.96it/s]
Adding requests:  83%|████████▎ | 3407/4096 [00:08<00:01, 401.55it/s]
Adding requests:  84%|████████▍ | 3448/4096 [00:08<00:01, 393.73it/s]
Adding requests:  85%|████████▌ | 3488/4096 [00:09<00:01, 387.63it/s]
Adding requests:  86%|████████▌ | 3527/4096 [00:09<00:01, 387.00it/s]
Adding requests:  87%|████████▋ | 3566/4096 [00:09<00:01, 386.31it/s]
Adding requests:  88%|████████▊ | 3607/4096 [00:09<00:01, 392.44it/s]
Adding requests:  89%|████████▉ | 3647/4096 [00:09<00:01, 389.79it/s]
Adding requests:  90%|█████████ | 3687/4096 [00:09<00:01, 392.50it/s]
Adding requests:  91%|█████████ | 3727/4096 [00:09<00:00, 393.88it/s]
Adding requests:  92%|█████████▏| 3767/4096 [00:09<00:00, 384.32it/s]
Adding requests:  93%|█████████▎| 3809/4096 [00:09<00:00, 392.52it/s]
Adding requests:  94%|█████████▍| 3850/4096 [00:09<00:00, 396.68it/s]
Adding requests:  95%|█████████▍| 3890/4096 [00:10<00:00, 393.27it/s]
Adding requests:  96%|█████████▌| 3931/4096 [00:10<00:00, 396.40it/s]
Adding requests:  97%|█████████▋| 3972/4096 [00:10<00:00, 398.16it/s]
Adding requests:  98%|█████████▊| 4013/4096 [00:10<00:00, 400.17it/s]
Adding requests:  99%|█████████▉| 4054/4096 [00:10<00:00, 397.34it/s]
Adding requests: 100%|█████████▉| 4095/4096 [00:10<00:00, 398.46it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 388.07it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:00<00:03, 1146.38it/s, est. speed input: 1174016.29 toks/s, output: 1146.39 toks/s]
Processed prompts:  18%|█▊        | 725/4096 [00:02<00:12, 275.91it/s, est. speed input: 349522.08 toks/s, output: 341.33 toks/s]   
Processed prompts:  19%|█▉        | 777/4096 [00:03<00:18, 177.14it/s, est. speed input: 249860.86 toks/s, output: 244.00 toks/s]
Processed prompts:  20%|█▉        | 808/4096 [00:03<00:22, 148.81it/s, est. speed input: 222587.91 toks/s, output: 217.37 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:04<00:26, 123.37it/s, est. speed input: 200905.54 toks/s, output: 196.20 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:04<00:30, 105.89it/s, est. speed input: 184908.02 toks/s, output: 180.57 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:05<00:34, 92.75it/s, est. speed input: 172181.10 toks/s, output: 168.14 toks/s] 
Processed prompts:  23%|██▎       | 930/4096 [00:05<00:38, 83.05it/s, est. speed input: 161799.42 toks/s, output: 158.01 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:06<00:40, 76.50it/s, est. speed input: 153467.12 toks/s, output: 149.87 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:06<00:43, 71.20it/s, est. speed input: 146122.97 toks/s, output: 142.70 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:07<00:45, 67.50it/s, est. speed input: 139873.31 toks/s, output: 136.59 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:08<00:46, 64.85it/s, est. speed input: 134460.74 toks/s, output: 131.31 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:08<00:47, 63.01it/s, est. speed input: 129742.48 toks/s, output: 126.70 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:09<00:48, 61.71it/s, est. speed input: 125586.35 toks/s, output: 122.64 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:09<00:47, 61.35it/s, est. speed input: 122106.50 toks/s, output: 119.24 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:10<00:48, 60.54it/s, est. speed input: 118797.10 toks/s, output: 116.01 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:10<00:47, 59.98it/s, est. speed input: 115821.71 toks/s, output: 113.11 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:11<00:47, 59.58it/s, est. speed input: 113134.26 toks/s, output: 110.48 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:11<00:47, 59.29it/s, est. speed input: 110688.51 toks/s, output: 108.09 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:12<00:47, 59.18it/s, est. speed input: 108483.39 toks/s, output: 105.94 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:12<00:46, 59.20it/s, est. speed input: 106487.40 toks/s, output: 103.99 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:13<00:45, 59.21it/s, est. speed input: 104651.94 toks/s, output: 102.20 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:14<00:45, 59.22it/s, est. speed input: 102956.70 toks/s, output: 100.54 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:14<00:44, 59.22it/s, est. speed input: 101386.71 toks/s, output: 99.01 toks/s] 
Processed prompts:  36%|███▌      | 1474/4096 [00:15<00:44, 59.22it/s, est. speed input: 99929.14 toks/s, output: 97.59 toks/s] 
Processed prompts:  37%|███▋      | 1506/4096 [00:15<00:43, 59.23it/s, est. speed input: 98573.50 toks/s, output: 96.26 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:16<00:43, 59.23it/s, est. speed input: 97307.86 toks/s, output: 95.03 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:16<00:42, 59.23it/s, est. speed input: 96123.12 toks/s, output: 93.87 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:17<00:42, 59.23it/s, est. speed input: 95013.02 toks/s, output: 92.79 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:17<00:41, 59.16it/s, est. speed input: 93960.04 toks/s, output: 91.76 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:18<00:41, 59.10it/s, est. speed input: 92966.01 toks/s, output: 90.79 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:20<01:24, 28.41it/s, est. speed input: 83423.88 toks/s, output: 81.47 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:21<01:10, 33.66it/s, est. speed input: 82848.94 toks/s, output: 80.91 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:21<01:00, 38.66it/s, est. speed input: 82299.29 toks/s, output: 80.37 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:22<00:53, 43.06it/s, est. speed input: 81759.02 toks/s, output: 79.84 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:23<00:48, 46.72it/s, est. speed input: 81232.06 toks/s, output: 79.33 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:23<00:44, 50.03it/s, est. speed input: 80782.52 toks/s, output: 78.89 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:24<00:42, 52.25it/s, est. speed input: 80302.04 toks/s, output: 78.42 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:24<00:40, 53.92it/s, est. speed input: 79841.65 toks/s, output: 77.97 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:25<00:38, 55.16it/s, est. speed input: 79402.46 toks/s, output: 77.54 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:25<00:37, 56.05it/s, est. speed input: 78980.49 toks/s, output: 77.13 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:26<00:36, 56.69it/s, est. speed input: 78576.50 toks/s, output: 76.73 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:26<00:35, 57.15it/s, est. speed input: 78189.09 toks/s, output: 76.36 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:27<00:35, 57.48it/s, est. speed input: 77817.72 toks/s, output: 75.99 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:27<00:34, 57.72it/s, est. speed input: 77461.48 toks/s, output: 75.65 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:28<00:33, 57.88it/s, est. speed input: 77118.52 toks/s, output: 75.31 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:29<00:33, 57.99it/s, est. speed input: 76787.60 toks/s, output: 74.99 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:29<00:32, 58.06it/s, est. speed input: 76469.38 toks/s, output: 74.68 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:30<00:31, 58.15it/s, est. speed input: 76165.46 toks/s, output: 74.38 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:30<00:30, 59.19it/s, est. speed input: 75947.36 toks/s, output: 74.17 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:31<00:30, 59.44it/s, est. speed input: 75700.15 toks/s, output: 73.93 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:31<00:29, 59.61it/s, est. speed input: 75460.23 toks/s, output: 73.69 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:32<00:28, 59.73it/s, est. speed input: 75229.09 toks/s, output: 73.47 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:32<00:28, 59.82it/s, est. speed input: 75005.06 toks/s, output: 73.25 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:33<00:27, 59.87it/s, est. speed input: 74787.81 toks/s, output: 73.03 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:33<00:27, 59.90it/s, est. speed input: 74577.19 toks/s, output: 72.83 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:34<00:26, 60.46it/s, est. speed input: 74407.18 toks/s, output: 72.66 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:34<00:25, 60.33it/s, est. speed input: 74209.43 toks/s, output: 72.47 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:35<00:25, 60.23it/s, est. speed input: 74017.34 toks/s, output: 72.28 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:35<00:24, 60.16it/s, est. speed input: 73831.08 toks/s, output: 72.10 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:36<00:24, 60.11it/s, est. speed input: 73649.78 toks/s, output: 71.92 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:37<00:23, 60.07it/s, est. speed input: 73473.88 toks/s, output: 71.75 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:37<00:23, 60.05it/s, est. speed input: 73303.04 toks/s, output: 71.58 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:38<00:22, 60.03it/s, est. speed input: 73136.97 toks/s, output: 71.42 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:38<00:22, 59.61it/s, est. speed input: 72952.29 toks/s, output: 71.24 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:39<00:22, 59.34it/s, est. speed input: 72773.76 toks/s, output: 71.07 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:39<00:21, 59.13it/s, est. speed input: 72598.99 toks/s, output: 70.90 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:40<00:21, 58.98it/s, est. speed input: 72428.80 toks/s, output: 70.73 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:40<00:20, 58.88it/s, est. speed input: 72263.63 toks/s, output: 70.57 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:41<00:20, 58.82it/s, est. speed input: 72103.15 toks/s, output: 70.41 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:41<00:19, 58.78it/s, est. speed input: 71946.85 toks/s, output: 70.26 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:42<00:19, 58.74it/s, est. speed input: 71794.34 toks/s, output: 70.11 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:43<00:18, 58.72it/s, est. speed input: 71645.71 toks/s, output: 69.97 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:43<00:17, 58.70it/s, est. speed input: 71500.83 toks/s, output: 69.82 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:44<00:17, 58.69it/s, est. speed input: 71359.48 toks/s, output: 69.69 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:44<00:16, 58.68it/s, est. speed input: 71221.31 toks/s, output: 69.55 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:45<00:16, 58.67it/s, est. speed input: 71086.74 toks/s, output: 69.42 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:45<00:15, 58.67it/s, est. speed input: 70955.41 toks/s, output: 69.29 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:46<00:15, 58.70it/s, est. speed input: 70828.44 toks/s, output: 69.17 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:46<00:14, 58.83it/s, est. speed input: 70709.73 toks/s, output: 69.05 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:47<00:14, 58.94it/s, est. speed input: 70594.54 toks/s, output: 68.94 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:47<00:13, 58.99it/s, est. speed input: 70481.07 toks/s, output: 68.83 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:48<00:12, 59.04it/s, est. speed input: 70370.22 toks/s, output: 68.72 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:48<00:12, 59.06it/s, est. speed input: 70261.64 toks/s, output: 68.61 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:49<00:11, 59.10it/s, est. speed input: 70156.29 toks/s, output: 68.51 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:50<00:11, 59.12it/s, est. speed input: 70052.97 toks/s, output: 68.41 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:50<00:10, 59.14it/s, est. speed input: 69951.95 toks/s, output: 68.31 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:51<00:10, 59.15it/s, est. speed input: 69853.17 toks/s, output: 68.22 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:51<00:09, 59.16it/s, est. speed input: 69756.24 toks/s, output: 68.12 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:52<00:09, 59.17it/s, est. speed input: 69661.92 toks/s, output: 68.03 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:52<00:08, 59.17it/s, est. speed input: 69568.93 toks/s, output: 67.94 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:55<00:16, 28.36it/s, est. speed input: 67016.23 toks/s, output: 65.45 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:55<00:13, 33.61it/s, est. speed input: 66953.83 toks/s, output: 65.38 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:56<00:10, 38.55it/s, est. speed input: 66887.79 toks/s, output: 65.32 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:56<00:08, 43.20it/s, est. speed input: 66837.51 toks/s, output: 65.27 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:57<00:07, 46.83it/s, est. speed input: 66769.25 toks/s, output: 65.20 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:57<00:06, 49.77it/s, est. speed input: 66702.05 toks/s, output: 65.14 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:58<00:05, 52.06it/s, est. speed input: 66636.63 toks/s, output: 65.07 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:59<00:04, 53.78it/s, est. speed input: 66571.83 toks/s, output: 65.01 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:59<00:04, 55.05it/s, est. speed input: 66508.36 toks/s, output: 64.95 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:00<00:03, 55.97it/s, est. speed input: 66445.77 toks/s, output: 64.89 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:00<00:02, 56.65it/s, est. speed input: 66384.58 toks/s, output: 64.83 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:01<00:02, 57.13it/s, est. speed input: 66324.54 toks/s, output: 64.77 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:01<00:01, 57.48it/s, est. speed input: 66265.94 toks/s, output: 64.71 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:02<00:01, 58.22it/s, est. speed input: 66224.86 toks/s, output: 64.67 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:02<00:00, 58.91it/s, est. speed input: 66189.89 toks/s, output: 64.64 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:02<00:00, 58.91it/s, est. speed input: 66677.73 toks/s, output: 65.11 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:02<00:00, 65.11it/s, est. speed input: 66677.73 toks/s, output: 65.11 toks/s]
[rank0]:[W128 11:34:23.587381967 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 138.6s

测试结果:
  Requests/s:   58.89
  Tokens/s:     60364.02
  Total Reqs:   4096
  Elapsed:      69.55s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     60305.13


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,16.7040,8569.1519,7.6628
1024,1024,1,128,128,16.7371,17155.5756,7.6477
2048,1024,2,256,128,32.9138,33736.6717,7.7779
4096,1024,4,512,128,56.6554,58071.7878,9.0371
8192,1024,8,1024,128,54.7630,56132.0866,18.6988
16384,1024,16,2048,128,58.9710,60445.2554,34.7289
32768,1024,32,4096,128,58.8917,60364.0182,69.5514

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:34:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:34:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23050) WARNING 01-28 11:34:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23050) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23050) WARNING 01-28 11:34:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.94 requests/s, 8688.70 total tokens/s, 16.94 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:34:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:34:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:34:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:34:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:34:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:34:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:34:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:34:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:34:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:34:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:34:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:34:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:34:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:34:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:34:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:34:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:34:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=23050) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=23050) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.09s/it]
(EngineCore_DP0 pid=23050) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.09s/it]
(EngineCore_DP0 pid=23050) 
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=23050) [2026-01-28 11:34:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=23050) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.68it/s]
(EngineCore_DP0 pid=23050) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 67/128 [00:00<00:00, 669.51it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 690.65it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 38.28it/s, est. speed input: 19605.95 toks/s, output: 38.29 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.75it/s, est. speed input: 11907.22 toks/s, output: 23.25 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:06, 19.42it/s, est. speed input: 10731.95 toks/s, output: 20.96 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 18.37it/s, est. speed input: 10180.19 toks/s, output: 19.88 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 18.03it/s, est. speed input: 9976.21 toks/s, output: 19.48 toks/s] 
Processed prompts:  14%|█▍        | 18/128 [00:00<00:06, 17.74it/s, est. speed input: 9817.51 toks/s, output: 19.17 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.55it/s, est. speed input: 9696.09 toks/s, output: 18.94 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.42it/s, est. speed input: 9602.86 toks/s, output: 18.76 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:05, 17.37it/s, est. speed input: 9533.13 toks/s, output: 18.62 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:05, 17.35it/s, est. speed input: 9477.31 toks/s, output: 18.51 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 17.31it/s, est. speed input: 9427.46 toks/s, output: 18.41 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 17.26it/s, est. speed input: 9381.28 toks/s, output: 18.32 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 17.26it/s, est. speed input: 9344.67 toks/s, output: 18.25 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 17.18it/s, est. speed input: 9304.65 toks/s, output: 18.17 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:05, 17.11it/s, est. speed input: 9266.94 toks/s, output: 18.10 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 17.13it/s, est. speed input: 9240.79 toks/s, output: 18.05 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 17.16it/s, est. speed input: 9218.73 toks/s, output: 18.01 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 17.18it/s, est. speed input: 9199.51 toks/s, output: 17.97 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:04, 17.07it/s, est. speed input: 9170.38 toks/s, output: 17.91 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 17.06it/s, est. speed input: 9150.26 toks/s, output: 17.87 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 17.10it/s, est. speed input: 9134.93 toks/s, output: 17.84 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 17.09it/s, est. speed input: 9118.51 toks/s, output: 17.81 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:04, 17.05it/s, est. speed input: 9100.73 toks/s, output: 17.77 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 17.03it/s, est. speed input: 9084.94 toks/s, output: 17.74 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 17.03it/s, est. speed input: 9071.66 toks/s, output: 17.72 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 17.03it/s, est. speed input: 9059.11 toks/s, output: 17.69 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:03, 17.04it/s, est. speed input: 9047.93 toks/s, output: 17.67 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 17.04it/s, est. speed input: 9036.88 toks/s, output: 17.65 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 17.07it/s, est. speed input: 9028.55 toks/s, output: 17.63 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 17.11it/s, est. speed input: 9021.87 toks/s, output: 17.62 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 17.08it/s, est. speed input: 9012.38 toks/s, output: 17.60 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:03<00:03, 17.03it/s, est. speed input: 9001.66 toks/s, output: 17.58 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 17.07it/s, est. speed input: 8995.87 toks/s, output: 17.57 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 17.15it/s, est. speed input: 8992.47 toks/s, output: 17.56 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 17.21it/s, est. speed input: 8989.56 toks/s, output: 17.56 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:02, 17.23it/s, est. speed input: 8985.94 toks/s, output: 17.55 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 17.14it/s, est. speed input: 8977.71 toks/s, output: 17.53 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 17.13it/s, est. speed input: 8972.12 toks/s, output: 17.52 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 17.15it/s, est. speed input: 8968.04 toks/s, output: 17.52 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:04<00:02, 17.07it/s, est. speed input: 8960.22 toks/s, output: 17.50 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 17.08it/s, est. speed input: 8955.40 toks/s, output: 17.49 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 17.12it/s, est. speed input: 8952.27 toks/s, output: 17.48 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 17.14it/s, est. speed input: 8948.93 toks/s, output: 17.48 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:01, 17.20it/s, est. speed input: 8947.32 toks/s, output: 17.48 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 17.23it/s, est. speed input: 8945.36 toks/s, output: 17.47 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 17.24it/s, est. speed input: 8943.16 toks/s, output: 17.47 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 17.24it/s, est. speed input: 8940.99 toks/s, output: 17.46 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:05<00:01, 17.05it/s, est. speed input: 8931.91 toks/s, output: 17.45 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:05<00:01, 16.94it/s, est. speed input: 8924.12 toks/s, output: 17.43 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 16.91it/s, est. speed input: 8918.15 toks/s, output: 17.42 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 16.99it/s, est. speed input: 8915.89 toks/s, output: 17.41 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 17.06it/s, est. speed input: 8914.23 toks/s, output: 17.41 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 17.07it/s, est. speed input: 8911.18 toks/s, output: 17.40 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 17.02it/s, est. speed input: 8906.67 toks/s, output: 17.40 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 17.04it/s, est. speed input: 8903.72 toks/s, output: 17.39 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:06<00:00, 17.08it/s, est. speed input: 8901.96 toks/s, output: 17.39 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:06<00:00, 17.04it/s, est. speed input: 8898.03 toks/s, output: 17.38 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 17.04it/s, est. speed input: 8895.09 toks/s, output: 17.37 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 17.09it/s, est. speed input: 8893.67 toks/s, output: 17.37 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 17.14it/s, est. speed input: 8892.84 toks/s, output: 17.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.16it/s, est. speed input: 8891.59 toks/s, output: 17.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.16it/s, est. speed input: 8891.59 toks/s, output: 17.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.37it/s, est. speed input: 8891.59 toks/s, output: 17.37 toks/s]
[rank0]:[W128 11:35:13.852533854 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.1s

测试结果:
  Requests/s:   16.94
  Tokens/s:     8688.70
  Total Reqs:   128
  Elapsed:      7.56s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8671.76

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:35:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:35:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23588) WARNING 01-28 11:35:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=23588) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23588) WARNING 01-28 11:35:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.27 requests/s, 16681.55 total tokens/s, 16.27 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:35:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:35:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:35:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:35:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:35:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:35:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:35:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:35:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:35:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:35:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:35:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:35:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:35:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:35:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:35:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:35:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:35:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=23588) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=23588) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=23588) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=23588) 
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=23588) [2026-01-28 11:35:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=23588) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.98it/s]
(EngineCore_DP0 pid=23588) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 369.80it/s]
Adding requests:  59%|█████▉    | 76/128 [00:00<00:00, 380.59it/s]
Adding requests:  90%|████████▉ | 115/128 [00:00<00:00, 379.92it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 379.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 42.97it/s, est. speed input: 44007.80 toks/s, output: 42.97 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 22.51it/s, est. speed input: 24826.01 toks/s, output: 24.24 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 20.23it/s, est. speed input: 22537.62 toks/s, output: 22.01 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 18.91it/s, est. speed input: 21252.40 toks/s, output: 20.75 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 18.18it/s, est. speed input: 20488.35 toks/s, output: 20.01 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.81it/s, est. speed input: 20104.49 toks/s, output: 19.63 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.55it/s, est. speed input: 19813.57 toks/s, output: 19.35 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.29it/s, est. speed input: 19555.20 toks/s, output: 19.10 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.13it/s, est. speed input: 19350.41 toks/s, output: 18.90 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.03it/s, est. speed input: 19183.89 toks/s, output: 18.73 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.91it/s, est. speed input: 19026.46 toks/s, output: 18.58 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.79it/s, est. speed input: 18882.82 toks/s, output: 18.44 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 16.70it/s, est. speed input: 18757.16 toks/s, output: 18.32 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.66it/s, est. speed input: 18648.74 toks/s, output: 18.21 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.50it/s, est. speed input: 18527.71 toks/s, output: 18.09 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:04<00:30,  2.81it/s, est. speed input: 9847.63 toks/s, output: 9.62 toks/s]  
Processed prompts:  34%|███▎      | 43/128 [00:04<00:22,  3.73it/s, est. speed input: 10042.40 toks/s, output: 9.81 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:04<00:17,  4.86it/s, est. speed input: 10228.79 toks/s, output: 9.99 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:04<00:13,  6.16it/s, est. speed input: 10404.84 toks/s, output: 10.16 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:04<00:10,  7.58it/s, est. speed input: 10570.68 toks/s, output: 10.32 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:04<00:08,  9.05it/s, est. speed input: 10728.83 toks/s, output: 10.48 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:04<00:07, 10.49it/s, est. speed input: 10881.61 toks/s, output: 10.63 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:05<00:06, 11.78it/s, est. speed input: 11024.88 toks/s, output: 10.77 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:05<00:05, 12.93it/s, est. speed input: 11164.58 toks/s, output: 10.90 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:05<00:04, 13.83it/s, est. speed input: 11294.63 toks/s, output: 11.03 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:05<00:04, 14.56it/s, est. speed input: 11420.61 toks/s, output: 11.15 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:05<00:04, 15.16it/s, est. speed input: 11543.39 toks/s, output: 11.27 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:05<00:04, 15.54it/s, est. speed input: 11656.81 toks/s, output: 11.38 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:05<00:03, 15.82it/s, est. speed input: 11766.22 toks/s, output: 11.49 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:05<00:03, 16.09it/s, est. speed input: 11874.34 toks/s, output: 11.60 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:06<00:03, 16.28it/s, est. speed input: 11978.11 toks/s, output: 11.70 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:06<00:03, 16.42it/s, est. speed input: 12077.70 toks/s, output: 11.79 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:06<00:03, 16.50it/s, est. speed input: 12173.14 toks/s, output: 11.89 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:06<00:03, 16.55it/s, est. speed input: 12264.27 toks/s, output: 11.98 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:06<00:02, 16.57it/s, est. speed input: 12351.85 toks/s, output: 12.06 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:06<00:02, 16.52it/s, est. speed input: 12433.13 toks/s, output: 12.14 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:06<00:02, 16.50it/s, est. speed input: 12511.85 toks/s, output: 12.22 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:06<00:02, 16.50it/s, est. speed input: 12588.67 toks/s, output: 12.29 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:07<00:02, 16.47it/s, est. speed input: 12661.52 toks/s, output: 12.36 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:07<00:02, 16.54it/s, est. speed input: 12736.02 toks/s, output: 12.44 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:07<00:02, 16.57it/s, est. speed input: 12807.18 toks/s, output: 12.51 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:07<00:02, 16.50it/s, est. speed input: 12871.91 toks/s, output: 12.57 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:07<00:02, 16.49it/s, est. speed input: 12936.26 toks/s, output: 12.63 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:07<00:01, 16.42it/s, est. speed input: 12996.26 toks/s, output: 12.69 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:07<00:01, 16.41it/s, est. speed input: 13055.50 toks/s, output: 12.75 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:07<00:01, 16.46it/s, est. speed input: 13115.70 toks/s, output: 12.81 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:08<00:01, 16.54it/s, est. speed input: 13175.56 toks/s, output: 12.87 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:08<00:01, 16.49it/s, est. speed input: 13229.60 toks/s, output: 12.92 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:08<00:01, 16.55it/s, est. speed input: 13285.86 toks/s, output: 12.97 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:08<00:01, 16.61it/s, est. speed input: 13340.79 toks/s, output: 13.03 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:08<00:01, 16.66it/s, est. speed input: 13394.97 toks/s, output: 13.08 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:08<00:00, 16.68it/s, est. speed input: 13446.67 toks/s, output: 13.13 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:08<00:00, 16.65it/s, est. speed input: 13495.72 toks/s, output: 13.18 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:08<00:00, 16.56it/s, est. speed input: 13540.41 toks/s, output: 13.22 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:08<00:00, 16.62it/s, est. speed input: 13588.88 toks/s, output: 13.27 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:09<00:00, 16.62it/s, est. speed input: 13634.17 toks/s, output: 13.31 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:09<00:00, 16.64it/s, est. speed input: 13678.95 toks/s, output: 13.36 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:09<00:00, 16.60it/s, est. speed input: 13720.87 toks/s, output: 13.40 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:09<00:00, 16.54it/s, est. speed input: 13760.66 toks/s, output: 13.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 16.54it/s, est. speed input: 13779.21 toks/s, output: 13.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 13.46it/s, est. speed input: 13779.21 toks/s, output: 13.46 toks/s]
[rank0]:[W128 11:36:04.371275209 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.5s

测试结果:
  Requests/s:   16.27
  Tokens/s:     16681.55
  Total Reqs:   128
  Elapsed:      7.86s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     16665.27

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:36:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:36:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24114) WARNING 01-28 11:36:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24114) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24114) WARNING 01-28 11:36:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.60 requests/s, 35462.85 total tokens/s, 34.60 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 11:36:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:36:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:36:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:36:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:36:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:36:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:36:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:36:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:36:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:36:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:36:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:36:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:36:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:36:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:36:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:36:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:36:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=24114) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=24114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=24114) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=24114) 
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=24114) [2026-01-28 11:36:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=24114) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.23it/s]
(EngineCore_DP0 pid=24114) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.36it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 37/256 [00:00<00:00, 362.54it/s]
Adding requests:  30%|███       | 77/256 [00:00<00:00, 382.28it/s]
Adding requests:  45%|████▌     | 116/256 [00:00<00:00, 372.27it/s]
Adding requests:  60%|██████    | 154/256 [00:00<00:00, 370.29it/s]
Adding requests:  75%|███████▌  | 192/256 [00:00<00:00, 368.35it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 371.72it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 373.70it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 147.82it/s, est. speed input: 151399.70 toks/s, output: 147.83 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:03, 59.37it/s, est. speed input: 67743.54 toks/s, output: 66.16 toks/s]   
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 47.00it/s, est. speed input: 55587.21 toks/s, output: 54.28 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:04, 43.96it/s, est. speed input: 52398.12 toks/s, output: 51.17 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:04, 41.81it/s, est. speed input: 50195.73 toks/s, output: 49.02 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:01<00:04, 42.56it/s, est. speed input: 49864.69 toks/s, output: 48.70 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:04, 38.75it/s, est. speed input: 47676.17 toks/s, output: 46.56 toks/s]
Processed prompts:  28%|██▊       | 71/256 [00:01<00:04, 40.08it/s, est. speed input: 47518.82 toks/s, output: 46.40 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:04, 36.72it/s, est. speed input: 45857.17 toks/s, output: 44.78 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 36.51it/s, est. speed input: 45293.57 toks/s, output: 44.23 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:04, 36.34it/s, est. speed input: 44793.19 toks/s, output: 43.74 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:04, 36.32it/s, est. speed input: 44376.37 toks/s, output: 43.34 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:04, 36.12it/s, est. speed input: 43961.61 toks/s, output: 42.93 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 35.92it/s, est. speed input: 43576.08 toks/s, output: 42.55 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 35.75it/s, est. speed input: 43222.17 toks/s, output: 42.21 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 35.67it/s, est. speed input: 42909.21 toks/s, output: 41.90 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:04, 35.73it/s, est. speed input: 42644.26 toks/s, output: 41.64 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:04, 35.72it/s, est. speed input: 42391.36 toks/s, output: 41.40 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 35.84it/s, est. speed input: 42179.70 toks/s, output: 41.19 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:03, 35.95it/s, est. speed input: 41987.14 toks/s, output: 41.00 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:03, 36.08it/s, est. speed input: 41815.95 toks/s, output: 40.84 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:03, 36.18it/s, est. speed input: 41657.64 toks/s, output: 40.68 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:03, 36.23it/s, est. speed input: 41508.17 toks/s, output: 40.54 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 36.22it/s, est. speed input: 41362.09 toks/s, output: 40.39 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:03, 36.14it/s, est. speed input: 41216.97 toks/s, output: 40.25 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:03, 35.72it/s, est. speed input: 41037.01 toks/s, output: 40.07 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:03, 35.74it/s, est. speed input: 40905.13 toks/s, output: 39.95 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 35.85it/s, est. speed input: 40790.34 toks/s, output: 39.83 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:03<00:02, 35.50it/s, est. speed input: 40636.92 toks/s, output: 39.68 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:02, 35.63it/s, est. speed input: 40530.95 toks/s, output: 39.58 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 35.82it/s, est. speed input: 40440.58 toks/s, output: 39.49 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 35.91it/s, est. speed input: 40351.11 toks/s, output: 39.41 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 35.85it/s, est. speed input: 40254.38 toks/s, output: 39.31 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 35.89it/s, est. speed input: 40170.38 toks/s, output: 39.23 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:02, 35.95it/s, est. speed input: 40093.09 toks/s, output: 39.15 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:04<00:01, 36.02it/s, est. speed input: 40020.92 toks/s, output: 39.08 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 36.08it/s, est. speed input: 39953.83 toks/s, output: 39.02 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:04<00:01, 36.20it/s, est. speed input: 39896.11 toks/s, output: 38.96 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 36.26it/s, est. speed input: 39839.40 toks/s, output: 38.91 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 36.05it/s, est. speed input: 39764.88 toks/s, output: 38.83 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 35.99it/s, est. speed input: 39699.68 toks/s, output: 38.77 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 35.93it/s, est. speed input: 39636.12 toks/s, output: 38.71 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 35.84it/s, est. speed input: 39571.62 toks/s, output: 38.64 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 35.76it/s, est. speed input: 39508.49 toks/s, output: 38.58 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:01, 35.78it/s, est. speed input: 39453.63 toks/s, output: 38.53 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 35.87it/s, est. speed input: 39405.56 toks/s, output: 38.48 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 35.75it/s, est. speed input: 39347.36 toks/s, output: 38.43 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:06<00:00, 35.79it/s, est. speed input: 39299.29 toks/s, output: 38.38 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:06<00:00, 35.88it/s, est. speed input: 39257.35 toks/s, output: 38.34 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:06<00:00, 36.02it/s, est. speed input: 39221.72 toks/s, output: 38.30 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:06<00:00, 36.06it/s, est. speed input: 39183.11 toks/s, output: 38.26 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 36.09it/s, est. speed input: 39146.32 toks/s, output: 38.23 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 36.00it/s, est. speed input: 39104.41 toks/s, output: 38.19 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 35.76it/s, est. speed input: 39052.54 toks/s, output: 38.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 35.76it/s, est. speed input: 39052.54 toks/s, output: 38.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 38.14it/s, est. speed input: 39052.54 toks/s, output: 38.14 toks/s]
[rank0]:[W128 11:36:53.021953597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.6s

测试结果:
  Requests/s:   34.60
  Tokens/s:     35462.85
  Total Reqs:   256
  Elapsed:      7.40s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     35428.25

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:37:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:37:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24646) WARNING 01-28 11:37:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=24646) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24646) WARNING 01-28 11:37:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.02 requests/s, 53319.95 total tokens/s, 52.02 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 11:37:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:37:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:37:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:37:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:37:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:37:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:37:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:37:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:37:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:37:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:37:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:37:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:37:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:37:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:37:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:37:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:37:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=24646) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=24646) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=24646) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=24646) 
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=24646) [2026-01-28 11:37:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=24646) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.05it/s]
(EngineCore_DP0 pid=24646) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.27it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.36it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 36/512 [00:00<00:01, 356.14it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 372.50it/s]
Adding requests:  22%|██▏       | 113/512 [00:00<00:01, 369.90it/s]
Adding requests:  29%|██▉       | 150/512 [00:00<00:00, 369.04it/s]
Adding requests:  37%|███▋      | 187/512 [00:00<00:00, 368.39it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 368.51it/s]
Adding requests:  51%|█████     | 262/512 [00:00<00:00, 369.41it/s]
Adding requests:  58%|█████▊    | 299/512 [00:00<00:00, 368.50it/s]
Adding requests:  66%|██████▌   | 336/512 [00:00<00:00, 368.87it/s]
Adding requests:  73%|███████▎  | 374/512 [00:01<00:00, 372.15it/s]
Adding requests:  80%|████████  | 412/512 [00:01<00:00, 373.30it/s]
Adding requests:  88%|████████▊ | 450/512 [00:01<00:00, 374.04it/s]
Adding requests:  95%|█████████▌| 488/512 [00:01<00:00, 370.00it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 368.90it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 520.83it/s, est. speed input: 533430.89 toks/s, output: 520.86 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:01<00:04, 93.42it/s, est. speed input: 110795.31 toks/s, output: 108.20 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:01<00:04, 79.20it/s, est. speed input: 95194.49 toks/s, output: 92.96 toks/s]  
Processed prompts:  31%|███▏      | 160/512 [00:01<00:04, 72.48it/s, est. speed input: 88702.15 toks/s, output: 86.62 toks/s]
Processed prompts:  34%|███▎      | 172/512 [00:02<00:04, 68.25it/s, est. speed input: 85000.32 toks/s, output: 83.01 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:02<00:05, 62.61it/s, est. speed input: 81173.10 toks/s, output: 79.27 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:02<00:05, 60.96it/s, est. speed input: 79575.43 toks/s, output: 77.71 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:02<00:05, 59.43it/s, est. speed input: 78159.76 toks/s, output: 76.33 toks/s]
Processed prompts:  40%|████      | 206/512 [00:02<00:05, 58.12it/s, est. speed input: 76902.85 toks/s, output: 75.10 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:02<00:05, 56.98it/s, est. speed input: 75762.25 toks/s, output: 73.99 toks/s]
Processed prompts:  43%|████▎     | 220/512 [00:05<00:24, 11.72it/s, est. speed input: 44761.39 toks/s, output: 43.71 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:20, 13.91it/s, est. speed input: 44652.75 toks/s, output: 43.61 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:15, 17.83it/s, est. speed input: 44941.33 toks/s, output: 43.89 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:12, 22.26it/s, est. speed input: 45211.04 toks/s, output: 44.15 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:09, 26.97it/s, est. speed input: 45469.21 toks/s, output: 44.40 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:08, 31.68it/s, est. speed input: 45712.91 toks/s, output: 44.64 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:05<00:06, 36.10it/s, est. speed input: 45944.29 toks/s, output: 44.87 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:05, 39.99it/s, est. speed input: 46160.24 toks/s, output: 45.08 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:05, 43.21it/s, est. speed input: 46361.60 toks/s, output: 45.27 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:04, 45.81it/s, est. speed input: 46555.19 toks/s, output: 45.46 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:04, 47.89it/s, est. speed input: 46745.36 toks/s, output: 45.65 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:06<00:04, 49.49it/s, est. speed input: 46928.57 toks/s, output: 45.83 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:06<00:03, 50.66it/s, est. speed input: 47102.90 toks/s, output: 46.00 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:06<00:03, 51.51it/s, est. speed input: 47269.84 toks/s, output: 46.16 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:03, 52.12it/s, est. speed input: 47428.96 toks/s, output: 46.32 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:03, 52.56it/s, est. speed input: 47582.76 toks/s, output: 46.47 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:07<00:03, 52.78it/s, est. speed input: 47724.14 toks/s, output: 46.61 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:07<00:02, 53.04it/s, est. speed input: 47866.08 toks/s, output: 46.74 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:07<00:02, 53.21it/s, est. speed input: 48001.92 toks/s, output: 46.88 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:07<00:02, 53.31it/s, est. speed input: 48131.81 toks/s, output: 47.00 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:08<00:02, 53.41it/s, est. speed input: 48258.29 toks/s, output: 47.13 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:08<00:02, 53.31it/s, est. speed input: 48371.49 toks/s, output: 47.24 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:08<00:02, 53.32it/s, est. speed input: 48483.28 toks/s, output: 47.35 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:08<00:02, 53.38it/s, est. speed input: 48594.94 toks/s, output: 47.46 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:08<00:01, 53.43it/s, est. speed input: 48702.95 toks/s, output: 47.56 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:08<00:01, 53.47it/s, est. speed input: 48807.67 toks/s, output: 47.66 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:08<00:01, 53.50it/s, est. speed input: 48908.84 toks/s, output: 47.76 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:09<00:01, 53.38it/s, est. speed input: 48999.94 toks/s, output: 47.85 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:09<00:01, 53.37it/s, est. speed input: 49091.11 toks/s, output: 47.94 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:09<00:01, 53.40it/s, est. speed input: 49181.56 toks/s, output: 48.03 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:09<00:01, 53.43it/s, est. speed input: 49269.68 toks/s, output: 48.11 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:09<00:00, 52.87it/s, est. speed input: 49326.85 toks/s, output: 48.17 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:09<00:00, 52.38it/s, est. speed input: 49377.25 toks/s, output: 48.22 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 52.01it/s, est. speed input: 49424.73 toks/s, output: 48.27 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:10<00:00, 51.79it/s, est. speed input: 49472.15 toks/s, output: 48.31 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:10<00:00, 51.53it/s, est. speed input: 49513.22 toks/s, output: 48.35 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:10<00:00, 51.42it/s, est. speed input: 49556.14 toks/s, output: 48.39 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 51.42it/s, est. speed input: 49842.94 toks/s, output: 48.67 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 48.67it/s, est. speed input: 49842.94 toks/s, output: 48.67 toks/s]
[rank0]:[W128 11:37:48.125875636 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.1s

测试结果:
  Requests/s:   52.02
  Tokens/s:     53319.95
  Total Reqs:   512
  Elapsed:      9.84s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     53267.93

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:38:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:38:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25211) WARNING 01-28 11:38:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25211) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25211) WARNING 01-28 11:38:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.22 requests/s, 55572.99 total tokens/s, 54.22 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 11:38:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:38:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:38:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:38:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:38:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:38:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:38:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:38:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:38:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:38:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:38:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:38:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:38:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:38:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:38:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:38:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:38:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=25211) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=25211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=25211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=25211) 
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=25211) [2026-01-28 11:38:11] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=25211) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.58it/s]
(EngineCore_DP0 pid=25211) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.34it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  9.11it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 378.43it/s]
Adding requests:   8%|▊         | 79/1024 [00:00<00:02, 394.30it/s]
Adding requests:  12%|█▏        | 119/1024 [00:00<00:02, 395.00it/s]
Adding requests:  16%|█▌        | 159/1024 [00:00<00:02, 390.72it/s]
Adding requests:  19%|█▉        | 199/1024 [00:00<00:02, 389.37it/s]
Adding requests:  23%|██▎       | 239/1024 [00:00<00:02, 390.64it/s]
Adding requests:  27%|██▋       | 280/1024 [00:00<00:01, 394.48it/s]
Adding requests:  31%|███▏      | 321/1024 [00:00<00:01, 396.63it/s]
Adding requests:  35%|███▌      | 362/1024 [00:00<00:01, 400.44it/s]
Adding requests:  39%|███▉      | 404/1024 [00:01<00:01, 404.62it/s]
Adding requests:  43%|████▎     | 445/1024 [00:01<00:01, 404.42it/s]
Adding requests:  48%|████▊     | 487/1024 [00:01<00:01, 404.72it/s]
Adding requests:  52%|█████▏    | 528/1024 [00:01<00:01, 393.80it/s]
Adding requests:  56%|█████▌    | 571/1024 [00:01<00:01, 402.49it/s]
Adding requests:  60%|█████▉    | 612/1024 [00:01<00:01, 402.45it/s]
Adding requests:  64%|██████▍   | 653/1024 [00:01<00:00, 393.68it/s]
Adding requests:  68%|██████▊   | 696/1024 [00:01<00:00, 402.37it/s]
Adding requests:  72%|███████▏  | 739/1024 [00:01<00:00, 408.15it/s]
Adding requests:  76%|███████▌  | 780/1024 [00:01<00:00, 405.37it/s]
Adding requests:  80%|████████  | 821/1024 [00:02<00:00, 395.47it/s]
Adding requests:  84%|████████▍ | 863/1024 [00:02<00:00, 400.16it/s]
Adding requests:  88%|████████▊ | 906/1024 [00:02<00:00, 408.11it/s]
Adding requests:  92%|█████████▏| 947/1024 [00:02<00:00, 407.03it/s]
Adding requests:  96%|█████████▋| 988/1024 [00:02<00:00, 407.15it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 400.91it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:00<00:00, 988.96it/s, est. speed input: 1012960.31 toks/s, output: 989.08 toks/s]
Processed prompts:  22%|██▏       | 229/1024 [00:01<00:07, 105.40it/s, est. speed input: 127302.96 toks/s, output: 124.32 toks/s] 
Processed prompts:  27%|██▋       | 274/1024 [00:02<00:08, 84.08it/s, est. speed input: 104019.77 toks/s, output: 101.58 toks/s] 
Processed prompts:  29%|██▉       | 301/1024 [00:03<00:09, 79.35it/s, est. speed input: 98602.92 toks/s, output: 96.29 toks/s]  
Processed prompts:  31%|███▏      | 320/1024 [00:03<00:09, 77.07it/s, est. speed input: 96036.93 toks/s, output: 93.79 toks/s]
Processed prompts:  33%|███▎      | 335/1024 [00:03<00:09, 72.14it/s, est. speed input: 92775.10 toks/s, output: 90.60 toks/s]
Processed prompts:  34%|███▍      | 347/1024 [00:03<00:10, 65.41it/s, est. speed input: 89201.22 toks/s, output: 87.11 toks/s]
Processed prompts:  35%|███▍      | 356/1024 [00:04<00:10, 65.09it/s, est. speed input: 88351.18 toks/s, output: 86.28 toks/s]
Processed prompts:  36%|███▌      | 365/1024 [00:04<00:10, 64.76it/s, est. speed input: 87555.80 toks/s, output: 85.50 toks/s]
Processed prompts:  36%|███▋      | 373/1024 [00:04<00:10, 63.13it/s, est. speed input: 86585.65 toks/s, output: 84.56 toks/s]
Processed prompts:  37%|███▋      | 380/1024 [00:04<00:10, 60.12it/s, est. speed input: 85444.39 toks/s, output: 83.44 toks/s]
Processed prompts:  38%|███▊      | 387/1024 [00:04<00:11, 57.52it/s, est. speed input: 84372.92 toks/s, output: 82.40 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:04<00:11, 55.40it/s, est. speed input: 83365.48 toks/s, output: 81.41 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:04<00:11, 55.57it/s, est. speed input: 82620.75 toks/s, output: 80.68 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:11, 55.69it/s, est. speed input: 81917.37 toks/s, output: 80.00 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:10, 55.73it/s, est. speed input: 81244.38 toks/s, output: 79.34 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:10, 55.78it/s, est. speed input: 80610.20 toks/s, output: 78.72 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:05<00:10, 55.82it/s, est. speed input: 80008.39 toks/s, output: 78.13 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:05<00:10, 55.83it/s, est. speed input: 79435.26 toks/s, output: 77.57 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:05<00:10, 55.85it/s, est. speed input: 78891.01 toks/s, output: 77.04 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:05<00:10, 55.88it/s, est. speed input: 78375.36 toks/s, output: 76.54 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:09, 55.91it/s, est. speed input: 77883.70 toks/s, output: 76.06 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:09, 55.96it/s, est. speed input: 77418.94 toks/s, output: 75.60 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:09, 55.97it/s, est. speed input: 76971.09 toks/s, output: 75.17 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:06<00:09, 56.03it/s, est. speed input: 76548.03 toks/s, output: 74.75 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:06<00:09, 56.02it/s, est. speed input: 76138.28 toks/s, output: 74.35 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:06<00:09, 56.00it/s, est. speed input: 75744.81 toks/s, output: 73.97 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:06<00:09, 55.98it/s, est. speed input: 75366.55 toks/s, output: 73.60 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:08, 55.97it/s, est. speed input: 75003.34 toks/s, output: 73.25 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:08, 55.95it/s, est. speed input: 74653.91 toks/s, output: 72.90 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:07<00:08, 55.95it/s, est. speed input: 74319.25 toks/s, output: 72.58 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:07<00:08, 55.96it/s, est. speed input: 73997.64 toks/s, output: 72.26 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:07<00:08, 55.98it/s, est. speed input: 73689.30 toks/s, output: 71.96 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:07<00:08, 55.98it/s, est. speed input: 73390.90 toks/s, output: 71.67 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:07<00:08, 55.95it/s, est. speed input: 73100.86 toks/s, output: 71.39 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:07, 55.95it/s, est. speed input: 72823.06 toks/s, output: 71.12 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:07, 55.92it/s, est. speed input: 72552.00 toks/s, output: 70.85 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:08<00:07, 55.90it/s, est. speed input: 72290.35 toks/s, output: 70.60 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:08<00:07, 55.89it/s, est. speed input: 72037.92 toks/s, output: 70.35 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:08<00:07, 55.93it/s, est. speed input: 71797.05 toks/s, output: 70.11 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:08<00:07, 55.93it/s, est. speed input: 71562.45 toks/s, output: 69.89 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:08<00:07, 55.94it/s, est. speed input: 71335.48 toks/s, output: 69.66 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:06, 55.94it/s, est. speed input: 71115.13 toks/s, output: 69.45 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:06, 55.92it/s, est. speed input: 70900.41 toks/s, output: 69.24 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:09<00:06, 54.55it/s, est. speed input: 70603.65 toks/s, output: 68.95 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:09<00:06, 53.51it/s, est. speed input: 70308.14 toks/s, output: 68.66 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:09<00:06, 52.77it/s, est. speed input: 70019.73 toks/s, output: 68.38 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:09<00:06, 52.27it/s, est. speed input: 69740.45 toks/s, output: 68.11 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 51.94it/s, est. speed input: 69471.03 toks/s, output: 67.84 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:06, 51.72it/s, est. speed input: 69210.44 toks/s, output: 67.59 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:10<00:06, 51.58it/s, est. speed input: 68958.36 toks/s, output: 67.34 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:10<00:06, 51.71it/s, est. speed input: 68728.99 toks/s, output: 67.12 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:10<00:05, 52.86it/s, est. speed input: 68572.60 toks/s, output: 66.97 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:10<00:05, 53.71it/s, est. speed input: 68420.78 toks/s, output: 66.82 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:10<00:05, 54.30it/s, est. speed input: 68272.12 toks/s, output: 66.67 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 54.72it/s, est. speed input: 68126.89 toks/s, output: 66.53 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:11<00:05, 54.98it/s, est. speed input: 67983.35 toks/s, output: 66.39 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:11<00:04, 55.15it/s, est. speed input: 67842.90 toks/s, output: 66.25 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:11<00:04, 55.27it/s, est. speed input: 67706.09 toks/s, output: 66.12 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:11<00:04, 55.35it/s, est. speed input: 67572.32 toks/s, output: 65.99 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:11<00:04, 55.45it/s, est. speed input: 67443.74 toks/s, output: 65.86 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:11<00:04, 55.53it/s, est. speed input: 67318.97 toks/s, output: 65.74 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 55.58it/s, est. speed input: 67196.77 toks/s, output: 65.62 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:12<00:03, 55.56it/s, est. speed input: 67075.24 toks/s, output: 65.50 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:12<00:03, 55.62it/s, est. speed input: 66959.40 toks/s, output: 65.39 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:12<00:03, 55.64it/s, est. speed input: 66845.68 toks/s, output: 65.28 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:12<00:03, 55.66it/s, est. speed input: 66734.61 toks/s, output: 65.17 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:12<00:03, 55.67it/s, est. speed input: 66625.68 toks/s, output: 65.06 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:12<00:03, 55.67it/s, est. speed input: 66519.25 toks/s, output: 64.96 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:03, 55.67it/s, est. speed input: 66415.06 toks/s, output: 64.86 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:15<00:17,  9.41it/s, est. speed input: 56305.15 toks/s, output: 54.99 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:15<00:12, 12.53it/s, est. speed input: 56311.73 toks/s, output: 54.99 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:15<00:09, 16.28it/s, est. speed input: 56300.50 toks/s, output: 54.98 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:16<00:06, 20.46it/s, est. speed input: 56260.31 toks/s, output: 54.94 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:16<00:05, 24.93it/s, est. speed input: 56220.22 toks/s, output: 54.90 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:16<00:04, 29.44it/s, est. speed input: 56181.34 toks/s, output: 54.86 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:16<00:03, 33.71it/s, est. speed input: 56142.90 toks/s, output: 54.83 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:16<00:02, 37.52it/s, est. speed input: 56105.78 toks/s, output: 54.79 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:16<00:02, 40.75it/s, est. speed input: 56069.55 toks/s, output: 54.76 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:16<00:02, 43.37it/s, est. speed input: 56034.42 toks/s, output: 54.72 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:17<00:01, 45.40it/s, est. speed input: 55999.31 toks/s, output: 54.69 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:17<00:01, 46.95it/s, est. speed input: 55965.23 toks/s, output: 54.65 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:17<00:01, 48.09it/s, est. speed input: 55931.52 toks/s, output: 54.62 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:17<00:01, 48.94it/s, est. speed input: 55898.97 toks/s, output: 54.59 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:17<00:01, 49.57it/s, est. speed input: 55867.33 toks/s, output: 54.56 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:17<00:00, 49.95it/s, est. speed input: 55834.31 toks/s, output: 54.53 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:18<00:00, 51.85it/s, est. speed input: 55853.11 toks/s, output: 54.54 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:18<00:00, 51.58it/s, est. speed input: 55821.55 toks/s, output: 54.51 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:18<00:00, 51.40it/s, est. speed input: 55790.65 toks/s, output: 54.48 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:18<00:00, 51.27it/s, est. speed input: 55760.37 toks/s, output: 54.45 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:18<00:00, 53.14it/s, est. speed input: 55787.59 toks/s, output: 54.48 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 53.14it/s, est. speed input: 56115.33 toks/s, output: 54.80 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 54.80it/s, est. speed input: 56115.33 toks/s, output: 54.80 toks/s]
[rank0]:[W128 11:38:55.729972614 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 67.2s

测试结果:
  Requests/s:   54.22
  Tokens/s:     55572.99
  Total Reqs:   1024
  Elapsed:      18.89s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     55518.78

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:39:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:39:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25888) WARNING 01-28 11:39:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=25888) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25888) WARNING 01-28 11:39:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.74 requests/s, 57135.06 total tokens/s, 55.74 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 11:39:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:39:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:39:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:39:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:39:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:39:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:39:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:39:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:39:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:39:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:39:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:39:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:39:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:39:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:39:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:39:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:39:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=25888) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=25888) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=25888) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]
(EngineCore_DP0 pid=25888) 
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=25888) [2026-01-28 11:39:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=25888) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.29it/s]
(EngineCore_DP0 pid=25888) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.76it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 34/2048 [00:00<00:06, 334.49it/s]
Adding requests:   3%|▎         | 71/2048 [00:00<00:05, 350.71it/s]
Adding requests:   5%|▌         | 108/2048 [00:00<00:05, 359.36it/s]
Adding requests:   7%|▋         | 145/2048 [00:00<00:05, 359.50it/s]
Adding requests:   9%|▉         | 181/2048 [00:00<00:05, 357.53it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:05, 355.69it/s]
Adding requests:  12%|█▎        | 256/2048 [00:00<00:04, 364.11it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:04, 364.07it/s]
Adding requests:  16%|█▌        | 330/2048 [00:00<00:04, 365.37it/s]
Adding requests:  18%|█▊        | 370/2048 [00:01<00:04, 375.31it/s]
Adding requests:  20%|██        | 410/2048 [00:01<00:04, 382.35it/s]
Adding requests:  22%|██▏       | 450/2048 [00:01<00:04, 384.88it/s]
Adding requests:  24%|██▍       | 490/2048 [00:01<00:04, 388.07it/s]
Adding requests:  26%|██▌       | 529/2048 [00:01<00:03, 381.28it/s]
Adding requests:  28%|██▊       | 569/2048 [00:01<00:03, 384.73it/s]
Adding requests:  30%|██▉       | 608/2048 [00:01<00:03, 384.85it/s]
Adding requests:  32%|███▏      | 649/2048 [00:01<00:03, 390.93it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:03, 392.06it/s]
Adding requests:  36%|███▌      | 730/2048 [00:01<00:03, 396.32it/s]
Adding requests:  38%|███▊      | 770/2048 [00:02<00:03, 389.83it/s]
Adding requests:  40%|███▉      | 810/2048 [00:02<00:03, 384.43it/s]
Adding requests:  41%|████▏     | 849/2048 [00:02<00:03, 375.71it/s]
Adding requests:  43%|████▎     | 890/2048 [00:02<00:03, 385.44it/s]
Adding requests:  45%|████▌     | 931/2048 [00:02<00:02, 392.49it/s]
Adding requests:  47%|████▋     | 971/2048 [00:02<00:02, 391.35it/s]
Adding requests:  49%|████▉     | 1011/2048 [00:02<00:02, 390.72it/s]
Adding requests:  51%|█████▏    | 1051/2048 [00:02<00:02, 388.76it/s]
Adding requests:  53%|█████▎    | 1090/2048 [00:02<00:02, 384.72it/s]
Adding requests:  55%|█████▌    | 1130/2048 [00:02<00:02, 386.51it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:03<00:02, 396.94it/s]
Adding requests:  59%|█████▉    | 1216/2048 [00:03<00:02, 404.79it/s]
Adding requests:  61%|██████▏   | 1257/2048 [00:03<00:01, 400.28it/s]
Adding requests:  63%|██████▎   | 1298/2048 [00:03<00:01, 402.78it/s]
Adding requests:  65%|██████▌   | 1339/2048 [00:03<00:01, 404.58it/s]
Adding requests:  67%|██████▋   | 1380/2048 [00:03<00:01, 401.45it/s]
Adding requests:  69%|██████▉   | 1421/2048 [00:03<00:01, 399.31it/s]
Adding requests:  71%|███████▏  | 1462/2048 [00:03<00:01, 401.34it/s]
Adding requests:  73%|███████▎  | 1503/2048 [00:03<00:01, 403.06it/s]
Adding requests:  75%|███████▌  | 1544/2048 [00:03<00:01, 402.03it/s]
Adding requests:  77%|███████▋  | 1585/2048 [00:04<00:01, 404.13it/s]
Adding requests:  79%|███████▉  | 1628/2048 [00:04<00:01, 409.53it/s]
Adding requests:  81%|████████▏ | 1669/2048 [00:04<00:00, 402.93it/s]
Adding requests:  83%|████████▎ | 1710/2048 [00:04<00:00, 401.88it/s]
Adding requests:  85%|████████▌ | 1751/2048 [00:04<00:00, 403.99it/s]
Adding requests:  88%|████████▊ | 1792/2048 [00:04<00:00, 399.47it/s]
Adding requests:  89%|████████▉ | 1832/2048 [00:04<00:00, 396.62it/s]
Adding requests:  91%|█████████▏| 1873/2048 [00:04<00:00, 397.59it/s]
Adding requests:  93%|█████████▎| 1913/2048 [00:04<00:00, 390.72it/s]
Adding requests:  95%|█████████▌| 1954/2048 [00:05<00:00, 395.38it/s]
Adding requests:  97%|█████████▋| 1995/2048 [00:05<00:00, 399.17it/s]
Adding requests:  99%|█████████▉| 2035/2048 [00:05<00:00, 389.54it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 389.09it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:00, 2441.40it/s, est. speed input: 2500538.73 toks/s, output: 2441.57 toks/s]
Processed prompts:  25%|██▌       | 519/2048 [00:04<00:15, 100.30it/s, est. speed input: 121097.02 toks/s, output: 118.26 toks/s]   
Processed prompts:  30%|███       | 623/2048 [00:06<00:16, 86.17it/s, est. speed input: 104657.01 toks/s, output: 102.20 toks/s] 
Processed prompts:  33%|███▎      | 683/2048 [00:07<00:17, 77.63it/s, est. speed input: 96671.63 toks/s, output: 94.41 toks/s]  
Processed prompts:  35%|███▌      | 721/2048 [00:09<00:25, 51.06it/s, est. speed input: 76858.73 toks/s, output: 75.06 toks/s]
Processed prompts:  36%|███▋      | 746/2048 [00:10<00:25, 50.13it/s, est. speed input: 75070.12 toks/s, output: 73.31 toks/s]
Processed prompts:  37%|███▋      | 764/2048 [00:10<00:25, 51.23it/s, est. speed input: 74786.77 toks/s, output: 73.03 toks/s]
Processed prompts:  38%|███▊      | 779/2048 [00:10<00:24, 51.39it/s, est. speed input: 74233.89 toks/s, output: 72.49 toks/s]
Processed prompts:  39%|███▊      | 791/2048 [00:11<00:25, 50.15it/s, est. speed input: 73430.63 toks/s, output: 71.71 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:11<00:25, 48.31it/s, est. speed input: 72576.60 toks/s, output: 70.88 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:11<00:24, 49.76it/s, est. speed input: 72205.19 toks/s, output: 70.51 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:11<00:24, 50.49it/s, est. speed input: 71752.90 toks/s, output: 70.07 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:12<00:23, 51.10it/s, est. speed input: 71322.00 toks/s, output: 69.65 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:12<00:22, 51.59it/s, est. speed input: 70911.27 toks/s, output: 69.25 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:12<00:22, 51.96it/s, est. speed input: 70518.22 toks/s, output: 68.87 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:13<00:22, 52.24it/s, est. speed input: 70143.16 toks/s, output: 68.50 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:13<00:21, 52.48it/s, est. speed input: 69788.38 toks/s, output: 68.15 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:13<00:20, 53.51it/s, est. speed input: 69534.72 toks/s, output: 67.90 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:13<00:20, 54.26it/s, est. speed input: 69289.66 toks/s, output: 67.67 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:14<00:19, 54.82it/s, est. speed input: 69055.37 toks/s, output: 67.44 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:14<00:19, 56.11it/s, est. speed input: 68903.72 toks/s, output: 67.29 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:14<00:18, 56.14it/s, est. speed input: 68685.80 toks/s, output: 67.08 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:15<00:18, 56.17it/s, est. speed input: 68477.12 toks/s, output: 66.87 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:15<00:18, 56.16it/s, est. speed input: 68273.53 toks/s, output: 66.67 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:15<00:17, 56.17it/s, est. speed input: 68078.09 toks/s, output: 66.48 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:15<00:17, 56.18it/s, est. speed input: 67890.29 toks/s, output: 66.30 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:16<00:17, 56.20it/s, est. speed input: 67709.88 toks/s, output: 66.12 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:16<00:17, 56.19it/s, est. speed input: 67534.35 toks/s, output: 65.95 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:16<00:16, 56.19it/s, est. speed input: 67365.06 toks/s, output: 65.79 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:17<00:16, 56.19it/s, est. speed input: 67201.32 toks/s, output: 65.63 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:17<00:16, 56.17it/s, est. speed input: 67042.06 toks/s, output: 65.47 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:17<00:15, 57.15it/s, est. speed input: 66949.95 toks/s, output: 65.38 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:17<00:15, 56.87it/s, est. speed input: 66801.02 toks/s, output: 65.24 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:18<00:15, 56.67it/s, est. speed input: 66656.50 toks/s, output: 65.09 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:18<00:14, 56.51it/s, est. speed input: 66515.63 toks/s, output: 64.96 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:18<00:14, 56.42it/s, est. speed input: 66379.65 toks/s, output: 64.82 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:19<00:14, 56.34it/s, est. speed input: 66247.41 toks/s, output: 64.69 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:19<00:14, 56.31it/s, est. speed input: 66119.79 toks/s, output: 64.57 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:19<00:13, 56.27it/s, est. speed input: 65995.07 toks/s, output: 64.45 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:19<00:13, 56.24it/s, est. speed input: 65874.11 toks/s, output: 64.33 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:20<00:13, 56.23it/s, est. speed input: 65757.14 toks/s, output: 64.22 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:20<00:13, 56.20it/s, est. speed input: 65642.08 toks/s, output: 64.10 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:20<00:12, 56.18it/s, est. speed input: 65530.19 toks/s, output: 63.99 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:21<00:12, 56.19it/s, est. speed input: 65422.27 toks/s, output: 63.89 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:21<00:12, 56.18it/s, est. speed input: 65316.82 toks/s, output: 63.79 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:21<00:11, 56.26it/s, est. speed input: 65218.54 toks/s, output: 63.69 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:21<00:11, 56.36it/s, est. speed input: 65124.88 toks/s, output: 63.60 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:22<00:11, 56.42it/s, est. speed input: 65032.95 toks/s, output: 63.51 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:22<00:11, 56.47it/s, est. speed input: 64943.89 toks/s, output: 63.42 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:22<00:10, 56.49it/s, est. speed input: 64856.32 toks/s, output: 63.34 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:23<00:10, 56.51it/s, est. speed input: 64771.20 toks/s, output: 63.25 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:23<00:10, 56.54it/s, est. speed input: 64688.69 toks/s, output: 63.17 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:23<00:09, 56.54it/s, est. speed input: 64607.43 toks/s, output: 63.09 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:23<00:09, 56.54it/s, est. speed input: 64528.09 toks/s, output: 63.02 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:24<00:09, 56.55it/s, est. speed input: 64450.87 toks/s, output: 62.94 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:24<00:09, 56.58it/s, est. speed input: 64376.24 toks/s, output: 62.87 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:24<00:08, 56.56it/s, est. speed input: 64302.00 toks/s, output: 62.79 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:25<00:08, 56.57it/s, est. speed input: 64230.04 toks/s, output: 62.72 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:25<00:08, 56.58it/s, est. speed input: 64160.39 toks/s, output: 62.66 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:25<00:07, 56.59it/s, est. speed input: 64092.00 toks/s, output: 62.59 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:25<00:07, 56.63it/s, est. speed input: 64026.58 toks/s, output: 62.53 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:26<00:07, 56.57it/s, est. speed input: 63958.89 toks/s, output: 62.46 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:26<00:07, 56.58it/s, est. speed input: 63894.67 toks/s, output: 62.40 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:26<00:06, 56.56it/s, est. speed input: 63830.83 toks/s, output: 62.33 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:27<00:06, 56.54it/s, est. speed input: 63768.31 toks/s, output: 62.27 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:27<00:06, 56.56it/s, est. speed input: 63708.23 toks/s, output: 62.21 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:27<00:05, 56.57it/s, est. speed input: 63649.03 toks/s, output: 62.16 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:27<00:05, 56.57it/s, est. speed input: 63591.13 toks/s, output: 62.10 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:28<00:05, 56.56it/s, est. speed input: 63533.70 toks/s, output: 62.04 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:28<00:05, 56.54it/s, est. speed input: 63477.25 toks/s, output: 61.99 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:28<00:04, 56.55it/s, est. speed input: 63422.56 toks/s, output: 61.94 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:28<00:04, 56.55it/s, est. speed input: 63368.79 toks/s, output: 61.88 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:29<00:04, 56.56it/s, est. speed input: 63316.13 toks/s, output: 61.83 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:29<00:03, 56.39it/s, est. speed input: 63258.34 toks/s, output: 61.78 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:29<00:03, 56.10it/s, est. speed input: 63195.47 toks/s, output: 61.71 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:30<00:03, 55.90it/s, est. speed input: 63134.20 toks/s, output: 61.65 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:30<00:03, 56.72it/s, est. speed input: 63107.28 toks/s, output: 61.63 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:30<00:02, 56.31it/s, est. speed input: 63047.07 toks/s, output: 61.57 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:30<00:02, 56.04it/s, est. speed input: 62988.46 toks/s, output: 61.51 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:31<00:02, 55.87it/s, est. speed input: 62931.36 toks/s, output: 61.46 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:31<00:01, 55.73it/s, est. speed input: 62874.75 toks/s, output: 61.40 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:31<00:01, 55.64it/s, est. speed input: 62819.38 toks/s, output: 61.35 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:32<00:01, 55.57it/s, est. speed input: 62764.53 toks/s, output: 61.29 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:32<00:01, 55.51it/s, est. speed input: 62710.63 toks/s, output: 61.24 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:32<00:00, 55.48it/s, est. speed input: 62657.90 toks/s, output: 61.19 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:33<00:00, 55.46it/s, est. speed input: 62606.30 toks/s, output: 61.14 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:33<00:00, 56.54it/s, est. speed input: 62590.49 toks/s, output: 61.12 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 56.54it/s, est. speed input: 63020.62 toks/s, output: 61.54 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:33<00:00, 61.54it/s, est. speed input: 63020.62 toks/s, output: 61.54 toks/s]
[rank0]:[W128 11:40:25.859146389 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 90.1s

测试结果:
  Requests/s:   55.74
  Tokens/s:     57135.06
  Total Reqs:   2048
  Elapsed:      36.74s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     57079.32

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:40:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:40:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=26753) WARNING 01-28 11:41:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=26753) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=26753) WARNING 01-28 11:41:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 57.59 requests/s, 59026.51 total tokens/s, 57.59 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 11:40:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:40:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:40:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:40:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:40:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:40:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:40:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:40:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:40:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:41:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:41:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 11:41:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 11:41:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 11:41:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:41:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:41:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:41:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:41:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=26753) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=26753) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=26753) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=26753) 
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=26753) [2026-01-28 11:41:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=26753) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.55it/s]
(EngineCore_DP0 pid=26753) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.92it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 36/4096 [00:00<00:11, 351.76it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:11, 350.16it/s]
Adding requests:   3%|▎         | 110/4096 [00:00<00:11, 361.25it/s]
Adding requests:   4%|▎         | 147/4096 [00:00<00:10, 363.42it/s]
Adding requests:   5%|▍         | 185/4096 [00:00<00:10, 368.56it/s]
Adding requests:   5%|▌         | 224/4096 [00:00<00:10, 373.43it/s]
Adding requests:   6%|▋         | 264/4096 [00:00<00:10, 379.31it/s]
Adding requests:   7%|▋         | 302/4096 [00:00<00:10, 378.62it/s]
Adding requests:   8%|▊         | 340/4096 [00:00<00:09, 377.76it/s]
Adding requests:   9%|▉         | 380/4096 [00:01<00:09, 384.40it/s]
Adding requests:  10%|█         | 420/4096 [00:01<00:09, 386.90it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:09, 387.20it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:10, 344.21it/s]
Adding requests:  13%|█▎        | 534/4096 [00:01<00:10, 343.65it/s]
Adding requests:  14%|█▍        | 574/4096 [00:01<00:09, 358.79it/s]
Adding requests:  15%|█▍        | 612/4096 [00:01<00:09, 364.17it/s]
Adding requests:  16%|█▌        | 653/4096 [00:01<00:09, 375.24it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:08, 385.33it/s]
Adding requests:  18%|█▊        | 733/4096 [00:01<00:08, 385.84it/s]
Adding requests:  19%|█▉        | 772/4096 [00:02<00:08, 381.45it/s]
Adding requests:  20%|█▉        | 811/4096 [00:02<00:08, 377.78it/s]
Adding requests:  21%|██        | 849/4096 [00:02<00:08, 375.05it/s]
Adding requests:  22%|██▏       | 890/4096 [00:02<00:08, 383.55it/s]
Adding requests:  23%|██▎       | 930/4096 [00:02<00:08, 387.38it/s]
Adding requests:  24%|██▎       | 969/4096 [00:02<00:08, 388.06it/s]
Adding requests:  25%|██▍       | 1009/4096 [00:02<00:07, 390.81it/s]
Adding requests:  26%|██▌       | 1049/4096 [00:02<00:07, 391.27it/s]
Adding requests:  27%|██▋       | 1089/4096 [00:02<00:08, 375.84it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:03<00:07, 378.09it/s]
Adding requests:  29%|██▊       | 1168/4096 [00:03<00:07, 382.99it/s]
Adding requests:  30%|██▉       | 1210/4096 [00:03<00:07, 390.67it/s]
Adding requests:  31%|███       | 1250/4096 [00:03<00:07, 391.44it/s]
Adding requests:  31%|███▏      | 1290/4096 [00:03<00:07, 389.80it/s]
Adding requests:  32%|███▏      | 1331/4096 [00:03<00:06, 395.41it/s]
Adding requests:  33%|███▎      | 1371/4096 [00:03<00:06, 395.59it/s]
Adding requests:  34%|███▍      | 1412/4096 [00:03<00:06, 398.12it/s]
Adding requests:  35%|███▌      | 1452/4096 [00:03<00:06, 396.51it/s]
Adding requests:  36%|███▋      | 1494/4096 [00:03<00:06, 400.75it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:04<00:06, 397.62it/s]
Adding requests:  38%|███▊      | 1575/4096 [00:04<00:06, 396.23it/s]
Adding requests:  39%|███▉      | 1616/4096 [00:04<00:06, 397.69it/s]
Adding requests:  40%|████      | 1656/4096 [00:04<00:06, 396.10it/s]
Adding requests:  41%|████▏     | 1696/4096 [00:04<00:06, 393.47it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:04<00:06, 389.96it/s]
Adding requests:  43%|████▎     | 1776/4096 [00:04<00:06, 386.51it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:04<00:05, 389.20it/s]
Adding requests:  45%|████▌     | 1855/4096 [00:04<00:05, 381.77it/s]
Adding requests:  46%|████▌     | 1894/4096 [00:04<00:05, 378.65it/s]
Adding requests:  47%|████▋     | 1932/4096 [00:05<00:05, 372.07it/s]
Adding requests:  48%|████▊     | 1970/4096 [00:05<00:05, 365.40it/s]
Adding requests:  49%|████▉     | 2009/4096 [00:05<00:05, 371.77it/s]
Adding requests:  50%|████▉     | 2047/4096 [00:05<00:05, 371.35it/s]
Adding requests:  51%|█████     | 2085/4096 [00:05<00:05, 373.69it/s]
Adding requests:  52%|█████▏    | 2123/4096 [00:05<00:05, 367.89it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:05<00:05, 361.00it/s]
Adding requests:  54%|█████▎    | 2197/4096 [00:05<00:05, 361.46it/s]
Adding requests:  55%|█████▍    | 2235/4096 [00:05<00:05, 365.60it/s]
Adding requests:  55%|█████▌    | 2273/4096 [00:05<00:04, 369.04it/s]
Adding requests:  56%|█████▋    | 2310/4096 [00:06<00:04, 359.17it/s]
Adding requests:  57%|█████▋    | 2348/4096 [00:06<00:04, 362.15it/s]
Adding requests:  58%|█████▊    | 2386/4096 [00:06<00:04, 366.68it/s]
Adding requests:  59%|█████▉    | 2423/4096 [00:06<00:04, 367.54it/s]
Adding requests:  60%|██████    | 2461/4096 [00:06<00:04, 368.76it/s]
Adding requests:  61%|██████    | 2500/4096 [00:06<00:04, 371.31it/s]
Adding requests:  62%|██████▏   | 2539/4096 [00:06<00:04, 374.35it/s]
Adding requests:  63%|██████▎   | 2579/4096 [00:06<00:03, 381.53it/s]
Adding requests:  64%|██████▍   | 2618/4096 [00:06<00:03, 381.67it/s]
Adding requests:  65%|██████▍   | 2657/4096 [00:07<00:03, 379.11it/s]
Adding requests:  66%|██████▌   | 2695/4096 [00:07<00:03, 375.52it/s]
Adding requests:  67%|██████▋   | 2733/4096 [00:07<00:03, 375.06it/s]
Adding requests:  68%|██████▊   | 2772/4096 [00:07<00:03, 376.04it/s]
Adding requests:  69%|██████▊   | 2810/4096 [00:07<00:03, 372.55it/s]
Adding requests:  70%|██████▉   | 2848/4096 [00:07<00:03, 371.75it/s]
Adding requests:  70%|███████   | 2886/4096 [00:07<00:03, 371.68it/s]
Adding requests:  71%|███████▏  | 2924/4096 [00:07<00:03, 367.90it/s]
Adding requests:  72%|███████▏  | 2961/4096 [00:07<00:03, 363.96it/s]
Adding requests:  73%|███████▎  | 3000/4096 [00:07<00:02, 368.92it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:08<00:02, 374.90it/s]
Adding requests:  75%|███████▌  | 3077/4096 [00:08<00:02, 370.29it/s]
Adding requests:  76%|███████▌  | 3119/4096 [00:08<00:02, 382.29it/s]
Adding requests:  77%|███████▋  | 3159/4096 [00:08<00:02, 385.68it/s]
Adding requests:  78%|███████▊  | 3198/4096 [00:08<00:02, 381.60it/s]
Adding requests:  79%|███████▉  | 3238/4096 [00:08<00:02, 386.00it/s]
Adding requests:  80%|████████  | 3277/4096 [00:08<00:02, 385.24it/s]
Adding requests:  81%|████████  | 3316/4096 [00:08<00:02, 384.39it/s]
Adding requests:  82%|████████▏ | 3357/4096 [00:08<00:01, 389.73it/s]
Adding requests:  83%|████████▎ | 3396/4096 [00:08<00:01, 386.99it/s]
Adding requests:  84%|████████▍ | 3437/4096 [00:09<00:01, 392.63it/s]
Adding requests:  85%|████████▍ | 3477/4096 [00:09<00:01, 386.16it/s]
Adding requests:  86%|████████▌ | 3517/4096 [00:09<00:01, 388.61it/s]
Adding requests:  87%|████████▋ | 3557/4096 [00:09<00:01, 391.20it/s]
Adding requests:  88%|████████▊ | 3597/4096 [00:09<00:01, 392.57it/s]
Adding requests:  89%|████████▉ | 3637/4096 [00:09<00:01, 376.76it/s]
Adding requests:  90%|████████▉ | 3677/4096 [00:09<00:01, 381.58it/s]
Adding requests:  91%|█████████ | 3717/4096 [00:09<00:00, 386.63it/s]
Adding requests:  92%|█████████▏| 3757/4096 [00:09<00:00, 390.45it/s]
Adding requests:  93%|█████████▎| 3800/4096 [00:10<00:00, 399.13it/s]
Adding requests:  94%|█████████▍| 3842/4096 [00:10<00:00, 402.33it/s]
Adding requests:  95%|█████████▍| 3883/4096 [00:10<00:00, 400.56it/s]
Adding requests:  96%|█████████▌| 3924/4096 [00:10<00:00, 401.46it/s]
Adding requests:  97%|█████████▋| 3965/4096 [00:10<00:00, 400.98it/s]
Adding requests:  98%|█████████▊| 4006/4096 [00:10<00:00, 396.66it/s]
Adding requests:  99%|█████████▉| 4046/4096 [00:10<00:00, 392.37it/s]
Adding requests: 100%|█████████▉| 4087/4096 [00:10<00:00, 395.22it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 380.89it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:00<00:03, 1017.63it/s, est. speed input: 1042107.04 toks/s, output: 1017.64 toks/s]
Processed prompts:  17%|█▋        | 712/4096 [00:04<00:25, 133.78it/s, est. speed input: 176356.15 toks/s, output: 172.22 toks/s]   
Processed prompts:  18%|█▊        | 756/4096 [00:04<00:26, 124.76it/s, est. speed input: 165167.18 toks/s, output: 161.30 toks/s]
Processed prompts:  19%|█▉        | 784/4096 [00:05<00:29, 110.59it/s, est. speed input: 153270.58 toks/s, output: 149.68 toks/s]
Processed prompts:  20%|█▉        | 803/4096 [00:05<00:34, 94.23it/s, est. speed input: 142036.64 toks/s, output: 138.71 toks/s] 
Processed prompts:  20%|██        | 834/4096 [00:06<00:38, 84.97it/s, est. speed input: 134499.93 toks/s, output: 131.35 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:06<00:41, 77.90it/s, est. speed input: 128336.93 toks/s, output: 125.33 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:07<00:44, 72.37it/s, est. speed input: 123095.20 toks/s, output: 120.21 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:08<00:46, 68.21it/s, est. speed input: 118606.82 toks/s, output: 115.83 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:08<00:48, 65.01it/s, est. speed input: 114657.80 toks/s, output: 111.97 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:09<00:49, 62.74it/s, est. speed input: 111216.48 toks/s, output: 108.61 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:09<00:50, 61.09it/s, est. speed input: 108170.26 toks/s, output: 105.63 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:10<00:50, 59.90it/s, est. speed input: 105453.87 toks/s, output: 102.98 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:10<00:50, 59.06it/s, est. speed input: 103020.23 toks/s, output: 100.61 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:11<00:50, 58.46it/s, est. speed input: 100822.56 toks/s, output: 98.46 toks/s] 
Processed prompts:  28%|██▊       | 1154/4096 [00:11<00:50, 58.53it/s, est. speed input: 98965.25 toks/s, output: 96.65 toks/s] 
Processed prompts:  29%|██▉       | 1186/4096 [00:12<00:50, 58.09it/s, est. speed input: 97146.66 toks/s, output: 94.87 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:13<00:49, 57.78it/s, est. speed input: 95482.94 toks/s, output: 93.24 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:13<00:49, 57.54it/s, est. speed input: 93953.60 toks/s, output: 91.75 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:14<00:48, 57.71it/s, est. speed input: 92615.49 toks/s, output: 90.44 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:14<00:47, 57.98it/s, est. speed input: 91407.07 toks/s, output: 89.26 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:15<00:47, 58.18it/s, est. speed input: 90286.31 toks/s, output: 88.17 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:15<00:46, 58.32it/s, est. speed input: 89243.36 toks/s, output: 87.15 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:16<00:45, 58.42it/s, est. speed input: 88269.46 toks/s, output: 86.20 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:16<00:45, 58.48it/s, est. speed input: 87357.22 toks/s, output: 85.31 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:17<00:44, 58.52it/s, est. speed input: 86502.85 toks/s, output: 84.48 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:17<00:44, 58.55it/s, est. speed input: 85699.65 toks/s, output: 83.69 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:18<00:43, 58.58it/s, est. speed input: 84945.10 toks/s, output: 82.95 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:19<00:43, 58.61it/s, est. speed input: 84234.84 toks/s, output: 82.26 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:19<00:42, 58.63it/s, est. speed input: 83563.75 toks/s, output: 81.61 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:20<00:42, 58.57it/s, est. speed input: 82919.95 toks/s, output: 80.98 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:20<00:41, 58.53it/s, est. speed input: 82310.13 toks/s, output: 80.38 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:21<00:40, 58.59it/s, est. speed input: 81741.78 toks/s, output: 79.83 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:21<00:40, 58.61it/s, est. speed input: 81199.61 toks/s, output: 79.30 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:22<00:40, 58.22it/s, est. speed input: 80638.14 toks/s, output: 78.75 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:22<00:39, 57.95it/s, est. speed input: 80105.01 toks/s, output: 78.23 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:23<00:39, 57.76it/s, est. speed input: 79595.98 toks/s, output: 77.73 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:24<00:38, 58.15it/s, est. speed input: 79165.72 toks/s, output: 77.31 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:24<00:38, 57.88it/s, est. speed input: 78699.49 toks/s, output: 76.85 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:25<00:37, 57.72it/s, est. speed input: 78255.69 toks/s, output: 76.42 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:25<00:37, 57.59it/s, est. speed input: 77830.55 toks/s, output: 76.01 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:26<00:36, 57.51it/s, est. speed input: 77424.06 toks/s, output: 75.61 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:26<00:36, 57.46it/s, est. speed input: 77034.50 toks/s, output: 75.23 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:27<00:35, 57.42it/s, est. speed input: 76661.02 toks/s, output: 74.86 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:27<00:35, 57.39it/s, est. speed input: 76302.43 toks/s, output: 74.51 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:28<00:34, 57.39it/s, est. speed input: 75958.81 toks/s, output: 74.18 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:29<00:33, 57.38it/s, est. speed input: 75628.27 toks/s, output: 73.86 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:29<00:33, 57.37it/s, est. speed input: 75309.92 toks/s, output: 73.54 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:30<00:32, 57.47it/s, est. speed input: 75011.91 toks/s, output: 73.25 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:30<00:32, 57.59it/s, est. speed input: 74728.24 toks/s, output: 72.98 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:31<00:31, 58.19it/s, est. speed input: 74493.44 toks/s, output: 72.75 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:31<00:30, 58.09it/s, est. speed input: 74228.66 toks/s, output: 72.49 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:32<00:30, 58.03it/s, est. speed input: 73973.43 toks/s, output: 72.24 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:32<00:29, 57.99it/s, est. speed input: 73726.89 toks/s, output: 72.00 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:33<00:29, 57.97it/s, est. speed input: 73488.97 toks/s, output: 71.77 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:34<00:28, 57.94it/s, est. speed input: 73257.57 toks/s, output: 71.54 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:34<00:28, 57.92it/s, est. speed input: 73033.62 toks/s, output: 71.32 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:35<00:27, 58.42it/s, est. speed input: 72850.47 toks/s, output: 71.14 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:37<00:54, 28.59it/s, est. speed input: 68965.65 toks/s, output: 67.35 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:38<00:45, 33.72it/s, est. speed input: 68827.77 toks/s, output: 67.21 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:38<00:38, 38.52it/s, est. speed input: 68687.72 toks/s, output: 67.08 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:39<00:34, 42.81it/s, est. speed input: 68554.23 toks/s, output: 66.95 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:39<00:30, 46.40it/s, est. speed input: 68422.21 toks/s, output: 66.82 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:40<00:28, 49.14it/s, est. speed input: 68282.19 toks/s, output: 66.68 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:40<00:26, 51.26it/s, est. speed input: 68145.63 toks/s, output: 66.55 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:41<00:25, 52.85it/s, est. speed input: 68012.73 toks/s, output: 66.42 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:42<00:24, 54.02it/s, est. speed input: 67883.65 toks/s, output: 66.29 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:42<00:23, 54.88it/s, est. speed input: 67758.01 toks/s, output: 66.17 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:43<00:22, 55.50it/s, est. speed input: 67635.69 toks/s, output: 66.05 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:43<00:21, 55.93it/s, est. speed input: 67516.28 toks/s, output: 65.93 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:44<00:21, 56.24it/s, est. speed input: 67399.97 toks/s, output: 65.82 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:44<00:20, 56.44it/s, est. speed input: 67285.65 toks/s, output: 65.71 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:45<00:19, 56.60it/s, est. speed input: 67174.99 toks/s, output: 65.60 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:45<00:19, 56.71it/s, est. speed input: 67067.05 toks/s, output: 65.50 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:46<00:18, 56.79it/s, est. speed input: 66961.46 toks/s, output: 65.39 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:47<00:17, 56.85it/s, est. speed input: 66859.07 toks/s, output: 65.29 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:47<00:17, 56.89it/s, est. speed input: 66758.88 toks/s, output: 65.19 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:48<00:16, 57.32it/s, est. speed input: 66678.97 toks/s, output: 65.12 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:48<00:16, 57.69it/s, est. speed input: 66603.97 toks/s, output: 65.04 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:49<00:15, 58.01it/s, est. speed input: 66532.73 toks/s, output: 64.97 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:49<00:14, 58.12it/s, est. speed input: 66458.42 toks/s, output: 64.90 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:50<00:14, 58.25it/s, est. speed input: 66388.04 toks/s, output: 64.83 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:50<00:13, 58.34it/s, est. speed input: 66319.11 toks/s, output: 64.76 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:51<00:13, 58.42it/s, est. speed input: 66252.06 toks/s, output: 64.70 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:52<00:12, 58.48it/s, est. speed input: 66186.63 toks/s, output: 64.64 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:52<00:12, 58.49it/s, est. speed input: 66121.41 toks/s, output: 64.57 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:53<00:11, 58.51it/s, est. speed input: 66058.27 toks/s, output: 64.51 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:53<00:10, 58.52it/s, est. speed input: 65995.91 toks/s, output: 64.45 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:54<00:10, 58.53it/s, est. speed input: 65934.92 toks/s, output: 64.39 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:54<00:09, 58.54it/s, est. speed input: 65875.31 toks/s, output: 64.33 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:55<00:09, 58.57it/s, est. speed input: 65818.06 toks/s, output: 64.28 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:55<00:08, 58.47it/s, est. speed input: 65757.18 toks/s, output: 64.22 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:56<00:08, 58.12it/s, est. speed input: 65687.22 toks/s, output: 64.15 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:56<00:07, 57.90it/s, est. speed input: 65619.46 toks/s, output: 64.08 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:57<00:07, 57.72it/s, est. speed input: 65552.19 toks/s, output: 64.02 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:58<00:06, 58.12it/s, est. speed input: 65504.83 toks/s, output: 63.97 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:58<00:06, 57.88it/s, est. speed input: 65440.02 toks/s, output: 63.91 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:59<00:05, 57.72it/s, est. speed input: 65376.70 toks/s, output: 63.84 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:59<00:04, 57.62it/s, est. speed input: 65314.90 toks/s, output: 63.78 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:00<00:04, 57.53it/s, est. speed input: 65253.70 toks/s, output: 63.72 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:00<00:03, 57.48it/s, est. speed input: 65193.91 toks/s, output: 63.67 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:01<00:03, 57.44it/s, est. speed input: 65135.21 toks/s, output: 63.61 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:01<00:02, 57.40it/s, est. speed input: 65077.20 toks/s, output: 63.55 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:02<00:02, 57.39it/s, est. speed input: 65020.71 toks/s, output: 63.50 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:03<00:01, 57.38it/s, est. speed input: 64965.05 toks/s, output: 63.44 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:03<00:01, 57.90it/s, est. speed input: 64927.53 toks/s, output: 63.41 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:04<00:00, 58.52it/s, est. speed input: 64898.90 toks/s, output: 63.38 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:04<00:00, 58.52it/s, est. speed input: 65377.24 toks/s, output: 63.84 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:04<00:00, 63.84it/s, est. speed input: 65377.24 toks/s, output: 63.84 toks/s]
[rank0]:[W128 11:42:45.605607369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 140.2s

测试结果:
  Requests/s:   57.59
  Tokens/s:     59026.51
  Total Reqs:   4096
  Elapsed:      71.13s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     58968.92


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,16.9370,8688.7018,7.5574
1024,1024,1,128,128,16.2747,16681.5491,7.8650
2048,1024,2,256,128,34.5979,35462.8504,7.3993
4096,1024,4,512,128,52.0195,53319.9513,9.8425
8192,1024,8,1024,128,54.2176,55572.9931,18.8869
16384,1024,16,2048,128,55.7415,57135.0572,36.7410
32768,1024,32,4096,128,57.5868,59026.5074,71.1274

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_110101.log
[SUCCESS] bitnet1.58-2b-int8 Prefill 完成 (2512.8s)

------------------------------------------------------------
  Prefill Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 4090                   ││
│ GPU (short):      RTX4090                                   │
│ Memory:           24.0 GB                                    │
│ CC:               cc89 (Ampere)                              ││
│ SM Code:          sm_89                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           581.80                                    │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['prefill']
  M_prefill:        [512, 1024, 2048, 4096, 8192, 16384, 32768]
  M_decode:         [512, 1024, 2048, 4096, 8192, 16384, 32768]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_114255.log


============================================================
  BitNet-2B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:43:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:43:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=27819) WARNING 01-28 11:43:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=27819) WARNING 01-28 11:43:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.72 requests/s, 8064.80 total tokens/s, 15.72 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:43:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:43:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:43:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:43:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:43:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:43:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:43:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:43:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:43:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:43:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:43:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:43:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:43:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:43:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:43:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:43:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:43:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:43:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=27819) [2026-01-28 11:43:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=27819) [2026-01-28 11:43:10] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=27819) [2026-01-28 11:43:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=27819) [2026-01-28 11:43:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=27819) [2026-01-28 11:43:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=27819) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=27819) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=27819) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.42it/s]
(EngineCore_DP0 pid=27819) 
(EngineCore_DP0 pid=27819) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]
(EngineCore_DP0 pid=27819) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  49%|████▉     | 63/128 [00:00<00:00, 627.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 660.72it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 31.40it/s, est. speed input: 16079.40 toks/s, output: 31.40 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:06, 19.80it/s, est. speed input: 10732.77 toks/s, output: 20.96 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:06, 17.92it/s, est. speed input: 9808.05 toks/s, output: 19.16 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:06, 17.28it/s, est. speed input: 9490.97 toks/s, output: 18.54 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 16.80it/s, est. speed input: 9265.60 toks/s, output: 18.10 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 16.46it/s, est. speed input: 9098.26 toks/s, output: 17.77 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.21it/s, est. speed input: 8968.09 toks/s, output: 17.52 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 15.95it/s, est. speed input: 8850.81 toks/s, output: 17.29 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 15.94it/s, est. speed input: 8784.32 toks/s, output: 17.16 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 15.83it/s, est. speed input: 8713.85 toks/s, output: 17.02 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 15.80it/s, est. speed input: 8661.76 toks/s, output: 16.92 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 15.75it/s, est. speed input: 8612.35 toks/s, output: 16.82 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:06, 15.77it/s, est. speed input: 8577.59 toks/s, output: 16.75 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:06, 15.73it/s, est. speed input: 8540.48 toks/s, output: 16.68 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 15.78it/s, est. speed input: 8516.43 toks/s, output: 16.63 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 15.82it/s, est. speed input: 8495.66 toks/s, output: 16.59 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 15.84it/s, est. speed input: 8476.96 toks/s, output: 16.56 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 15.82it/s, est. speed input: 8455.69 toks/s, output: 16.51 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 15.74it/s, est. speed input: 8431.60 toks/s, output: 16.47 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 15.71it/s, est. speed input: 8412.01 toks/s, output: 16.43 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:05, 15.75it/s, est. speed input: 8398.84 toks/s, output: 16.40 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:05, 15.77it/s, est. speed input: 8386.06 toks/s, output: 16.38 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 15.80it/s, est. speed input: 8375.70 toks/s, output: 16.36 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 15.86it/s, est. speed input: 8368.54 toks/s, output: 16.34 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 15.86it/s, est. speed input: 8359.37 toks/s, output: 16.33 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 15.90it/s, est. speed input: 8353.20 toks/s, output: 16.31 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.03it/s, est. speed input: 8353.52 toks/s, output: 16.32 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.07it/s, est. speed input: 8351.16 toks/s, output: 16.31 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:04, 16.04it/s, est. speed input: 8345.67 toks/s, output: 16.30 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.04it/s, est. speed input: 8341.60 toks/s, output: 16.29 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.04it/s, est. speed input: 8337.44 toks/s, output: 16.28 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.11it/s, est. speed input: 8337.47 toks/s, output: 16.28 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.16it/s, est. speed input: 8337.28 toks/s, output: 16.28 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.17it/s, est. speed input: 8336.11 toks/s, output: 16.28 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.18it/s, est. speed input: 8334.98 toks/s, output: 16.28 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.15it/s, est. speed input: 8332.27 toks/s, output: 16.27 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:03, 15.91it/s, est. speed input: 8319.96 toks/s, output: 16.25 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 15.96it/s, est. speed input: 8317.82 toks/s, output: 16.25 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.04it/s, est. speed input: 8317.75 toks/s, output: 16.25 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 15.85it/s, est. speed input: 8307.10 toks/s, output: 16.22 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 15.89it/s, est. speed input: 8304.46 toks/s, output: 16.22 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 15.96it/s, est. speed input: 8303.32 toks/s, output: 16.22 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 15.96it/s, est. speed input: 8300.50 toks/s, output: 16.21 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.03it/s, est. speed input: 8300.07 toks/s, output: 16.21 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:02, 16.04it/s, est. speed input: 8298.49 toks/s, output: 16.21 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.10it/s, est. speed input: 8298.89 toks/s, output: 16.21 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.05it/s, est. speed input: 8295.95 toks/s, output: 16.20 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 15.95it/s, est. speed input: 8291.09 toks/s, output: 16.19 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 15.86it/s, est. speed input: 8285.59 toks/s, output: 16.18 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 15.71it/s, est. speed input: 8277.17 toks/s, output: 16.17 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 15.64it/s, est. speed input: 8270.36 toks/s, output: 16.15 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 15.53it/s, est. speed input: 8261.62 toks/s, output: 16.14 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 15.59it/s, est. speed input: 8257.76 toks/s, output: 16.13 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:07<00:00, 15.69it/s, est. speed input: 8255.85 toks/s, output: 16.12 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 15.76it/s, est. speed input: 8254.24 toks/s, output: 16.12 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 15.79it/s, est. speed input: 8251.86 toks/s, output: 16.12 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 15.83it/s, est. speed input: 8250.05 toks/s, output: 16.11 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 15.80it/s, est. speed input: 8246.68 toks/s, output: 16.11 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 15.83it/s, est. speed input: 8244.99 toks/s, output: 16.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 15.91it/s, est. speed input: 8245.06 toks/s, output: 16.10 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.00it/s, est. speed input: 8245.92 toks/s, output: 16.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.00it/s, est. speed input: 8246.86 toks/s, output: 16.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.11it/s, est. speed input: 8246.86 toks/s, output: 16.11 toks/s]
[rank0]:[W128 11:43:41.572699397 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.8s

测试结果:
  Requests/s:   15.72
  Tokens/s:     8064.80
  Total Reqs:   128
  Elapsed:      8.14s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8049.08

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:43:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:43:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=28481) WARNING 01-28 11:43:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=28481) WARNING 01-28 11:44:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.64 requests/s, 16029.04 total tokens/s, 15.64 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:43:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:43:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:43:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:43:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:43:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:43:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:43:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:43:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:43:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:43:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:43:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:43:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:43:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:43:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:43:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:43:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:43:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:43:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:43:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=28481) [2026-01-28 11:43:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=28481) [2026-01-28 11:43:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=28481) [2026-01-28 11:43:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=28481) [2026-01-28 11:43:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=28481) [2026-01-28 11:43:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=28481) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=28481) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.46s/it]
(EngineCore_DP0 pid=28481) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.46s/it]
(EngineCore_DP0 pid=28481) 
(EngineCore_DP0 pid=28481) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.16it/s]
(EngineCore_DP0 pid=28481) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 34/128 [00:00<00:00, 339.84it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 367.48it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 370.98it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 370.32it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 49.60it/s, est. speed input: 50819.90 toks/s, output: 49.61 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:05, 22.82it/s, est. speed input: 25632.26 toks/s, output: 25.03 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:05, 20.05it/s, est. speed input: 22823.29 toks/s, output: 22.29 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 18.45it/s, est. speed input: 21245.76 toks/s, output: 20.75 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.53it/s, est. speed input: 20283.74 toks/s, output: 19.81 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.00it/s, est. speed input: 19772.12 toks/s, output: 19.31 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 16.72it/s, est. speed input: 19425.92 toks/s, output: 18.97 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.46it/s, est. speed input: 19127.94 toks/s, output: 18.68 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:06, 16.28it/s, est. speed input: 18881.68 toks/s, output: 18.44 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:06, 16.14it/s, est. speed input: 18674.65 toks/s, output: 18.24 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 16.02it/s, est. speed input: 18487.86 toks/s, output: 18.05 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 15.97it/s, est. speed input: 18339.46 toks/s, output: 17.91 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 15.95it/s, est. speed input: 18210.29 toks/s, output: 17.78 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 15.94it/s, est. speed input: 18099.71 toks/s, output: 17.68 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 15.92it/s, est. speed input: 17997.47 toks/s, output: 17.58 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 15.84it/s, est. speed input: 17891.84 toks/s, output: 17.47 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 15.86it/s, est. speed input: 17812.15 toks/s, output: 17.39 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:05, 15.82it/s, est. speed input: 17731.24 toks/s, output: 17.32 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:05, 15.78it/s, est. speed input: 17654.97 toks/s, output: 17.24 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 15.77it/s, est. speed input: 17587.84 toks/s, output: 17.18 toks/s]
Processed prompts:  41%|████      | 52/128 [00:03<00:04, 15.72it/s, est. speed input: 17520.76 toks/s, output: 17.11 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 15.74it/s, est. speed input: 17466.31 toks/s, output: 17.06 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 15.68it/s, est. speed input: 17406.14 toks/s, output: 17.00 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 15.73it/s, est. speed input: 17361.71 toks/s, output: 16.95 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 15.78it/s, est. speed input: 17323.68 toks/s, output: 16.92 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:04, 15.84it/s, est. speed input: 17291.13 toks/s, output: 16.89 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:04, 15.80it/s, est. speed input: 17250.91 toks/s, output: 16.85 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 15.74it/s, est. speed input: 17209.62 toks/s, output: 16.81 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:04<00:03, 15.80it/s, est. speed input: 17181.94 toks/s, output: 16.78 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 15.84it/s, est. speed input: 17155.83 toks/s, output: 16.75 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 15.85it/s, est. speed input: 17129.04 toks/s, output: 16.73 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 15.89it/s, est. speed input: 17107.95 toks/s, output: 16.71 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 15.89it/s, est. speed input: 17085.02 toks/s, output: 16.68 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 15.89it/s, est. speed input: 17063.02 toks/s, output: 16.66 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:03, 15.93it/s, est. speed input: 17045.73 toks/s, output: 16.65 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 15.91it/s, est. speed input: 17025.75 toks/s, output: 16.63 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:05<00:02, 15.93it/s, est. speed input: 17009.26 toks/s, output: 16.61 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 15.83it/s, est. speed input: 16983.74 toks/s, output: 16.59 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 15.80it/s, est. speed input: 16962.78 toks/s, output: 16.57 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 15.77it/s, est. speed input: 16941.89 toks/s, output: 16.54 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 15.82it/s, est. speed input: 16927.62 toks/s, output: 16.53 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 15.85it/s, est. speed input: 16914.34 toks/s, output: 16.52 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:02, 15.82it/s, est. speed input: 16896.67 toks/s, output: 16.50 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 15.82it/s, est. speed input: 16881.84 toks/s, output: 16.49 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:06<00:01, 15.87it/s, est. speed input: 16871.29 toks/s, output: 16.48 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 15.88it/s, est. speed input: 16859.51 toks/s, output: 16.46 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 15.89it/s, est. speed input: 16848.62 toks/s, output: 16.45 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 15.88it/s, est. speed input: 16836.23 toks/s, output: 16.44 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 15.89it/s, est. speed input: 16825.91 toks/s, output: 16.43 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 15.92it/s, est. speed input: 16817.23 toks/s, output: 16.42 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:01, 15.91it/s, est. speed input: 16807.30 toks/s, output: 16.41 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 15.89it/s, est. speed input: 16796.68 toks/s, output: 16.40 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:07<00:00, 15.89it/s, est. speed input: 16787.30 toks/s, output: 16.39 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 15.66it/s, est. speed input: 16764.07 toks/s, output: 16.37 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 15.64it/s, est. speed input: 16750.03 toks/s, output: 16.36 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 15.73it/s, est. speed input: 16743.22 toks/s, output: 16.35 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 15.79it/s, est. speed input: 16736.08 toks/s, output: 16.34 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 15.85it/s, est. speed input: 16730.05 toks/s, output: 16.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 15.87it/s, est. speed input: 16723.09 toks/s, output: 16.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 15.87it/s, est. speed input: 16723.09 toks/s, output: 16.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.33it/s, est. speed input: 16723.09 toks/s, output: 16.33 toks/s]
[rank0]:[W128 11:44:29.112805439 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.5s

测试结果:
  Requests/s:   15.64
  Tokens/s:     16029.04
  Total Reqs:   128
  Elapsed:      8.19s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     16013.40

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:44:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:44:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=29488) WARNING 01-28 11:44:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=29488) WARNING 01-28 11:44:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.42 requests/s, 31178.52 total tokens/s, 30.42 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 11:44:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:44:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:44:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:44:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:44:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:44:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:44:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:44:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:44:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:44:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:44:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:44:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:44:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:44:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:44:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:44:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:44:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:44:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:44:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=29488) [2026-01-28 11:44:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=29488) [2026-01-28 11:44:50] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=29488) [2026-01-28 11:44:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=29488) [2026-01-28 11:44:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=29488) [2026-01-28 11:44:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=29488) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=29488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=29488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=29488) 
(EngineCore_DP0 pid=29488) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:02<00:00,  1.03s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:02<00:00,  1.27it/s]
(EngineCore_DP0 pid=29488) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.56it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 36/256 [00:00<00:00, 359.08it/s]
Adding requests:  29%|██▉       | 74/256 [00:00<00:00, 371.10it/s]
Adding requests:  44%|████▍     | 112/256 [00:00<00:00, 373.12it/s]
Adding requests:  59%|█████▊    | 150/256 [00:00<00:00, 370.08it/s]
Adding requests:  73%|███████▎  | 188/256 [00:00<00:00, 368.94it/s]
Adding requests:  88%|████████▊ | 225/256 [00:00<00:00, 368.57it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 369.95it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:02, 117.62it/s, est. speed input: 120468.99 toks/s, output: 117.63 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:00<00:04, 49.12it/s, est. speed input: 56194.61 toks/s, output: 54.87 toks/s]   
Processed prompts:  14%|█▍        | 37/256 [00:00<00:04, 44.54it/s, est. speed input: 51205.01 toks/s, output: 50.00 toks/s]
Processed prompts:  17%|█▋        | 43/256 [00:00<00:05, 40.19it/s, est. speed input: 47326.29 toks/s, output: 46.22 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:01<00:05, 35.74it/s, est. speed input: 43884.84 toks/s, output: 42.86 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:05, 34.76it/s, est. speed input: 42711.20 toks/s, output: 41.71 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:05, 33.87it/s, est. speed input: 41710.53 toks/s, output: 40.73 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:01<00:05, 33.14it/s, est. speed input: 40870.57 toks/s, output: 39.91 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:05, 32.71it/s, est. speed input: 40207.68 toks/s, output: 39.26 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:05, 32.38it/s, est. speed input: 39634.86 toks/s, output: 38.71 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:05, 32.22it/s, est. speed input: 39164.57 toks/s, output: 38.25 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:02<00:05, 32.12it/s, est. speed input: 38755.94 toks/s, output: 37.85 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:02<00:05, 31.84it/s, est. speed input: 38345.30 toks/s, output: 37.45 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:02<00:05, 31.68it/s, est. speed input: 37989.69 toks/s, output: 37.10 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:05, 31.67it/s, est. speed input: 37694.20 toks/s, output: 36.81 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:05, 31.59it/s, est. speed input: 37414.42 toks/s, output: 36.54 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:05, 31.58it/s, est. speed input: 37169.32 toks/s, output: 36.30 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 31.44it/s, est. speed input: 36922.86 toks/s, output: 36.06 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 31.24it/s, est. speed input: 36680.78 toks/s, output: 35.82 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:03<00:04, 31.21it/s, est. speed input: 36478.60 toks/s, output: 35.62 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:03<00:04, 31.18it/s, est. speed input: 36289.70 toks/s, output: 35.44 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:03<00:04, 30.91it/s, est. speed input: 36079.67 toks/s, output: 35.23 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:03<00:04, 30.67it/s, est. speed input: 35875.73 toks/s, output: 35.03 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:04, 30.54it/s, est. speed input: 35694.35 toks/s, output: 34.86 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:04, 30.43it/s, est. speed input: 35521.43 toks/s, output: 34.69 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:04, 30.45it/s, est. speed input: 35373.86 toks/s, output: 34.54 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 30.46it/s, est. speed input: 35235.44 toks/s, output: 34.41 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:04<00:03, 30.56it/s, est. speed input: 35117.53 toks/s, output: 34.29 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:04<00:03, 30.69it/s, est. speed input: 35013.95 toks/s, output: 34.19 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:04<00:03, 30.84it/s, est. speed input: 34924.08 toks/s, output: 34.11 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:04<00:03, 31.02it/s, est. speed input: 34846.16 toks/s, output: 34.03 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:04<00:03, 31.16it/s, est. speed input: 34774.75 toks/s, output: 33.96 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:03, 31.21it/s, est. speed input: 34701.42 toks/s, output: 33.89 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 31.34it/s, est. speed input: 34641.63 toks/s, output: 33.83 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 31.44it/s, est. speed input: 34585.69 toks/s, output: 33.77 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:05<00:02, 31.52it/s, est. speed input: 34533.53 toks/s, output: 33.72 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:05<00:02, 31.54it/s, est. speed input: 34480.39 toks/s, output: 33.67 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:05<00:02, 31.52it/s, est. speed input: 34426.64 toks/s, output: 33.62 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:05<00:02, 31.31it/s, est. speed input: 34358.98 toks/s, output: 33.55 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:05<00:02, 31.02it/s, est. speed input: 34282.45 toks/s, output: 33.48 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:05<00:02, 31.38it/s, est. speed input: 34255.15 toks/s, output: 33.45 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 31.74it/s, est. speed input: 34237.15 toks/s, output: 33.43 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 32.03it/s, est. speed input: 34222.48 toks/s, output: 33.42 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:06<00:01, 32.18it/s, est. speed input: 34204.07 toks/s, output: 33.40 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:06<00:01, 32.06it/s, est. speed input: 34170.65 toks/s, output: 33.37 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:06<00:01, 32.08it/s, est. speed input: 34145.43 toks/s, output: 33.35 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:06<00:01, 32.06it/s, est. speed input: 34119.21 toks/s, output: 33.32 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:06<00:01, 32.11it/s, est. speed input: 34098.48 toks/s, output: 33.30 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:06<00:00, 32.15it/s, est. speed input: 34078.16 toks/s, output: 33.28 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:06<00:00, 32.12it/s, est. speed input: 34055.48 toks/s, output: 33.26 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:06<00:00, 32.20it/s, est. speed input: 34039.83 toks/s, output: 33.24 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:07<00:00, 32.29it/s, est. speed input: 34026.50 toks/s, output: 33.23 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:07<00:00, 32.22it/s, est. speed input: 34006.06 toks/s, output: 33.21 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:07<00:00, 32.34it/s, est. speed input: 33995.66 toks/s, output: 33.20 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:07<00:00, 32.34it/s, est. speed input: 33981.52 toks/s, output: 33.18 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:07<00:00, 32.22it/s, est. speed input: 33960.57 toks/s, output: 33.16 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 32.23it/s, est. speed input: 33945.72 toks/s, output: 33.15 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 32.23it/s, est. speed input: 33945.72 toks/s, output: 33.15 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 33.15it/s, est. speed input: 33945.72 toks/s, output: 33.15 toks/s]
[rank0]:[W128 11:45:20.281776022 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.0s

测试结果:
  Requests/s:   30.42
  Tokens/s:     31178.52
  Total Reqs:   256
  Elapsed:      8.42s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     31148.10

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:45:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:45:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=30051) WARNING 01-28 11:45:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=30051) WARNING 01-28 11:45:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.01 requests/s, 46134.76 total tokens/s, 45.01 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 11:45:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:45:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:45:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:45:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:45:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:45:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:45:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:45:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:45:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:45:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:45:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:45:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:45:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:45:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:45:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:45:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:45:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:45:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:45:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=30051) [2026-01-28 11:45:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=30051) [2026-01-28 11:45:40] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=30051) [2026-01-28 11:45:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=30051) [2026-01-28 11:45:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=30051) [2026-01-28 11:45:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=30051) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=30051) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.84it/s]
(EngineCore_DP0 pid=30051) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.83it/s]
(EngineCore_DP0 pid=30051) 
(EngineCore_DP0 pid=30051) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.96it/s]
(EngineCore_DP0 pid=30051) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.22it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.30it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 37/512 [00:00<00:01, 363.69it/s]
Adding requests:  15%|█▌        | 77/512 [00:00<00:01, 382.94it/s]
Adding requests:  23%|██▎       | 116/512 [00:00<00:01, 384.95it/s]
Adding requests:  30%|███       | 155/512 [00:00<00:00, 378.72it/s]
Adding requests:  38%|███▊      | 193/512 [00:00<00:00, 376.33it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 366.91it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 369.48it/s]
Adding requests:  60%|█████▉    | 306/512 [00:00<00:00, 367.20it/s]
Adding requests:  67%|██████▋   | 345/512 [00:00<00:00, 373.85it/s]
Adding requests:  75%|███████▌  | 385/512 [00:01<00:00, 379.35it/s]
Adding requests:  83%|████████▎ | 425/512 [00:01<00:00, 383.68it/s]
Adding requests:  91%|█████████ | 464/512 [00:01<00:00, 383.07it/s]
Adding requests:  98%|█████████▊| 503/512 [00:01<00:00, 384.34it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 378.43it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:00<00:01, 437.53it/s, est. speed input: 448117.68 toks/s, output: 437.56 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:01<00:05, 80.43it/s, est. speed input: 95217.96 toks/s, output: 92.98 toks/s]   
Processed prompts:  23%|██▎       | 119/512 [00:01<00:05, 68.98it/s, est. speed input: 82524.75 toks/s, output: 80.59 toks/s]
Processed prompts:  26%|██▌       | 133/512 [00:01<00:05, 65.65it/s, est. speed input: 78714.09 toks/s, output: 76.87 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:01<00:06, 59.87it/s, est. speed input: 74293.49 toks/s, output: 72.55 toks/s]
Processed prompts:  30%|██▉       | 153/512 [00:02<00:06, 58.51it/s, est. speed input: 72712.93 toks/s, output: 71.01 toks/s]
Processed prompts:  31%|███▏      | 161/512 [00:02<00:06, 55.94it/s, est. speed input: 70899.24 toks/s, output: 69.24 toks/s]
Processed prompts:  33%|███▎      | 168/512 [00:02<00:06, 52.44it/s, est. speed input: 68952.82 toks/s, output: 67.34 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:02<00:07, 48.13it/s, est. speed input: 66879.00 toks/s, output: 65.31 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:02<00:06, 47.86it/s, est. speed input: 65761.96 toks/s, output: 64.22 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:06, 47.71it/s, est. speed input: 64786.55 toks/s, output: 63.27 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:06, 47.61it/s, est. speed input: 63920.10 toks/s, output: 62.42 toks/s]
Processed prompts:  40%|████      | 206/512 [00:03<00:06, 47.51it/s, est. speed input: 63133.79 toks/s, output: 61.65 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:03<00:06, 47.40it/s, est. speed input: 62414.02 toks/s, output: 60.95 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:03<00:06, 47.33it/s, est. speed input: 61763.15 toks/s, output: 60.32 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:03<00:05, 47.29it/s, est. speed input: 61171.94 toks/s, output: 59.74 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:05, 46.38it/s, est. speed input: 60467.39 toks/s, output: 59.05 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:04<00:05, 45.43it/s, est. speed input: 59764.71 toks/s, output: 58.36 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:04<00:05, 44.77it/s, est. speed input: 59115.48 toks/s, output: 57.73 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:04<00:05, 44.68it/s, est. speed input: 58582.69 toks/s, output: 57.21 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:04<00:05, 44.78it/s, est. speed input: 58115.37 toks/s, output: 56.75 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:04<00:05, 44.87it/s, est. speed input: 57684.50 toks/s, output: 56.33 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:05<00:05, 45.00it/s, est. speed input: 57295.03 toks/s, output: 55.95 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:05<00:04, 45.10it/s, est. speed input: 56931.27 toks/s, output: 55.60 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:05<00:04, 45.19it/s, est. speed input: 56597.43 toks/s, output: 55.27 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:05<00:04, 45.20it/s, est. speed input: 56272.53 toks/s, output: 54.95 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:05<00:04, 45.27it/s, est. speed input: 55977.04 toks/s, output: 54.66 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:05<00:04, 45.22it/s, est. speed input: 55688.17 toks/s, output: 54.38 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:06<00:03, 45.28it/s, est. speed input: 55426.20 toks/s, output: 54.13 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:06<00:03, 46.44it/s, est. speed input: 55302.15 toks/s, output: 54.01 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:06<00:03, 46.12it/s, est. speed input: 55063.55 toks/s, output: 53.77 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:06<00:03, 45.88it/s, est. speed input: 54834.89 toks/s, output: 53.55 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:06<00:03, 45.68it/s, est. speed input: 54614.43 toks/s, output: 53.33 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:07<00:03, 45.57it/s, est. speed input: 54409.87 toks/s, output: 53.13 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:07<00:02, 45.50it/s, est. speed input: 54214.04 toks/s, output: 52.94 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:07<00:02, 45.46it/s, est. speed input: 54028.30 toks/s, output: 52.76 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:07<00:02, 45.39it/s, est. speed input: 53847.74 toks/s, output: 52.59 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:07<00:02, 45.34it/s, est. speed input: 53676.19 toks/s, output: 52.42 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:07<00:02, 45.31it/s, est. speed input: 53512.00 toks/s, output: 52.26 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:08<00:01, 45.29it/s, est. speed input: 53355.39 toks/s, output: 52.10 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:08<00:01, 45.30it/s, est. speed input: 53206.97 toks/s, output: 51.96 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:08<00:01, 45.33it/s, est. speed input: 53066.91 toks/s, output: 51.82 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:08<00:01, 45.33it/s, est. speed input: 52930.74 toks/s, output: 51.69 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:08<00:01, 45.32it/s, est. speed input: 52799.23 toks/s, output: 51.56 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:08<00:01, 45.29it/s, est. speed input: 52671.49 toks/s, output: 51.44 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:09<00:00, 45.29it/s, est. speed input: 52549.93 toks/s, output: 51.32 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:09<00:00, 45.27it/s, est. speed input: 52431.45 toks/s, output: 51.20 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:09<00:00, 45.30it/s, est. speed input: 52320.70 toks/s, output: 51.09 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:09<00:00, 45.30it/s, est. speed input: 52212.57 toks/s, output: 50.99 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:09<00:00, 45.29it/s, est. speed input: 52107.61 toks/s, output: 50.89 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:10<00:00, 46.97it/s, est. speed input: 52116.26 toks/s, output: 50.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 46.97it/s, est. speed input: 52319.55 toks/s, output: 51.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 51.09it/s, est. speed input: 52319.55 toks/s, output: 51.09 toks/s]
[rank0]:[W128 11:46:14.781854188 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.5s

测试结果:
  Requests/s:   45.01
  Tokens/s:     46134.76
  Total Reqs:   512
  Elapsed:      11.38s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     46089.75

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:46:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:46:31 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=30628) WARNING 01-28 11:46:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=30628) WARNING 01-28 11:46:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.10 requests/s, 48279.42 total tokens/s, 47.10 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 11:46:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:46:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:46:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:46:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:46:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:46:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:46:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:46:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:46:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:46:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:46:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:46:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:46:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:46:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:46:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:46:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:46:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:46:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:46:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=30628) [2026-01-28 11:46:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=30628) [2026-01-28 11:46:38] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=30628) [2026-01-28 11:46:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=30628) [2026-01-28 11:46:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=30628) [2026-01-28 11:46:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=30628) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=30628) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=30628) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=30628) 
(EngineCore_DP0 pid=30628) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  7.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  7.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  7.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  7.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.23it/s]
(EngineCore_DP0 pid=30628) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  6.65it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  7.42it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  7.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  7.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  7.61it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 38/1024 [00:00<00:02, 372.55it/s]
Adding requests:   8%|▊         | 78/1024 [00:00<00:02, 386.79it/s]
Adding requests:  12%|█▏        | 118/1024 [00:00<00:02, 388.38it/s]
Adding requests:  15%|█▌        | 157/1024 [00:00<00:02, 385.14it/s]
Adding requests:  19%|█▉        | 197/1024 [00:00<00:02, 387.28it/s]
Adding requests:  23%|██▎       | 238/1024 [00:00<00:02, 392.72it/s]
Adding requests:  27%|██▋       | 278/1024 [00:00<00:01, 392.86it/s]
Adding requests:  31%|███       | 318/1024 [00:00<00:01, 391.68it/s]
Adding requests:  35%|███▍      | 358/1024 [00:00<00:01, 393.79it/s]
Adding requests:  39%|███▉      | 399/1024 [00:01<00:01, 397.24it/s]
Adding requests:  43%|████▎     | 439/1024 [00:01<00:01, 392.46it/s]
Adding requests:  47%|████▋     | 480/1024 [00:01<00:01, 396.64it/s]
Adding requests:  51%|█████     | 520/1024 [00:01<00:01, 384.32it/s]
Adding requests:  55%|█████▍    | 561/1024 [00:01<00:01, 389.28it/s]
Adding requests:  59%|█████▉    | 602/1024 [00:01<00:01, 394.80it/s]
Adding requests:  63%|██████▎   | 642/1024 [00:01<00:00, 394.75it/s]
Adding requests:  67%|██████▋   | 685/1024 [00:01<00:00, 403.68it/s]
Adding requests:  71%|███████   | 727/1024 [00:01<00:00, 406.28it/s]
Adding requests:  75%|███████▌  | 768/1024 [00:01<00:00, 406.16it/s]
Adding requests:  79%|███████▉  | 809/1024 [00:02<00:00, 402.08it/s]
Adding requests:  83%|████████▎ | 850/1024 [00:02<00:00, 394.54it/s]
Adding requests:  87%|████████▋ | 893/1024 [00:02<00:00, 401.84it/s]
Adding requests:  91%|█████████ | 934/1024 [00:02<00:00, 398.40it/s]
Adding requests:  95%|█████████▌| 974/1024 [00:02<00:00, 396.70it/s]
Adding requests:  99%|█████████▉| 1014/1024 [00:02<00:00, 397.39it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 395.31it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:00<00:01, 815.28it/s, est. speed input: 835083.69 toks/s, output: 815.33 toks/s]
Processed prompts:  19%|█▉        | 196/1024 [00:01<00:09, 90.78it/s, est. speed input: 110021.16 toks/s, output: 107.44 toks/s] 
Processed prompts:  23%|██▎       | 233/1024 [00:02<00:10, 78.18it/s, est. speed input: 95486.93 toks/s, output: 93.25 toks/s]  
Processed prompts:  25%|██▌       | 256/1024 [00:03<00:11, 69.19it/s, est. speed input: 87212.66 toks/s, output: 85.17 toks/s]
Processed prompts:  27%|██▋       | 272/1024 [00:03<00:11, 64.69it/s, est. speed input: 83306.17 toks/s, output: 81.35 toks/s]
Processed prompts:  28%|██▊       | 284/1024 [00:03<00:12, 58.04it/s, est. speed input: 78997.34 toks/s, output: 77.15 toks/s]
Processed prompts:  29%|██▊       | 293/1024 [00:03<00:12, 57.36it/s, est. speed input: 77922.79 toks/s, output: 76.10 toks/s]
Processed prompts:  29%|██▉       | 301/1024 [00:04<00:12, 55.68it/s, est. speed input: 76681.59 toks/s, output: 74.88 toks/s]
Processed prompts:  30%|███       | 308/1024 [00:04<00:13, 52.95it/s, est. speed input: 75302.79 toks/s, output: 73.54 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:04<00:14, 49.18it/s, est. speed input: 73789.72 toks/s, output: 72.06 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:04<00:14, 48.74it/s, est. speed input: 72842.31 toks/s, output: 71.13 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:04<00:14, 48.35it/s, est. speed input: 71958.78 toks/s, output: 70.27 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:04<00:14, 48.07it/s, est. speed input: 71139.15 toks/s, output: 69.47 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:05<00:14, 47.86it/s, est. speed input: 70376.65 toks/s, output: 68.73 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:05<00:14, 47.73it/s, est. speed input: 69668.62 toks/s, output: 68.04 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:05<00:13, 47.61it/s, est. speed input: 69001.24 toks/s, output: 67.38 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:05<00:13, 47.52it/s, est. speed input: 68373.17 toks/s, output: 66.77 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:05<00:13, 47.46it/s, est. speed input: 67782.66 toks/s, output: 66.19 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:13, 47.38it/s, est. speed input: 67221.38 toks/s, output: 65.65 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:06<00:13, 47.35it/s, est. speed input: 66695.11 toks/s, output: 65.13 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:06<00:13, 47.33it/s, est. speed input: 66198.48 toks/s, output: 64.65 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:06<00:12, 47.32it/s, est. speed input: 65727.68 toks/s, output: 64.19 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:06<00:12, 47.33it/s, est. speed input: 65283.12 toks/s, output: 63.75 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:06<00:12, 47.31it/s, est. speed input: 64859.08 toks/s, output: 63.34 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:12, 47.32it/s, est. speed input: 64456.76 toks/s, output: 62.95 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:07<00:12, 47.27it/s, est. speed input: 64069.17 toks/s, output: 62.57 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:07<00:12, 47.26it/s, est. speed input: 63701.04 toks/s, output: 62.21 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:07<00:11, 47.29it/s, est. speed input: 63354.69 toks/s, output: 61.87 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:07<00:11, 47.30it/s, est. speed input: 63021.74 toks/s, output: 61.54 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:07<00:11, 47.28it/s, est. speed input: 62701.57 toks/s, output: 61.23 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:07<00:11, 47.28it/s, est. speed input: 62395.48 toks/s, output: 60.93 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:08<00:11, 47.26it/s, est. speed input: 62100.92 toks/s, output: 60.65 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:08<00:11, 47.24it/s, est. speed input: 61818.31 toks/s, output: 60.37 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:08<00:10, 47.22it/s, est. speed input: 61545.86 toks/s, output: 60.10 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:08<00:10, 47.22it/s, est. speed input: 61285.49 toks/s, output: 59.85 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:08<00:10, 47.23it/s, est. speed input: 61036.58 toks/s, output: 59.61 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:08<00:10, 47.22it/s, est. speed input: 60794.64 toks/s, output: 59.37 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:09<00:10, 47.20it/s, est. speed input: 60561.34 toks/s, output: 59.14 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:09<00:10, 47.18it/s, est. speed input: 60335.97 toks/s, output: 58.92 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:09<00:09, 47.17it/s, est. speed input: 60119.67 toks/s, output: 58.71 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:09<00:09, 47.19it/s, est. speed input: 59911.95 toks/s, output: 58.51 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:09<00:09, 47.17it/s, est. speed input: 59709.88 toks/s, output: 58.31 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:09<00:09, 47.16it/s, est. speed input: 59514.79 toks/s, output: 58.12 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:10<00:09, 47.16it/s, est. speed input: 59326.48 toks/s, output: 57.94 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:10<00:09, 47.15it/s, est. speed input: 59143.54 toks/s, output: 57.76 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:10<00:08, 47.14it/s, est. speed input: 58966.74 toks/s, output: 57.58 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:10<00:08, 47.17it/s, est. speed input: 58797.51 toks/s, output: 57.42 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:10<00:08, 47.17it/s, est. speed input: 58632.51 toks/s, output: 57.26 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:10<00:08, 47.14it/s, est. speed input: 58471.04 toks/s, output: 57.10 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:11<00:08, 47.09it/s, est. speed input: 58312.60 toks/s, output: 56.95 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:11<00:08, 47.07it/s, est. speed input: 58159.39 toks/s, output: 56.80 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:11<00:07, 47.07it/s, est. speed input: 58011.73 toks/s, output: 56.65 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:11<00:07, 47.09it/s, est. speed input: 57869.72 toks/s, output: 56.51 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:11<00:07, 47.10it/s, est. speed input: 57731.52 toks/s, output: 56.38 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:11<00:07, 47.10it/s, est. speed input: 57596.78 toks/s, output: 56.25 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:12<00:07, 47.11it/s, est. speed input: 57466.28 toks/s, output: 56.12 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:12<00:07, 47.07it/s, est. speed input: 57337.31 toks/s, output: 55.99 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:12<00:06, 47.08it/s, est. speed input: 57213.21 toks/s, output: 55.87 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:12<00:06, 47.08it/s, est. speed input: 57092.54 toks/s, output: 55.75 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:12<00:06, 46.59it/s, est. speed input: 56948.38 toks/s, output: 55.61 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:13<00:06, 46.97it/s, est. speed input: 56846.92 toks/s, output: 55.51 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:13<00:06, 47.25it/s, est. speed input: 56747.92 toks/s, output: 55.42 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:13<00:06, 47.42it/s, est. speed input: 56650.50 toks/s, output: 55.32 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:13<00:05, 47.54it/s, est. speed input: 56555.59 toks/s, output: 55.23 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:13<00:05, 47.63it/s, est. speed input: 56462.76 toks/s, output: 55.14 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:13<00:05, 47.67it/s, est. speed input: 56371.66 toks/s, output: 55.05 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:14<00:05, 47.73it/s, est. speed input: 56283.84 toks/s, output: 54.96 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:14<00:05, 47.76it/s, est. speed input: 56197.77 toks/s, output: 54.88 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:14<00:04, 47.79it/s, est. speed input: 56114.00 toks/s, output: 54.80 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:14<00:04, 47.79it/s, est. speed input: 56031.30 toks/s, output: 54.72 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:14<00:04, 47.85it/s, est. speed input: 55952.77 toks/s, output: 54.64 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:14<00:04, 47.87it/s, est. speed input: 55875.27 toks/s, output: 54.57 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:15<00:04, 47.88it/s, est. speed input: 55799.47 toks/s, output: 54.49 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:15<00:04, 47.88it/s, est. speed input: 55725.08 toks/s, output: 54.42 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:15<00:03, 47.87it/s, est. speed input: 55651.54 toks/s, output: 54.35 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:15<00:03, 47.87it/s, est. speed input: 55580.14 toks/s, output: 54.28 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:15<00:03, 47.88it/s, est. speed input: 55510.77 toks/s, output: 54.21 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:15<00:03, 47.88it/s, est. speed input: 55442.29 toks/s, output: 54.14 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:16<00:03, 47.89it/s, est. speed input: 55375.94 toks/s, output: 54.08 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:16<00:03, 47.91it/s, est. speed input: 55311.24 toks/s, output: 54.01 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:16<00:02, 47.88it/s, est. speed input: 55246.06 toks/s, output: 53.95 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:16<00:02, 47.85it/s, est. speed input: 55182.02 toks/s, output: 53.89 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:16<00:02, 47.85it/s, est. speed input: 55120.28 toks/s, output: 53.83 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:16<00:02, 47.85it/s, est. speed input: 55059.23 toks/s, output: 53.77 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:17<00:02, 47.86it/s, est. speed input: 55000.09 toks/s, output: 53.71 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:17<00:02, 47.85it/s, est. speed input: 54941.52 toks/s, output: 53.65 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:17<00:01, 47.85it/s, est. speed input: 54884.27 toks/s, output: 53.60 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:17<00:01, 47.86it/s, est. speed input: 54828.57 toks/s, output: 53.54 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:17<00:01, 47.89it/s, est. speed input: 54774.40 toks/s, output: 53.49 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:17<00:01, 47.88it/s, est. speed input: 54720.58 toks/s, output: 53.44 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:18<00:01, 47.88it/s, est. speed input: 54667.64 toks/s, output: 53.39 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:18<00:01, 47.88it/s, est. speed input: 54615.73 toks/s, output: 53.34 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:18<00:00, 47.86it/s, est. speed input: 54564.26 toks/s, output: 53.29 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:18<00:00, 49.65it/s, est. speed input: 54573.33 toks/s, output: 53.29 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:18<00:00, 49.11it/s, est. speed input: 54523.86 toks/s, output: 53.25 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:18<00:00, 48.72it/s, est. speed input: 54474.58 toks/s, output: 53.20 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:19<00:00, 48.46it/s, est. speed input: 54426.46 toks/s, output: 53.15 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:19<00:00, 50.26it/s, est. speed input: 54441.02 toks/s, output: 53.17 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:19<00:00, 50.26it/s, est. speed input: 54761.26 toks/s, output: 53.48 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:19<00:00, 53.48it/s, est. speed input: 54761.26 toks/s, output: 53.48 toks/s]
[rank0]:[W128 11:47:23.681693195 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.1s

测试结果:
  Requests/s:   47.10
  Tokens/s:     48279.42
  Total Reqs:   1024
  Elapsed:      21.74s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     48232.31

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:47:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:47:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=31369) WARNING 01-28 11:47:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=31369) WARNING 01-28 11:48:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.32 requests/s, 46451.95 total tokens/s, 45.32 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 11:47:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:47:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:47:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:47:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:47:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:47:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:47:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:47:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:47:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:47:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:47:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:47:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:47:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:47:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:47:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:47:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:47:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:47:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:47:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=31369) [2026-01-28 11:47:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=31369) [2026-01-28 11:47:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=31369) [2026-01-28 11:47:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=31369) [2026-01-28 11:47:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=31369) [2026-01-28 11:47:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=31369) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=31369) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=31369) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=31369) 
(EngineCore_DP0 pid=31369) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.76it/s]
(EngineCore_DP0 pid=31369) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.56it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  7.44it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  7.84it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  7.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  7.89it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 37/2048 [00:00<00:05, 367.21it/s]
Adding requests:   4%|▎         | 75/2048 [00:00<00:05, 374.69it/s]
Adding requests:   6%|▌         | 114/2048 [00:00<00:05, 379.71it/s]
Adding requests:   7%|▋         | 152/2048 [00:00<00:05, 377.95it/s]
Adding requests:   9%|▉         | 190/2048 [00:00<00:04, 377.39it/s]
Adding requests:  11%|█         | 229/2048 [00:00<00:04, 379.99it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:04, 380.29it/s]
Adding requests:  15%|█▍        | 307/2048 [00:00<00:04, 379.64it/s]
Adding requests:  17%|█▋        | 345/2048 [00:00<00:04, 377.97it/s]
Adding requests:  19%|█▊        | 383/2048 [00:01<00:04, 378.32it/s]
Adding requests:  21%|██        | 423/2048 [00:01<00:04, 384.26it/s]
Adding requests:  23%|██▎       | 462/2048 [00:01<00:04, 383.87it/s]
Adding requests:  24%|██▍       | 501/2048 [00:01<00:04, 381.62it/s]
Adding requests:  26%|██▋       | 540/2048 [00:01<00:04, 376.47it/s]
Adding requests:  28%|██▊       | 580/2048 [00:01<00:03, 381.82it/s]
Adding requests:  30%|███       | 619/2048 [00:01<00:03, 382.97it/s]
Adding requests:  32%|███▏      | 658/2048 [00:01<00:03, 382.67it/s]
Adding requests:  34%|███▍      | 697/2048 [00:01<00:03, 380.72it/s]
Adding requests:  36%|███▌      | 738/2048 [00:01<00:03, 386.88it/s]
Adding requests:  38%|███▊      | 777/2048 [00:02<00:03, 376.49it/s]
Adding requests:  40%|███▉      | 815/2048 [00:02<00:03, 376.92it/s]
Adding requests:  42%|████▏     | 853/2048 [00:02<00:03, 377.54it/s]
Adding requests:  44%|████▎     | 894/2048 [00:02<00:02, 386.90it/s]
Adding requests:  46%|████▌     | 934/2048 [00:02<00:02, 390.68it/s]
Adding requests:  48%|████▊     | 974/2048 [00:02<00:02, 392.14it/s]
Adding requests:  50%|████▉     | 1016/2048 [00:02<00:02, 397.91it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:02<00:02, 396.49it/s]
Adding requests:  54%|█████▎    | 1096/2048 [00:02<00:02, 395.69it/s]
Adding requests:  55%|█████▌    | 1136/2048 [00:02<00:02, 389.52it/s]
Adding requests:  58%|█████▊    | 1178/2048 [00:03<00:02, 396.65it/s]
Adding requests:  60%|█████▉    | 1220/2048 [00:03<00:02, 402.46it/s]
Adding requests:  62%|██████▏   | 1261/2048 [00:03<00:01, 397.05it/s]
Adding requests:  64%|██████▎   | 1302/2048 [00:03<00:01, 400.81it/s]
Adding requests:  66%|██████▌   | 1343/2048 [00:03<00:01, 402.44it/s]
Adding requests:  68%|██████▊   | 1384/2048 [00:03<00:01, 402.22it/s]
Adding requests:  70%|██████▉   | 1425/2048 [00:03<00:01, 404.36it/s]
Adding requests:  72%|███████▏  | 1467/2048 [00:03<00:01, 407.54it/s]
Adding requests:  74%|███████▎  | 1508/2048 [00:03<00:01, 405.10it/s]
Adding requests:  76%|███████▌  | 1549/2048 [00:03<00:01, 401.95it/s]
Adding requests:  78%|███████▊  | 1592/2048 [00:04<00:01, 408.26it/s]
Adding requests:  80%|███████▉  | 1633/2048 [00:04<00:01, 407.47it/s]
Adding requests:  82%|████████▏ | 1674/2048 [00:04<00:00, 404.37it/s]
Adding requests:  84%|████████▎ | 1715/2048 [00:04<00:00, 402.24it/s]
Adding requests:  86%|████████▌ | 1756/2048 [00:04<00:00, 399.61it/s]
Adding requests:  88%|████████▊ | 1796/2048 [00:04<00:00, 397.53it/s]
Adding requests:  90%|████████▉ | 1836/2048 [00:04<00:00, 394.66it/s]
Adding requests:  92%|█████████▏| 1877/2048 [00:04<00:00, 399.16it/s]
Adding requests:  94%|█████████▎| 1917/2048 [00:04<00:00, 385.36it/s]
Adding requests:  96%|█████████▌| 1957/2048 [00:05<00:00, 387.71it/s]
Adding requests:  98%|█████████▊| 1998/2048 [00:05<00:00, 393.04it/s]
Adding requests: 100%|█████████▉| 2038/2048 [00:05<00:00, 394.80it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 390.96it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:00<00:02, 818.37it/s, est. speed input: 838096.67 toks/s, output: 818.40 toks/s]
Processed prompts:  15%|█▌        | 308/2048 [00:02<00:14, 123.64it/s, est. speed input: 155707.35 toks/s, output: 152.06 toks/s]
Processed prompts:  17%|█▋        | 345/2048 [00:02<00:17, 99.55it/s, est. speed input: 129581.84 toks/s, output: 126.54 toks/s] 
Processed prompts:  18%|█▊        | 368/2048 [00:03<00:18, 92.92it/s, est. speed input: 122474.10 toks/s, output: 119.60 toks/s]
Processed prompts:  19%|█▉        | 385/2048 [00:03<00:19, 83.25it/s, est. speed input: 115047.35 toks/s, output: 112.35 toks/s]
Processed prompts:  19%|█▉        | 398/2048 [00:03<00:22, 72.36it/s, est. speed input: 107942.10 toks/s, output: 105.41 toks/s]
Processed prompts:  20%|█▉        | 408/2048 [00:04<00:26, 61.33it/s, est. speed input: 101308.42 toks/s, output: 98.93 toks/s] 
Processed prompts:  20%|██        | 418/2048 [00:04<00:30, 52.68it/s, est. speed input: 95707.29 toks/s, output: 93.46 toks/s] 
Processed prompts:  21%|██        | 434/2048 [00:04<00:31, 50.82it/s, est. speed input: 92185.97 toks/s, output: 90.03 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:05<00:32, 49.43it/s, est. speed input: 89140.84 toks/s, output: 87.05 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:05<00:32, 48.41it/s, est. speed input: 86474.98 toks/s, output: 84.45 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:05<00:32, 47.67it/s, est. speed input: 84127.05 toks/s, output: 82.16 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:06<00:32, 47.13it/s, est. speed input: 82038.50 toks/s, output: 80.12 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:06<00:32, 46.79it/s, est. speed input: 80183.23 toks/s, output: 78.30 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:06<00:32, 46.47it/s, est. speed input: 78494.34 toks/s, output: 76.65 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:07<00:32, 46.27it/s, est. speed input: 76975.71 toks/s, output: 75.17 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:07<00:32, 46.13it/s, est. speed input: 75594.88 toks/s, output: 73.82 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:07<00:31, 46.02it/s, est. speed input: 74333.45 toks/s, output: 72.59 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:08<00:31, 45.93it/s, est. speed input: 73175.97 toks/s, output: 71.46 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:08<00:31, 45.87it/s, est. speed input: 72111.75 toks/s, output: 70.42 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:09<00:31, 45.84it/s, est. speed input: 71132.51 toks/s, output: 69.47 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:09<00:30, 45.80it/s, est. speed input: 70222.11 toks/s, output: 68.58 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:09<00:30, 45.78it/s, est. speed input: 69378.83 toks/s, output: 67.75 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:10<00:30, 45.77it/s, est. speed input: 68595.52 toks/s, output: 66.99 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:10<00:29, 45.75it/s, est. speed input: 67863.79 toks/s, output: 66.27 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:10<00:29, 45.74it/s, est. speed input: 67178.51 toks/s, output: 65.60 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:11<00:28, 45.73it/s, est. speed input: 66536.97 toks/s, output: 64.98 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:11<00:28, 45.73it/s, est. speed input: 65934.75 toks/s, output: 64.39 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:11<00:28, 45.65it/s, est. speed input: 65358.38 toks/s, output: 63.83 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:12<00:28, 45.60it/s, est. speed input: 64815.30 toks/s, output: 63.30 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:12<00:27, 45.55it/s, est. speed input: 64300.28 toks/s, output: 62.79 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:12<00:27, 45.53it/s, est. speed input: 63815.20 toks/s, output: 62.32 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:13<00:27, 45.50it/s, est. speed input: 63354.60 toks/s, output: 61.87 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:13<00:26, 45.49it/s, est. speed input: 62919.38 toks/s, output: 61.44 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:13<00:26, 45.49it/s, est. speed input: 62506.11 toks/s, output: 61.04 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<01:10, 16.76it/s, est. speed input: 54452.31 toks/s, output: 53.18 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:16<00:56, 20.68it/s, est. speed input: 54284.58 toks/s, output: 53.01 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:16<00:46, 24.72it/s, est. speed input: 54123.31 toks/s, output: 52.85 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:17<00:39, 28.64it/s, est. speed input: 53968.95 toks/s, output: 52.70 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:34, 32.21it/s, est. speed input: 53820.88 toks/s, output: 52.56 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:18<00:31, 35.29it/s, est. speed input: 53677.84 toks/s, output: 52.42 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:18<00:28, 37.82it/s, est. speed input: 53541.23 toks/s, output: 52.29 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:18<00:26, 40.44it/s, est. speed input: 53467.53 toks/s, output: 52.21 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:19<00:25, 41.82it/s, est. speed input: 53339.43 toks/s, output: 52.09 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:19<00:24, 42.84it/s, est. speed input: 53215.86 toks/s, output: 51.97 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:19<00:23, 43.58it/s, est. speed input: 53096.14 toks/s, output: 51.85 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:20<00:22, 44.12it/s, est. speed input: 52981.10 toks/s, output: 51.74 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:22, 44.50it/s, est. speed input: 52869.94 toks/s, output: 51.63 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:20<00:21, 44.76it/s, est. speed input: 52761.84 toks/s, output: 51.53 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:21<00:21, 44.96it/s, est. speed input: 52657.96 toks/s, output: 51.42 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:20, 45.09it/s, est. speed input: 52556.84 toks/s, output: 51.32 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:21<00:20, 45.16it/s, est. speed input: 52457.89 toks/s, output: 51.23 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:20, 45.21it/s, est. speed input: 52362.15 toks/s, output: 51.13 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:22<00:19, 46.04it/s, est. speed input: 52316.19 toks/s, output: 51.09 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:22<00:19, 45.83it/s, est. speed input: 52225.64 toks/s, output: 51.00 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:18, 45.67it/s, est. speed input: 52137.20 toks/s, output: 50.92 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:23<00:18, 45.56it/s, est. speed input: 52051.69 toks/s, output: 50.83 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:24<00:18, 45.48it/s, est. speed input: 51968.06 toks/s, output: 50.75 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:17, 45.42it/s, est. speed input: 51886.83 toks/s, output: 50.67 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:24<00:17, 45.39it/s, est. speed input: 51808.28 toks/s, output: 50.59 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:25<00:17, 45.36it/s, est. speed input: 51731.98 toks/s, output: 50.52 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:25<00:16, 45.34it/s, est. speed input: 51657.77 toks/s, output: 50.45 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:25<00:16, 45.33it/s, est. speed input: 51585.69 toks/s, output: 50.38 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:26<00:16, 45.34it/s, est. speed input: 51516.10 toks/s, output: 50.31 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:26<00:15, 45.33it/s, est. speed input: 51447.60 toks/s, output: 50.24 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:15, 45.32it/s, est. speed input: 51381.32 toks/s, output: 50.18 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:27<00:15, 45.32it/s, est. speed input: 51316.56 toks/s, output: 50.11 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:14, 45.32it/s, est. speed input: 51253.46 toks/s, output: 50.05 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:27<00:14, 45.30it/s, est. speed input: 51191.11 toks/s, output: 49.99 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:28<00:14, 45.29it/s, est. speed input: 51130.75 toks/s, output: 49.93 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:13, 45.28it/s, est. speed input: 51071.49 toks/s, output: 49.87 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:28<00:13, 45.31it/s, est. speed input: 51015.41 toks/s, output: 49.82 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:29<00:13, 45.28it/s, est. speed input: 50958.18 toks/s, output: 49.76 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:12, 45.28it/s, est. speed input: 50903.77 toks/s, output: 49.71 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:30<00:12, 45.29it/s, est. speed input: 50850.49 toks/s, output: 49.66 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:30<00:11, 45.30it/s, est. speed input: 50798.85 toks/s, output: 49.61 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 45.30it/s, est. speed input: 50748.39 toks/s, output: 49.56 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:31<00:11, 45.31it/s, est. speed input: 50698.97 toks/s, output: 49.51 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:31<00:10, 45.32it/s, est. speed input: 50651.08 toks/s, output: 49.46 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:31<00:10, 45.34it/s, est. speed input: 50604.90 toks/s, output: 49.42 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:32<00:10, 45.33it/s, est. speed input: 50558.62 toks/s, output: 49.37 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:32<00:09, 45.31it/s, est. speed input: 50512.86 toks/s, output: 49.33 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:32<00:09, 45.32it/s, est. speed input: 50468.78 toks/s, output: 49.29 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:33<00:09, 45.32it/s, est. speed input: 50425.74 toks/s, output: 49.24 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:33<00:08, 45.32it/s, est. speed input: 50383.55 toks/s, output: 49.20 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:33<00:08, 45.32it/s, est. speed input: 50342.19 toks/s, output: 49.16 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:34<00:08, 45.32it/s, est. speed input: 50301.55 toks/s, output: 49.12 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:34<00:07, 45.32it/s, est. speed input: 50261.77 toks/s, output: 49.08 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:34<00:07, 45.32it/s, est. speed input: 50222.81 toks/s, output: 49.05 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:35<00:07, 45.31it/s, est. speed input: 50184.14 toks/s, output: 49.01 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:35<00:06, 45.32it/s, est. speed input: 50146.93 toks/s, output: 48.97 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:36<00:06, 45.32it/s, est. speed input: 50110.26 toks/s, output: 48.94 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:36<00:05, 45.32it/s, est. speed input: 50074.40 toks/s, output: 48.90 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:36<00:05, 45.32it/s, est. speed input: 50039.21 toks/s, output: 48.87 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:37<00:05, 45.33it/s, est. speed input: 50004.97 toks/s, output: 48.83 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:37<00:04, 45.34it/s, est. speed input: 49971.52 toks/s, output: 48.80 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:37<00:04, 45.34it/s, est. speed input: 49938.34 toks/s, output: 48.77 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:38<00:04, 45.34it/s, est. speed input: 49906.03 toks/s, output: 48.74 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:38<00:03, 46.14it/s, est. speed input: 49900.50 toks/s, output: 48.73 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:38<00:03, 45.91it/s, est. speed input: 49869.28 toks/s, output: 48.70 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:39<00:03, 45.74it/s, est. speed input: 49838.38 toks/s, output: 48.67 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:39<00:02, 45.63it/s, est. speed input: 49808.27 toks/s, output: 48.64 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:39<00:02, 45.55it/s, est. speed input: 49778.61 toks/s, output: 48.61 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:40<00:02, 45.50it/s, est. speed input: 49749.51 toks/s, output: 48.58 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:40<00:01, 45.46it/s, est. speed input: 49720.97 toks/s, output: 48.56 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:40<00:01, 45.43it/s, est. speed input: 49692.88 toks/s, output: 48.53 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:41<00:01, 45.42it/s, est. speed input: 49665.47 toks/s, output: 48.50 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:41<00:00, 45.43it/s, est. speed input: 49639.04 toks/s, output: 48.48 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:41<00:00, 46.35it/s, est. speed input: 49640.32 toks/s, output: 48.48 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 46.35it/s, est. speed input: 49981.56 toks/s, output: 48.81 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 48.81it/s, est. speed input: 49981.56 toks/s, output: 48.81 toks/s]
[rank0]:[W128 11:49:04.875174827 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 101.1s

测试结果:
  Requests/s:   45.32
  Tokens/s:     46451.95
  Total Reqs:   2048
  Elapsed:      45.19s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     46406.63

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:49:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:49:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=32301) WARNING 01-28 11:49:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=32301) WARNING 01-28 11:50:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.07 requests/s, 46199.36 total tokens/s, 45.07 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 11:49:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:49:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:49:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:49:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:49:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:49:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:49:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:49:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:49:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:49:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:49:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:49:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:49:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:49:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:49:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:49:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:49:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:49:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:49:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 11:49:54.617076893 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=32301) [2026-01-28 11:49:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=32301) [2026-01-28 11:49:55] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=32301) [2026-01-28 11:49:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=32301) [2026-01-28 11:49:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=32301) [2026-01-28 11:49:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=32301) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=32301) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=32301) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=32301) 
(EngineCore_DP0 pid=32301) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  7.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.77it/s]
(EngineCore_DP0 pid=32301) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.88it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  7.51it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  7.86it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  7.94it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.13it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.04it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 35/4096 [00:00<00:11, 346.28it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:11, 358.00it/s]
Adding requests:   3%|▎         | 109/4096 [00:00<00:11, 362.40it/s]
Adding requests:   4%|▎         | 147/4096 [00:00<00:10, 366.55it/s]
Adding requests:   4%|▍         | 184/4096 [00:00<00:10, 364.98it/s]
Adding requests:   5%|▌         | 223/4096 [00:00<00:10, 372.16it/s]
Adding requests:   6%|▋         | 263/4096 [00:00<00:10, 380.59it/s]
Adding requests:   7%|▋         | 302/4096 [00:00<00:10, 377.12it/s]
Adding requests:   8%|▊         | 341/4096 [00:00<00:09, 379.74it/s]
Adding requests:   9%|▉         | 381/4096 [00:01<00:09, 383.01it/s]
Adding requests:  10%|█         | 420/4096 [00:01<00:09, 385.00it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:09, 384.70it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:09, 385.50it/s]
Adding requests:  13%|█▎        | 537/4096 [00:01<00:09, 376.39it/s]
Adding requests:  14%|█▍        | 579/4096 [00:01<00:09, 386.75it/s]
Adding requests:  15%|█▌        | 620/4096 [00:01<00:08, 390.76it/s]
Adding requests:  16%|█▌        | 660/4096 [00:01<00:08, 391.63it/s]
Adding requests:  17%|█▋        | 702/4096 [00:01<00:08, 397.79it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:08, 397.13it/s]
Adding requests:  19%|█▉        | 782/4096 [00:02<00:08, 397.26it/s]
Adding requests:  20%|██        | 822/4096 [00:02<00:08, 384.08it/s]
Adding requests:  21%|██        | 862/4096 [00:02<00:08, 387.64it/s]
Adding requests:  22%|██▏       | 901/4096 [00:02<00:08, 388.28it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:08, 388.51it/s]
Adding requests:  24%|██▍       | 980/4096 [00:02<00:07, 391.69it/s]
Adding requests:  25%|██▍       | 1020/4096 [00:02<00:07, 386.38it/s]
Adding requests:  26%|██▌       | 1061/4096 [00:02<00:07, 393.10it/s]
Adding requests:  27%|██▋       | 1101/4096 [00:04<00:53, 55.74it/s] 
Adding requests:  28%|██▊       | 1141/4096 [00:05<00:39, 75.01it/s]
Adding requests:  29%|██▉       | 1182/4096 [00:05<00:29, 99.77it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:05<00:22, 129.32it/s]
Adding requests:  31%|███       | 1261/4096 [00:05<00:17, 158.98it/s]
Adding requests:  32%|███▏      | 1301/4096 [00:05<00:14, 193.61it/s]
Adding requests:  33%|███▎      | 1343/4096 [00:05<00:11, 232.47it/s]
Adding requests:  34%|███▍      | 1384/4096 [00:05<00:10, 266.82it/s]
Adding requests:  35%|███▍      | 1426/4096 [00:05<00:08, 299.00it/s]
Adding requests:  36%|███▌      | 1468/4096 [00:05<00:08, 326.80it/s]
Adding requests:  37%|███▋      | 1509/4096 [00:05<00:07, 346.95it/s]
Adding requests:  38%|███▊      | 1550/4096 [00:06<00:07, 363.19it/s]
Adding requests:  39%|███▉      | 1592/4096 [00:06<00:06, 377.90it/s]
Adding requests:  40%|███▉      | 1633/4096 [00:06<00:06, 385.79it/s]
Adding requests:  41%|████      | 1674/4096 [00:06<00:06, 387.09it/s]
Adding requests:  42%|████▏     | 1715/4096 [00:06<00:06, 392.49it/s]
Adding requests:  43%|████▎     | 1756/4096 [00:06<00:05, 394.59it/s]
Adding requests:  44%|████▍     | 1797/4096 [00:06<00:05, 396.32it/s]
Adding requests:  45%|████▍     | 1838/4096 [00:06<00:05, 399.97it/s]
Adding requests:  46%|████▌     | 1879/4096 [00:06<00:05, 401.90it/s]
Adding requests:  47%|████▋     | 1920/4096 [00:06<00:05, 399.96it/s]
Adding requests:  48%|████▊     | 1961/4096 [00:07<00:05, 400.57it/s]
Adding requests:  49%|████▉     | 2003/4096 [00:07<00:05, 404.58it/s]
Adding requests:  50%|████▉     | 2044/4096 [00:07<00:05, 403.31it/s]
Adding requests:  51%|█████     | 2085/4096 [00:07<00:04, 402.61it/s]
Adding requests:  52%|█████▏    | 2126/4096 [00:07<00:04, 402.21it/s]
Adding requests:  53%|█████▎    | 2167/4096 [00:07<00:04, 400.67it/s]
Adding requests:  54%|█████▍    | 2208/4096 [00:07<00:04, 384.35it/s]
Adding requests:  55%|█████▍    | 2251/4096 [00:07<00:04, 395.86it/s]
Adding requests:  56%|█████▌    | 2291/4096 [00:07<00:04, 395.09it/s]
Adding requests:  57%|█████▋    | 2331/4096 [00:07<00:04, 390.58it/s]
Adding requests:  58%|█████▊    | 2371/4096 [00:08<00:04, 390.54it/s]
Adding requests:  59%|█████▉    | 2411/4096 [00:08<00:04, 385.37it/s]
Adding requests:  60%|█████▉    | 2451/4096 [00:08<00:04, 388.71it/s]
Adding requests:  61%|██████    | 2493/4096 [00:08<00:04, 394.45it/s]
Adding requests:  62%|██████▏   | 2534/4096 [00:08<00:03, 398.17it/s]
Adding requests:  63%|██████▎   | 2575/4096 [00:08<00:03, 400.41it/s]
Adding requests:  64%|██████▍   | 2616/4096 [00:08<00:03, 399.03it/s]
Adding requests:  65%|██████▍   | 2657/4096 [00:08<00:03, 401.95it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:08<00:03, 397.98it/s]
Adding requests:  67%|██████▋   | 2738/4096 [00:09<00:03, 395.34it/s]
Adding requests:  68%|██████▊   | 2778/4096 [00:09<00:03, 396.18it/s]
Adding requests:  69%|██████▉   | 2818/4096 [00:09<00:03, 395.95it/s]
Adding requests:  70%|██████▉   | 2858/4096 [00:09<00:03, 396.24it/s]
Adding requests:  71%|███████   | 2898/4096 [00:09<00:03, 396.85it/s]
Adding requests:  72%|███████▏  | 2938/4096 [00:09<00:02, 390.34it/s]
Adding requests:  73%|███████▎  | 2979/4096 [00:09<00:02, 395.34it/s]
Adding requests:  74%|███████▎  | 3019/4096 [00:09<00:02, 396.42it/s]
Adding requests:  75%|███████▍  | 3059/4096 [00:09<00:02, 397.33it/s]
Adding requests:  76%|███████▌  | 3099/4096 [00:09<00:02, 392.00it/s]
Adding requests:  77%|███████▋  | 3139/4096 [00:10<00:02, 393.54it/s]
Adding requests:  78%|███████▊  | 3179/4096 [00:10<00:02, 394.91it/s]
Adding requests:  79%|███████▊  | 3219/4096 [00:10<00:02, 393.63it/s]
Adding requests:  80%|███████▉  | 3259/4096 [00:10<00:02, 395.24it/s]
Adding requests:  81%|████████  | 3299/4096 [00:10<00:02, 393.64it/s]
Adding requests:  82%|████████▏ | 3339/4096 [00:10<00:01, 393.70it/s]
Adding requests:  83%|████████▎ | 3380/4096 [00:10<00:01, 397.90it/s]
Adding requests:  83%|████████▎ | 3420/4096 [00:10<00:01, 397.22it/s]
Adding requests:  84%|████████▍ | 3460/4096 [00:10<00:01, 387.23it/s]
Adding requests:  85%|████████▌ | 3499/4096 [00:10<00:01, 385.98it/s]
Adding requests:  86%|████████▋ | 3538/4096 [00:11<00:01, 370.96it/s]
Adding requests:  87%|████████▋ | 3579/4096 [00:11<00:01, 380.06it/s]
Adding requests:  88%|████████▊ | 3619/4096 [00:11<00:01, 382.26it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:11<00:01, 382.51it/s]
Adding requests:  90%|█████████ | 3699/4096 [00:11<00:01, 389.45it/s]
Adding requests:  91%|█████████▏| 3739/4096 [00:11<00:00, 389.12it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:11<00:00, 397.06it/s]
Adding requests:  93%|█████████▎| 3821/4096 [00:11<00:00, 397.29it/s]
Adding requests:  94%|█████████▍| 3863/4096 [00:11<00:00, 403.51it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:11<00:00, 399.78it/s]
Adding requests:  96%|█████████▋| 3945/4096 [00:12<00:00, 401.34it/s]
Adding requests:  97%|█████████▋| 3986/4096 [00:12<00:00, 397.97it/s]
Adding requests:  98%|█████████▊| 4026/4096 [00:12<00:00, 396.89it/s]
Adding requests:  99%|█████████▉| 4066/4096 [00:12<00:00, 389.22it/s]
Adding requests: 100%|██████████| 4096/4096 [00:12<00:00, 328.22it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:00<00:03, 1160.64it/s, est. speed input: 1188572.45 toks/s, output: 1160.66 toks/s]
Processed prompts:  14%|█▍        | 567/4096 [00:02<00:19, 180.79it/s, est. speed input: 231704.98 toks/s, output: 226.27 toks/s]   
Processed prompts:  15%|█▌        | 619/4096 [00:03<00:30, 114.81it/s, est. speed input: 161701.96 toks/s, output: 157.91 toks/s]
Processed prompts:  16%|█▌        | 649/4096 [00:04<00:35, 96.95it/s, est. speed input: 143611.62 toks/s, output: 140.25 toks/s] 
Processed prompts:  16%|█▋        | 674/4096 [00:05<00:42, 80.90it/s, est. speed input: 129367.87 toks/s, output: 126.34 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:06<00:47, 71.20it/s, est. speed input: 119631.31 toks/s, output: 116.83 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:06<00:52, 63.93it/s, est. speed input: 111935.96 toks/s, output: 109.31 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:07<00:56, 58.57it/s, est. speed input: 105692.15 toks/s, output: 103.21 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:08<01:00, 54.68it/s, est. speed input: 100532.60 toks/s, output: 98.18 toks/s] 
Processed prompts:  20%|██        | 834/4096 [00:08<01:02, 51.88it/s, est. speed input: 96191.98 toks/s, output: 93.94 toks/s] 
Processed prompts:  21%|██        | 866/4096 [00:09<01:04, 49.92it/s, est. speed input: 92511.51 toks/s, output: 90.34 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:10<01:06, 48.44it/s, est. speed input: 89295.63 toks/s, output: 87.20 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:11<01:06, 47.42it/s, est. speed input: 86507.36 toks/s, output: 84.48 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:11<01:06, 47.09it/s, est. speed input: 84197.42 toks/s, output: 82.22 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:12<01:06, 46.48it/s, est. speed input: 82017.88 toks/s, output: 80.10 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:13<01:06, 46.04it/s, est. speed input: 80072.32 toks/s, output: 78.20 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:13<01:06, 45.73it/s, est. speed input: 78325.21 toks/s, output: 76.49 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:14<01:06, 45.51it/s, est. speed input: 76748.39 toks/s, output: 74.95 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:15<01:05, 45.36it/s, est. speed input: 75319.28 toks/s, output: 73.55 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:15<01:04, 45.62it/s, est. speed input: 74106.32 toks/s, output: 72.37 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:16<01:04, 45.43it/s, est. speed input: 72909.87 toks/s, output: 71.20 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:17<01:03, 45.30it/s, est. speed input: 71809.83 toks/s, output: 70.13 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:18<01:02, 45.21it/s, est. speed input: 70797.96 toks/s, output: 69.14 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:18<01:02, 45.14it/s, est. speed input: 69860.68 toks/s, output: 68.22 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:19<01:01, 45.09it/s, est. speed input: 68993.15 toks/s, output: 67.38 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:20<01:01, 45.06it/s, est. speed input: 68186.71 toks/s, output: 66.59 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:20<01:00, 45.04it/s, est. speed input: 67434.37 toks/s, output: 65.85 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:21<00:59, 45.03it/s, est. speed input: 66732.63 toks/s, output: 65.17 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:22<00:58, 45.02it/s, est. speed input: 66075.13 toks/s, output: 64.53 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:23<00:58, 45.01it/s, est. speed input: 65458.19 toks/s, output: 63.92 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:23<00:57, 45.01it/s, est. speed input: 64878.28 toks/s, output: 63.36 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:24<00:56, 45.00it/s, est. speed input: 64331.66 toks/s, output: 62.82 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:27<01:44, 24.08it/s, est. speed input: 58995.73 toks/s, output: 57.61 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:27<01:29, 27.99it/s, est. speed input: 58668.36 toks/s, output: 57.29 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:28<01:17, 31.58it/s, est. speed input: 58357.51 toks/s, output: 56.99 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:29<01:10, 34.69it/s, est. speed input: 58062.32 toks/s, output: 56.70 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:30<01:04, 37.27it/s, est. speed input: 57781.61 toks/s, output: 56.43 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:30<01:00, 39.30it/s, est. speed input: 57512.86 toks/s, output: 56.16 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:31<00:57, 40.86it/s, est. speed input: 57255.59 toks/s, output: 55.91 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:32<00:54, 42.04it/s, est. speed input: 57010.98 toks/s, output: 55.67 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:32<00:52, 42.91it/s, est. speed input: 56777.31 toks/s, output: 55.45 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:33<00:50, 43.88it/s, est. speed input: 56585.64 toks/s, output: 55.26 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:34<00:49, 44.25it/s, est. speed input: 56371.06 toks/s, output: 55.05 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:35<00:48, 44.50it/s, est. speed input: 56164.94 toks/s, output: 54.85 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:35<00:47, 44.68it/s, est. speed input: 55966.56 toks/s, output: 54.65 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:36<00:47, 44.80it/s, est. speed input: 55775.83 toks/s, output: 54.47 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:37<00:46, 44.89it/s, est. speed input: 55592.47 toks/s, output: 54.29 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:37<00:45, 44.95it/s, est. speed input: 55416.10 toks/s, output: 54.12 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:38<00:44, 44.99it/s, est. speed input: 55246.26 toks/s, output: 53.95 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:39<00:44, 45.02it/s, est. speed input: 55082.60 toks/s, output: 53.79 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:40<00:43, 45.05it/s, est. speed input: 54924.66 toks/s, output: 53.64 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:40<00:42, 45.06it/s, est. speed input: 54772.08 toks/s, output: 53.49 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:41<00:41, 45.07it/s, est. speed input: 54624.78 toks/s, output: 53.34 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:42<00:41, 45.07it/s, est. speed input: 54482.17 toks/s, output: 53.21 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:42<00:40, 45.45it/s, est. speed input: 54368.97 toks/s, output: 53.09 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:43<00:39, 45.34it/s, est. speed input: 54235.42 toks/s, output: 52.96 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:44<00:38, 45.27it/s, est. speed input: 54106.32 toks/s, output: 52.84 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:44<00:38, 45.21it/s, est. speed input: 53980.90 toks/s, output: 52.72 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:45<00:37, 45.17it/s, est. speed input: 53859.26 toks/s, output: 52.60 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:46<00:36, 45.14it/s, est. speed input: 53741.44 toks/s, output: 52.48 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:47<00:36, 45.13it/s, est. speed input: 53627.39 toks/s, output: 52.37 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:47<00:35, 45.48it/s, est. speed input: 53538.04 toks/s, output: 52.28 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:48<00:34, 45.35it/s, est. speed input: 53429.63 toks/s, output: 52.18 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:49<00:33, 45.27it/s, est. speed input: 53324.43 toks/s, output: 52.07 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:49<00:33, 45.21it/s, est. speed input: 53222.65 toks/s, output: 51.98 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:50<00:32, 45.21it/s, est. speed input: 53125.48 toks/s, output: 51.88 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:51<00:31, 45.13it/s, est. speed input: 53027.17 toks/s, output: 51.78 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:52<00:31, 45.11it/s, est. speed input: 52933.15 toks/s, output: 51.69 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:52<00:30, 45.10it/s, est. speed input: 52842.18 toks/s, output: 51.60 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:53<00:29, 45.10it/s, est. speed input: 52753.79 toks/s, output: 51.52 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:54<00:29, 45.09it/s, est. speed input: 52667.36 toks/s, output: 51.43 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:54<00:28, 45.09it/s, est. speed input: 52583.18 toks/s, output: 51.35 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:55<00:27, 45.08it/s, est. speed input: 52500.85 toks/s, output: 51.27 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:56<00:26, 45.08it/s, est. speed input: 52420.74 toks/s, output: 51.19 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:57<00:26, 45.07it/s, est. speed input: 52342.52 toks/s, output: 51.12 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:57<00:25, 45.07it/s, est. speed input: 52266.31 toks/s, output: 51.04 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:58<00:24, 45.07it/s, est. speed input: 52192.02 toks/s, output: 50.97 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:59<00:24, 45.07it/s, est. speed input: 52119.53 toks/s, output: 50.90 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:01<00:43, 24.10it/s, est. speed input: 50316.94 toks/s, output: 49.14 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:02<00:36, 28.01it/s, est. speed input: 50269.92 toks/s, output: 49.09 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:03<00:31, 31.60it/s, est. speed input: 50223.96 toks/s, output: 49.05 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:04<00:27, 34.71it/s, est. speed input: 50178.97 toks/s, output: 49.00 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:04<00:24, 37.28it/s, est. speed input: 50134.60 toks/s, output: 48.96 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:05<00:22, 39.31it/s, est. speed input: 50091.02 toks/s, output: 48.92 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:06<00:21, 40.88it/s, est. speed input: 50048.58 toks/s, output: 48.88 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:06<00:19, 42.04it/s, est. speed input: 50006.81 toks/s, output: 48.83 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:07<00:18, 42.90it/s, est. speed input: 49966.00 toks/s, output: 48.79 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:08<00:17, 43.52it/s, est. speed input: 49926.10 toks/s, output: 48.76 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:09<00:16, 43.97it/s, est. speed input: 49887.18 toks/s, output: 48.72 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:09<00:15, 44.29it/s, est. speed input: 49849.03 toks/s, output: 48.68 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:10<00:15, 44.51it/s, est. speed input: 49811.27 toks/s, output: 48.64 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:11<00:14, 44.67it/s, est. speed input: 49774.29 toks/s, output: 48.61 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:11<00:13, 44.78it/s, est. speed input: 49738.19 toks/s, output: 48.57 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:12<00:12, 44.85it/s, est. speed input: 49702.65 toks/s, output: 48.54 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:13<00:12, 44.93it/s, est. speed input: 49668.57 toks/s, output: 48.50 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:13<00:11, 44.98it/s, est. speed input: 49635.02 toks/s, output: 48.47 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:14<00:10, 45.01it/s, est. speed input: 49602.27 toks/s, output: 48.44 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:15<00:09, 45.04it/s, est. speed input: 49570.05 toks/s, output: 48.41 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:16<00:09, 45.05it/s, est. speed input: 49538.21 toks/s, output: 48.38 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:16<00:08, 45.42it/s, est. speed input: 49519.29 toks/s, output: 48.36 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:17<00:07, 45.33it/s, est. speed input: 49488.72 toks/s, output: 48.33 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:18<00:07, 45.26it/s, est. speed input: 49458.69 toks/s, output: 48.30 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:18<00:06, 45.21it/s, est. speed input: 49429.21 toks/s, output: 48.27 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:19<00:05, 45.17it/s, est. speed input: 49400.00 toks/s, output: 48.24 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:20<00:04, 45.14it/s, est. speed input: 49371.25 toks/s, output: 48.21 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:21<00:04, 45.11it/s, est. speed input: 49342.70 toks/s, output: 48.19 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:21<00:03, 45.10it/s, est. speed input: 49315.07 toks/s, output: 48.16 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:22<00:02, 45.08it/s, est. speed input: 49287.41 toks/s, output: 48.13 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:23<00:02, 45.06it/s, est. speed input: 49260.16 toks/s, output: 48.11 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:23<00:01, 45.42it/s, est. speed input: 49244.78 toks/s, output: 48.09 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:24<00:00, 45.82it/s, est. speed input: 49233.92 toks/s, output: 48.08 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:24<00:00, 45.82it/s, est. speed input: 49596.89 toks/s, output: 48.43 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:24<00:00, 48.43it/s, est. speed input: 49596.89 toks/s, output: 48.43 toks/s]
[rank0]:[W128 11:51:54.789861051 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 170.3s

测试结果:
  Requests/s:   45.07
  Tokens/s:     46199.36
  Total Reqs:   4096
  Elapsed:      90.88s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     46154.29


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cublaslt/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,15.7209,8064.8010,8.1420
1024,1024,1,128,128,15.6381,16029.0405,8.1851
2048,1024,2,256,128,30.4181,31178.5197,8.4161
4096,1024,4,512,128,45.0095,46134.7605,11.3754
8192,1024,8,1024,128,47.1019,48279.4158,21.7401
16384,1024,16,2048,128,45.3190,46451.9465,45.1908
32768,1024,32,4096,128,45.0725,46199.3590,90.8757

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:52:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:52:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=33488) WARNING 01-28 11:52:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=33488) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=33488) WARNING 01-28 11:52:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.43 requests/s, 8429.96 total tokens/s, 16.43 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:52:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:52:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:52:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:52:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:52:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:52:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:52:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:52:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:52:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:52:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:52:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:52:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:52:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:52:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:52:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:52:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:52:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:52:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=33488) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=33488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.15s/it]
(EngineCore_DP0 pid=33488) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.15s/it]
(EngineCore_DP0 pid=33488) 
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=33488) [2026-01-28 11:52:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=33488) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  6.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.98it/s]
(EngineCore_DP0 pid=33488) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 648.04it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 673.31it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 36.32it/s, est. speed input: 18597.80 toks/s, output: 36.32 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.55it/s, est. speed input: 11751.44 toks/s, output: 22.95 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:06, 19.44it/s, est. speed input: 10691.45 toks/s, output: 20.88 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:06, 18.34it/s, est. speed input: 10134.72 toks/s, output: 19.79 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:06, 17.87it/s, est. speed input: 9897.28 toks/s, output: 19.33 toks/s] 
Processed prompts:  14%|█▍        | 18/128 [00:00<00:06, 17.55it/s, est. speed input: 9729.68 toks/s, output: 19.00 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.31it/s, est. speed input: 9598.61 toks/s, output: 18.75 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.07it/s, est. speed input: 9479.36 toks/s, output: 18.51 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 16.89it/s, est. speed input: 9382.64 toks/s, output: 18.33 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.80it/s, est. speed input: 9307.65 toks/s, output: 18.18 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:05, 16.76it/s, est. speed input: 9246.98 toks/s, output: 18.06 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.72it/s, est. speed input: 9194.85 toks/s, output: 17.96 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:03<00:35,  2.73it/s, est. speed input: 4252.63 toks/s, output: 8.31 toks/s] 
Processed prompts:  27%|██▋       | 34/128 [00:03<00:25,  3.64it/s, est. speed input: 4381.02 toks/s, output: 8.56 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:04<00:19,  4.74it/s, est. speed input: 4502.71 toks/s, output: 8.79 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:04<00:14,  6.04it/s, est. speed input: 4618.21 toks/s, output: 9.02 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:04<00:11,  7.47it/s, est. speed input: 4727.42 toks/s, output: 9.23 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:04<00:09,  8.96it/s, est. speed input: 4831.07 toks/s, output: 9.44 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:04<00:08, 10.41it/s, est. speed input: 4929.22 toks/s, output: 9.63 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:04<00:06, 11.75it/s, est. speed input: 5022.27 toks/s, output: 9.81 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:04<00:06, 12.90it/s, est. speed input: 5110.47 toks/s, output: 9.98 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:04<00:05, 13.82it/s, est. speed input: 5193.07 toks/s, output: 10.14 toks/s]
Processed prompts:  41%|████      | 52/128 [00:05<00:05, 14.52it/s, est. speed input: 5270.91 toks/s, output: 10.29 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:05<00:04, 15.14it/s, est. speed input: 5347.75 toks/s, output: 10.44 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:05<00:04, 15.61it/s, est. speed input: 5421.21 toks/s, output: 10.59 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:05<00:04, 15.98it/s, est. speed input: 5491.93 toks/s, output: 10.73 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:05<00:04, 16.17it/s, est. speed input: 5557.68 toks/s, output: 10.85 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:05<00:04, 16.34it/s, est. speed input: 5621.54 toks/s, output: 10.98 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:05<00:03, 16.47it/s, est. speed input: 5682.95 toks/s, output: 11.10 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:05<00:03, 16.60it/s, est. speed input: 5742.79 toks/s, output: 11.22 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:06<00:03, 16.67it/s, est. speed input: 5799.69 toks/s, output: 11.33 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:06<00:03, 16.49it/s, est. speed input: 5849.22 toks/s, output: 11.42 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:06<00:03, 16.54it/s, est. speed input: 5900.65 toks/s, output: 11.52 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:06<00:03, 16.61it/s, est. speed input: 5950.94 toks/s, output: 11.62 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:06<00:03, 16.63it/s, est. speed input: 5998.78 toks/s, output: 11.72 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:06<00:03, 16.56it/s, est. speed input: 6043.14 toks/s, output: 11.80 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:06<00:02, 16.57it/s, est. speed input: 6087.07 toks/s, output: 11.89 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:06<00:02, 16.57it/s, est. speed input: 6129.37 toks/s, output: 11.97 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:06<00:02, 16.50it/s, est. speed input: 6168.47 toks/s, output: 12.05 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:07<00:02, 16.51it/s, est. speed input: 6207.70 toks/s, output: 12.12 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:07<00:02, 16.52it/s, est. speed input: 6245.71 toks/s, output: 12.20 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:07<00:02, 16.54it/s, est. speed input: 6282.56 toks/s, output: 12.27 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:07<00:02, 16.56it/s, est. speed input: 6318.35 toks/s, output: 12.34 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:07<00:02, 16.55it/s, est. speed input: 6352.62 toks/s, output: 12.41 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:07<00:01, 16.57it/s, est. speed input: 6386.43 toks/s, output: 12.47 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:07<00:01, 16.65it/s, est. speed input: 6420.27 toks/s, output: 12.54 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:07<00:01, 16.68it/s, est. speed input: 6452.81 toks/s, output: 12.60 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:08<00:01, 16.72it/s, est. speed input: 6484.66 toks/s, output: 12.67 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:08<00:01, 16.77it/s, est. speed input: 6515.99 toks/s, output: 12.73 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:08<00:01, 16.70it/s, est. speed input: 6544.51 toks/s, output: 12.78 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:08<00:01, 16.58it/s, est. speed input: 6570.69 toks/s, output: 12.83 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:08<00:01, 16.52it/s, est. speed input: 6596.80 toks/s, output: 12.88 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:08<00:00, 16.39it/s, est. speed input: 6620.25 toks/s, output: 12.93 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:08<00:00, 16.33it/s, est. speed input: 6643.86 toks/s, output: 12.98 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:08<00:00, 16.28it/s, est. speed input: 6666.53 toks/s, output: 13.02 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:09<00:00, 16.24it/s, est. speed input: 6688.53 toks/s, output: 13.06 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:09<00:00, 16.21it/s, est. speed input: 6709.80 toks/s, output: 13.11 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:09<00:00, 16.16it/s, est. speed input: 6729.95 toks/s, output: 13.14 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:09<00:00, 16.07it/s, est. speed input: 6748.71 toks/s, output: 13.18 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:09<00:00, 16.00it/s, est. speed input: 6766.66 toks/s, output: 13.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 15.95it/s, est. speed input: 6784.19 toks/s, output: 13.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 15.95it/s, est. speed input: 6784.19 toks/s, output: 13.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 13.25it/s, est. speed input: 6784.19 toks/s, output: 13.25 toks/s]
[rank0]:[W128 11:52:46.511390359 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.8s

测试结果:
  Requests/s:   16.43
  Tokens/s:     8429.96
  Total Reqs:   128
  Elapsed:      7.79s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8413.53

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:52:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:52:57 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=34107) WARNING 01-28 11:53:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=34107) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=34107) WARNING 01-28 11:53:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.08 requests/s, 16479.99 total tokens/s, 16.08 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 11:52:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:52:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:52:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:52:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:52:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:52:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:52:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:52:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:52:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:52:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:53:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:53:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:53:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:53:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:53:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:53:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:53:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:53:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:53:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=34107) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=34107) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=34107) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=34107) 
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=34107) [2026-01-28 11:53:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=34107) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.95it/s]
(EngineCore_DP0 pid=34107) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 351.99it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 372.37it/s]
Adding requests:  89%|████████▉ | 114/128 [00:00<00:00, 376.60it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 375.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:03, 38.74it/s, est. speed input: 39677.59 toks/s, output: 38.74 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:05, 23.27it/s, est. speed input: 25678.47 toks/s, output: 25.08 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 20.42it/s, est. speed input: 22947.98 toks/s, output: 22.41 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 18.86it/s, est. speed input: 21447.73 toks/s, output: 20.94 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:06, 18.05it/s, est. speed input: 20713.67 toks/s, output: 20.23 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:01<00:06, 17.46it/s, est. speed input: 20169.21 toks/s, output: 19.70 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:06, 17.11it/s, est. speed input: 19780.65 toks/s, output: 19.32 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:01<00:06, 16.85it/s, est. speed input: 19471.28 toks/s, output: 19.01 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:06, 16.70it/s, est. speed input: 19227.98 toks/s, output: 18.78 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:01<00:06, 16.60it/s, est. speed input: 19026.81 toks/s, output: 18.58 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:01<00:05, 16.45it/s, est. speed input: 18833.25 toks/s, output: 18.39 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:05, 16.39it/s, est. speed input: 18678.98 toks/s, output: 18.24 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:01<00:05, 16.32it/s, est. speed input: 18538.55 toks/s, output: 18.10 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:02<00:05, 16.30it/s, est. speed input: 18422.68 toks/s, output: 17.99 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:02<00:05, 16.25it/s, est. speed input: 18310.92 toks/s, output: 17.88 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:02<00:05, 16.22it/s, est. speed input: 18213.36 toks/s, output: 17.79 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:02<00:05, 16.28it/s, est. speed input: 18140.92 toks/s, output: 17.72 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:02<00:05, 16.41it/s, est. speed input: 18091.68 toks/s, output: 17.67 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:02<00:04, 16.44it/s, est. speed input: 18036.66 toks/s, output: 17.61 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:02<00:04, 16.45it/s, est. speed input: 17985.77 toks/s, output: 17.56 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:02<00:04, 16.37it/s, est. speed input: 17924.28 toks/s, output: 17.50 toks/s]
Processed prompts:  41%|████      | 52/128 [00:02<00:04, 16.47it/s, est. speed input: 17890.81 toks/s, output: 17.47 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:03<00:04, 16.53it/s, est. speed input: 17860.14 toks/s, output: 17.44 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:03<00:04, 16.60it/s, est. speed input: 17834.33 toks/s, output: 17.42 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:03<00:04, 16.56it/s, est. speed input: 17799.04 toks/s, output: 17.38 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:03<00:04, 16.53it/s, est. speed input: 17765.35 toks/s, output: 17.35 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:03<00:03, 16.50it/s, est. speed input: 17733.86 toks/s, output: 17.32 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:03<00:03, 16.48it/s, est. speed input: 17704.26 toks/s, output: 17.29 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:03<00:03, 16.50it/s, est. speed input: 17680.08 toks/s, output: 17.27 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:03<00:03, 16.56it/s, est. speed input: 17662.04 toks/s, output: 17.25 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:04<00:03, 16.58it/s, est. speed input: 17643.78 toks/s, output: 17.23 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:04<00:03, 16.61it/s, est. speed input: 17627.62 toks/s, output: 17.21 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:04<00:03, 16.58it/s, est. speed input: 17607.06 toks/s, output: 17.19 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:04<00:03, 16.51it/s, est. speed input: 17583.68 toks/s, output: 17.17 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:04<00:03, 16.42it/s, est. speed input: 17556.54 toks/s, output: 17.14 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:04<00:02, 16.48it/s, est. speed input: 17542.43 toks/s, output: 17.13 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:04<00:02, 16.44it/s, est. speed input: 17522.66 toks/s, output: 17.11 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:04<00:02, 16.42it/s, est. speed input: 17503.87 toks/s, output: 17.09 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:05<00:02, 16.47it/s, est. speed input: 17491.52 toks/s, output: 17.08 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:05<00:02, 16.50it/s, est. speed input: 17479.20 toks/s, output: 17.07 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:05<00:02, 16.55it/s, est. speed input: 17469.98 toks/s, output: 17.06 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:05<00:02, 16.61it/s, est. speed input: 17462.72 toks/s, output: 17.05 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:05<00:02, 16.50it/s, est. speed input: 17444.14 toks/s, output: 17.04 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:05<00:01, 16.40it/s, est. speed input: 17425.16 toks/s, output: 17.02 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:05<00:01, 16.30it/s, est. speed input: 17404.42 toks/s, output: 17.00 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:05<00:01, 16.29it/s, est. speed input: 17388.36 toks/s, output: 16.98 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:06<00:01, 16.26it/s, est. speed input: 17371.95 toks/s, output: 16.96 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:06<00:01, 16.26it/s, est. speed input: 17357.74 toks/s, output: 16.95 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:06<00:01, 16.19it/s, est. speed input: 17338.86 toks/s, output: 16.93 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:06<00:01, 16.16it/s, est. speed input: 17322.27 toks/s, output: 16.92 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:06<00:01, 16.10it/s, est. speed input: 17303.40 toks/s, output: 16.90 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:06<00:00, 16.15it/s, est. speed input: 17291.58 toks/s, output: 16.89 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:06<00:00, 16.17it/s, est. speed input: 17279.08 toks/s, output: 16.87 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:06<00:00, 16.17it/s, est. speed input: 17266.11 toks/s, output: 16.86 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:07<00:00, 16.09it/s, est. speed input: 17248.65 toks/s, output: 16.84 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:07<00:00, 16.14it/s, est. speed input: 17238.30 toks/s, output: 16.83 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:07<00:00, 16.19it/s, est. speed input: 17229.12 toks/s, output: 16.83 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:07<00:00, 16.24it/s, est. speed input: 17221.19 toks/s, output: 16.82 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:07<00:00, 16.26it/s, est. speed input: 17212.72 toks/s, output: 16.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.27it/s, est. speed input: 17204.07 toks/s, output: 16.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.27it/s, est. speed input: 17204.07 toks/s, output: 16.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.80it/s, est. speed input: 17204.07 toks/s, output: 16.80 toks/s]
[rank0]:[W128 11:53:34.769841286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.3s

测试结果:
  Requests/s:   16.08
  Tokens/s:     16479.99
  Total Reqs:   128
  Elapsed:      7.96s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     16463.91

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:53:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:53:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=34627) WARNING 01-28 11:53:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=34627) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=34627) WARNING 01-28 11:54:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.63 requests/s, 33441.90 total tokens/s, 32.63 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 11:53:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:53:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:53:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:53:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:53:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:53:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:53:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:53:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:53:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:53:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:53:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:53:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:53:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:53:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:53:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:53:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:53:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:53:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:53:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=34627) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=34627) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=34627) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=34627) 
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=34627) [2026-01-28 11:53:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=34627) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.77it/s]
(EngineCore_DP0 pid=34627) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.08it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 37/256 [00:00<00:00, 366.67it/s]
Adding requests:  30%|███       | 77/256 [00:00<00:00, 386.01it/s]
Adding requests:  45%|████▌     | 116/256 [00:00<00:00, 379.99it/s]
Adding requests:  61%|██████    | 155/256 [00:00<00:00, 379.85it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 379.37it/s]
Adding requests:  91%|█████████▏| 234/256 [00:00<00:00, 385.52it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 383.49it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 129.40it/s, est. speed input: 132523.50 toks/s, output: 129.40 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:00<00:03, 58.65it/s, est. speed input: 66689.34 toks/s, output: 65.13 toks/s]   
Processed prompts:  16%|█▌        | 41/256 [00:00<00:04, 48.25it/s, est. speed input: 56579.69 toks/s, output: 55.25 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:00<00:04, 43.67it/s, est. speed input: 52292.39 toks/s, output: 51.07 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:05, 38.56it/s, est. speed input: 48290.60 toks/s, output: 47.16 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:05, 39.16it/s, est. speed input: 47681.10 toks/s, output: 46.56 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 35.14it/s, est. speed input: 45068.10 toks/s, output: 44.01 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:05, 34.59it/s, est. speed input: 44156.17 toks/s, output: 43.12 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 34.30it/s, est. speed input: 43439.68 toks/s, output: 42.42 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:05, 34.03it/s, est. speed input: 42802.93 toks/s, output: 41.80 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:05, 33.86it/s, est. speed input: 42259.66 toks/s, output: 41.27 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:02<00:05, 33.72it/s, est. speed input: 41775.00 toks/s, output: 40.80 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:05, 33.87it/s, est. speed input: 41408.50 toks/s, output: 40.44 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 33.98it/s, est. speed input: 41078.74 toks/s, output: 40.12 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 34.05it/s, est. speed input: 40779.90 toks/s, output: 39.82 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 34.02it/s, est. speed input: 40492.88 toks/s, output: 39.54 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 34.08it/s, est. speed input: 40247.12 toks/s, output: 39.30 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 34.03it/s, est. speed input: 40008.23 toks/s, output: 39.07 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:04, 34.12it/s, est. speed input: 39807.39 toks/s, output: 38.87 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:04, 34.19it/s, est. speed input: 39625.25 toks/s, output: 38.70 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:04, 34.20it/s, est. speed input: 39450.74 toks/s, output: 38.53 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:03<00:03, 34.03it/s, est. speed input: 39263.71 toks/s, output: 38.34 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 34.08it/s, est. speed input: 39112.44 toks/s, output: 38.20 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 33.82it/s, est. speed input: 38934.46 toks/s, output: 38.02 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 33.68it/s, est. speed input: 38772.20 toks/s, output: 37.86 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 33.53it/s, est. speed input: 38613.69 toks/s, output: 37.71 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 33.47it/s, est. speed input: 38472.12 toks/s, output: 37.57 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:03, 33.43it/s, est. speed input: 38338.87 toks/s, output: 37.44 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:04<00:03, 33.24it/s, est. speed input: 38194.26 toks/s, output: 37.30 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:04<00:03, 33.18it/s, est. speed input: 38066.62 toks/s, output: 37.17 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:04<00:02, 33.16it/s, est. speed input: 37949.65 toks/s, output: 37.06 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 33.21it/s, est. speed input: 37844.86 toks/s, output: 36.96 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 33.16it/s, est. speed input: 37736.96 toks/s, output: 36.85 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 33.09it/s, est. speed input: 37631.48 toks/s, output: 36.75 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 33.00it/s, est. speed input: 37527.37 toks/s, output: 36.65 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 33.11it/s, est. speed input: 37444.88 toks/s, output: 36.57 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 33.21it/s, est. speed input: 37367.68 toks/s, output: 36.49 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:05<00:02, 33.22it/s, est. speed input: 37289.69 toks/s, output: 36.42 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:05<00:01, 33.16it/s, est. speed input: 37209.25 toks/s, output: 36.34 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:05<00:01, 33.26it/s, est. speed input: 37143.92 toks/s, output: 36.27 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 33.15it/s, est. speed input: 37067.59 toks/s, output: 36.20 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 33.20it/s, est. speed input: 37004.12 toks/s, output: 36.14 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:07<00:09,  5.37it/s, est. speed input: 27066.48 toks/s, output: 26.43 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:07<00:06,  7.18it/s, est. speed input: 27182.16 toks/s, output: 26.54 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:08<00:04,  9.41it/s, est. speed input: 27297.06 toks/s, output: 26.66 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:08<00:03, 12.03it/s, est. speed input: 27408.44 toks/s, output: 26.77 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:08<00:02, 14.94it/s, est. speed input: 27516.87 toks/s, output: 26.87 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:08<00:01, 17.96it/s, est. speed input: 27620.21 toks/s, output: 26.97 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:08<00:01, 20.95it/s, est. speed input: 27722.04 toks/s, output: 27.07 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:08<00:00, 23.69it/s, est. speed input: 27820.54 toks/s, output: 27.17 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:08<00:00, 26.07it/s, est. speed input: 27915.46 toks/s, output: 27.26 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:08<00:00, 27.82it/s, est. speed input: 27995.71 toks/s, output: 27.34 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:08<00:00, 29.39it/s, est. speed input: 28083.30 toks/s, output: 27.42 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:09<00:00, 30.64it/s, est. speed input: 28170.63 toks/s, output: 27.51 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:09<00:00, 31.56it/s, est. speed input: 28255.03 toks/s, output: 27.59 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 31.56it/s, est. speed input: 28296.71 toks/s, output: 27.63 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 27.63it/s, est. speed input: 28296.71 toks/s, output: 27.63 toks/s]
[rank0]:[W128 11:54:26.684490937 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.1s

测试结果:
  Requests/s:   32.63
  Tokens/s:     33441.90
  Total Reqs:   256
  Elapsed:      7.85s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     33409.27

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:54:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:54:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=35164) WARNING 01-28 11:54:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=35164) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=35164) WARNING 01-28 11:54:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 59.78 requests/s, 61270.31 total tokens/s, 59.78 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 11:54:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:54:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:54:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:54:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:54:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:54:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:54:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:54:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:54:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:54:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:54:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:54:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:54:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:54:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:54:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:54:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:54:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:54:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:54:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=35164) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=35164) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=35164) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=35164) 
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=35164) [2026-01-28 11:54:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=35164) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.21it/s]
(EngineCore_DP0 pid=35164) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.61it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 37/512 [00:00<00:01, 363.12it/s]
Adding requests:  15%|█▌        | 77/512 [00:00<00:01, 381.08it/s]
Adding requests:  23%|██▎       | 116/512 [00:00<00:01, 384.45it/s]
Adding requests:  30%|███       | 155/512 [00:00<00:00, 381.64it/s]
Adding requests:  38%|███▊      | 194/512 [00:00<00:00, 380.23it/s]
Adding requests:  46%|████▌     | 234/512 [00:00<00:00, 385.84it/s]
Adding requests:  53%|█████▎    | 273/512 [00:00<00:00, 383.40it/s]
Adding requests:  61%|██████    | 312/512 [00:00<00:00, 381.40it/s]
Adding requests:  69%|██████▊   | 351/512 [00:00<00:00, 382.38it/s]
Adding requests:  76%|███████▌  | 390/512 [00:01<00:00, 384.00it/s]
Adding requests:  84%|████████▍ | 429/512 [00:01<00:00, 385.30it/s]
Adding requests:  91%|█████████▏| 468/512 [00:01<00:00, 383.31it/s]
Adding requests:  99%|█████████▉| 507/512 [00:01<00:00, 382.81it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 382.50it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:01, 387.62it/s, est. speed input: 396985.54 toks/s, output: 387.64 toks/s]
Processed prompts:  20%|█▉        | 101/512 [00:00<00:03, 121.45it/s, est. speed input: 142370.09 toks/s, output: 139.03 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:04, 92.33it/s, est. speed input: 113190.26 toks/s, output: 110.54 toks/s] 
Processed prompts:  27%|██▋       | 136/512 [00:01<00:04, 87.97it/s, est. speed input: 107753.34 toks/s, output: 105.23 toks/s]
Processed prompts:  29%|██▉       | 148/512 [00:01<00:04, 81.71it/s, est. speed input: 102305.47 toks/s, output: 99.91 toks/s] 
Processed prompts:  31%|███       | 158/512 [00:01<00:04, 74.04it/s, est. speed input: 96880.12 toks/s, output: 94.61 toks/s] 
Processed prompts:  32%|███▏      | 166/512 [00:01<00:04, 71.93it/s, est. speed input: 94659.74 toks/s, output: 92.44 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:01<00:04, 69.99it/s, est. speed input: 92702.11 toks/s, output: 90.53 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:02<00:04, 68.44it/s, est. speed input: 91008.47 toks/s, output: 88.87 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:02<00:04, 67.19it/s, est. speed input: 89510.60 toks/s, output: 87.41 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:02<00:04, 66.27it/s, est. speed input: 88187.68 toks/s, output: 86.12 toks/s]
Processed prompts:  40%|████      | 206/512 [00:02<00:04, 65.57it/s, est. speed input: 87001.84 toks/s, output: 84.96 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:02<00:04, 65.05it/s, est. speed input: 85932.31 toks/s, output: 83.92 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:02<00:04, 64.65it/s, est. speed input: 84958.05 toks/s, output: 82.97 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:02<00:04, 64.30it/s, est. speed input: 84059.46 toks/s, output: 82.09 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:02<00:04, 64.00it/s, est. speed input: 83228.85 toks/s, output: 81.28 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:03<00:04, 63.80it/s, est. speed input: 82468.52 toks/s, output: 80.54 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:03<00:04, 63.67it/s, est. speed input: 81769.55 toks/s, output: 79.85 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:03<00:03, 63.63it/s, est. speed input: 81133.46 toks/s, output: 79.23 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:03<00:03, 63.64it/s, est. speed input: 80549.34 toks/s, output: 78.66 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:03<00:03, 63.56it/s, est. speed input: 79994.81 toks/s, output: 78.12 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:03<00:03, 63.56it/s, est. speed input: 79485.68 toks/s, output: 77.62 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:03, 63.56it/s, est. speed input: 79009.96 toks/s, output: 77.16 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:03<00:03, 63.51it/s, est. speed input: 78557.93 toks/s, output: 76.72 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:04<00:03, 63.51it/s, est. speed input: 78138.67 toks/s, output: 76.31 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:04<00:03, 63.57it/s, est. speed input: 77752.04 toks/s, output: 75.93 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:04<00:02, 63.55it/s, est. speed input: 77380.08 toks/s, output: 75.57 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:04<00:02, 63.58it/s, est. speed input: 77035.15 toks/s, output: 75.23 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:04<00:02, 63.52it/s, est. speed input: 76699.67 toks/s, output: 74.90 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:04<00:02, 63.53it/s, est. speed input: 76387.99 toks/s, output: 74.60 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:04<00:02, 63.50it/s, est. speed input: 76088.10 toks/s, output: 74.30 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:04<00:02, 63.50it/s, est. speed input: 75806.80 toks/s, output: 74.03 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:05<00:02, 63.57it/s, est. speed input: 75545.40 toks/s, output: 73.77 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:05<00:02, 63.56it/s, est. speed input: 75290.92 toks/s, output: 73.53 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:05<00:01, 63.53it/s, est. speed input: 75047.46 toks/s, output: 73.29 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:05<00:01, 63.41it/s, est. speed input: 74804.89 toks/s, output: 73.05 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:05<00:01, 63.28it/s, est. speed input: 74569.72 toks/s, output: 72.82 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:05<00:01, 63.13it/s, est. speed input: 74339.98 toks/s, output: 72.60 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:05<00:01, 62.97it/s, est. speed input: 74115.63 toks/s, output: 72.38 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:05<00:01, 62.89it/s, est. speed input: 73903.69 toks/s, output: 72.17 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:06<00:01, 62.89it/s, est. speed input: 73704.82 toks/s, output: 71.98 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:06<00:01, 62.92it/s, est. speed input: 73516.42 toks/s, output: 71.79 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:06<00:00, 62.89it/s, est. speed input: 73332.10 toks/s, output: 71.61 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:06<00:00, 62.89it/s, est. speed input: 73156.64 toks/s, output: 71.44 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:06<00:00, 62.96it/s, est. speed input: 72992.70 toks/s, output: 71.28 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:06<00:00, 62.90it/s, est. speed input: 72827.14 toks/s, output: 71.12 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:06<00:00, 62.94it/s, est. speed input: 72673.95 toks/s, output: 70.97 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:06<00:00, 62.99it/s, est. speed input: 72527.47 toks/s, output: 70.83 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:07<00:00, 63.04it/s, est. speed input: 72387.66 toks/s, output: 70.69 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:07<00:00, 63.53it/s, est. speed input: 72282.31 toks/s, output: 70.59 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 63.53it/s, est. speed input: 72563.87 toks/s, output: 70.86 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 70.86it/s, est. speed input: 72563.87 toks/s, output: 70.86 toks/s]
[rank0]:[W128 11:55:17.620099655 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.1s

测试结果:
  Requests/s:   59.78
  Tokens/s:     61270.31
  Total Reqs:   512
  Elapsed:      8.57s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     61210.53

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:55:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:55:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=35716) WARNING 01-28 11:55:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=35716) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=35716) WARNING 01-28 11:55:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.75 requests/s, 63289.73 total tokens/s, 61.75 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 11:55:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:55:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:55:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:55:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:55:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:55:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:55:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:55:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:55:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:55:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:55:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:55:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:55:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:55:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:55:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:55:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:55:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:55:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:55:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=35716) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=35716) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]
(EngineCore_DP0 pid=35716) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]
(EngineCore_DP0 pid=35716) 
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=35716) [2026-01-28 11:55:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=35716) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.14it/s]
(EngineCore_DP0 pid=35716) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.24it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.42it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 352.39it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 372.58it/s]
Adding requests:  11%|█         | 115/1024 [00:00<00:02, 381.11it/s]
Adding requests:  15%|█▌        | 154/1024 [00:00<00:02, 381.13it/s]
Adding requests:  19%|█▉        | 193/1024 [00:00<00:02, 377.91it/s]
Adding requests:  23%|██▎       | 232/1024 [00:00<00:02, 379.38it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:01, 382.18it/s]
Adding requests:  30%|███       | 310/1024 [00:00<00:01, 381.92it/s]
Adding requests:  34%|███▍      | 349/1024 [00:00<00:01, 382.74it/s]
Adding requests:  38%|███▊      | 388/1024 [00:01<00:01, 381.65it/s]
Adding requests:  42%|████▏     | 428/1024 [00:01<00:01, 383.74it/s]
Adding requests:  46%|████▌     | 468/1024 [00:01<00:01, 385.77it/s]
Adding requests:  50%|████▉     | 507/1024 [00:01<00:01, 382.77it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 367.72it/s]
Adding requests:  57%|█████▋    | 586/1024 [00:01<00:01, 376.61it/s]
Adding requests:  61%|██████    | 625/1024 [00:01<00:01, 380.46it/s]
Adding requests:  65%|██████▍   | 664/1024 [00:01<00:00, 383.03it/s]
Adding requests:  69%|██████▉   | 705/1024 [00:01<00:00, 390.33it/s]
Adding requests:  73%|███████▎  | 745/1024 [00:01<00:00, 388.16it/s]
Adding requests:  77%|███████▋  | 785/1024 [00:02<00:00, 389.82it/s]
Adding requests:  81%|████████  | 825/1024 [00:02<00:00, 380.92it/s]
Adding requests:  84%|████████▍ | 865/1024 [00:02<00:00, 384.63it/s]
Adding requests:  88%|████████▊ | 906/1024 [00:02<00:00, 389.55it/s]
Adding requests:  92%|█████████▏| 946/1024 [00:02<00:00, 388.52it/s]
Adding requests:  96%|█████████▋| 986/1024 [00:02<00:00, 390.35it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 382.89it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:00<00:01, 756.14it/s, est. speed input: 774396.21 toks/s, output: 756.18 toks/s]
Processed prompts:  22%|██▏       | 230/1024 [00:03<00:16, 49.34it/s, est. speed input: 62197.88 toks/s, output: 60.74 toks/s]   
Processed prompts:  26%|██▌       | 263/1024 [00:04<00:14, 51.72it/s, est. speed input: 62565.18 toks/s, output: 61.10 toks/s]
Processed prompts:  28%|██▊       | 284/1024 [00:04<00:14, 52.07it/s, est. speed input: 61965.43 toks/s, output: 60.51 toks/s]
Processed prompts:  29%|██▉       | 299/1024 [00:04<00:13, 52.79it/s, est. speed input: 61821.64 toks/s, output: 60.37 toks/s]
Processed prompts:  30%|███       | 311/1024 [00:05<00:12, 56.07it/s, est. speed input: 62664.64 toks/s, output: 61.20 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:13, 53.47it/s, est. speed input: 61731.37 toks/s, output: 60.28 toks/s]
Processed prompts:  32%|███▏      | 331/1024 [00:05<00:12, 55.34it/s, est. speed input: 61943.15 toks/s, output: 60.49 toks/s]
Processed prompts:  33%|███▎      | 340/1024 [00:05<00:11, 58.07it/s, est. speed input: 62269.16 toks/s, output: 60.81 toks/s]
Processed prompts:  34%|███▍      | 348/1024 [00:05<00:11, 58.71it/s, est. speed input: 62291.60 toks/s, output: 60.83 toks/s]
Processed prompts:  35%|███▍      | 356/1024 [00:05<00:11, 59.32it/s, est. speed input: 62312.78 toks/s, output: 60.85 toks/s]
Processed prompts:  35%|███▌      | 363/1024 [00:05<00:11, 58.14it/s, est. speed input: 62160.09 toks/s, output: 60.70 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:11, 57.17it/s, est. speed input: 62017.70 toks/s, output: 60.56 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:11, 58.31it/s, est. speed input: 62039.69 toks/s, output: 60.59 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:06<00:10, 59.22it/s, est. speed input: 62063.41 toks/s, output: 60.61 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:06<00:10, 59.90it/s, est. speed input: 62085.25 toks/s, output: 60.63 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:06<00:10, 60.37it/s, est. speed input: 62103.53 toks/s, output: 60.65 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:06<00:10, 60.77it/s, est. speed input: 62125.38 toks/s, output: 60.67 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:06<00:09, 61.01it/s, est. speed input: 62143.49 toks/s, output: 60.69 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:09, 61.20it/s, est. speed input: 62161.38 toks/s, output: 60.70 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:07<00:09, 61.35it/s, est. speed input: 62180.16 toks/s, output: 60.72 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:07<00:09, 61.48it/s, est. speed input: 62199.52 toks/s, output: 60.74 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:07<00:09, 61.57it/s, est. speed input: 62218.06 toks/s, output: 60.76 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:07<00:09, 61.63it/s, est. speed input: 62236.29 toks/s, output: 60.78 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:07<00:09, 61.66it/s, est. speed input: 62252.60 toks/s, output: 60.79 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:07<00:08, 61.68it/s, est. speed input: 62268.28 toks/s, output: 60.81 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:07<00:08, 61.71it/s, est. speed input: 62284.86 toks/s, output: 60.82 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:08<00:08, 61.67it/s, est. speed input: 62297.02 toks/s, output: 60.84 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:08<00:08, 61.61it/s, est. speed input: 62307.38 toks/s, output: 60.85 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:08<00:08, 61.54it/s, est. speed input: 62316.06 toks/s, output: 60.86 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:08<00:08, 61.56it/s, est. speed input: 62327.56 toks/s, output: 60.87 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:08<00:08, 61.57it/s, est. speed input: 62338.87 toks/s, output: 60.88 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:08<00:08, 61.57it/s, est. speed input: 62349.27 toks/s, output: 60.89 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:07, 61.62it/s, est. speed input: 62362.21 toks/s, output: 60.90 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:07, 61.62it/s, est. speed input: 62372.91 toks/s, output: 60.91 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:09<00:07, 61.59it/s, est. speed input: 62381.65 toks/s, output: 60.92 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:09<00:07, 61.58it/s, est. speed input: 62390.84 toks/s, output: 60.93 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:09<00:07, 61.55it/s, est. speed input: 62398.77 toks/s, output: 60.94 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:09<00:07, 61.59it/s, est. speed input: 62409.22 toks/s, output: 60.95 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:09<00:07, 61.66it/s, est. speed input: 62421.36 toks/s, output: 60.96 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:06, 61.67it/s, est. speed input: 62431.36 toks/s, output: 60.97 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:06, 61.68it/s, est. speed input: 62441.04 toks/s, output: 60.98 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:10<00:06, 61.68it/s, est. speed input: 62450.47 toks/s, output: 60.99 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:10<00:06, 61.69it/s, est. speed input: 62459.93 toks/s, output: 61.00 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:10<00:06, 61.64it/s, est. speed input: 62466.89 toks/s, output: 61.00 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:10<00:06, 61.61it/s, est. speed input: 62473.70 toks/s, output: 61.01 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:10<00:06, 61.65it/s, est. speed input: 62482.95 toks/s, output: 61.02 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:05, 62.44it/s, est. speed input: 62522.84 toks/s, output: 61.06 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:05, 63.08it/s, est. speed input: 62564.75 toks/s, output: 61.10 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:05, 63.49it/s, est. speed input: 62604.14 toks/s, output: 61.14 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:11<00:05, 63.78it/s, est. speed input: 62642.61 toks/s, output: 61.17 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:11<00:05, 63.93it/s, est. speed input: 62678.06 toks/s, output: 61.21 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:11<00:05, 64.07it/s, est. speed input: 62714.02 toks/s, output: 61.24 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:05, 64.21it/s, est. speed input: 62750.78 toks/s, output: 61.28 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:04, 64.27it/s, est. speed input: 62785.44 toks/s, output: 61.31 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:04, 64.34it/s, est. speed input: 62820.14 toks/s, output: 61.35 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:04, 64.42it/s, est. speed input: 62855.39 toks/s, output: 61.38 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:04, 64.45it/s, est. speed input: 62888.95 toks/s, output: 61.41 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:12<00:04, 64.49it/s, est. speed input: 62922.27 toks/s, output: 61.45 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:12<00:04, 64.54it/s, est. speed input: 62955.68 toks/s, output: 61.48 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:04, 64.56it/s, est. speed input: 62988.02 toks/s, output: 61.51 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:04, 64.56it/s, est. speed input: 63019.35 toks/s, output: 61.54 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:03, 64.58it/s, est. speed input: 63050.52 toks/s, output: 61.57 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:03, 64.57it/s, est. speed input: 63080.32 toks/s, output: 61.60 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:12<00:03, 64.49it/s, est. speed input: 63107.27 toks/s, output: 61.63 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:03, 64.49it/s, est. speed input: 63135.71 toks/s, output: 61.66 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:03, 64.49it/s, est. speed input: 63163.24 toks/s, output: 61.68 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:03, 64.44it/s, est. speed input: 63189.02 toks/s, output: 61.71 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 64.48it/s, est. speed input: 63216.51 toks/s, output: 61.73 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 64.51it/s, est. speed input: 63243.33 toks/s, output: 61.76 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:02, 64.51it/s, est. speed input: 63269.19 toks/s, output: 61.79 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:13<00:02, 64.48it/s, est. speed input: 63293.81 toks/s, output: 61.81 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:02, 64.44it/s, est. speed input: 63317.34 toks/s, output: 61.83 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:13<00:02, 64.44it/s, est. speed input: 63341.07 toks/s, output: 61.86 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:13<00:02, 64.41it/s, est. speed input: 63363.81 toks/s, output: 61.88 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 64.45it/s, est. speed input: 63387.81 toks/s, output: 61.90 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 64.48it/s, est. speed input: 63411.27 toks/s, output: 61.92 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 64.48it/s, est. speed input: 63433.91 toks/s, output: 61.95 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:14<00:01, 64.47it/s, est. speed input: 63455.83 toks/s, output: 61.97 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:14<00:01, 64.47it/s, est. speed input: 63477.50 toks/s, output: 61.99 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:14<00:01, 64.45it/s, est. speed input: 63498.47 toks/s, output: 62.01 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:14<00:01, 64.43it/s, est. speed input: 63518.71 toks/s, output: 62.03 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:14<00:01, 64.43it/s, est. speed input: 63539.04 toks/s, output: 62.05 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 64.43it/s, est. speed input: 63559.21 toks/s, output: 62.07 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:15<00:01, 64.43it/s, est. speed input: 63578.73 toks/s, output: 62.09 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:15<00:01, 64.44it/s, est. speed input: 63598.33 toks/s, output: 62.11 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:15<00:00, 64.45it/s, est. speed input: 63617.87 toks/s, output: 62.13 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:15<00:00, 64.50it/s, est. speed input: 63638.09 toks/s, output: 62.15 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:15<00:00, 64.52it/s, est. speed input: 63657.44 toks/s, output: 62.17 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:15<00:00, 66.54it/s, est. speed input: 63726.82 toks/s, output: 62.23 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:15<00:00, 65.94it/s, est. speed input: 63745.59 toks/s, output: 62.25 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:16<00:00, 65.54it/s, est. speed input: 63764.29 toks/s, output: 62.27 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:16<00:00, 65.16it/s, est. speed input: 63780.08 toks/s, output: 62.29 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:16<00:00, 67.27it/s, est. speed input: 63852.34 toks/s, output: 62.36 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:16<00:00, 67.27it/s, est. speed input: 64227.81 toks/s, output: 62.72 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:16<00:00, 62.72it/s, est. speed input: 64227.81 toks/s, output: 62.72 toks/s]
[rank0]:[W128 11:56:21.523070175 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 64.6s

测试结果:
  Requests/s:   61.75
  Tokens/s:     63289.73
  Total Reqs:   1024
  Elapsed:      16.58s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     63227.98

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:56:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:56:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=36384) WARNING 01-28 11:56:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=36384) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=36384) WARNING 01-28 11:57:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.32 requests/s, 63874.43 total tokens/s, 62.32 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 11:56:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:56:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:56:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:56:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:56:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:56:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:56:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:56:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:56:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:56:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:56:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:56:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:56:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:56:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:56:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:56:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:56:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:56:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:56:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=36384) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=36384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.99it/s]
(EngineCore_DP0 pid=36384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.99it/s]
(EngineCore_DP0 pid=36384) 
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=36384) [2026-01-28 11:56:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=36384) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.13it/s]
(EngineCore_DP0 pid=36384) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.98it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  7.94it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.47it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 37/2048 [00:00<00:05, 364.33it/s]
Adding requests:   4%|▎         | 76/2048 [00:00<00:05, 374.38it/s]
Adding requests:   6%|▌         | 116/2048 [00:00<00:05, 382.92it/s]
Adding requests:   8%|▊         | 155/2048 [00:00<00:04, 383.11it/s]
Adding requests:   9%|▉         | 194/2048 [00:00<00:04, 384.60it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:04, 383.01it/s]
Adding requests:  13%|█▎        | 272/2048 [00:00<00:04, 384.33it/s]
Adding requests:  15%|█▌        | 311/2048 [00:00<00:04, 381.94it/s]
Adding requests:  17%|█▋        | 350/2048 [00:00<00:04, 381.43it/s]
Adding requests:  19%|█▉        | 390/2048 [00:01<00:04, 384.14it/s]
Adding requests:  21%|██        | 429/2048 [00:01<00:04, 385.18it/s]
Adding requests:  23%|██▎       | 468/2048 [00:01<00:04, 385.87it/s]
Adding requests:  25%|██▍       | 507/2048 [00:01<00:04, 384.98it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:04, 374.17it/s]
Adding requests:  29%|██▊       | 586/2048 [00:01<00:03, 380.74it/s]
Adding requests:  31%|███       | 625/2048 [00:01<00:03, 376.05it/s]
Adding requests:  32%|███▏      | 665/2048 [00:01<00:03, 381.95it/s]
Adding requests:  34%|███▍      | 706/2048 [00:01<00:03, 389.49it/s]
Adding requests:  36%|███▋      | 745/2048 [00:01<00:03, 381.60it/s]
Adding requests:  38%|███▊      | 784/2048 [00:02<00:03, 382.68it/s]
Adding requests:  40%|████      | 823/2048 [00:02<00:03, 374.14it/s]
Adding requests:  42%|████▏     | 861/2048 [00:02<00:03, 369.61it/s]
Adding requests:  44%|████▍     | 901/2048 [00:02<00:03, 376.95it/s]
Adding requests:  46%|████▌     | 939/2048 [00:02<00:02, 377.75it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 381.96it/s]
Adding requests:  50%|████▉     | 1020/2048 [00:02<00:02, 387.57it/s]
Adding requests:  52%|█████▏    | 1059/2048 [00:02<00:02, 387.42it/s]
Adding requests:  54%|█████▎    | 1099/2048 [00:02<00:02, 390.34it/s]
Adding requests:  56%|█████▌    | 1139/2048 [00:02<00:02, 382.42it/s]
Adding requests:  58%|█████▊    | 1180/2048 [00:03<00:02, 389.85it/s]
Adding requests:  60%|█████▉    | 1222/2048 [00:03<00:02, 395.37it/s]
Adding requests:  62%|██████▏   | 1262/2048 [00:03<00:02, 389.53it/s]
Adding requests:  64%|██████▎   | 1302/2048 [00:03<00:01, 391.71it/s]
Adding requests:  66%|██████▌   | 1342/2048 [00:03<00:01, 394.00it/s]
Adding requests:  67%|██████▋   | 1382/2048 [00:06<00:15, 44.15it/s] 
Adding requests:  69%|██████▉   | 1421/2048 [00:06<00:10, 59.71it/s]
Adding requests:  71%|███████▏  | 1461/2048 [00:06<00:07, 80.23it/s]
Adding requests:  73%|███████▎  | 1502/2048 [00:06<00:05, 106.35it/s]
Adding requests:  75%|███████▌  | 1540/2048 [00:06<00:03, 134.11it/s]
Adding requests:  77%|███████▋  | 1578/2048 [00:06<00:02, 165.25it/s]
Adding requests:  79%|███████▉  | 1618/2048 [00:06<00:02, 201.03it/s]
Adding requests:  81%|████████  | 1657/2048 [00:06<00:01, 234.06it/s]
Adding requests:  83%|████████▎ | 1695/2048 [00:07<00:01, 263.45it/s]
Adding requests:  85%|████████▍ | 1735/2048 [00:07<00:01, 294.10it/s]
Adding requests:  87%|████████▋ | 1774/2048 [00:07<00:00, 311.11it/s]
Adding requests:  89%|████████▊ | 1814/2048 [00:07<00:00, 333.64it/s]
Adding requests:  91%|█████████ | 1854/2048 [00:07<00:00, 350.15it/s]
Adding requests:  92%|█████████▏| 1894/2048 [00:07<00:00, 362.37it/s]
Adding requests:  94%|█████████▍| 1934/2048 [00:07<00:00, 371.45it/s]
Adding requests:  96%|█████████▋| 1974/2048 [00:07<00:00, 376.01it/s]
Adding requests:  98%|█████████▊| 2013/2048 [00:07<00:00, 371.80it/s]
Adding requests: 100%|██████████| 2048/2048 [00:08<00:00, 255.80it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:00<00:01, 1138.84it/s, est. speed input: 1166291.43 toks/s, output: 1138.87 toks/s]
Processed prompts:  21%|██        | 420/2048 [00:02<00:10, 159.59it/s, est. speed input: 201246.97 toks/s, output: 196.53 toks/s]   
Processed prompts:  23%|██▎       | 471/2048 [00:02<00:12, 125.27it/s, est. speed input: 164125.16 toks/s, output: 160.28 toks/s]
Processed prompts:  25%|██▍       | 502/2048 [00:03<00:14, 107.94it/s, est. speed input: 147810.39 toks/s, output: 144.35 toks/s]
Processed prompts:  26%|██▌       | 523/2048 [00:03<00:14, 104.02it/s, est. speed input: 143380.95 toks/s, output: 140.02 toks/s]
Processed prompts:  26%|██▋       | 540/2048 [00:03<00:15, 97.35it/s, est. speed input: 138484.38 toks/s, output: 135.24 toks/s] 
Processed prompts:  27%|██▋       | 553/2048 [00:04<00:16, 87.97it/s, est. speed input: 133226.85 toks/s, output: 130.10 toks/s]
Processed prompts:  28%|██▊       | 564/2048 [00:04<00:19, 77.92it/s, est. speed input: 128115.94 toks/s, output: 125.11 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:04<00:20, 72.21it/s, est. speed input: 124191.35 toks/s, output: 121.28 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:05<00:20, 69.59it/s, est. speed input: 121071.92 toks/s, output: 118.23 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:05<00:21, 67.56it/s, est. speed input: 118260.05 toks/s, output: 115.49 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:05<00:21, 66.05it/s, est. speed input: 115718.25 toks/s, output: 113.01 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:05<00:21, 64.94it/s, est. speed input: 113403.01 toks/s, output: 110.74 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:06<00:21, 64.11it/s, est. speed input: 111280.21 toks/s, output: 108.67 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:06<00:21, 63.52it/s, est. speed input: 109333.02 toks/s, output: 106.77 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:06<00:21, 63.10it/s, est. speed input: 107539.61 toks/s, output: 105.02 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:06<00:21, 62.80it/s, est. speed input: 105878.73 toks/s, output: 103.40 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:07<00:21, 62.56it/s, est. speed input: 104335.78 toks/s, output: 101.89 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:07<00:20, 62.42it/s, est. speed input: 102906.04 toks/s, output: 100.49 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:07<00:20, 62.31it/s, est. speed input: 101569.42 toks/s, output: 99.19 toks/s] 
Processed prompts:  38%|███▊      | 770/2048 [00:07<00:20, 62.22it/s, est. speed input: 100320.17 toks/s, output: 97.97 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:08<00:20, 62.15it/s, est. speed input: 99147.65 toks/s, output: 96.82 toks/s] 
Processed prompts:  39%|███▉      | 802/2048 [00:08<00:20, 62.11it/s, est. speed input: 98049.65 toks/s, output: 95.75 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:08<00:19, 62.09it/s, est. speed input: 97018.71 toks/s, output: 94.74 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:08<00:19, 62.08it/s, est. speed input: 96048.13 toks/s, output: 93.80 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:09<00:19, 62.05it/s, est. speed input: 95129.46 toks/s, output: 92.90 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:09<00:19, 62.07it/s, est. speed input: 94266.32 toks/s, output: 92.06 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:09<00:18, 62.07it/s, est. speed input: 93446.80 toks/s, output: 91.26 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:09<00:18, 62.05it/s, est. speed input: 92668.45 toks/s, output: 90.50 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:10<00:18, 62.06it/s, est. speed input: 91931.64 toks/s, output: 89.78 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:10<00:18, 62.08it/s, est. speed input: 91233.59 toks/s, output: 89.10 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:10<00:17, 62.01it/s, est. speed input: 90557.94 toks/s, output: 88.44 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:10<00:17, 62.02it/s, est. speed input: 89922.25 toks/s, output: 87.81 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:11<00:16, 63.06it/s, est. speed input: 89427.69 toks/s, output: 87.33 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:11<00:16, 62.69it/s, est. speed input: 88838.60 toks/s, output: 86.76 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:11<00:16, 62.47it/s, est. speed input: 88279.04 toks/s, output: 86.21 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:11<00:16, 62.36it/s, est. speed input: 87747.35 toks/s, output: 85.69 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:12<00:16, 62.23it/s, est. speed input: 87233.85 toks/s, output: 85.19 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:12<00:15, 62.43it/s, est. speed input: 86768.49 toks/s, output: 84.73 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:12<00:15, 63.19it/s, est. speed input: 86378.57 toks/s, output: 84.35 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:12<00:15, 63.71it/s, est. speed input: 86001.89 toks/s, output: 83.99 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:13<00:14, 64.09it/s, est. speed input: 85639.80 toks/s, output: 83.63 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:13<00:14, 64.33it/s, est. speed input: 85288.73 toks/s, output: 83.29 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:13<00:14, 64.53it/s, est. speed input: 84952.48 toks/s, output: 82.96 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:13<00:13, 65.74it/s, est. speed input: 84709.80 toks/s, output: 82.72 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:14<00:13, 65.52it/s, est. speed input: 84395.54 toks/s, output: 82.42 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:14<00:13, 65.37it/s, est. speed input: 84091.84 toks/s, output: 82.12 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:14<00:12, 65.26it/s, est. speed input: 83797.92 toks/s, output: 81.83 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:14<00:12, 65.17it/s, est. speed input: 83513.02 toks/s, output: 81.56 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:15<00:12, 65.11it/s, est. speed input: 83237.59 toks/s, output: 81.29 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:15<00:12, 65.03it/s, est. speed input: 82968.57 toks/s, output: 81.02 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:15<00:12, 65.02it/s, est. speed input: 82710.79 toks/s, output: 80.77 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:15<00:11, 65.00it/s, est. speed input: 82460.10 toks/s, output: 80.53 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:16<00:11, 64.98it/s, est. speed input: 82216.35 toks/s, output: 80.29 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:16<00:11, 64.97it/s, est. speed input: 81980.59 toks/s, output: 80.06 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:16<00:11, 64.95it/s, est. speed input: 81751.30 toks/s, output: 79.84 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:16<00:10, 64.95it/s, est. speed input: 81529.06 toks/s, output: 79.62 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:17<00:10, 64.94it/s, est. speed input: 81312.80 toks/s, output: 79.41 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:17<00:10, 64.95it/s, est. speed input: 81103.69 toks/s, output: 79.20 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:17<00:10, 64.95it/s, est. speed input: 80899.77 toks/s, output: 79.00 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:17<00:09, 64.95it/s, est. speed input: 80701.43 toks/s, output: 78.81 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:18<00:09, 64.96it/s, est. speed input: 80509.10 toks/s, output: 78.62 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:18<00:09, 64.94it/s, est. speed input: 80320.53 toks/s, output: 78.44 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:18<00:09, 64.94it/s, est. speed input: 80137.68 toks/s, output: 78.26 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:18<00:08, 64.89it/s, est. speed input: 79957.19 toks/s, output: 78.08 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:19<00:08, 64.87it/s, est. speed input: 79781.89 toks/s, output: 77.91 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:19<00:08, 64.86it/s, est. speed input: 79611.14 toks/s, output: 77.75 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:19<00:08, 64.86it/s, est. speed input: 79445.17 toks/s, output: 77.58 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:19<00:07, 64.87it/s, est. speed input: 79284.00 toks/s, output: 77.43 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:20<00:07, 64.90it/s, est. speed input: 79127.59 toks/s, output: 77.27 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:20<00:07, 64.89it/s, est. speed input: 78973.87 toks/s, output: 77.12 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:20<00:07, 64.75it/s, est. speed input: 78817.29 toks/s, output: 76.97 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:20<00:06, 64.62it/s, est. speed input: 78662.46 toks/s, output: 76.82 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:21<00:06, 64.51it/s, est. speed input: 78510.89 toks/s, output: 76.67 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:21<00:06, 64.43it/s, est. speed input: 78362.44 toks/s, output: 76.53 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:21<00:06, 64.36it/s, est. speed input: 78216.93 toks/s, output: 76.38 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:21<00:05, 64.32it/s, est. speed input: 78074.95 toks/s, output: 76.24 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:22<00:05, 64.31it/s, est. speed input: 77936.96 toks/s, output: 76.11 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:22<00:05, 64.29it/s, est. speed input: 77801.70 toks/s, output: 75.98 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:22<00:05, 64.26it/s, est. speed input: 77668.55 toks/s, output: 75.85 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:22<00:04, 64.29it/s, est. speed input: 77540.62 toks/s, output: 75.72 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:23<00:04, 64.29it/s, est. speed input: 77414.30 toks/s, output: 75.60 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:23<00:04, 64.27it/s, est. speed input: 77290.12 toks/s, output: 75.48 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:23<00:04, 64.28it/s, est. speed input: 77169.22 toks/s, output: 75.36 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:23<00:03, 64.29it/s, est. speed input: 77051.37 toks/s, output: 75.25 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:24<00:03, 64.29it/s, est. speed input: 76935.32 toks/s, output: 75.13 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:24<00:03, 64.27it/s, est. speed input: 76820.84 toks/s, output: 75.02 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:24<00:03, 64.26it/s, est. speed input: 76709.05 toks/s, output: 74.91 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:24<00:02, 64.27it/s, est. speed input: 76600.34 toks/s, output: 74.80 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:25<00:02, 65.33it/s, est. speed input: 76534.28 toks/s, output: 74.74 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:25<00:02, 65.02it/s, est. speed input: 76429.30 toks/s, output: 74.64 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:25<00:02, 64.78it/s, est. speed input: 76325.58 toks/s, output: 74.54 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:25<00:01, 64.64it/s, est. speed input: 76224.57 toks/s, output: 74.44 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:26<00:01, 64.52it/s, est. speed input: 76124.97 toks/s, output: 74.34 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:26<00:01, 64.45it/s, est. speed input: 76027.51 toks/s, output: 74.25 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:26<00:01, 64.39it/s, est. speed input: 75931.44 toks/s, output: 74.15 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:26<00:00, 64.34it/s, est. speed input: 75837.10 toks/s, output: 74.06 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:27<00:00, 64.31it/s, est. speed input: 75744.57 toks/s, output: 73.97 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:27<00:00, 64.30it/s, est. speed input: 75654.00 toks/s, output: 73.88 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:27<00:00, 65.53it/s, est. speed input: 75608.30 toks/s, output: 73.84 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:27<00:00, 65.53it/s, est. speed input: 76127.68 toks/s, output: 74.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:27<00:00, 74.34it/s, est. speed input: 76127.68 toks/s, output: 74.34 toks/s]
[rank0]:[W128 11:57:48.965258290 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 86.7s

测试结果:
  Requests/s:   62.32
  Tokens/s:     63874.43
  Total Reqs:   2048
  Elapsed:      32.86s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     63812.11

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 11:58:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 11:58:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=37227) WARNING 01-28 11:58:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=37227) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=37227) WARNING 01-28 11:58:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.42 requests/s, 63978.71 total tokens/s, 62.42 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 11:58:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:58:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:58:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:58:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:58:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:58:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:58:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:58:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:58:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 11:58:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 11:58:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 11:58:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 11:58:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 11:58:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 11:58:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 11:58:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 11:58:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 11:58:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 11:58:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=37227) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=37227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=37227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=37227) 
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=37227) [2026-01-28 11:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=37227) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.40it/s]
(EngineCore_DP0 pid=37227) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.63it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 37/4096 [00:00<00:11, 363.74it/s]
Adding requests:   2%|▏         | 76/4096 [00:00<00:10, 373.59it/s]
Adding requests:   3%|▎         | 116/4096 [00:00<00:10, 384.15it/s]
Adding requests:   4%|▍         | 155/4096 [00:00<00:10, 384.45it/s]
Adding requests:   5%|▍         | 195/4096 [00:00<00:10, 387.08it/s]
Adding requests:   6%|▌         | 236/4096 [00:00<00:09, 392.87it/s]
Adding requests:   7%|▋         | 277/4096 [00:00<00:09, 394.39it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:09, 387.60it/s]
Adding requests:   9%|▊         | 356/4096 [00:00<00:09, 384.41it/s]
Adding requests:  10%|▉         | 396/4096 [00:01<00:09, 387.35it/s]
Adding requests:  11%|█         | 435/4096 [00:01<00:09, 380.70it/s]
Adding requests:  12%|█▏        | 475/4096 [00:01<00:09, 385.70it/s]
Adding requests:  13%|█▎        | 514/4096 [00:01<00:09, 383.64it/s]
Adding requests:  14%|█▎        | 553/4096 [00:01<00:09, 380.77it/s]
Adding requests:  15%|█▍        | 595/4096 [00:01<00:08, 391.67it/s]
Adding requests:  16%|█▌        | 636/4096 [00:01<00:08, 396.51it/s]
Adding requests:  17%|█▋        | 678/4096 [00:01<00:08, 402.93it/s]
Adding requests:  18%|█▊        | 719/4096 [00:01<00:08, 404.02it/s]
Adding requests:  19%|█▊        | 760/4096 [00:01<00:08, 401.32it/s]
Adding requests:  20%|█▉        | 801/4096 [00:02<00:08, 398.70it/s]
Adding requests:  21%|██        | 841/4096 [00:02<00:08, 395.00it/s]
Adding requests:  22%|██▏       | 883/4096 [00:02<00:08, 401.28it/s]
Adding requests:  23%|██▎       | 924/4096 [00:02<00:07, 401.78it/s]
Adding requests:  24%|██▎       | 965/4096 [00:02<00:07, 398.34it/s]
Adding requests:  25%|██▍       | 1005/4096 [00:02<00:07, 396.17it/s]
Adding requests:  26%|██▌       | 1046/4096 [00:02<00:07, 399.59it/s]
Adding requests:  27%|██▋       | 1087/4096 [00:02<00:07, 400.39it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:02<00:07, 397.16it/s]
Adding requests:  29%|██▊       | 1170/4096 [00:02<00:07, 403.69it/s]
Adding requests:  30%|██▉       | 1211/4096 [00:03<00:07, 399.19it/s]
Adding requests:  31%|███       | 1251/4096 [00:03<00:07, 397.84it/s]
Adding requests:  32%|███▏      | 1292/4096 [00:03<00:07, 399.69it/s]
Adding requests:  33%|███▎      | 1333/4096 [00:03<00:06, 402.73it/s]
Adding requests:  34%|███▎      | 1374/4096 [00:03<00:06, 402.35it/s]
Adding requests:  35%|███▍      | 1416/4096 [00:03<00:06, 405.64it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:06, 405.65it/s]
Adding requests:  37%|███▋      | 1498/4096 [00:03<00:06, 403.66it/s]
Adding requests:  38%|███▊      | 1539/4096 [00:03<00:06, 401.99it/s]
Adding requests:  39%|███▊      | 1580/4096 [00:03<00:06, 396.00it/s]
Adding requests:  40%|███▉      | 1623/4096 [00:04<00:06, 403.94it/s]
Adding requests:  41%|████      | 1665/4096 [00:04<00:06, 404.93it/s]
Adding requests:  42%|████▏     | 1707/4096 [00:04<00:05, 408.12it/s]
Adding requests:  43%|████▎     | 1749/4096 [00:04<00:05, 409.75it/s]
Adding requests:  44%|████▎     | 1790/4096 [00:04<00:05, 408.69it/s]
Adding requests:  45%|████▍     | 1832/4096 [00:04<00:05, 410.62it/s]
Adding requests:  46%|████▌     | 1874/4096 [00:04<00:05, 410.98it/s]
Adding requests:  47%|████▋     | 1916/4096 [00:04<00:05, 409.43it/s]
Adding requests:  48%|████▊     | 1957/4096 [00:04<00:05, 407.70it/s]
Adding requests:  49%|████▉     | 1998/4096 [00:05<00:05, 407.79it/s]
Adding requests:  50%|████▉     | 2041/4096 [00:05<00:04, 411.11it/s]
Adding requests:  51%|█████     | 2084/4096 [00:05<00:04, 414.62it/s]
Adding requests:  52%|█████▏    | 2126/4096 [00:05<00:04, 409.34it/s]
Adding requests:  53%|█████▎    | 2167/4096 [00:05<00:04, 404.27it/s]
Adding requests:  54%|█████▍    | 2208/4096 [00:05<00:04, 398.62it/s]
Adding requests:  55%|█████▍    | 2251/4096 [00:05<00:04, 405.39it/s]
Adding requests:  56%|█████▌    | 2292/4096 [00:05<00:04, 405.18it/s]
Adding requests:  57%|█████▋    | 2333/4096 [00:05<00:04, 405.74it/s]
Adding requests:  58%|█████▊    | 2375/4096 [00:05<00:04, 407.60it/s]
Adding requests:  59%|█████▉    | 2416/4096 [00:06<00:04, 394.30it/s]
Adding requests:  60%|██████    | 2458/4096 [00:06<00:04, 399.92it/s]
Adding requests:  61%|██████    | 2500/4096 [00:06<00:03, 404.06it/s]
Adding requests:  62%|██████▏   | 2541/4096 [00:06<00:03, 403.80it/s]
Adding requests:  63%|██████▎   | 2582/4096 [00:06<00:03, 398.94it/s]
Adding requests:  64%|██████▍   | 2622/4096 [00:06<00:03, 394.35it/s]
Adding requests:  65%|██████▌   | 2664/4096 [00:06<00:03, 398.94it/s]
Adding requests:  66%|██████▌   | 2704/4096 [00:06<00:03, 392.23it/s]
Adding requests:  67%|██████▋   | 2745/4096 [00:06<00:03, 395.14it/s]
Adding requests:  68%|██████▊   | 2786/4096 [00:06<00:03, 396.49it/s]
Adding requests:  69%|██████▉   | 2826/4096 [00:07<00:03, 395.46it/s]
Adding requests:  70%|██████▉   | 2867/4096 [00:07<00:03, 398.08it/s]
Adding requests:  71%|███████   | 2909/4096 [00:07<00:02, 403.11it/s]
Adding requests:  72%|███████▏  | 2950/4096 [00:07<00:02, 394.20it/s]
Adding requests:  73%|███████▎  | 2990/4096 [00:07<00:02, 395.19it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:07<00:02, 392.19it/s]
Adding requests:  75%|███████▍  | 3070/4096 [00:07<00:02, 393.76it/s]
Adding requests:  76%|███████▌  | 3111/4096 [00:07<00:02, 395.55it/s]
Adding requests:  77%|███████▋  | 3151/4096 [00:10<00:18, 52.34it/s] 
Adding requests:  78%|███████▊  | 3191/4096 [00:10<00:12, 70.62it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:10<00:09, 93.64it/s]
Adding requests:  80%|███████▉  | 3271/4096 [00:10<00:06, 121.40it/s]
Adding requests:  81%|████████  | 3312/4096 [00:10<00:05, 154.15it/s]
Adding requests:  82%|████████▏ | 3354/4096 [00:10<00:03, 191.00it/s]
Adding requests:  83%|████████▎ | 3394/4096 [00:10<00:03, 225.35it/s]
Adding requests:  84%|████████▍ | 3435/4096 [00:10<00:02, 260.82it/s]
Adding requests:  85%|████████▍ | 3475/4096 [00:10<00:02, 287.68it/s]
Adding requests:  86%|████████▌ | 3516/4096 [00:11<00:01, 315.55it/s]
Adding requests:  87%|████████▋ | 3557/4096 [00:11<00:01, 337.85it/s]
Adding requests:  88%|████████▊ | 3597/4096 [00:11<00:01, 353.41it/s]
Adding requests:  89%|████████▉ | 3637/4096 [00:11<00:01, 361.57it/s]
Adding requests:  90%|████████▉ | 3677/4096 [00:11<00:01, 370.25it/s]
Adding requests:  91%|█████████ | 3717/4096 [00:11<00:01, 375.71it/s]
Adding requests:  92%|█████████▏| 3757/4096 [00:11<00:00, 365.98it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:11<00:00, 373.69it/s]
Adding requests:  94%|█████████▎| 3836/4096 [00:11<00:00, 378.22it/s]
Adding requests:  95%|█████████▍| 3878/4096 [00:11<00:00, 387.94it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:12<00:00, 389.15it/s]
Adding requests:  97%|█████████▋| 3959/4096 [00:12<00:00, 394.17it/s]
Adding requests:  98%|█████████▊| 3999/4096 [00:12<00:00, 391.05it/s]
Adding requests:  99%|█████████▊| 4039/4096 [00:12<00:00, 391.66it/s]
Adding requests: 100%|█████████▉| 4079/4096 [00:12<00:00, 391.98it/s]
Adding requests: 100%|██████████| 4096/4096 [00:12<00:00, 326.88it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:00<00:01, 2813.29it/s, est. speed input: 2881136.79 toks/s, output: 2813.39 toks/s]
Processed prompts:  22%|██▏       | 892/4096 [00:04<00:19, 167.37it/s, est. speed input: 212360.28 toks/s, output: 207.38 toks/s]   
Processed prompts:  25%|██▍       | 1012/4096 [00:06<00:25, 123.33it/s, est. speed input: 163627.08 toks/s, output: 159.79 toks/s]
Processed prompts:  26%|██▋       | 1080/4096 [00:07<00:27, 110.46it/s, est. speed input: 150336.71 toks/s, output: 146.81 toks/s]
Processed prompts:  27%|██▋       | 1124/4096 [00:08<00:31, 93.99it/s, est. speed input: 137362.39 toks/s, output: 134.14 toks/s] 
Processed prompts:  28%|██▊       | 1154/4096 [00:08<00:33, 89.09it/s, est. speed input: 133116.88 toks/s, output: 130.00 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:09<00:34, 84.42it/s, est. speed input: 129352.10 toks/s, output: 126.32 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:09<00:35, 80.03it/s, est. speed input: 125977.84 toks/s, output: 123.02 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:10<00:37, 76.15it/s, est. speed input: 122938.67 toks/s, output: 120.06 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:10<00:38, 72.94it/s, est. speed input: 120203.08 toks/s, output: 117.39 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:11<00:39, 70.32it/s, est. speed input: 117704.49 toks/s, output: 114.95 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:11<00:40, 68.27it/s, est. speed input: 115416.19 toks/s, output: 112.71 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:12<00:40, 66.74it/s, est. speed input: 113320.49 toks/s, output: 110.66 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:12<00:40, 65.60it/s, est. speed input: 111386.86 toks/s, output: 108.78 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:13<00:40, 64.76it/s, est. speed input: 109598.78 toks/s, output: 107.03 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:13<00:40, 64.15it/s, est. speed input: 107940.00 toks/s, output: 105.41 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:14<00:40, 63.72it/s, est. speed input: 106399.39 toks/s, output: 103.91 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:15<00:40, 63.40it/s, est. speed input: 104960.75 toks/s, output: 102.50 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:15<00:39, 63.18it/s, est. speed input: 103618.18 toks/s, output: 101.19 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:16<00:39, 63.02it/s, est. speed input: 102359.68 toks/s, output: 99.96 toks/s] 
Processed prompts:  40%|███▉      | 1634/4096 [00:16<00:39, 62.92it/s, est. speed input: 101180.91 toks/s, output: 98.81 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:17<00:38, 62.84it/s, est. speed input: 100072.73 toks/s, output: 97.73 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:17<00:38, 62.79it/s, est. speed input: 99028.13 toks/s, output: 96.71 toks/s] 
Processed prompts:  42%|████▏     | 1730/4096 [00:18<00:37, 62.74it/s, est. speed input: 98040.85 toks/s, output: 95.74 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:18<00:37, 62.70it/s, est. speed input: 97108.36 toks/s, output: 94.83 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:19<00:36, 62.67it/s, est. speed input: 96223.69 toks/s, output: 93.97 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:19<00:36, 62.64it/s, est. speed input: 95385.54 toks/s, output: 93.15 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:20<00:35, 63.13it/s, est. speed input: 94654.06 toks/s, output: 92.44 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:20<00:35, 62.98it/s, est. speed input: 93897.21 toks/s, output: 91.70 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:21<00:34, 62.87it/s, est. speed input: 93176.63 toks/s, output: 90.99 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:21<00:34, 62.78it/s, est. speed input: 92488.60 toks/s, output: 90.32 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:22<00:33, 62.71it/s, est. speed input: 91831.62 toks/s, output: 89.68 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:22<00:33, 62.68it/s, est. speed input: 91205.87 toks/s, output: 89.07 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:23<00:32, 62.64it/s, est. speed input: 90607.11 toks/s, output: 88.48 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:23<00:32, 62.63it/s, est. speed input: 90034.49 toks/s, output: 87.92 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:24<00:31, 62.60it/s, est. speed input: 89484.75 toks/s, output: 87.39 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:24<00:31, 62.59it/s, est. speed input: 88958.27 toks/s, output: 86.87 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:25<00:30, 62.58it/s, est. speed input: 88453.42 toks/s, output: 86.38 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:25<00:30, 62.57it/s, est. speed input: 87967.86 toks/s, output: 85.91 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:26<00:29, 62.57it/s, est. speed input: 87502.37 toks/s, output: 85.45 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:26<00:28, 63.06it/s, est. speed input: 87097.64 toks/s, output: 85.06 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:27<00:28, 62.91it/s, est. speed input: 86664.65 toks/s, output: 84.63 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:27<00:27, 62.81it/s, est. speed input: 86248.72 toks/s, output: 84.23 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:28<00:27, 62.74it/s, est. speed input: 85848.01 toks/s, output: 83.84 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:28<00:27, 62.69it/s, est. speed input: 85461.36 toks/s, output: 83.46 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:29<00:26, 62.65it/s, est. speed input: 85087.10 toks/s, output: 83.09 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:29<00:26, 62.61it/s, est. speed input: 84725.37 toks/s, output: 82.74 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:32<00:58, 27.12it/s, est. speed input: 78597.62 toks/s, output: 76.76 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:33<00:47, 32.67it/s, est. speed input: 78372.27 toks/s, output: 76.54 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:33<00:40, 38.14it/s, est. speed input: 78154.87 toks/s, output: 76.32 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:34<00:34, 43.20it/s, est. speed input: 77943.48 toks/s, output: 76.12 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:34<00:30, 47.63it/s, est. speed input: 77739.10 toks/s, output: 75.92 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:35<00:28, 51.31it/s, est. speed input: 77540.24 toks/s, output: 75.72 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:35<00:25, 54.23it/s, est. speed input: 77346.18 toks/s, output: 75.53 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:36<00:24, 56.49it/s, est. speed input: 77158.62 toks/s, output: 75.35 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:36<00:23, 58.17it/s, est. speed input: 76975.02 toks/s, output: 75.17 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:37<00:22, 59.41it/s, est. speed input: 76796.00 toks/s, output: 75.00 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:37<00:21, 60.29it/s, est. speed input: 76621.32 toks/s, output: 74.83 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:38<00:20, 60.92it/s, est. speed input: 76451.35 toks/s, output: 74.66 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:38<00:19, 61.38it/s, est. speed input: 76285.98 toks/s, output: 74.50 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:39<00:19, 61.70it/s, est. speed input: 76124.82 toks/s, output: 74.34 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:39<00:18, 61.93it/s, est. speed input: 75968.27 toks/s, output: 74.19 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:40<00:18, 62.09it/s, est. speed input: 75815.40 toks/s, output: 74.04 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:40<00:17, 62.20it/s, est. speed input: 75666.36 toks/s, output: 73.89 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:41<00:16, 62.28it/s, est. speed input: 75520.76 toks/s, output: 73.75 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:41<00:16, 62.34it/s, est. speed input: 75379.16 toks/s, output: 73.61 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:42<00:15, 62.37it/s, est. speed input: 75240.63 toks/s, output: 73.48 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:42<00:15, 62.39it/s, est. speed input: 75105.25 toks/s, output: 73.34 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:43<00:14, 62.40it/s, est. speed input: 74972.61 toks/s, output: 73.22 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:43<00:14, 62.41it/s, est. speed input: 74843.46 toks/s, output: 73.09 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:44<00:13, 62.42it/s, est. speed input: 74717.57 toks/s, output: 72.97 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:44<00:13, 62.47it/s, est. speed input: 74596.11 toks/s, output: 72.85 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:45<00:12, 62.53it/s, est. speed input: 74478.88 toks/s, output: 72.73 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:45<00:12, 62.57it/s, est. speed input: 74364.16 toks/s, output: 72.62 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:46<00:11, 62.59it/s, est. speed input: 74251.26 toks/s, output: 72.51 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:46<00:11, 62.61it/s, est. speed input: 74141.34 toks/s, output: 72.40 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:47<00:10, 62.62it/s, est. speed input: 74033.83 toks/s, output: 72.30 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:47<00:10, 62.63it/s, est. speed input: 73928.39 toks/s, output: 72.20 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:48<00:09, 62.64it/s, est. speed input: 73825.31 toks/s, output: 72.09 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:48<00:09, 62.63it/s, est. speed input: 73723.96 toks/s, output: 72.00 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:49<00:08, 62.63it/s, est. speed input: 73624.95 toks/s, output: 71.90 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:49<00:08, 62.63it/s, est. speed input: 73527.66 toks/s, output: 71.80 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:50<00:07, 62.63it/s, est. speed input: 73432.77 toks/s, output: 71.71 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:50<00:07, 62.63it/s, est. speed input: 73339.37 toks/s, output: 71.62 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:51<00:06, 62.63it/s, est. speed input: 73248.10 toks/s, output: 71.53 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:51<00:06, 63.12it/s, est. speed input: 73176.94 toks/s, output: 71.46 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:52<00:05, 62.97it/s, est. speed input: 73088.84 toks/s, output: 71.38 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:52<00:05, 62.86it/s, est. speed input: 73002.06 toks/s, output: 71.29 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:53<00:04, 62.77it/s, est. speed input: 72916.63 toks/s, output: 71.21 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:54<00:04, 62.71it/s, est. speed input: 72833.00 toks/s, output: 71.13 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:54<00:03, 62.67it/s, est. speed input: 72750.90 toks/s, output: 71.05 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:55<00:03, 62.64it/s, est. speed input: 72670.20 toks/s, output: 70.97 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:55<00:02, 62.62it/s, est. speed input: 72591.27 toks/s, output: 70.89 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:56<00:02, 62.63it/s, est. speed input: 72514.16 toks/s, output: 70.81 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:56<00:01, 62.62it/s, est. speed input: 72438.27 toks/s, output: 70.74 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:57<00:00, 63.12it/s, est. speed input: 72380.77 toks/s, output: 70.68 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:57<00:00, 63.65it/s, est. speed input: 72330.46 toks/s, output: 70.64 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:57<00:00, 63.65it/s, est. speed input: 72863.45 toks/s, output: 71.16 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:57<00:00, 71.16it/s, est. speed input: 72863.45 toks/s, output: 71.16 toks/s]
[rank0]:[W128 12:00:03.031794635 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 135.0s

测试结果:
  Requests/s:   62.42
  Tokens/s:     63978.71
  Total Reqs:   4096
  Elapsed:      65.62s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     63916.29


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,16.4327,8429.9638,7.7894
1024,1024,1,128,128,16.0780,16479.9890,7.9612
2048,1024,2,256,128,32.6262,33441.9008,7.8464
4096,1024,4,512,128,59.7759,61270.3077,8.5653
8192,1024,8,1024,128,61.7461,63289.7273,16.5840
16384,1024,16,2048,128,62.3165,63874.4284,32.8645
32768,1024,32,4096,128,62.4183,63978.7118,65.6218

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:00:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:00:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=38256) WARNING 01-28 12:00:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=38256) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=38256) WARNING 01-28 12:00:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.13 requests/s, 8273.80 total tokens/s, 16.13 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 12:00:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:00:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:00:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:00:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:00:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:00:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:00:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:00:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:00:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:00:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:00:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:00:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:00:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:00:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:00:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:00:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:00:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:00:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:00:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=38256) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=38256) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.69s/it]
(EngineCore_DP0 pid=38256) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.69s/it]
(EngineCore_DP0 pid=38256) 
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=38256) [2026-01-28 12:00:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=38256) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=38256) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 641.80it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 658.87it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:19,  6.55it/s, est. speed input: 3354.71 toks/s, output: 6.55 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:10, 11.89it/s, est. speed input: 5630.56 toks/s, output: 11.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 13.97it/s, est. speed input: 6524.40 toks/s, output: 12.74 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 15.12it/s, est. speed input: 7029.46 toks/s, output: 13.73 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 15.71it/s, est. speed input: 7326.40 toks/s, output: 14.31 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 16.06it/s, est. speed input: 7525.35 toks/s, output: 14.70 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 16.36it/s, est. speed input: 7686.55 toks/s, output: 15.01 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 16.49it/s, est. speed input: 7794.27 toks/s, output: 15.22 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 16.58it/s, est. speed input: 7879.96 toks/s, output: 15.39 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.69it/s, est. speed input: 7956.24 toks/s, output: 15.54 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.49it/s, est. speed input: 7981.38 toks/s, output: 15.59 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.59it/s, est. speed input: 8032.24 toks/s, output: 15.69 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.66it/s, est. speed input: 8075.49 toks/s, output: 15.77 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.77it/s, est. speed input: 8120.51 toks/s, output: 15.86 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 16.84it/s, est. speed input: 8158.19 toks/s, output: 15.93 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.85it/s, est. speed input: 8187.99 toks/s, output: 15.99 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.90it/s, est. speed input: 8217.63 toks/s, output: 16.05 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.88it/s, est. speed input: 8239.48 toks/s, output: 16.09 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.84it/s, est. speed input: 8257.14 toks/s, output: 16.13 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.80it/s, est. speed input: 8271.99 toks/s, output: 16.16 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.83it/s, est. speed input: 8289.69 toks/s, output: 16.19 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.78it/s, est. speed input: 8300.79 toks/s, output: 16.21 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 16.78it/s, est. speed input: 8313.41 toks/s, output: 16.24 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.75it/s, est. speed input: 8322.62 toks/s, output: 16.26 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 16.70it/s, est. speed input: 8329.28 toks/s, output: 16.27 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.72it/s, est. speed input: 8338.87 toks/s, output: 16.29 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.77it/s, est. speed input: 8350.36 toks/s, output: 16.31 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.79it/s, est. speed input: 8359.57 toks/s, output: 16.33 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.72it/s, est. speed input: 8363.74 toks/s, output: 16.34 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.68it/s, est. speed input: 8367.85 toks/s, output: 16.34 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.65it/s, est. speed input: 8371.71 toks/s, output: 16.35 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.68it/s, est. speed input: 8377.95 toks/s, output: 16.36 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.68it/s, est. speed input: 8383.16 toks/s, output: 16.37 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.69it/s, est. speed input: 8388.07 toks/s, output: 16.38 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.67it/s, est. speed input: 8391.43 toks/s, output: 16.39 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.66it/s, est. speed input: 8394.98 toks/s, output: 16.40 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.57it/s, est. speed input: 8394.53 toks/s, output: 16.40 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.59it/s, est. speed input: 8397.91 toks/s, output: 16.40 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.59it/s, est. speed input: 8400.40 toks/s, output: 16.41 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.55it/s, est. speed input: 8400.83 toks/s, output: 16.41 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.59it/s, est. speed input: 8404.26 toks/s, output: 16.41 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.62it/s, est. speed input: 8407.55 toks/s, output: 16.42 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.67it/s, est. speed input: 8411.99 toks/s, output: 16.43 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.74it/s, est. speed input: 8417.37 toks/s, output: 16.44 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.75it/s, est. speed input: 8421.29 toks/s, output: 16.45 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.76it/s, est. speed input: 8424.86 toks/s, output: 16.45 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.80it/s, est. speed input: 8429.64 toks/s, output: 16.46 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.66it/s, est. speed input: 8428.16 toks/s, output: 16.46 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.73it/s, est. speed input: 8432.84 toks/s, output: 16.47 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.80it/s, est. speed input: 8437.87 toks/s, output: 16.48 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.84it/s, est. speed input: 8442.34 toks/s, output: 16.49 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.81it/s, est. speed input: 8444.69 toks/s, output: 16.49 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.75it/s, est. speed input: 8445.79 toks/s, output: 16.50 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.72it/s, est. speed input: 8447.29 toks/s, output: 16.50 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.76it/s, est. speed input: 8450.50 toks/s, output: 16.50 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.77it/s, est. speed input: 8453.29 toks/s, output: 16.51 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.77it/s, est. speed input: 8455.61 toks/s, output: 16.51 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 16.71it/s, est. speed input: 8456.13 toks/s, output: 16.52 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.70it/s, est. speed input: 8457.48 toks/s, output: 16.52 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.64it/s, est. speed input: 8457.27 toks/s, output: 16.52 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.64it/s, est. speed input: 8458.33 toks/s, output: 16.52 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.65it/s, est. speed input: 8459.70 toks/s, output: 16.52 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.69it/s, est. speed input: 8461.65 toks/s, output: 16.53 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.75it/s, est. speed input: 8464.70 toks/s, output: 16.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.75it/s, est. speed input: 8466.49 toks/s, output: 16.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.54it/s, est. speed input: 8466.49 toks/s, output: 16.54 toks/s]
[rank0]:[W128 12:01:01.830449085 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.2s

测试结果:
  Requests/s:   16.13
  Tokens/s:     8273.80
  Total Reqs:   128
  Elapsed:      7.94s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8257.67

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:01:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:01:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=38902) WARNING 01-28 12:01:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=38902) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=38902) WARNING 01-28 12:01:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 15.91 requests/s, 16304.71 total tokens/s, 15.91 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 12:01:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:01:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:01:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:01:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:01:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:01:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:01:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:01:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:01:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:01:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:01:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:01:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:01:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:01:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:01:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:01:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:01:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:01:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:01:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=38902) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=38902) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.86s/it]
(EngineCore_DP0 pid=38902) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.86s/it]
(EngineCore_DP0 pid=38902) 
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=38902) [2026-01-28 12:01:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=38902) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.04it/s]
(EngineCore_DP0 pid=38902) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 365.29it/s]
Adding requests:  59%|█████▉    | 76/128 [00:00<00:00, 375.62it/s]
Adding requests:  89%|████████▉ | 114/128 [00:00<00:00, 372.67it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 373.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:06, 19.02it/s, est. speed input: 19482.87 toks/s, output: 19.02 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 17.57it/s, est. speed input: 18243.06 toks/s, output: 17.81 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:07, 16.87it/s, est. speed input: 17657.85 toks/s, output: 17.24 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 16.70it/s, est. speed input: 17460.12 toks/s, output: 17.05 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 16.53it/s, est. speed input: 17299.12 toks/s, output: 16.89 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:06, 16.46it/s, est. speed input: 17208.19 toks/s, output: 16.80 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 16.45it/s, est. speed input: 17155.91 toks/s, output: 16.75 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 16.40it/s, est. speed input: 17098.16 toks/s, output: 16.70 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.38it/s, est. speed input: 17058.76 toks/s, output: 16.66 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.36it/s, est. speed input: 17025.03 toks/s, output: 16.63 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.39it/s, est. speed input: 17010.81 toks/s, output: 16.61 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.41it/s, est. speed input: 16997.71 toks/s, output: 16.60 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.40it/s, est. speed input: 16980.83 toks/s, output: 16.58 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 16.43it/s, est. speed input: 16974.92 toks/s, output: 16.58 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.44it/s, est. speed input: 16966.59 toks/s, output: 16.57 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 16.46it/s, est. speed input: 16962.20 toks/s, output: 16.56 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.49it/s, est. speed input: 16963.16 toks/s, output: 16.57 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.48it/s, est. speed input: 16956.44 toks/s, output: 16.56 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.24it/s, est. speed input: 16909.63 toks/s, output: 16.51 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.17it/s, est. speed input: 16882.85 toks/s, output: 16.49 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.13it/s, est. speed input: 16861.24 toks/s, output: 16.47 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:05, 16.09it/s, est. speed input: 16840.22 toks/s, output: 16.45 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.29it/s, est. speed input: 16853.70 toks/s, output: 16.46 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 16.40it/s, est. speed input: 16862.84 toks/s, output: 16.47 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.49it/s, est. speed input: 16871.17 toks/s, output: 16.48 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.56it/s, est. speed input: 16880.57 toks/s, output: 16.48 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.69it/s, est. speed input: 16899.11 toks/s, output: 16.50 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.78it/s, est. speed input: 16916.03 toks/s, output: 16.52 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.84it/s, est. speed input: 16932.22 toks/s, output: 16.54 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 16.82it/s, est. speed input: 16940.23 toks/s, output: 16.54 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.87it/s, est. speed input: 16953.80 toks/s, output: 16.56 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.89it/s, est. speed input: 16965.82 toks/s, output: 16.57 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.92it/s, est. speed input: 16978.55 toks/s, output: 16.58 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.94it/s, est. speed input: 16990.03 toks/s, output: 16.59 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.98it/s, est. speed input: 17003.91 toks/s, output: 16.61 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.01it/s, est. speed input: 17016.90 toks/s, output: 16.62 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.03it/s, est. speed input: 17029.35 toks/s, output: 16.63 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.02it/s, est. speed input: 17038.86 toks/s, output: 16.64 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.82it/s, est. speed input: 17031.38 toks/s, output: 16.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.54it/s, est. speed input: 17013.18 toks/s, output: 16.61 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 16.45it/s, est. speed input: 17003.92 toks/s, output: 16.61 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.51it/s, est. speed input: 17005.11 toks/s, output: 16.61 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.58it/s, est. speed input: 17008.18 toks/s, output: 16.61 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.70it/s, est. speed input: 17016.52 toks/s, output: 16.62 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.74it/s, est. speed input: 17021.75 toks/s, output: 16.62 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.73it/s, est. speed input: 17023.22 toks/s, output: 16.62 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.53it/s, est. speed input: 17011.16 toks/s, output: 16.61 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.46it/s, est. speed input: 17004.65 toks/s, output: 16.61 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 16.46it/s, est. speed input: 17001.43 toks/s, output: 16.60 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.54it/s, est. speed input: 17003.88 toks/s, output: 16.61 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.49it/s, est. speed input: 16999.32 toks/s, output: 16.60 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.48it/s, est. speed input: 16996.15 toks/s, output: 16.60 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.46it/s, est. speed input: 16992.62 toks/s, output: 16.59 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.46it/s, est. speed input: 16990.23 toks/s, output: 16.59 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.57it/s, est. speed input: 16994.39 toks/s, output: 16.60 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.62it/s, est. speed input: 16997.00 toks/s, output: 16.60 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:09<00:05,  2.51it/s, est. speed input: 12818.49 toks/s, output: 12.52 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:09<00:03,  3.36it/s, est. speed input: 12873.15 toks/s, output: 12.57 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:09<00:02,  4.43it/s, est. speed input: 12927.81 toks/s, output: 12.62 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:09<00:01,  5.68it/s, est. speed input: 12979.95 toks/s, output: 12.68 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:09<00:00,  7.09it/s, est. speed input: 13033.57 toks/s, output: 12.73 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:09<00:00,  8.60it/s, est. speed input: 13087.19 toks/s, output: 12.78 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:09<00:00, 10.08it/s, est. speed input: 13137.08 toks/s, output: 12.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 10.08it/s, est. speed input: 13162.54 toks/s, output: 12.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 12.85it/s, est. speed input: 13162.54 toks/s, output: 12.85 toks/s]
[rank0]:[W128 12:01:57.465118610 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.2s

测试结果:
  Requests/s:   15.91
  Tokens/s:     16304.71
  Total Reqs:   128
  Elapsed:      8.05s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     16288.80

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:02:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:02:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=39504) WARNING 01-28 12:02:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=39504) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=39504) WARNING 01-28 12:02:25 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.56 requests/s, 33372.25 total tokens/s, 32.56 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 12:02:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:02:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:02:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:02:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:02:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:02:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:02:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:02:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:02:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:02:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:02:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:02:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:02:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:02:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:02:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:02:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:02:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:02:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:02:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=39504) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=39504) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.65it/s]
(EngineCore_DP0 pid=39504) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.65it/s]
(EngineCore_DP0 pid=39504) 
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=39504) [2026-01-28 12:02:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=39504) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.77it/s]
(EngineCore_DP0 pid=39504) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.98it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 36/256 [00:00<00:00, 355.54it/s]
Adding requests:  30%|██▉       | 76/256 [00:00<00:00, 379.17it/s]
Adding requests:  45%|████▍     | 114/256 [00:00<00:00, 374.87it/s]
Adding requests:  59%|█████▉    | 152/256 [00:00<00:00, 376.15it/s]
Adding requests:  74%|███████▍  | 190/256 [00:00<00:00, 375.06it/s]
Adding requests:  89%|████████▉ | 228/256 [00:00<00:00, 375.88it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 375.32it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 165.77it/s, est. speed input: 169782.06 toks/s, output: 165.78 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:04, 52.91it/s, est. speed input: 60542.54 toks/s, output: 59.12 toks/s]   
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 43.07it/s, est. speed input: 50599.70 toks/s, output: 49.41 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:01<00:04, 42.14it/s, est. speed input: 48935.04 toks/s, output: 47.79 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 39.97it/s, est. speed input: 46982.39 toks/s, output: 45.88 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 36.73it/s, est. speed input: 44779.39 toks/s, output: 43.73 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 37.98it/s, est. speed input: 44692.83 toks/s, output: 43.65 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:05, 34.80it/s, est. speed input: 43013.12 toks/s, output: 42.00 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:05, 34.62it/s, est. speed input: 42486.37 toks/s, output: 41.49 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:05, 34.45it/s, est. speed input: 42018.13 toks/s, output: 41.03 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:02<00:05, 34.21it/s, est. speed input: 41575.85 toks/s, output: 40.60 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:04, 34.02it/s, est. speed input: 41178.96 toks/s, output: 40.21 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:04, 34.01it/s, est. speed input: 40855.05 toks/s, output: 39.90 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 33.97it/s, est. speed input: 40553.48 toks/s, output: 39.60 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 33.79it/s, est. speed input: 40252.17 toks/s, output: 39.31 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 33.87it/s, est. speed input: 40014.47 toks/s, output: 39.08 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:04, 33.91it/s, est. speed input: 39795.18 toks/s, output: 38.86 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:04, 33.92it/s, est. speed input: 39590.86 toks/s, output: 38.66 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:03<00:04, 33.93it/s, est. speed input: 39401.28 toks/s, output: 38.48 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:03<00:04, 33.89it/s, est. speed input: 39219.97 toks/s, output: 38.30 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:03, 33.92it/s, est. speed input: 39060.13 toks/s, output: 38.14 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:03, 33.92it/s, est. speed input: 38909.28 toks/s, output: 38.00 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:03, 33.77it/s, est. speed input: 38749.03 toks/s, output: 37.84 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 33.79it/s, est. speed input: 38614.14 toks/s, output: 37.71 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:03, 33.61it/s, est. speed input: 38465.31 toks/s, output: 37.56 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:03, 33.61it/s, est. speed input: 38339.29 toks/s, output: 37.44 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:03, 33.71it/s, est. speed input: 38232.76 toks/s, output: 37.34 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:04<00:03, 33.80it/s, est. speed input: 38134.75 toks/s, output: 37.24 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:04<00:02, 33.74it/s, est. speed input: 38028.94 toks/s, output: 37.14 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:02, 33.77it/s, est. speed input: 37936.25 toks/s, output: 37.05 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 33.84it/s, est. speed input: 37853.77 toks/s, output: 36.97 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 33.89it/s, est. speed input: 37775.22 toks/s, output: 36.89 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 33.92it/s, est. speed input: 37700.74 toks/s, output: 36.82 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 33.75it/s, est. speed input: 37611.91 toks/s, output: 36.73 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:02, 33.80it/s, est. speed input: 37543.24 toks/s, output: 36.66 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:05<00:02, 33.87it/s, est. speed input: 37479.83 toks/s, output: 36.60 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:05<00:02, 33.93it/s, est. speed input: 37420.78 toks/s, output: 36.54 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:05<00:01, 33.96it/s, est. speed input: 37363.72 toks/s, output: 36.49 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 33.97it/s, est. speed input: 37307.55 toks/s, output: 36.43 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 33.98it/s, est. speed input: 37254.32 toks/s, output: 36.38 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 34.00it/s, est. speed input: 37204.32 toks/s, output: 36.33 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 33.94it/s, est. speed input: 37150.81 toks/s, output: 36.28 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 33.64it/s, est. speed input: 37080.48 toks/s, output: 36.21 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 33.72it/s, est. speed input: 37033.98 toks/s, output: 36.17 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:06<00:01, 33.83it/s, est. speed input: 36993.01 toks/s, output: 36.13 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:06<00:00, 33.60it/s, est. speed input: 36932.31 toks/s, output: 36.07 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:06<00:00, 33.61it/s, est. speed input: 36885.17 toks/s, output: 36.02 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:06<00:00, 33.29it/s, est. speed input: 36818.09 toks/s, output: 35.96 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:06<00:00, 33.12it/s, est. speed input: 36756.54 toks/s, output: 35.89 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:06<00:00, 32.99it/s, est. speed input: 36696.56 toks/s, output: 35.84 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:06<00:00, 33.03it/s, est. speed input: 36647.42 toks/s, output: 35.79 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 33.07it/s, est. speed input: 36600.62 toks/s, output: 35.74 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:07<00:00, 33.10it/s, est. speed input: 36555.83 toks/s, output: 35.70 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 33.16it/s, est. speed input: 36514.78 toks/s, output: 35.66 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 33.16it/s, est. speed input: 36514.78 toks/s, output: 35.66 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 35.66it/s, est. speed input: 36514.78 toks/s, output: 35.66 toks/s]
[rank0]:[W128 12:02:52.765029948 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.6s

测试结果:
  Requests/s:   32.56
  Tokens/s:     33372.25
  Total Reqs:   256
  Elapsed:      7.86s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     33339.69

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:03:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:03:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=40131) WARNING 01-28 12:03:13 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=40131) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=40131) WARNING 01-28 12:03:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.54 requests/s, 54880.48 total tokens/s, 53.54 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 12:03:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:03:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:03:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:03:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:03:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:03:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:03:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:03:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:03:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:03:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:03:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:03:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:03:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:03:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:03:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:03:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:03:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:03:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:03:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=40131) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=40131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=40131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=40131) 
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=40131) [2026-01-28 12:03:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=40131) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.07it/s]
(EngineCore_DP0 pid=40131) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.24it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.21it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 36/512 [00:00<00:01, 359.83it/s]
Adding requests:  15%|█▍        | 75/512 [00:00<00:01, 375.34it/s]
Adding requests:  22%|██▏       | 114/512 [00:00<00:01, 379.67it/s]
Adding requests:  30%|██▉       | 152/512 [00:00<00:00, 368.96it/s]
Adding requests:  37%|███▋      | 190/512 [00:00<00:00, 370.52it/s]
Adding requests:  45%|████▍     | 228/512 [00:00<00:00, 370.30it/s]
Adding requests:  52%|█████▏    | 267/512 [00:00<00:00, 375.80it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 371.72it/s]
Adding requests:  67%|██████▋   | 343/512 [00:00<00:00, 366.71it/s]
Adding requests:  74%|███████▍  | 381/512 [00:01<00:00, 368.85it/s]
Adding requests:  82%|████████▏ | 419/512 [00:01<00:00, 369.82it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 367.93it/s]
Adding requests:  96%|█████████▋| 494/512 [00:01<00:00, 371.29it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 370.92it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 449.37it/s, est. speed input: 460227.37 toks/s, output: 449.39 toks/s]
Processed prompts:  22%|██▏       | 111/512 [00:00<00:03, 101.48it/s, est. speed input: 120561.61 toks/s, output: 117.74 toks/s]
Processed prompts:  26%|██▌       | 133/512 [00:01<00:04, 86.51it/s, est. speed input: 104289.85 toks/s, output: 101.85 toks/s] 
Processed prompts:  29%|██▉       | 148/512 [00:01<00:04, 76.28it/s, est. speed input: 94973.31 toks/s, output: 92.75 toks/s]  
Processed prompts:  31%|███       | 159/512 [00:01<00:05, 70.13it/s, est. speed input: 89801.91 toks/s, output: 87.70 toks/s]
Processed prompts:  33%|███▎      | 168/512 [00:01<00:05, 68.63it/s, est. speed input: 87859.44 toks/s, output: 85.80 toks/s]
Processed prompts:  34%|███▍      | 176/512 [00:02<00:05, 65.85it/s, est. speed input: 85716.22 toks/s, output: 83.71 toks/s]
Processed prompts:  36%|███▌      | 184/512 [00:02<00:05, 63.39it/s, est. speed input: 83834.12 toks/s, output: 81.87 toks/s]
Processed prompts:  37%|███▋      | 191/512 [00:02<00:05, 59.60it/s, est. speed input: 81735.18 toks/s, output: 79.82 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:02<00:05, 56.64it/s, est. speed input: 79890.00 toks/s, output: 78.02 toks/s]
Processed prompts:  40%|████      | 206/512 [00:02<00:05, 56.19it/s, est. speed input: 78612.91 toks/s, output: 76.77 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:02<00:05, 55.94it/s, est. speed input: 77486.18 toks/s, output: 75.67 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:02<00:05, 55.72it/s, est. speed input: 76464.84 toks/s, output: 74.67 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:03<00:05, 55.55it/s, est. speed input: 75533.07 toks/s, output: 73.76 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:03<00:04, 55.44it/s, est. speed input: 74687.00 toks/s, output: 72.94 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:03<00:04, 55.36it/s, est. speed input: 73912.55 toks/s, output: 72.18 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:03<00:04, 55.29it/s, est. speed input: 73199.21 toks/s, output: 71.48 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:03<00:04, 55.19it/s, est. speed input: 72534.28 toks/s, output: 70.83 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:03<00:04, 55.21it/s, est. speed input: 71931.67 toks/s, output: 70.25 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:03<00:04, 55.22it/s, est. speed input: 71376.85 toks/s, output: 69.70 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:04<00:04, 55.22it/s, est. speed input: 70853.78 toks/s, output: 69.19 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:04<00:03, 55.21it/s, est. speed input: 70367.91 toks/s, output: 68.72 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:04<00:03, 55.21it/s, est. speed input: 69916.77 toks/s, output: 68.28 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:04<00:03, 55.22it/s, est. speed input: 69492.36 toks/s, output: 67.86 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:04<00:03, 55.23it/s, est. speed input: 69096.53 toks/s, output: 67.48 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:04<00:03, 55.21it/s, est. speed input: 68720.23 toks/s, output: 67.11 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:05<00:03, 55.23it/s, est. speed input: 68368.21 toks/s, output: 66.77 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:05<00:03, 55.18it/s, est. speed input: 68030.20 toks/s, output: 66.44 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:05<00:02, 55.07it/s, est. speed input: 67702.27 toks/s, output: 66.12 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:05<00:02, 55.07it/s, est. speed input: 67400.95 toks/s, output: 65.82 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:05<00:02, 55.08it/s, est. speed input: 67114.77 toks/s, output: 65.54 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:05<00:02, 55.05it/s, est. speed input: 66839.89 toks/s, output: 65.27 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:05<00:02, 55.10it/s, est. speed input: 66586.10 toks/s, output: 65.03 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:06<00:02, 55.05it/s, est. speed input: 66335.91 toks/s, output: 64.78 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:06<00:02, 55.06it/s, est. speed input: 66102.54 toks/s, output: 64.55 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:06<00:01, 55.11it/s, est. speed input: 65882.83 toks/s, output: 64.34 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:06<00:01, 55.21it/s, est. speed input: 65679.63 toks/s, output: 64.14 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:06<00:01, 55.22it/s, est. speed input: 65479.91 toks/s, output: 63.95 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:06<00:01, 55.26it/s, est. speed input: 65291.86 toks/s, output: 63.76 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:06<00:01, 55.29it/s, est. speed input: 65110.76 toks/s, output: 63.58 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:07<00:01, 55.27it/s, est. speed input: 64934.50 toks/s, output: 63.41 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:07<00:01, 55.31it/s, est. speed input: 64769.73 toks/s, output: 63.25 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:07<00:00, 55.31it/s, est. speed input: 64608.84 toks/s, output: 63.09 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:07<00:00, 55.33it/s, est. speed input: 64456.04 toks/s, output: 62.95 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:07<00:00, 55.36it/s, est. speed input: 64310.04 toks/s, output: 62.80 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:07<00:00, 55.37it/s, est. speed input: 64168.84 toks/s, output: 62.66 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:07<00:00, 55.33it/s, est. speed input: 64029.31 toks/s, output: 62.53 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:08<00:00, 55.23it/s, est. speed input: 63890.06 toks/s, output: 62.39 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:08<00:00, 56.43it/s, est. speed input: 63840.25 toks/s, output: 62.34 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 56.43it/s, est. speed input: 64089.16 toks/s, output: 62.59 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 62.59it/s, est. speed input: 64089.16 toks/s, output: 62.59 toks/s]
[rank0]:[W128 12:03:52.113997964 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.8s

测试结果:
  Requests/s:   53.54
  Tokens/s:     54880.48
  Total Reqs:   512
  Elapsed:      9.56s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     54826.94

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:04:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:04:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=40762) WARNING 01-28 12:04:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=40762) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=40762) WARNING 01-28 12:04:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.60 requests/s, 54938.78 total tokens/s, 53.60 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 12:04:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:04:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:04:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:04:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:04:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:04:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:04:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:04:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:04:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:04:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:04:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:04:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:04:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:04:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:04:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:04:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:04:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:04:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:04:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=40762) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=40762) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.64it/s]
(EngineCore_DP0 pid=40762) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.64it/s]
(EngineCore_DP0 pid=40762) 
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=40762) [2026-01-28 12:04:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=40762) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  6.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  7.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.77it/s]
(EngineCore_DP0 pid=40762) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.13it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.15it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.40it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 37/1024 [00:00<00:02, 368.24it/s]
Adding requests:   7%|▋         | 76/1024 [00:00<00:02, 376.25it/s]
Adding requests:  11%|█         | 114/1024 [00:00<00:02, 361.88it/s]
Adding requests:  15%|█▍        | 151/1024 [00:00<00:02, 354.27it/s]
Adding requests:  19%|█▊        | 190/1024 [00:00<00:02, 365.40it/s]
Adding requests:  22%|██▏       | 230/1024 [00:00<00:02, 374.63it/s]
Adding requests:  26%|██▋       | 270/1024 [00:00<00:01, 379.97it/s]
Adding requests:  30%|███       | 309/1024 [00:00<00:01, 378.64it/s]
Adding requests:  34%|███▍      | 347/1024 [00:00<00:01, 375.68it/s]
Adding requests:  38%|███▊      | 386/1024 [00:01<00:01, 379.61it/s]
Adding requests:  42%|████▏     | 426/1024 [00:01<00:01, 383.33it/s]
Adding requests:  45%|████▌     | 465/1024 [00:01<00:01, 379.30it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 377.75it/s]
Adding requests:  53%|█████▎    | 541/1024 [00:01<00:01, 368.35it/s]
Adding requests:  57%|█████▋    | 581/1024 [00:01<00:01, 377.28it/s]
Adding requests:  61%|██████    | 620/1024 [00:01<00:01, 380.99it/s]
Adding requests:  64%|██████▍   | 660/1024 [00:01<00:00, 384.99it/s]
Adding requests:  68%|██████▊   | 700/1024 [00:01<00:00, 388.59it/s]
Adding requests:  72%|███████▏  | 739/1024 [00:01<00:00, 383.23it/s]
Adding requests:  76%|███████▌  | 778/1024 [00:02<00:00, 384.76it/s]
Adding requests:  80%|███████▉  | 817/1024 [00:02<00:00, 382.16it/s]
Adding requests:  84%|████████▎ | 856/1024 [00:02<00:00, 383.37it/s]
Adding requests:  88%|████████▊ | 897/1024 [00:02<00:00, 390.27it/s]
Adding requests:  92%|█████████▏| 937/1024 [00:02<00:00, 388.74it/s]
Adding requests:  96%|█████████▌| 978/1024 [00:02<00:00, 392.27it/s]
Adding requests: 100%|█████████▉| 1019/1024 [00:02<00:00, 396.53it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 381.59it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:00<00:01, 636.66it/s, est. speed input: 652025.46 toks/s, output: 636.69 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:01<00:06, 120.94it/s, est. speed input: 148495.26 toks/s, output: 145.01 toks/s]
Processed prompts:  23%|██▎       | 232/1024 [00:01<00:07, 103.80it/s, est. speed input: 129430.15 toks/s, output: 126.40 toks/s]
Processed prompts:  25%|██▍       | 252/1024 [00:02<00:09, 85.36it/s, est. speed input: 113312.50 toks/s, output: 110.66 toks/s] 
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:27, 27.16it/s, est. speed input: 55638.20 toks/s, output: 54.33 toks/s]  
Processed prompts:  27%|██▋       | 275/1024 [00:05<00:25, 29.02it/s, est. speed input: 55827.70 toks/s, output: 54.52 toks/s]
Processed prompts:  28%|██▊       | 283/1024 [00:05<00:24, 30.86it/s, est. speed input: 55814.88 toks/s, output: 54.51 toks/s]
Processed prompts:  28%|██▊       | 291/1024 [00:05<00:22, 33.05it/s, est. speed input: 55796.88 toks/s, output: 54.49 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:20, 34.75it/s, est. speed input: 55595.59 toks/s, output: 54.29 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:19, 37.56it/s, est. speed input: 55588.89 toks/s, output: 54.29 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:17, 40.37it/s, est. speed input: 55577.33 toks/s, output: 54.27 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:16, 43.02it/s, est. speed input: 55564.11 toks/s, output: 54.26 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:15, 45.43it/s, est. speed input: 55556.44 toks/s, output: 54.25 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:14, 47.42it/s, est. speed input: 55542.87 toks/s, output: 54.24 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:13, 49.09it/s, est. speed input: 55536.69 toks/s, output: 54.23 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:13, 50.37it/s, est. speed input: 55529.67 toks/s, output: 54.23 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:12, 51.32it/s, est. speed input: 55519.01 toks/s, output: 54.22 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:12, 52.03it/s, est. speed input: 55510.91 toks/s, output: 54.21 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:12, 52.53it/s, est. speed input: 55502.22 toks/s, output: 54.20 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:12, 52.93it/s, est. speed input: 55494.89 toks/s, output: 54.19 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:11, 53.25it/s, est. speed input: 55492.05 toks/s, output: 54.19 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:11, 53.45it/s, est. speed input: 55485.50 toks/s, output: 54.18 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:11, 53.55it/s, est. speed input: 55477.17 toks/s, output: 54.18 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:11, 53.61it/s, est. speed input: 55469.59 toks/s, output: 54.17 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:11, 53.73it/s, est. speed input: 55465.97 toks/s, output: 54.17 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:10, 53.79it/s, est. speed input: 55461.23 toks/s, output: 54.16 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:10, 53.81it/s, est. speed input: 55455.81 toks/s, output: 54.16 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:10, 53.81it/s, est. speed input: 55450.16 toks/s, output: 54.15 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:10, 53.81it/s, est. speed input: 55442.99 toks/s, output: 54.14 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:10, 53.85it/s, est. speed input: 55439.48 toks/s, output: 54.14 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:10, 53.84it/s, est. speed input: 55435.12 toks/s, output: 54.13 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:08<00:10, 53.87it/s, est. speed input: 55430.98 toks/s, output: 54.13 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:09, 53.87it/s, est. speed input: 55426.58 toks/s, output: 54.13 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:09, 53.88it/s, est. speed input: 55422.57 toks/s, output: 54.12 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:09, 53.85it/s, est. speed input: 55416.97 toks/s, output: 54.12 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:09, 53.82it/s, est. speed input: 55411.03 toks/s, output: 54.11 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:09, 53.81it/s, est. speed input: 55405.88 toks/s, output: 54.11 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:09<00:09, 53.81it/s, est. speed input: 55401.36 toks/s, output: 54.10 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:09<00:09, 53.83it/s, est. speed input: 55397.64 toks/s, output: 54.10 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:08, 53.78it/s, est. speed input: 55391.46 toks/s, output: 54.09 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:08, 53.81it/s, est. speed input: 55388.07 toks/s, output: 54.09 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:08, 53.79it/s, est. speed input: 55382.74 toks/s, output: 54.08 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:10<00:08, 53.81it/s, est. speed input: 55379.51 toks/s, output: 54.08 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:10<00:08, 53.78it/s, est. speed input: 55374.46 toks/s, output: 54.08 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:10<00:08, 53.84it/s, est. speed input: 55373.09 toks/s, output: 54.08 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:10<00:07, 53.83it/s, est. speed input: 55369.46 toks/s, output: 54.07 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:07, 53.81it/s, est. speed input: 55365.20 toks/s, output: 54.07 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:07, 53.85it/s, est. speed input: 55363.38 toks/s, output: 54.07 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:11<00:07, 53.79it/s, est. speed input: 55358.35 toks/s, output: 54.06 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:11<00:07, 53.82it/s, est. speed input: 55356.22 toks/s, output: 54.06 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:11<00:07, 53.83it/s, est. speed input: 55353.13 toks/s, output: 54.06 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:11<00:07, 53.81it/s, est. speed input: 55349.30 toks/s, output: 54.05 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:06, 53.78it/s, est. speed input: 55345.14 toks/s, output: 54.05 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:06, 53.79it/s, est. speed input: 55342.25 toks/s, output: 54.05 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:12<00:06, 53.75it/s, est. speed input: 55337.32 toks/s, output: 54.04 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:12<00:06, 53.70it/s, est. speed input: 55331.78 toks/s, output: 54.03 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:12<00:06, 53.75it/s, est. speed input: 55330.17 toks/s, output: 54.03 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:12<00:06, 53.98it/s, est. speed input: 55335.20 toks/s, output: 54.04 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:12<00:06, 54.15it/s, est. speed input: 55341.29 toks/s, output: 54.04 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:13<00:05, 54.32it/s, est. speed input: 55349.15 toks/s, output: 54.05 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:13<00:05, 54.42it/s, est. speed input: 55355.72 toks/s, output: 54.06 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:13<00:05, 54.50it/s, est. speed input: 55363.62 toks/s, output: 54.07 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:13<00:05, 54.58it/s, est. speed input: 55370.73 toks/s, output: 54.07 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:13<00:05, 54.66it/s, est. speed input: 55379.19 toks/s, output: 54.08 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:13<00:05, 54.68it/s, est. speed input: 55386.03 toks/s, output: 54.09 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:13<00:04, 54.65it/s, est. speed input: 55391.46 toks/s, output: 54.09 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:14<00:04, 54.67it/s, est. speed input: 55398.03 toks/s, output: 54.10 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:14<00:04, 54.71it/s, est. speed input: 55405.31 toks/s, output: 54.11 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:14<00:04, 54.68it/s, est. speed input: 55411.33 toks/s, output: 54.11 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:14<00:04, 54.69it/s, est. speed input: 55416.89 toks/s, output: 54.12 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:14<00:04, 54.69it/s, est. speed input: 55422.52 toks/s, output: 54.12 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:14<00:04, 54.70it/s, est. speed input: 55428.72 toks/s, output: 54.13 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:14<00:03, 54.70it/s, est. speed input: 55434.19 toks/s, output: 54.13 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:15<00:03, 54.64it/s, est. speed input: 55438.48 toks/s, output: 54.14 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:15<00:03, 54.63it/s, est. speed input: 55443.00 toks/s, output: 54.14 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:15<00:03, 54.66it/s, est. speed input: 55448.20 toks/s, output: 54.15 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:15<00:03, 54.71it/s, est. speed input: 55454.91 toks/s, output: 54.16 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:15<00:03, 54.68it/s, est. speed input: 55459.44 toks/s, output: 54.16 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:15<00:03, 54.70it/s, est. speed input: 55464.61 toks/s, output: 54.16 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:15<00:02, 54.65it/s, est. speed input: 55468.31 toks/s, output: 54.17 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:16<00:02, 54.63it/s, est. speed input: 55471.97 toks/s, output: 54.17 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:16<00:02, 54.65it/s, est. speed input: 55476.90 toks/s, output: 54.18 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:16<00:02, 54.63it/s, est. speed input: 55480.42 toks/s, output: 54.18 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:16<00:02, 54.59it/s, est. speed input: 55483.53 toks/s, output: 54.18 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:16<00:02, 54.61it/s, est. speed input: 55488.19 toks/s, output: 54.19 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:16<00:02, 54.63it/s, est. speed input: 55492.07 toks/s, output: 54.19 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:17<00:01, 54.60it/s, est. speed input: 55495.15 toks/s, output: 54.19 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:17<00:01, 54.65it/s, est. speed input: 55500.07 toks/s, output: 54.20 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:17<00:01, 54.65it/s, est. speed input: 55504.38 toks/s, output: 54.20 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:17<00:01, 54.65it/s, est. speed input: 55507.93 toks/s, output: 54.21 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:17<00:01, 54.66it/s, est. speed input: 55511.89 toks/s, output: 54.21 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:17<00:01, 54.61it/s, est. speed input: 55514.36 toks/s, output: 54.21 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:17<00:00, 54.54it/s, est. speed input: 55515.57 toks/s, output: 54.21 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:18<00:00, 54.58it/s, est. speed input: 55519.37 toks/s, output: 54.22 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:18<00:00, 56.18it/s, est. speed input: 55565.35 toks/s, output: 54.26 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:18<00:00, 55.69it/s, est. speed input: 55567.84 toks/s, output: 54.27 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:18<00:00, 55.34it/s, est. speed input: 55569.93 toks/s, output: 54.27 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:18<00:00, 55.11it/s, est. speed input: 55572.49 toks/s, output: 54.27 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:18<00:00, 56.82it/s, est. speed input: 55622.25 toks/s, output: 54.32 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 56.82it/s, est. speed input: 55949.41 toks/s, output: 54.64 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 54.64it/s, est. speed input: 55949.41 toks/s, output: 54.64 toks/s]
[rank0]:[W128 12:05:04.227007978 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 72.8s

测试结果:
  Requests/s:   53.60
  Tokens/s:     54938.78
  Total Reqs:   1024
  Elapsed:      19.10s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     54885.19

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:05:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:05:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=41497) WARNING 01-28 12:05:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=41497) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=41497) WARNING 01-28 12:05:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.26 requests/s, 55619.69 total tokens/s, 54.26 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 12:05:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:05:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:05:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:05:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:05:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:05:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:05:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:05:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:05:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:05:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:05:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:05:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:05:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:05:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:05:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:05:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:05:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:05:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:05:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=41497) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=41497) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
(EngineCore_DP0 pid=41497) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
(EngineCore_DP0 pid=41497) 
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=41497) [2026-01-28 12:05:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=41497) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.03it/s]
(EngineCore_DP0 pid=41497) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.03it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.47it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 36/2048 [00:00<00:05, 357.32it/s]
Adding requests:   4%|▎         | 74/2048 [00:00<00:05, 370.32it/s]
Adding requests:   5%|▌         | 112/2048 [00:00<00:05, 374.37it/s]
Adding requests:   7%|▋         | 150/2048 [00:00<00:05, 375.02it/s]
Adding requests:   9%|▉         | 188/2048 [00:00<00:05, 367.86it/s]
Adding requests:  11%|█         | 226/2048 [00:00<00:04, 370.60it/s]
Adding requests:  13%|█▎        | 264/2048 [00:00<00:04, 366.91it/s]
Adding requests:  15%|█▍        | 301/2048 [00:00<00:04, 364.85it/s]
Adding requests:  17%|█▋        | 341/2048 [00:00<00:04, 373.21it/s]
Adding requests:  19%|█▊        | 381/2048 [00:01<00:04, 379.07it/s]
Adding requests:  21%|██        | 421/2048 [00:01<00:04, 385.08it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:04, 383.08it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:04, 382.11it/s]
Adding requests:  26%|██▋       | 538/2048 [00:01<00:04, 376.43it/s]
Adding requests:  28%|██▊       | 578/2048 [00:01<00:03, 383.15it/s]
Adding requests:  30%|███       | 619/2048 [00:01<00:03, 389.09it/s]
Adding requests:  32%|███▏      | 660/2048 [00:01<00:03, 393.71it/s]
Adding requests:  34%|███▍      | 702/2048 [00:01<00:03, 398.87it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:03, 398.03it/s]
Adding requests:  38%|███▊      | 782/2048 [00:02<00:03, 398.27it/s]
Adding requests:  40%|████      | 822/2048 [00:02<00:03, 389.66it/s]
Adding requests:  42%|████▏     | 862/2048 [00:02<00:03, 384.92it/s]
Adding requests:  44%|████▍     | 904/2048 [00:02<00:02, 393.70it/s]
Adding requests:  46%|████▌     | 944/2048 [00:02<00:02, 391.89it/s]
Adding requests:  48%|████▊     | 985/2048 [00:02<00:02, 395.30it/s]
Adding requests:  50%|█████     | 1026/2048 [00:02<00:02, 398.78it/s]
Adding requests:  52%|█████▏    | 1066/2048 [00:02<00:02, 391.99it/s]
Adding requests:  54%|█████▍    | 1106/2048 [00:02<00:02, 388.85it/s]
Adding requests:  56%|█████▌    | 1146/2048 [00:02<00:02, 389.55it/s]
Adding requests:  58%|█████▊    | 1188/2048 [00:03<00:02, 397.15it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:03<00:02, 383.76it/s]
Adding requests:  62%|██████▏   | 1267/2048 [00:03<00:02, 364.09it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:03<00:02, 367.31it/s]
Adding requests:  66%|██████▌   | 1346/2048 [00:03<00:01, 378.14it/s]
Adding requests:  68%|██████▊   | 1386/2048 [00:03<00:01, 382.05it/s]
Adding requests:  70%|██████▉   | 1427/2048 [00:03<00:01, 388.65it/s]
Adding requests:  72%|███████▏  | 1470/2048 [00:03<00:01, 399.50it/s]
Adding requests:  74%|███████▍  | 1514/2048 [00:03<00:01, 411.19it/s]
Adding requests:  76%|███████▌  | 1557/2048 [00:04<00:01, 415.44it/s]
Adding requests:  78%|███████▊  | 1603/2048 [00:04<00:01, 425.68it/s]
Adding requests:  80%|████████  | 1646/2048 [00:04<00:00, 422.84it/s]
Adding requests:  82%|████████▏ | 1689/2048 [00:04<00:00, 421.42it/s]
Adding requests:  85%|████████▍ | 1733/2048 [00:04<00:00, 424.64it/s]
Adding requests:  87%|████████▋ | 1776/2048 [00:04<00:00, 417.54it/s]
Adding requests:  89%|████████▉ | 1820/2048 [00:04<00:00, 422.49it/s]
Adding requests:  91%|█████████ | 1863/2048 [00:04<00:00, 419.43it/s]
Adding requests:  93%|█████████▎| 1905/2048 [00:04<00:00, 419.57it/s]
Adding requests:  95%|█████████▌| 1948/2048 [00:04<00:00, 422.41it/s]
Adding requests:  97%|█████████▋| 1991/2048 [00:05<00:00, 423.61it/s]
Adding requests:  99%|█████████▉| 2034/2048 [00:05<00:00, 414.97it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 394.99it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:02, 749.35it/s, est. speed input: 767397.56 toks/s, output: 749.37 toks/s]
Processed prompts:  17%|█▋        | 349/2048 [00:01<00:08, 191.73it/s, est. speed input: 238057.09 toks/s, output: 232.48 toks/s]
Processed prompts:  19%|█▉        | 384/2048 [00:02<00:11, 143.80it/s, est. speed input: 190020.73 toks/s, output: 185.57 toks/s]
Processed prompts:  20%|█▉        | 406/2048 [00:02<00:15, 107.60it/s, est. speed input: 157664.18 toks/s, output: 153.97 toks/s]
Processed prompts:  21%|██        | 421/2048 [00:02<00:16, 96.79it/s, est. speed input: 147600.76 toks/s, output: 144.14 toks/s] 
Processed prompts:  21%|██        | 434/2048 [00:03<00:18, 85.56it/s, est. speed input: 138681.84 toks/s, output: 135.43 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:03<00:20, 78.14it/s, est. speed input: 131844.20 toks/s, output: 128.75 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:03<00:21, 72.60it/s, est. speed input: 126267.59 toks/s, output: 123.31 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:04<00:22, 68.26it/s, est. speed input: 121466.93 toks/s, output: 118.62 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:04<00:23, 64.95it/s, est. speed input: 117287.10 toks/s, output: 114.54 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:04<00:24, 62.50it/s, est. speed input: 113624.21 toks/s, output: 110.96 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:04<00:24, 60.73it/s, est. speed input: 110392.59 toks/s, output: 107.80 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:05<00:25, 59.44it/s, est. speed input: 107511.24 toks/s, output: 104.99 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:05<00:25, 58.53it/s, est. speed input: 104931.47 toks/s, output: 102.47 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:05<00:25, 57.89it/s, est. speed input: 102607.39 toks/s, output: 100.20 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:06<00:25, 57.42it/s, est. speed input: 100496.77 toks/s, output: 98.14 toks/s] 
Processed prompts:  30%|██▉       | 610/2048 [00:06<00:25, 57.09it/s, est. speed input: 98577.54 toks/s, output: 96.27 toks/s] 
Processed prompts:  31%|███       | 626/2048 [00:06<00:25, 56.81it/s, est. speed input: 96810.53 toks/s, output: 94.54 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:06<00:24, 56.62it/s, est. speed input: 95189.92 toks/s, output: 92.96 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:07<00:24, 56.52it/s, est. speed input: 93704.30 toks/s, output: 91.51 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:07<00:24, 56.35it/s, est. speed input: 92312.51 toks/s, output: 90.15 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:07<00:24, 56.30it/s, est. speed input: 91035.69 toks/s, output: 88.90 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:08<00:23, 56.24it/s, est. speed input: 89845.21 toks/s, output: 87.74 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:08<00:23, 56.20it/s, est. speed input: 88736.85 toks/s, output: 86.66 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:08<00:23, 56.18it/s, est. speed input: 87703.14 toks/s, output: 85.65 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:08<00:23, 56.17it/s, est. speed input: 86736.28 toks/s, output: 84.70 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:09<00:22, 56.14it/s, est. speed input: 85825.95 toks/s, output: 83.81 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:09<00:22, 56.11it/s, est. speed input: 84968.90 toks/s, output: 82.98 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:09<00:22, 56.12it/s, est. speed input: 84166.18 toks/s, output: 82.19 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:10<00:21, 56.12it/s, est. speed input: 83407.97 toks/s, output: 81.45 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:10<00:21, 56.11it/s, est. speed input: 82691.04 toks/s, output: 80.75 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:10<00:21, 56.11it/s, est. speed input: 82013.01 toks/s, output: 80.09 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:10<00:21, 56.10it/s, est. speed input: 81368.75 toks/s, output: 79.46 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:11<00:20, 56.10it/s, est. speed input: 80758.68 toks/s, output: 78.87 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:11<00:20, 56.11it/s, est. speed input: 80179.85 toks/s, output: 78.30 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:11<00:20, 56.10it/s, est. speed input: 79627.49 toks/s, output: 77.76 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:12<00:19, 56.10it/s, est. speed input: 79101.67 toks/s, output: 77.25 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:12<00:19, 56.09it/s, est. speed input: 78600.04 toks/s, output: 76.76 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:12<00:19, 56.09it/s, est. speed input: 78120.71 toks/s, output: 76.29 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:12<00:18, 56.88it/s, est. speed input: 77742.70 toks/s, output: 75.92 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:13<00:18, 56.62it/s, est. speed input: 77301.40 toks/s, output: 75.49 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:13<00:18, 56.46it/s, est. speed input: 76879.71 toks/s, output: 75.08 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:13<00:18, 55.81it/s, est. speed input: 76425.17 toks/s, output: 74.63 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:14<00:18, 55.10it/s, est. speed input: 75964.60 toks/s, output: 74.18 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:14<00:18, 54.60it/s, est. speed input: 75522.52 toks/s, output: 73.75 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:14<00:17, 54.25it/s, est. speed input: 75097.81 toks/s, output: 73.34 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:14<00:17, 54.01it/s, est. speed input: 74690.00 toks/s, output: 72.94 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:15<00:17, 53.84it/s, est. speed input: 74298.23 toks/s, output: 72.56 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:15<00:17, 53.72it/s, est. speed input: 73921.26 toks/s, output: 72.19 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:15<00:16, 53.65it/s, est. speed input: 73559.85 toks/s, output: 71.84 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:16<00:16, 54.41it/s, est. speed input: 73278.28 toks/s, output: 71.56 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:16<00:16, 54.12it/s, est. speed input: 72940.40 toks/s, output: 71.23 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:16<00:15, 53.90it/s, est. speed input: 72613.24 toks/s, output: 70.91 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:17<00:15, 53.74it/s, est. speed input: 72297.31 toks/s, output: 70.60 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:17<00:15, 53.64it/s, est. speed input: 71992.23 toks/s, output: 70.30 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:17<00:15, 53.58it/s, est. speed input: 71699.30 toks/s, output: 70.02 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:17<00:14, 53.55it/s, est. speed input: 71416.26 toks/s, output: 69.74 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:18<00:14, 53.49it/s, est. speed input: 71139.91 toks/s, output: 69.47 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:18<00:14, 53.48it/s, est. speed input: 70874.83 toks/s, output: 69.21 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:18<00:14, 53.47it/s, est. speed input: 70617.91 toks/s, output: 68.96 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:19<00:13, 53.46it/s, est. speed input: 70368.85 toks/s, output: 68.72 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:19<00:13, 53.49it/s, est. speed input: 70130.02 toks/s, output: 68.49 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:19<00:13, 53.44it/s, est. speed input: 69894.15 toks/s, output: 68.26 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:20<00:12, 53.45it/s, est. speed input: 69667.95 toks/s, output: 68.04 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:20<00:12, 53.44it/s, est. speed input: 69447.38 toks/s, output: 67.82 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:20<00:12, 53.45it/s, est. speed input: 69234.09 toks/s, output: 67.61 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:20<00:11, 53.46it/s, est. speed input: 69026.82 toks/s, output: 67.41 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:21<00:11, 53.45it/s, est. speed input: 68825.00 toks/s, output: 67.21 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:21<00:11, 53.44it/s, est. speed input: 68628.41 toks/s, output: 67.02 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:21<00:11, 53.44it/s, est. speed input: 68437.58 toks/s, output: 66.83 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:22<00:10, 53.45it/s, est. speed input: 68252.22 toks/s, output: 66.65 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:22<00:10, 53.36it/s, est. speed input: 68066.80 toks/s, output: 66.47 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:22<00:10, 53.23it/s, est. speed input: 67882.00 toks/s, output: 66.29 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:23<00:09, 53.13it/s, est. speed input: 67702.26 toks/s, output: 66.12 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:23<00:09, 53.05it/s, est. speed input: 67525.85 toks/s, output: 65.94 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:23<00:09, 52.99it/s, est. speed input: 67354.07 toks/s, output: 65.78 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:23<00:09, 52.96it/s, est. speed input: 67187.18 toks/s, output: 65.61 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:26<00:32, 14.42it/s, est. speed input: 60327.71 toks/s, output: 58.91 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:27<00:24, 18.45it/s, est. speed input: 60259.20 toks/s, output: 58.85 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:27<00:18, 22.93it/s, est. speed input: 60191.47 toks/s, output: 58.78 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:27<00:14, 27.61it/s, est. speed input: 60124.80 toks/s, output: 58.72 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:28<00:12, 32.23it/s, est. speed input: 60060.12 toks/s, output: 58.65 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:28<00:10, 36.50it/s, est. speed input: 59996.60 toks/s, output: 58.59 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:28<00:09, 40.23it/s, est. speed input: 59934.17 toks/s, output: 58.53 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:29<00:08, 43.33it/s, est. speed input: 59873.63 toks/s, output: 58.47 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:29<00:07, 45.82it/s, est. speed input: 59814.98 toks/s, output: 58.41 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:29<00:06, 47.73it/s, est. speed input: 59757.05 toks/s, output: 58.36 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:29<00:06, 49.17it/s, est. speed input: 59700.35 toks/s, output: 58.30 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:30<00:05, 50.27it/s, est. speed input: 59646.73 toks/s, output: 58.25 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:30<00:05, 50.98it/s, est. speed input: 59590.43 toks/s, output: 58.19 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:30<00:04, 51.54it/s, est. speed input: 59537.36 toks/s, output: 58.14 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:31<00:04, 51.93it/s, est. speed input: 59484.83 toks/s, output: 58.09 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:31<00:04, 52.21it/s, est. speed input: 59433.37 toks/s, output: 58.04 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:31<00:03, 52.44it/s, est. speed input: 59384.23 toks/s, output: 57.99 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:32<00:03, 52.53it/s, est. speed input: 59333.39 toks/s, output: 57.94 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:32<00:03, 53.43it/s, est. speed input: 59312.42 toks/s, output: 57.92 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:32<00:02, 53.27it/s, est. speed input: 59264.92 toks/s, output: 57.88 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:32<00:02, 53.17it/s, est. speed input: 59218.43 toks/s, output: 57.83 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:33<00:02, 53.11it/s, est. speed input: 59173.39 toks/s, output: 57.79 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:33<00:02, 53.95it/s, est. speed input: 59157.92 toks/s, output: 57.77 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:33<00:01, 54.55it/s, est. speed input: 59142.75 toks/s, output: 57.76 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:34<00:01, 54.97it/s, est. speed input: 59127.38 toks/s, output: 57.74 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:34<00:01, 55.27it/s, est. speed input: 59112.34 toks/s, output: 57.73 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:34<00:00, 55.48it/s, est. speed input: 59097.73 toks/s, output: 57.71 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:34<00:00, 55.64it/s, est. speed input: 59083.43 toks/s, output: 57.70 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:35<00:00, 56.73it/s, est. speed input: 59097.12 toks/s, output: 57.71 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:35<00:00, 56.73it/s, est. speed input: 59503.31 toks/s, output: 58.11 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:35<00:00, 58.11it/s, est. speed input: 59503.31 toks/s, output: 58.11 toks/s]
[rank0]:[W128 12:06:44.265444746 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 99.5s

测试结果:
  Requests/s:   54.26
  Tokens/s:     55619.69
  Total Reqs:   2048
  Elapsed:      37.74s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     55565.43

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:07:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:07:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=42417) WARNING 01-28 12:07:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=42417) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=42417) WARNING 01-28 12:07:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.07 requests/s, 56447.19 total tokens/s, 55.07 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 12:07:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:07:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:07:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:07:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:07:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:07:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:07:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:07:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:07:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:07:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:07:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:07:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:07:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:07:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:07:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:07:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:07:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:07:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:07:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 12:07:34.355347584 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=42417) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=42417) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=42417) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=42417) 
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=42417) [2026-01-28 12:07:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=42417) [rank0]:W0128 12:07:55.366000 42417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=42417) [rank0]:W0128 12:07:55.472000 42417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=42417) [rank0]:W0128 12:07:56.826000 42417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=42417) [rank0]:W0128 12:07:56.982000 42417 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=42417) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.57it/s]
(EngineCore_DP0 pid=42417) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.21it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.86it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 37/4096 [00:00<00:11, 368.00it/s]
Adding requests:   2%|▏         | 77/4096 [00:00<00:10, 383.93it/s]
Adding requests:   3%|▎         | 116/4096 [00:00<00:10, 385.75it/s]
Adding requests:   4%|▍         | 155/4096 [00:00<00:10, 383.83it/s]
Adding requests:   5%|▍         | 194/4096 [00:00<00:10, 383.28it/s]
Adding requests:   6%|▌         | 234/4096 [00:00<00:09, 387.21it/s]
Adding requests:   7%|▋         | 276/4096 [00:00<00:09, 395.91it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:09, 395.54it/s]
Adding requests:   9%|▊         | 357/4096 [00:00<00:09, 399.20it/s]
Adding requests:  10%|▉         | 399/4096 [00:01<00:09, 404.77it/s]
Adding requests:  11%|█         | 440/4096 [00:01<00:09, 399.89it/s]
Adding requests:  12%|█▏        | 482/4096 [00:01<00:08, 403.45it/s]
Adding requests:  13%|█▎        | 523/4096 [00:01<00:09, 396.73it/s]
Adding requests:  14%|█▍        | 566/4096 [00:01<00:08, 404.26it/s]
Adding requests:  15%|█▍        | 608/4096 [00:01<00:08, 406.09it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:08, 413.53it/s]
Adding requests:  17%|█▋        | 696/4096 [00:01<00:08, 420.04it/s]
Adding requests:  18%|█▊        | 739/4096 [00:01<00:07, 422.54it/s]
Adding requests:  19%|█▉        | 782/4096 [00:01<00:07, 418.90it/s]
Adding requests:  20%|██        | 824/4096 [00:02<00:08, 407.98it/s]
Adding requests:  21%|██        | 866/4096 [00:02<00:07, 408.92it/s]
Adding requests:  22%|██▏       | 909/4096 [00:02<00:07, 414.67it/s]
Adding requests:  23%|██▎       | 952/4096 [00:02<00:07, 416.50it/s]
Adding requests:  24%|██▍       | 995/4096 [00:02<00:07, 419.09it/s]
Adding requests:  25%|██▌       | 1038/4096 [00:02<00:07, 419.91it/s]
Adding requests:  26%|██▋       | 1081/4096 [00:02<00:07, 416.71it/s]
Adding requests:  27%|██▋       | 1123/4096 [00:02<00:07, 414.12it/s]
Adding requests:  28%|██▊       | 1167/4096 [00:02<00:06, 420.55it/s]
Adding requests:  30%|██▉       | 1210/4096 [00:02<00:07, 411.41it/s]
Adding requests:  31%|███       | 1252/4096 [00:03<00:07, 404.91it/s]
Adding requests:  32%|███▏      | 1293/4096 [00:03<00:07, 399.40it/s]
Adding requests:  33%|███▎      | 1335/4096 [00:03<00:06, 403.94it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:03<00:06, 408.43it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:03<00:06, 413.25it/s]
Adding requests:  36%|███▌      | 1464/4096 [00:03<00:06, 418.05it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:03<00:06, 417.52it/s]
Adding requests:  38%|███▊      | 1549/4096 [00:03<00:06, 418.85it/s]
Adding requests:  39%|███▉      | 1592/4096 [00:03<00:05, 421.58it/s]
Adding requests:  40%|███▉      | 1636/4096 [00:03<00:05, 424.26it/s]
Adding requests:  41%|████      | 1679/4096 [00:04<00:05, 415.37it/s]
Adding requests:  42%|████▏     | 1721/4096 [00:04<00:05, 414.19it/s]
Adding requests:  43%|████▎     | 1763/4096 [00:04<00:05, 414.69it/s]
Adding requests:  44%|████▍     | 1805/4096 [00:04<00:05, 415.76it/s]
Adding requests:  45%|████▌     | 1848/4096 [00:04<00:05, 417.59it/s]
Adding requests:  46%|████▌     | 1891/4096 [00:04<00:05, 418.85it/s]
Adding requests:  47%|████▋     | 1934/4096 [00:04<00:05, 419.59it/s]
Adding requests:  48%|████▊     | 1976/4096 [00:04<00:05, 418.50it/s]
Adding requests:  49%|████▉     | 2019/4096 [00:04<00:04, 419.85it/s]
Adding requests:  50%|█████     | 2062/4096 [00:05<00:04, 420.71it/s]
Adding requests:  51%|█████▏    | 2105/4096 [00:05<00:04, 421.06it/s]
Adding requests:  52%|█████▏    | 2148/4096 [00:05<00:04, 412.73it/s]
Adding requests:  53%|█████▎    | 2190/4096 [00:05<00:04, 411.72it/s]
Adding requests:  55%|█████▍    | 2233/4096 [00:05<00:04, 415.48it/s]
Adding requests:  56%|█████▌    | 2275/4096 [00:05<00:04, 415.99it/s]
Adding requests:  57%|█████▋    | 2318/4096 [00:05<00:04, 420.06it/s]
Adding requests:  58%|█████▊    | 2361/4096 [00:05<00:04, 411.34it/s]
Adding requests:  59%|█████▊    | 2403/4096 [00:05<00:04, 410.90it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:05<00:04, 396.75it/s]
Adding requests:  61%|██████    | 2486/4096 [00:06<00:04, 400.33it/s]
Adding requests:  62%|██████▏   | 2527/4096 [00:06<00:03, 401.54it/s]
Adding requests:  63%|██████▎   | 2570/4096 [00:06<00:03, 407.58it/s]
Adding requests:  64%|██████▍   | 2612/4096 [00:06<00:03, 410.30it/s]
Adding requests:  65%|██████▍   | 2655/4096 [00:06<00:03, 415.22it/s]
Adding requests:  66%|██████▌   | 2697/4096 [00:06<00:03, 412.05it/s]
Adding requests:  67%|██████▋   | 2739/4096 [00:06<00:03, 409.02it/s]
Adding requests:  68%|██████▊   | 2780/4096 [00:06<00:03, 407.91it/s]
Adding requests:  69%|██████▉   | 2821/4096 [00:06<00:03, 405.77it/s]
Adding requests:  70%|██████▉   | 2863/4096 [00:06<00:03, 409.16it/s]
Adding requests:  71%|███████   | 2905/4096 [00:07<00:02, 412.08it/s]
Adding requests:  72%|███████▏  | 2947/4096 [00:07<00:02, 407.02it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:07<00:02, 406.52it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:07<00:02, 407.66it/s]
Adding requests:  75%|███████▌  | 3072/4096 [00:07<00:02, 408.79it/s]
Adding requests:  76%|███████▌  | 3114/4096 [00:07<00:02, 410.65it/s]
Adding requests:  77%|███████▋  | 3156/4096 [00:07<00:02, 406.77it/s]
Adding requests:  78%|███████▊  | 3197/4096 [00:07<00:02, 405.49it/s]
Adding requests:  79%|███████▉  | 3240/4096 [00:07<00:02, 411.01it/s]
Adding requests:  80%|████████  | 3283/4096 [00:08<00:01, 414.54it/s]
Adding requests:  81%|████████  | 3325/4096 [00:08<00:01, 410.88it/s]
Adding requests:  82%|████████▏ | 3368/4096 [00:08<00:01, 415.36it/s]
Adding requests:  83%|████████▎ | 3410/4096 [00:08<00:01, 409.08it/s]
Adding requests:  84%|████████▍ | 3451/4096 [00:08<00:01, 408.52it/s]
Adding requests:  85%|████████▌ | 3492/4096 [00:08<00:01, 401.38it/s]
Adding requests:  86%|████████▋ | 3533/4096 [00:08<00:01, 401.08it/s]
Adding requests:  87%|████████▋ | 3574/4096 [00:08<00:01, 400.99it/s]
Adding requests:  88%|████████▊ | 3615/4096 [00:08<00:01, 401.85it/s]
Adding requests:  89%|████████▉ | 3656/4096 [00:08<00:01, 401.83it/s]
Adding requests:  90%|█████████ | 3699/4096 [00:09<00:00, 408.10it/s]
Adding requests:  91%|█████████▏| 3740/4096 [00:09<00:00, 406.64it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:09<00:00, 400.65it/s]
Adding requests:  93%|█████████▎| 3822/4096 [00:09<00:00, 399.32it/s]
Adding requests:  94%|█████████▍| 3864/4096 [00:09<00:00, 403.52it/s]
Adding requests:  95%|█████████▌| 3905/4096 [00:09<00:00, 402.33it/s]
Adding requests:  96%|█████████▋| 3947/4096 [00:09<00:00, 406.75it/s]
Adding requests:  97%|█████████▋| 3988/4096 [00:09<00:00, 404.34it/s]
Adding requests:  98%|█████████▊| 4030/4096 [00:09<00:00, 408.89it/s]
Adding requests:  99%|█████████▉| 4071/4096 [00:09<00:00, 408.12it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 408.99it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:00<00:03, 1076.23it/s, est. speed input: 1102112.13 toks/s, output: 1076.25 toks/s]
Processed prompts:  16%|█▌        | 654/4096 [00:02<00:15, 224.05it/s, est. speed input: 286184.79 toks/s, output: 279.48 toks/s]   
Processed prompts:  17%|█▋        | 702/4096 [00:02<00:18, 182.55it/s, est. speed input: 243575.61 toks/s, output: 237.87 toks/s]
Processed prompts:  18%|█▊        | 732/4096 [00:06<00:53, 62.95it/s, est. speed input: 119403.72 toks/s, output: 116.61 toks/s] 
Processed prompts:  18%|█▊        | 751/4096 [00:06<00:57, 58.04it/s, est. speed input: 111606.42 toks/s, output: 108.99 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:07<01:02, 53.20it/s, est. speed input: 105103.58 toks/s, output: 102.64 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:08<01:02, 53.02it/s, est. speed input: 101218.43 toks/s, output: 98.85 toks/s] 
Processed prompts:  20%|██        | 834/4096 [00:08<01:01, 53.29it/s, est. speed input: 98125.76 toks/s, output: 95.83 toks/s] 
Processed prompts:  21%|██        | 866/4096 [00:09<01:00, 53.56it/s, est. speed input: 95443.14 toks/s, output: 93.21 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:09<00:59, 53.78it/s, est. speed input: 93078.88 toks/s, output: 90.90 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:10<00:58, 53.94it/s, est. speed input: 90976.39 toks/s, output: 88.84 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:11<00:57, 54.56it/s, est. speed input: 89256.11 toks/s, output: 87.16 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:11<00:56, 54.95it/s, est. speed input: 87681.30 toks/s, output: 85.63 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:12<00:55, 55.22it/s, est. speed input: 86250.44 toks/s, output: 84.23 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:12<00:54, 55.41it/s, est. speed input: 84948.01 toks/s, output: 82.96 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:13<00:54, 55.56it/s, est. speed input: 83758.91 toks/s, output: 81.80 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:13<00:53, 55.64it/s, est. speed input: 82662.82 toks/s, output: 80.73 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:14<00:52, 56.19it/s, est. speed input: 81749.35 toks/s, output: 79.83 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:15<00:51, 56.08it/s, est. speed input: 80810.80 toks/s, output: 78.92 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:15<00:51, 55.97it/s, est. speed input: 79936.72 toks/s, output: 78.06 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:16<00:50, 55.94it/s, est. speed input: 79131.08 toks/s, output: 77.28 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:16<00:50, 55.92it/s, est. speed input: 78381.53 toks/s, output: 76.54 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:17<00:49, 55.89it/s, est. speed input: 77680.11 toks/s, output: 75.86 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:17<00:49, 55.88it/s, est. speed input: 77024.20 toks/s, output: 75.22 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:18<00:48, 55.88it/s, est. speed input: 76410.14 toks/s, output: 74.62 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:19<00:48, 55.91it/s, est. speed input: 75837.56 toks/s, output: 74.06 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:19<00:47, 56.02it/s, est. speed input: 75309.07 toks/s, output: 73.54 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:20<00:46, 56.10it/s, est. speed input: 74810.73 toks/s, output: 73.06 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:20<00:46, 56.19it/s, est. speed input: 74344.98 toks/s, output: 72.60 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:21<00:45, 56.16it/s, est. speed input: 73892.13 toks/s, output: 72.16 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:21<00:44, 56.19it/s, est. speed input: 73468.96 toks/s, output: 71.75 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:22<00:44, 56.20it/s, est. speed input: 73066.26 toks/s, output: 71.35 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:23<00:43, 56.22it/s, est. speed input: 72684.21 toks/s, output: 70.98 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:23<00:43, 56.22it/s, est. speed input: 72319.78 toks/s, output: 70.62 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:24<00:42, 56.23it/s, est. speed input: 71973.15 toks/s, output: 70.29 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:24<00:42, 56.23it/s, est. speed input: 71641.84 toks/s, output: 69.96 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:25<00:41, 56.23it/s, est. speed input: 71325.32 toks/s, output: 69.65 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:25<00:40, 56.24it/s, est. speed input: 71024.04 toks/s, output: 69.36 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:26<00:40, 56.17it/s, est. speed input: 70728.66 toks/s, output: 69.07 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:26<00:39, 56.50it/s, est. speed input: 70478.34 toks/s, output: 68.83 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:27<00:39, 55.99it/s, est. speed input: 70175.45 toks/s, output: 68.53 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:28<00:39, 55.63it/s, est. speed input: 69884.41 toks/s, output: 68.25 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:28<00:38, 55.38it/s, est. speed input: 69604.78 toks/s, output: 67.97 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:29<00:38, 55.21it/s, est. speed input: 69336.57 toks/s, output: 67.71 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:29<00:37, 55.09it/s, est. speed input: 69078.62 toks/s, output: 67.46 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:30<00:37, 55.01it/s, est. speed input: 68830.95 toks/s, output: 67.22 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:31<00:36, 54.96it/s, est. speed input: 68593.20 toks/s, output: 66.99 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:31<00:36, 54.95it/s, est. speed input: 68365.87 toks/s, output: 66.76 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:32<00:35, 54.89it/s, est. speed input: 68143.00 toks/s, output: 66.55 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:32<00:34, 54.87it/s, est. speed input: 67929.74 toks/s, output: 66.34 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:33<00:34, 54.86it/s, est. speed input: 67723.84 toks/s, output: 66.14 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:33<00:33, 54.85it/s, est. speed input: 67524.89 toks/s, output: 65.94 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:34<00:32, 55.27it/s, est. speed input: 67362.08 toks/s, output: 65.78 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:35<00:32, 54.81it/s, est. speed input: 67153.81 toks/s, output: 65.58 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:35<00:32, 54.07it/s, est. speed input: 66923.67 toks/s, output: 65.36 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:36<00:32, 53.57it/s, est. speed input: 66702.22 toks/s, output: 65.14 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:36<00:31, 53.20it/s, est. speed input: 66485.78 toks/s, output: 64.93 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:37<00:31, 52.95it/s, est. speed input: 66277.20 toks/s, output: 64.72 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:40<01:13, 22.28it/s, est. speed input: 61614.88 toks/s, output: 60.17 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:41<00:59, 27.03it/s, est. speed input: 61521.20 toks/s, output: 60.08 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:42<00:49, 31.62it/s, est. speed input: 61406.71 toks/s, output: 59.97 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:42<00:42, 35.89it/s, est. speed input: 61295.83 toks/s, output: 59.86 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:43<00:37, 39.63it/s, est. speed input: 61187.91 toks/s, output: 59.75 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:44<00:34, 43.11it/s, est. speed input: 61112.11 toks/s, output: 59.68 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:44<00:31, 45.94it/s, est. speed input: 61038.17 toks/s, output: 59.61 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:45<00:29, 48.15it/s, est. speed input: 60966.43 toks/s, output: 59.54 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:45<00:27, 49.88it/s, est. speed input: 60899.41 toks/s, output: 59.47 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:46<00:26, 51.49it/s, est. speed input: 60851.66 toks/s, output: 59.43 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:46<00:24, 52.61it/s, est. speed input: 60801.25 toks/s, output: 59.38 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:47<00:23, 53.47it/s, est. speed input: 60753.93 toks/s, output: 59.33 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:48<00:23, 54.08it/s, est. speed input: 60707.86 toks/s, output: 59.28 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:48<00:22, 54.51it/s, est. speed input: 60662.38 toks/s, output: 59.24 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:49<00:21, 54.82it/s, est. speed input: 60618.10 toks/s, output: 59.20 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:49<00:20, 55.03it/s, est. speed input: 60574.89 toks/s, output: 59.16 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:50<00:20, 55.19it/s, est. speed input: 60532.62 toks/s, output: 59.11 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:50<00:19, 55.30it/s, est. speed input: 60491.49 toks/s, output: 59.07 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:51<00:19, 55.38it/s, est. speed input: 60451.29 toks/s, output: 59.03 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:52<00:18, 55.43it/s, est. speed input: 60411.73 toks/s, output: 59.00 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:52<00:17, 55.47it/s, est. speed input: 60373.33 toks/s, output: 58.96 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:53<00:17, 55.50it/s, est. speed input: 60336.07 toks/s, output: 58.92 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:53<00:16, 55.58it/s, est. speed input: 60301.68 toks/s, output: 58.89 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:54<00:16, 55.79it/s, est. speed input: 60273.73 toks/s, output: 58.86 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:54<00:15, 55.94it/s, est. speed input: 60246.39 toks/s, output: 58.83 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:55<00:14, 56.03it/s, est. speed input: 60219.30 toks/s, output: 58.81 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:56<00:14, 56.11it/s, est. speed input: 60193.19 toks/s, output: 58.78 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:56<00:13, 56.17it/s, est. speed input: 60167.66 toks/s, output: 58.76 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:57<00:13, 56.20it/s, est. speed input: 60142.30 toks/s, output: 58.73 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:57<00:12, 56.22it/s, est. speed input: 60117.56 toks/s, output: 58.71 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:58<00:11, 56.24it/s, est. speed input: 60093.54 toks/s, output: 58.69 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:58<00:11, 56.26it/s, est. speed input: 60069.93 toks/s, output: 58.66 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:59<00:10, 56.27it/s, est. speed input: 60046.66 toks/s, output: 58.64 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:00<00:10, 56.27it/s, est. speed input: 60023.86 toks/s, output: 58.62 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:00<00:09, 56.33it/s, est. speed input: 60003.38 toks/s, output: 58.60 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:01<00:09, 56.27it/s, est. speed input: 59979.95 toks/s, output: 58.57 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:01<00:08, 56.23it/s, est. speed input: 59956.89 toks/s, output: 58.55 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:02<00:07, 55.97it/s, est. speed input: 59926.59 toks/s, output: 58.52 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:02<00:07, 55.79it/s, est. speed input: 59897.21 toks/s, output: 58.49 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:03<00:06, 56.11it/s, est. speed input: 59882.46 toks/s, output: 58.48 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:04<00:06, 55.89it/s, est. speed input: 59853.90 toks/s, output: 58.45 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:04<00:05, 55.73it/s, est. speed input: 59825.58 toks/s, output: 58.42 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:05<00:05, 55.62it/s, est. speed input: 59797.89 toks/s, output: 58.40 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:05<00:04, 55.54it/s, est. speed input: 59770.59 toks/s, output: 58.37 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:06<00:03, 55.54it/s, est. speed input: 59745.49 toks/s, output: 58.35 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:06<00:03, 55.45it/s, est. speed input: 59718.04 toks/s, output: 58.32 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:07<00:02, 55.43it/s, est. speed input: 59692.16 toks/s, output: 58.29 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:08<00:02, 55.40it/s, est. speed input: 59666.49 toks/s, output: 58.27 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:08<00:01, 55.36it/s, est. speed input: 59640.42 toks/s, output: 58.24 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:09<00:01, 55.79it/s, est. speed input: 59628.59 toks/s, output: 58.23 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:09<00:00, 56.00it/s, est. speed input: 59614.32 toks/s, output: 58.22 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:09<00:00, 56.00it/s, est. speed input: 60053.74 toks/s, output: 58.65 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:09<00:00, 58.65it/s, est. speed input: 60053.74 toks/s, output: 58.65 toks/s]
[rank0]:[W128 12:09:25.624669599 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 161.5s

测试结果:
  Requests/s:   55.07
  Tokens/s:     56447.19
  Total Reqs:   4096
  Elapsed:      74.38s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     56392.12


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,16.1283,8273.8014,7.9364
1024,1024,1,128,128,15.9070,16304.7055,8.0468
2048,1024,2,256,128,32.5583,33372.2512,7.8628
4096,1024,4,512,128,53.5419,54880.4837,9.5626
8192,1024,8,1024,128,53.5988,54938.7848,19.1049
16384,1024,16,2048,128,54.2631,55619.6939,37.7420
32768,1024,32,4096,128,55.0704,56447.1919,74.3775

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:09:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:09:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=43548) WARNING 01-28 12:09:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=43548) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=43548) WARNING 01-28 12:10:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.65 requests/s, 8541.60 total tokens/s, 16.65 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 12:09:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:09:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:09:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:09:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:09:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:09:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:09:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:09:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:09:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:09:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:09:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:09:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:09:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:09:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:09:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:09:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:09:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:09:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:09:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 12:09:55.071706305 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=43548) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=43548) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.38s/it]
(EngineCore_DP0 pid=43548) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.38s/it]
(EngineCore_DP0 pid=43548) 
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=43548) [2026-01-28 12:09:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=43548) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=43548) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.78it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  53%|█████▎    | 68/128 [00:00<00:00, 669.92it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 683.48it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:19,  6.65it/s, est. speed input: 3408.01 toks/s, output: 6.66 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:10, 12.08it/s, est. speed input: 5721.20 toks/s, output: 11.17 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 14.30it/s, est. speed input: 6665.86 toks/s, output: 13.02 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:07, 15.48it/s, est. speed input: 7184.59 toks/s, output: 14.03 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 16.08it/s, est. speed input: 7489.89 toks/s, output: 14.63 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 16.55it/s, est. speed input: 7720.84 toks/s, output: 15.08 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:06, 16.88it/s, est. speed input: 7892.52 toks/s, output: 15.41 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:06, 17.09it/s, est. speed input: 8022.68 toks/s, output: 15.67 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 17.22it/s, est. speed input: 8123.83 toks/s, output: 15.87 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 17.33it/s, est. speed input: 8207.29 toks/s, output: 16.03 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.39it/s, est. speed input: 8275.24 toks/s, output: 16.16 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 17.33it/s, est. speed input: 8318.53 toks/s, output: 16.25 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.40it/s, est. speed input: 8369.01 toks/s, output: 16.35 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.47it/s, est. speed input: 8414.66 toks/s, output: 16.43 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.41it/s, est. speed input: 8442.77 toks/s, output: 16.49 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.33it/s, est. speed input: 8463.16 toks/s, output: 16.53 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.29it/s, est. speed input: 8483.92 toks/s, output: 16.57 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 17.30it/s, est. speed input: 8505.11 toks/s, output: 16.61 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 17.34it/s, est. speed input: 8526.42 toks/s, output: 16.65 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.36it/s, est. speed input: 8545.15 toks/s, output: 16.69 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 17.37it/s, est. speed input: 8562.42 toks/s, output: 16.72 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.31it/s, est. speed input: 8573.03 toks/s, output: 16.74 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.27it/s, est. speed input: 8582.35 toks/s, output: 16.76 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.25it/s, est. speed input: 8591.90 toks/s, output: 16.78 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.30it/s, est. speed input: 8604.79 toks/s, output: 16.81 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 17.32it/s, est. speed input: 8615.61 toks/s, output: 16.83 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 17.33it/s, est. speed input: 8625.48 toks/s, output: 16.85 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 17.36it/s, est. speed input: 8636.17 toks/s, output: 16.87 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 17.30it/s, est. speed input: 8641.45 toks/s, output: 16.88 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.31it/s, est. speed input: 8648.88 toks/s, output: 16.89 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.30it/s, est. speed input: 8655.22 toks/s, output: 16.90 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.25it/s, est. speed input: 8658.75 toks/s, output: 16.91 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 17.26it/s, est. speed input: 8664.52 toks/s, output: 16.92 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 17.21it/s, est. speed input: 8666.97 toks/s, output: 16.93 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 17.24it/s, est. speed input: 8672.76 toks/s, output: 16.94 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 17.24it/s, est. speed input: 8677.13 toks/s, output: 16.95 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 17.30it/s, est. speed input: 8683.63 toks/s, output: 16.96 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.32it/s, est. speed input: 8689.00 toks/s, output: 16.97 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.28it/s, est. speed input: 8692.09 toks/s, output: 16.98 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.17it/s, est. speed input: 8691.27 toks/s, output: 16.98 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.17it/s, est. speed input: 8693.81 toks/s, output: 16.98 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.25it/s, est. speed input: 8699.36 toks/s, output: 16.99 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 17.22it/s, est. speed input: 8701.27 toks/s, output: 16.99 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 17.25it/s, est. speed input: 8705.00 toks/s, output: 17.00 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 17.31it/s, est. speed input: 8709.87 toks/s, output: 17.01 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.36it/s, est. speed input: 8714.92 toks/s, output: 17.02 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.37it/s, est. speed input: 8719.14 toks/s, output: 17.03 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.37it/s, est. speed input: 8722.78 toks/s, output: 17.04 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.39it/s, est. speed input: 8726.81 toks/s, output: 17.04 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 17.42it/s, est. speed input: 8731.21 toks/s, output: 17.05 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.38it/s, est. speed input: 8733.49 toks/s, output: 17.06 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 17.33it/s, est. speed input: 8735.08 toks/s, output: 17.06 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 17.36it/s, est. speed input: 8738.71 toks/s, output: 17.07 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 17.38it/s, est. speed input: 8742.14 toks/s, output: 17.07 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 17.42it/s, est. speed input: 8745.99 toks/s, output: 17.08 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.45it/s, est. speed input: 8750.00 toks/s, output: 17.09 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.42it/s, est. speed input: 8752.39 toks/s, output: 17.09 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.35it/s, est. speed input: 8753.16 toks/s, output: 17.10 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 17.36it/s, est. speed input: 8755.63 toks/s, output: 17.10 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.37it/s, est. speed input: 8758.15 toks/s, output: 17.11 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 17.36it/s, est. speed input: 8759.97 toks/s, output: 17.11 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 17.20it/s, est. speed input: 8757.72 toks/s, output: 17.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.96it/s, est. speed input: 8752.00 toks/s, output: 17.09 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.67it/s, est. speed input: 8742.72 toks/s, output: 17.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.67it/s, est. speed input: 8739.42 toks/s, output: 17.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.07it/s, est. speed input: 8739.42 toks/s, output: 17.07 toks/s]
[rank0]:[W128 12:10:35.668856359 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.5s

测试结果:
  Requests/s:   16.65
  Tokens/s:     8541.60
  Total Reqs:   128
  Elapsed:      7.69s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8524.95

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:10:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:10:48 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=44233) WARNING 01-28 12:10:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=44233) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=44233) WARNING 01-28 12:11:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.13 requests/s, 17559.83 total tokens/s, 17.13 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 12:10:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:10:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:10:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:10:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:10:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:10:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:10:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:10:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:10:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:10:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:10:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:10:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:10:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:10:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:10:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:10:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:10:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:10:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:10:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=44233) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=44233) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=44233) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=44233) 
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=44233) [2026-01-28 12:10:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=44233) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.07it/s]
(EngineCore_DP0 pid=44233) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 374.80it/s]
Adding requests:  62%|██████▏   | 79/128 [00:00<00:00, 394.04it/s]
Adding requests:  93%|█████████▎| 119/128 [00:00<00:00, 394.33it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 393.56it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 55.53it/s, est. speed input: 56871.54 toks/s, output: 55.53 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 23.53it/s, est. speed input: 26374.59 toks/s, output: 25.76 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:05, 20.75it/s, est. speed input: 23428.97 toks/s, output: 22.88 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:05, 19.71it/s, est. speed input: 22330.20 toks/s, output: 21.81 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:01<00:05, 19.03it/s, est. speed input: 21606.32 toks/s, output: 21.10 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 18.49it/s, est. speed input: 21045.81 toks/s, output: 20.55 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 18.24it/s, est. speed input: 20767.13 toks/s, output: 20.28 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 18.07it/s, est. speed input: 20546.95 toks/s, output: 20.07 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.87it/s, est. speed input: 20335.54 toks/s, output: 19.86 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.73it/s, est. speed input: 20158.89 toks/s, output: 19.69 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.66it/s, est. speed input: 20015.71 toks/s, output: 19.55 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:05, 17.60it/s, est. speed input: 19886.03 toks/s, output: 19.42 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.58it/s, est. speed input: 19777.52 toks/s, output: 19.31 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:04, 17.58it/s, est. speed input: 19682.65 toks/s, output: 19.22 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.56it/s, est. speed input: 19594.25 toks/s, output: 19.13 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.57it/s, est. speed input: 19518.26 toks/s, output: 19.06 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.58it/s, est. speed input: 19448.59 toks/s, output: 18.99 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.34it/s, est. speed input: 19345.48 toks/s, output: 18.89 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 17.17it/s, est. speed input: 19250.60 toks/s, output: 18.80 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 17.23it/s, est. speed input: 19191.78 toks/s, output: 18.74 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:02<00:04, 17.23it/s, est. speed input: 19130.48 toks/s, output: 18.68 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 17.26it/s, est. speed input: 19077.99 toks/s, output: 18.63 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:03, 17.25it/s, est. speed input: 19025.86 toks/s, output: 18.58 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 17.22it/s, est. speed input: 18974.21 toks/s, output: 18.53 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 17.26it/s, est. speed input: 18933.21 toks/s, output: 18.49 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 17.33it/s, est. speed input: 18900.45 toks/s, output: 18.46 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 17.35it/s, est. speed input: 18865.61 toks/s, output: 18.42 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 17.42it/s, est. speed input: 18839.50 toks/s, output: 18.40 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:03<00:03, 17.42it/s, est. speed input: 18809.56 toks/s, output: 18.37 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:03<00:03, 17.42it/s, est. speed input: 18781.41 toks/s, output: 18.34 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 17.42it/s, est. speed input: 18755.40 toks/s, output: 18.32 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:02, 17.48it/s, est. speed input: 18736.02 toks/s, output: 18.30 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 17.47it/s, est. speed input: 18712.72 toks/s, output: 18.27 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 17.35it/s, est. speed input: 18680.72 toks/s, output: 18.24 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 17.28it/s, est. speed input: 18651.23 toks/s, output: 18.21 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 17.13it/s, est. speed input: 18614.29 toks/s, output: 18.18 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:02, 17.14it/s, est. speed input: 18589.10 toks/s, output: 18.15 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:04<00:02, 17.22it/s, est. speed input: 18571.16 toks/s, output: 18.14 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 17.23it/s, est. speed input: 18550.19 toks/s, output: 18.12 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 17.33it/s, est. speed input: 18538.02 toks/s, output: 18.10 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 17.30it/s, est. speed input: 18517.96 toks/s, output: 18.08 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 17.35it/s, est. speed input: 18504.88 toks/s, output: 18.07 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 17.39it/s, est. speed input: 18492.24 toks/s, output: 18.06 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 17.42it/s, est. speed input: 18480.38 toks/s, output: 18.05 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 17.45it/s, est. speed input: 18469.51 toks/s, output: 18.04 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:05<00:01, 17.38it/s, est. speed input: 18453.11 toks/s, output: 18.02 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:05<00:01, 17.34it/s, est. speed input: 18437.77 toks/s, output: 18.01 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 17.33it/s, est. speed input: 18423.89 toks/s, output: 17.99 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:00, 17.38it/s, est. speed input: 18414.43 toks/s, output: 17.98 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 17.42it/s, est. speed input: 18405.89 toks/s, output: 17.97 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 17.48it/s, est. speed input: 18399.28 toks/s, output: 17.97 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 17.51it/s, est. speed input: 18392.72 toks/s, output: 17.96 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.52it/s, est. speed input: 18385.06 toks/s, output: 17.95 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:06<00:00, 17.36it/s, est. speed input: 18368.21 toks/s, output: 17.94 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:06<00:00, 17.39it/s, est. speed input: 18360.03 toks/s, output: 17.93 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:06<00:00, 17.41it/s, est. speed input: 18352.11 toks/s, output: 17.92 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 17.46it/s, est. speed input: 18346.27 toks/s, output: 17.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.46it/s, est. speed input: 18344.98 toks/s, output: 17.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.91it/s, est. speed input: 18344.98 toks/s, output: 17.91 toks/s]
[rank0]:[W128 12:11:31.408559921 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.3s

测试结果:
  Requests/s:   17.13
  Tokens/s:     17559.83
  Total Reqs:   128
  Elapsed:      7.47s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     17542.70

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:11:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:11:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=44796) WARNING 01-28 12:11:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=44796) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=44796) WARNING 01-28 12:12:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.59 requests/s, 31352.23 total tokens/s, 30.59 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 12:11:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:11:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:11:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:11:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:11:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:11:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:11:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:11:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:11:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:11:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:11:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:11:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:11:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:11:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:11:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:11:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:11:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:11:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:11:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=44796) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=44796) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.53s/it]
(EngineCore_DP0 pid=44796) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.53s/it]
(EngineCore_DP0 pid=44796) 
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=44796) [2026-01-28 12:11:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=44796) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.78it/s]
(EngineCore_DP0 pid=44796) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.11it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  15%|█▍        | 38/256 [00:00<00:00, 377.29it/s]
Adding requests:  30%|██▉       | 76/256 [00:00<00:00, 373.71it/s]
Adding requests:  45%|████▌     | 116/256 [00:00<00:00, 383.23it/s]
Adding requests:  61%|██████    | 155/256 [00:00<00:00, 384.00it/s]
Adding requests:  76%|███████▌  | 194/256 [00:00<00:00, 378.45it/s]
Adding requests:  92%|█████████▏| 235/256 [00:00<00:00, 387.94it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 385.27it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 177.64it/s, est. speed input: 181938.74 toks/s, output: 177.65 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:04, 51.22it/s, est. speed input: 58719.09 toks/s, output: 57.34 toks/s]   
Processed prompts:  18%|█▊        | 46/256 [00:00<00:04, 44.13it/s, est. speed input: 51112.94 toks/s, output: 49.91 toks/s]
Processed prompts:  21%|██        | 53/256 [00:01<00:04, 41.69it/s, est. speed input: 48517.57 toks/s, output: 47.38 toks/s]
Processed prompts:  23%|██▎       | 59/256 [00:01<00:05, 38.25it/s, est. speed input: 45792.03 toks/s, output: 44.72 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:05, 34.38it/s, est. speed input: 43149.61 toks/s, output: 42.14 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:05, 33.43it/s, est. speed input: 42147.30 toks/s, output: 41.16 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:05, 32.83it/s, est. speed input: 41381.34 toks/s, output: 40.41 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:05, 32.38it/s, est. speed input: 40730.79 toks/s, output: 39.78 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:02<00:05, 32.05it/s, est. speed input: 40172.11 toks/s, output: 39.23 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:02<00:05, 31.74it/s, est. speed input: 39663.50 toks/s, output: 38.73 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:05, 31.49it/s, est. speed input: 39208.73 toks/s, output: 38.29 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:05, 31.33it/s, est. speed input: 38806.15 toks/s, output: 37.90 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:05, 31.27it/s, est. speed input: 38457.43 toks/s, output: 37.56 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:05, 31.08it/s, est. speed input: 38113.92 toks/s, output: 37.22 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 31.05it/s, est. speed input: 37820.01 toks/s, output: 36.93 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:04, 30.83it/s, est. speed input: 37516.33 toks/s, output: 36.64 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:03<00:04, 30.75it/s, est. speed input: 37251.97 toks/s, output: 36.38 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:03<00:04, 30.54it/s, est. speed input: 36983.89 toks/s, output: 36.12 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:03<00:04, 30.43it/s, est. speed input: 36743.33 toks/s, output: 35.88 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:04, 30.47it/s, est. speed input: 36537.36 toks/s, output: 35.68 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:04, 30.37it/s, est. speed input: 36329.43 toks/s, output: 35.48 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:04, 30.26it/s, est. speed input: 36129.69 toks/s, output: 35.28 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 30.32it/s, est. speed input: 35961.90 toks/s, output: 35.12 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:04<00:03, 30.39it/s, est. speed input: 35808.96 toks/s, output: 34.97 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:04<00:03, 30.30it/s, est. speed input: 35649.03 toks/s, output: 34.81 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:04<00:03, 30.25it/s, est. speed input: 35500.06 toks/s, output: 34.67 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:04<00:03, 30.15it/s, est. speed input: 35352.04 toks/s, output: 34.52 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:04<00:03, 30.14it/s, est. speed input: 35219.62 toks/s, output: 34.39 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:03, 30.17it/s, est. speed input: 35099.50 toks/s, output: 34.28 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:03, 30.21it/s, est. speed input: 34987.83 toks/s, output: 34.17 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 30.30it/s, est. speed input: 34887.63 toks/s, output: 34.07 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:05<00:02, 30.38it/s, est. speed input: 34795.39 toks/s, output: 33.98 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:05<00:02, 30.40it/s, est. speed input: 34704.21 toks/s, output: 33.89 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:05<00:02, 30.27it/s, est. speed input: 34603.16 toks/s, output: 33.79 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:05<00:02, 30.22it/s, est. speed input: 34511.80 toks/s, output: 33.70 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:05<00:02, 30.30it/s, est. speed input: 34434.06 toks/s, output: 33.63 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:05<00:02, 30.18it/s, est. speed input: 34345.04 toks/s, output: 33.54 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 30.23it/s, est. speed input: 34271.13 toks/s, output: 33.47 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 30.32it/s, est. speed input: 34205.83 toks/s, output: 33.40 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:06<00:01, 30.41it/s, est. speed input: 34144.46 toks/s, output: 33.34 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:06<00:01, 30.42it/s, est. speed input: 34082.47 toks/s, output: 33.28 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:06<00:01, 30.47it/s, est. speed input: 34025.66 toks/s, output: 33.23 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:06<00:01, 31.29it/s, est. speed input: 34028.89 toks/s, output: 33.23 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:06<00:01, 31.95it/s, est. speed input: 34035.68 toks/s, output: 33.24 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:06<00:00, 32.36it/s, est. speed input: 34037.57 toks/s, output: 33.24 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:06<00:00, 32.75it/s, est. speed input: 34046.10 toks/s, output: 33.25 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:06<00:00, 33.02it/s, est. speed input: 34053.17 toks/s, output: 33.25 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:09<00:04,  4.20it/s, est. speed input: 24483.01 toks/s, output: 23.91 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:09<00:02,  5.69it/s, est. speed input: 24596.62 toks/s, output: 24.02 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:10<00:01,  7.55it/s, est. speed input: 24700.52 toks/s, output: 24.12 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:10<00:00,  9.81it/s, est. speed input: 24802.48 toks/s, output: 24.22 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:10<00:00, 12.45it/s, est. speed input: 24911.99 toks/s, output: 24.33 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:10<00:00, 15.34it/s, est. speed input: 25017.90 toks/s, output: 24.43 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:10<00:00, 15.34it/s, est. speed input: 25017.90 toks/s, output: 24.43 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:10<00:00, 24.43it/s, est. speed input: 25017.90 toks/s, output: 24.43 toks/s]
[rank0]:[W128 12:12:29.238732923 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.5s

测试结果:
  Requests/s:   30.59
  Tokens/s:     31352.23
  Total Reqs:   256
  Elapsed:      8.37s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     31321.65

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:12:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:12:41 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=45377) WARNING 01-28 12:12:48 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=45377) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=45377) WARNING 01-28 12:12:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.58 requests/s, 53897.03 total tokens/s, 52.58 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 12:12:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:12:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:12:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:12:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:12:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:12:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:12:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:12:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:12:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:12:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:12:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:12:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:12:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:12:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:12:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:12:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:12:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:12:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:12:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=45377) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=45377) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=45377) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]
(EngineCore_DP0 pid=45377) 
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=45377) [2026-01-28 12:12:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=45377) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  7.84it/s]
(EngineCore_DP0 pid=45377) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.11it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  7.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.15it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 37/512 [00:00<00:01, 365.68it/s]
Adding requests:  15%|█▌        | 77/512 [00:00<00:01, 382.92it/s]
Adding requests:  23%|██▎       | 117/512 [00:00<00:01, 388.23it/s]
Adding requests:  30%|███       | 156/512 [00:00<00:00, 380.20it/s]
Adding requests:  38%|███▊      | 195/512 [00:00<00:00, 377.69it/s]
Adding requests:  46%|████▌     | 236/512 [00:00<00:00, 386.16it/s]
Adding requests:  54%|█████▎    | 275/512 [00:00<00:00, 387.38it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 385.98it/s]
Adding requests:  69%|██████▉   | 354/512 [00:00<00:00, 388.88it/s]
Adding requests:  77%|███████▋  | 393/512 [00:01<00:00, 387.42it/s]
Adding requests:  84%|████████▍ | 432/512 [00:01<00:00, 385.43it/s]
Adding requests:  92%|█████████▏| 471/512 [00:01<00:00, 384.24it/s]
Adding requests: 100%|█████████▉| 510/512 [00:01<00:00, 377.81it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 383.01it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:01, 427.87it/s, est. speed input: 438217.97 toks/s, output: 427.89 toks/s]
Processed prompts:  21%|██        | 105/512 [00:00<00:03, 103.12it/s, est. speed input: 121999.38 toks/s, output: 119.14 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:01<00:04, 79.94it/s, est. speed input: 98031.35 toks/s, output: 95.73 toks/s]   
Processed prompts:  28%|██▊       | 141/512 [00:01<00:04, 76.09it/s, est. speed input: 93326.34 toks/s, output: 91.14 toks/s]
Processed prompts:  30%|██▉       | 152/512 [00:01<00:05, 69.54it/s, est. speed input: 88061.90 toks/s, output: 86.00 toks/s]
Processed prompts:  31%|███▏      | 161/512 [00:01<00:05, 67.92it/s, est. speed input: 86103.96 toks/s, output: 84.09 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:02<00:05, 65.00it/s, est. speed input: 83934.06 toks/s, output: 81.97 toks/s]
Processed prompts:  35%|███▍      | 177/512 [00:02<00:05, 62.48it/s, est. speed input: 82048.12 toks/s, output: 80.12 toks/s]
Processed prompts:  36%|███▌      | 184/512 [00:02<00:05, 58.71it/s, est. speed input: 79961.68 toks/s, output: 78.09 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:02<00:05, 53.94it/s, est. speed input: 77708.89 toks/s, output: 75.89 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:02<00:05, 54.04it/s, est. speed input: 76479.80 toks/s, output: 74.69 toks/s]
Processed prompts:  40%|████      | 206/512 [00:02<00:05, 53.99it/s, est. speed input: 75353.45 toks/s, output: 73.58 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:02<00:05, 54.02it/s, est. speed input: 74348.04 toks/s, output: 72.60 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:03<00:05, 54.11it/s, est. speed input: 73457.20 toks/s, output: 71.74 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:03<00:05, 54.15it/s, est. speed input: 72643.10 toks/s, output: 70.94 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:03<00:05, 54.20it/s, est. speed input: 71903.33 toks/s, output: 70.22 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:03<00:04, 54.25it/s, est. speed input: 71228.64 toks/s, output: 69.56 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:03<00:04, 54.31it/s, est. speed input: 70610.58 toks/s, output: 68.96 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:03<00:04, 54.30it/s, est. speed input: 70032.07 toks/s, output: 68.39 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:03<00:04, 54.30it/s, est. speed input: 69497.91 toks/s, output: 67.87 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:04<00:04, 54.30it/s, est. speed input: 69001.60 toks/s, output: 67.38 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:04<00:04, 54.32it/s, est. speed input: 68542.55 toks/s, output: 66.94 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:04<00:04, 54.20it/s, est. speed input: 68095.65 toks/s, output: 66.50 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:04<00:03, 54.23it/s, est. speed input: 67692.70 toks/s, output: 66.11 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:04<00:03, 54.27it/s, est. speed input: 67317.47 toks/s, output: 65.74 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:04<00:03, 54.30it/s, est. speed input: 66965.67 toks/s, output: 65.40 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:05<00:03, 54.30it/s, est. speed input: 66630.70 toks/s, output: 65.07 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:05<00:03, 54.27it/s, est. speed input: 66312.36 toks/s, output: 64.76 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:05<00:03, 54.13it/s, est. speed input: 65998.32 toks/s, output: 64.45 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:05<00:02, 54.13it/s, est. speed input: 65711.41 toks/s, output: 64.17 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:05<00:02, 54.09it/s, est. speed input: 65436.08 toks/s, output: 63.90 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:05<00:02, 54.12it/s, est. speed input: 65181.17 toks/s, output: 63.65 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:05<00:02, 54.20it/s, est. speed input: 64944.30 toks/s, output: 63.42 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:06<00:02, 54.23it/s, est. speed input: 64716.89 toks/s, output: 63.20 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:06<00:02, 54.22it/s, est. speed input: 64497.11 toks/s, output: 62.99 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:06<00:02, 54.22it/s, est. speed input: 64287.75 toks/s, output: 62.78 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:06<00:01, 54.22it/s, est. speed input: 64088.18 toks/s, output: 62.59 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:06<00:01, 54.23it/s, est. speed input: 63899.16 toks/s, output: 62.40 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:06<00:01, 54.25it/s, est. speed input: 63718.64 toks/s, output: 62.22 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:06<00:01, 54.17it/s, est. speed input: 63537.71 toks/s, output: 62.05 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:07<00:01, 54.14it/s, est. speed input: 63367.03 toks/s, output: 61.88 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:07<00:01, 54.13it/s, est. speed input: 63204.52 toks/s, output: 61.72 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:07<00:01, 54.16it/s, est. speed input: 63050.83 toks/s, output: 61.57 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:07<00:00, 54.18it/s, est. speed input: 62903.36 toks/s, output: 61.43 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:07<00:00, 54.20it/s, est. speed input: 62761.49 toks/s, output: 61.29 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:07<00:00, 54.20it/s, est. speed input: 62624.69 toks/s, output: 61.16 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:07<00:00, 54.20it/s, est. speed input: 62492.39 toks/s, output: 61.03 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:08<00:00, 54.11it/s, est. speed input: 62359.27 toks/s, output: 60.90 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:08<00:00, 54.07it/s, est. speed input: 62231.60 toks/s, output: 60.77 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:08<00:00, 55.14it/s, est. speed input: 62181.73 toks/s, output: 60.72 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 55.14it/s, est. speed input: 62424.24 toks/s, output: 60.96 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 60.96it/s, est. speed input: 62424.24 toks/s, output: 60.96 toks/s]
[rank0]:[W128 12:13:27.640452792 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.9s

测试结果:
  Requests/s:   52.58
  Tokens/s:     53897.03
  Total Reqs:   512
  Elapsed:      9.74s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     53844.44

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:13:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:13:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=45993) WARNING 01-28 12:13:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=45993) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=45993) WARNING 01-28 12:14:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.41 requests/s, 52698.40 total tokens/s, 51.41 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 12:13:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:13:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:13:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:13:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:13:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:13:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:13:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:13:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:13:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:13:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:13:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:13:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:13:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:13:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:13:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:13:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:13:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:13:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:13:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:52] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=45993) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=45993) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=45993) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=45993) 
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=45993) [2026-01-28 12:13:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=45993) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  6.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  7.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.75it/s]
(EngineCore_DP0 pid=45993) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.19it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.42it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 357.59it/s]
Adding requests:   7%|▋         | 75/1024 [00:00<00:02, 375.59it/s]
Adding requests:  11%|█         | 114/1024 [00:00<00:02, 381.62it/s]
Adding requests:  15%|█▍        | 153/1024 [00:00<00:02, 378.39it/s]
Adding requests:  19%|█▉        | 192/1024 [00:00<00:02, 380.67it/s]
Adding requests:  23%|██▎       | 232/1024 [00:00<00:02, 386.83it/s]
Adding requests:  27%|██▋       | 272/1024 [00:00<00:01, 389.96it/s]
Adding requests:  30%|███       | 312/1024 [00:00<00:01, 388.47it/s]
Adding requests:  34%|███▍      | 351/1024 [00:00<00:01, 387.93it/s]
Adding requests:  38%|███▊      | 390/1024 [00:01<00:01, 385.62it/s]
Adding requests:  42%|████▏     | 429/1024 [00:01<00:01, 385.57it/s]
Adding requests:  46%|████▌     | 468/1024 [00:01<00:01, 382.34it/s]
Adding requests:  50%|████▉     | 507/1024 [00:01<00:01, 379.96it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 371.07it/s]
Adding requests:  57%|█████▋    | 586/1024 [00:01<00:01, 378.20it/s]
Adding requests:  61%|██████    | 626/1024 [00:01<00:01, 383.33it/s]
Adding requests:  65%|██████▌   | 667/1024 [00:01<00:00, 389.47it/s]
Adding requests:  69%|██████▉   | 709/1024 [00:01<00:00, 398.27it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:01<00:00, 397.19it/s]
Adding requests:  77%|███████▋  | 789/1024 [00:02<00:00, 396.41it/s]
Adding requests:  81%|████████  | 829/1024 [00:02<00:00, 390.74it/s]
Adding requests:  85%|████████▍ | 870/1024 [00:02<00:00, 395.77it/s]
Adding requests:  89%|████████▉ | 911/1024 [00:02<00:00, 398.33it/s]
Adding requests:  93%|█████████▎| 951/1024 [00:02<00:00, 397.47it/s]
Adding requests:  97%|█████████▋| 991/1024 [00:02<00:00, 398.11it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 388.88it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:00<00:01, 577.16it/s, est. speed input: 591095.39 toks/s, output: 577.20 toks/s]
Processed prompts:  18%|█▊        | 188/1024 [00:01<00:06, 121.05it/s, est. speed input: 148258.27 toks/s, output: 144.78 toks/s]
Processed prompts:  21%|██        | 215/1024 [00:01<00:08, 99.32it/s, est. speed input: 125170.84 toks/s, output: 122.24 toks/s] 
Processed prompts:  23%|██▎       | 233/1024 [00:02<00:08, 89.20it/s, est. speed input: 115509.59 toks/s, output: 112.80 toks/s]
Processed prompts:  24%|██▍       | 246/1024 [00:02<00:10, 76.92it/s, est. speed input: 106183.83 toks/s, output: 103.69 toks/s]
Processed prompts:  25%|██▌       | 256/1024 [00:02<00:10, 75.07it/s, est. speed input: 103788.19 toks/s, output: 101.36 toks/s]
Processed prompts:  26%|██▌       | 265/1024 [00:02<00:10, 72.06it/s, est. speed input: 101288.34 toks/s, output: 98.91 toks/s] 
Processed prompts:  27%|██▋       | 273/1024 [00:02<00:11, 67.91it/s, est. speed input: 98692.94 toks/s, output: 96.38 toks/s] 
Processed prompts:  27%|██▋       | 280/1024 [00:02<00:11, 62.81it/s, est. speed input: 96025.98 toks/s, output: 93.78 toks/s]
Processed prompts:  28%|██▊       | 287/1024 [00:03<00:12, 58.57it/s, est. speed input: 93617.06 toks/s, output: 91.42 toks/s]
Processed prompts:  29%|██▊       | 293/1024 [00:03<00:13, 53.49it/s, est. speed input: 91119.34 toks/s, output: 88.98 toks/s]
Processed prompts:  29%|██▉       | 299/1024 [00:03<00:14, 49.62it/s, est. speed input: 88854.50 toks/s, output: 86.77 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:03<00:14, 48.50it/s, est. speed input: 87056.56 toks/s, output: 85.02 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:03<00:14, 49.54it/s, est. speed input: 85681.96 toks/s, output: 83.67 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:03<00:13, 50.31it/s, est. speed input: 84416.09 toks/s, output: 82.44 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:04<00:13, 50.79it/s, est. speed input: 83232.87 toks/s, output: 81.28 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:04<00:13, 51.17it/s, est. speed input: 82142.42 toks/s, output: 80.22 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:04<00:13, 51.44it/s, est. speed input: 81128.52 toks/s, output: 79.23 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:04<00:12, 51.64it/s, est. speed input: 80186.01 toks/s, output: 78.31 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:04<00:12, 51.79it/s, est. speed input: 79306.51 toks/s, output: 77.45 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:04<00:12, 51.86it/s, est. speed input: 78477.82 toks/s, output: 76.64 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:04<00:12, 51.94it/s, est. speed input: 77704.67 toks/s, output: 75.88 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:12, 51.99it/s, est. speed input: 76976.99 toks/s, output: 75.17 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:05<00:12, 52.03it/s, est. speed input: 76291.29 toks/s, output: 74.50 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:05<00:11, 52.07it/s, est. speed input: 75646.78 toks/s, output: 73.87 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:11, 52.08it/s, est. speed input: 75035.62 toks/s, output: 73.28 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:11, 52.10it/s, est. speed input: 74458.11 toks/s, output: 72.71 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:11, 52.13it/s, est. speed input: 73911.96 toks/s, output: 72.18 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:11, 52.13it/s, est. speed input: 73391.48 toks/s, output: 71.67 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:11, 52.16it/s, est. speed input: 72900.69 toks/s, output: 71.19 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:11, 52.16it/s, est. speed input: 72431.08 toks/s, output: 70.73 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:10, 52.15it/s, est. speed input: 71982.47 toks/s, output: 70.30 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:10, 52.13it/s, est. speed input: 71552.86 toks/s, output: 69.88 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:10, 52.10it/s, est. speed input: 71140.64 toks/s, output: 69.47 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:10, 52.06it/s, est. speed input: 70745.74 toks/s, output: 69.09 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:10, 52.02it/s, est. speed input: 70365.45 toks/s, output: 68.72 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:10, 52.06it/s, est. speed input: 70008.37 toks/s, output: 68.37 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:09, 52.08it/s, est. speed input: 69665.09 toks/s, output: 68.03 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:09, 52.09it/s, est. speed input: 69335.72 toks/s, output: 67.71 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:09, 52.08it/s, est. speed input: 69017.35 toks/s, output: 67.40 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:09, 52.04it/s, est. speed input: 68709.27 toks/s, output: 67.10 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:09, 52.01it/s, est. speed input: 68412.02 toks/s, output: 66.81 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:09, 51.99it/s, est. speed input: 68126.35 toks/s, output: 66.53 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:09, 52.00it/s, est. speed input: 67853.55 toks/s, output: 66.26 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:08, 52.02it/s, est. speed input: 67590.75 toks/s, output: 66.01 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 51.99it/s, est. speed input: 67334.87 toks/s, output: 65.76 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:08, 51.99it/s, est. speed input: 67088.62 toks/s, output: 65.52 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:08, 51.96it/s, est. speed input: 66849.06 toks/s, output: 65.28 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:08, 51.99it/s, est. speed input: 66621.01 toks/s, output: 65.06 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:08, 51.95it/s, est. speed input: 66396.53 toks/s, output: 64.84 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:07, 51.95it/s, est. speed input: 66181.28 toks/s, output: 64.63 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:07, 51.93it/s, est. speed input: 65971.06 toks/s, output: 64.42 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 51.91it/s, est. speed input: 65767.65 toks/s, output: 64.23 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:07, 51.91it/s, est. speed input: 65571.54 toks/s, output: 64.03 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:10<00:07, 51.95it/s, est. speed input: 65383.48 toks/s, output: 63.85 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:07, 51.94it/s, est. speed input: 65199.00 toks/s, output: 63.67 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:07, 51.95it/s, est. speed input: 65020.91 toks/s, output: 63.50 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:06, 51.94it/s, est. speed input: 64847.50 toks/s, output: 63.33 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 51.95it/s, est. speed input: 64679.45 toks/s, output: 63.16 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 51.93it/s, est. speed input: 64514.97 toks/s, output: 63.00 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:06, 51.90it/s, est. speed input: 64354.34 toks/s, output: 62.85 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:06, 51.90it/s, est. speed input: 64198.76 toks/s, output: 62.69 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:06, 51.87it/s, est. speed input: 64046.55 toks/s, output: 62.55 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:05, 51.88it/s, est. speed input: 63899.52 toks/s, output: 62.40 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 51.87it/s, est. speed input: 63755.86 toks/s, output: 62.26 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 51.88it/s, est. speed input: 63616.67 toks/s, output: 62.13 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 51.86it/s, est. speed input: 63479.53 toks/s, output: 61.99 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:12<00:05, 51.84it/s, est. speed input: 63345.72 toks/s, output: 61.86 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:05, 51.82it/s, est. speed input: 63215.19 toks/s, output: 61.73 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:05, 51.83it/s, est. speed input: 63088.79 toks/s, output: 61.61 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:04, 51.77it/s, est. speed input: 62962.48 toks/s, output: 61.49 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 51.78it/s, est. speed input: 62841.63 toks/s, output: 61.37 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:12<00:04, 51.80it/s, est. speed input: 62724.44 toks/s, output: 61.25 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 51.82it/s, est. speed input: 62610.37 toks/s, output: 61.14 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:04, 51.75it/s, est. speed input: 62495.01 toks/s, output: 61.03 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:04, 51.74it/s, est. speed input: 62383.89 toks/s, output: 60.92 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 51.74it/s, est. speed input: 62275.81 toks/s, output: 60.82 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 51.75it/s, est. speed input: 62170.73 toks/s, output: 60.71 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 51.79it/s, est. speed input: 62069.51 toks/s, output: 60.61 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:13<00:03, 51.78it/s, est. speed input: 61968.72 toks/s, output: 60.52 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:14<00:03, 51.78it/s, est. speed input: 61870.44 toks/s, output: 60.42 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:03, 51.78it/s, est. speed input: 61774.43 toks/s, output: 60.33 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:03, 51.76it/s, est. speed input: 61679.52 toks/s, output: 60.23 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 51.80it/s, est. speed input: 61588.93 toks/s, output: 60.15 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 51.80it/s, est. speed input: 61499.09 toks/s, output: 60.06 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 51.78it/s, est. speed input: 61410.01 toks/s, output: 59.97 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:14<00:02, 51.78it/s, est. speed input: 61323.57 toks/s, output: 59.89 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:15<00:02, 51.74it/s, est. speed input: 61237.32 toks/s, output: 59.80 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:15<00:02, 51.72it/s, est. speed input: 61153.08 toks/s, output: 59.72 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:01, 51.72it/s, est. speed input: 61071.15 toks/s, output: 59.64 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 51.72it/s, est. speed input: 60990.82 toks/s, output: 59.56 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 51.70it/s, est. speed input: 60911.43 toks/s, output: 59.48 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:15<00:01, 51.68it/s, est. speed input: 60833.24 toks/s, output: 59.41 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:16<00:01, 51.71it/s, est. speed input: 60758.04 toks/s, output: 59.33 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:16<00:01, 51.73it/s, est. speed input: 60684.38 toks/s, output: 59.26 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:01, 51.74it/s, est. speed input: 60612.06 toks/s, output: 59.19 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 51.77it/s, est. speed input: 60541.64 toks/s, output: 59.12 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 53.41it/s, est. speed input: 60529.51 toks/s, output: 59.11 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:16<00:00, 52.88it/s, est. speed input: 60459.35 toks/s, output: 59.04 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:16<00:00, 52.50it/s, est. speed input: 60390.11 toks/s, output: 58.97 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:17<00:00, 52.23it/s, est. speed input: 60322.13 toks/s, output: 58.91 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:17<00:00, 53.98it/s, est. speed input: 60319.28 toks/s, output: 58.91 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 53.98it/s, est. speed input: 60674.00 toks/s, output: 59.25 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 59.25it/s, est. speed input: 60674.00 toks/s, output: 59.25 toks/s]
[rank0]:[W128 12:14:41.271331895 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 73.8s

测试结果:
  Requests/s:   51.41
  Tokens/s:     52698.40
  Total Reqs:   1024
  Elapsed:      19.92s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     52646.98

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:15:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:15:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=46749) WARNING 01-28 12:15:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=46749) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=46749) WARNING 01-28 12:15:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.81 requests/s, 53104.73 total tokens/s, 51.81 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 12:15:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:15:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:15:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:15:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:15:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:15:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:15:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:15:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:15:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:15:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:15:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:15:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:15:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:15:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:15:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:15:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:15:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:15:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:15:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=46749) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=46749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.53it/s]
(EngineCore_DP0 pid=46749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.53it/s]
(EngineCore_DP0 pid=46749) 
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=46749) [2026-01-28 12:15:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=46749) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.14it/s]
(EngineCore_DP0 pid=46749) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.00it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.54it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 37/2048 [00:00<00:05, 366.00it/s]
Adding requests:   4%|▍         | 77/2048 [00:00<00:05, 382.83it/s]
Adding requests:   6%|▌         | 117/2048 [00:00<00:04, 386.90it/s]
Adding requests:   8%|▊         | 156/2048 [00:00<00:04, 385.32it/s]
Adding requests:  10%|▉         | 195/2048 [00:00<00:04, 386.25it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:04, 395.34it/s]
Adding requests:  14%|█▎        | 277/2048 [00:00<00:04, 394.13it/s]
Adding requests:  15%|█▌        | 317/2048 [00:00<00:04, 394.16it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:04, 395.85it/s]
Adding requests:  19%|█▉        | 399/2048 [00:01<00:04, 398.34it/s]
Adding requests:  21%|██▏       | 440/2048 [00:01<00:04, 399.86it/s]
Adding requests:  23%|██▎       | 480/2048 [00:01<00:03, 399.10it/s]
Adding requests:  25%|██▌       | 520/2048 [00:01<00:03, 389.45it/s]
Adding requests:  27%|██▋       | 561/2048 [00:01<00:03, 393.67it/s]
Adding requests:  29%|██▉       | 601/2048 [00:01<00:03, 392.37it/s]
Adding requests:  31%|███▏      | 643/2048 [00:01<00:03, 398.99it/s]
Adding requests:  33%|███▎      | 685/2048 [00:01<00:03, 404.18it/s]
Adding requests:  35%|███▌      | 726/2048 [00:01<00:03, 403.07it/s]
Adding requests:  37%|███▋      | 767/2048 [00:01<00:03, 391.68it/s]
Adding requests:  39%|███▉      | 807/2048 [00:02<00:03, 388.88it/s]
Adding requests:  41%|████▏     | 846/2048 [00:02<00:03, 386.94it/s]
Adding requests:  43%|████▎     | 888/2048 [00:02<00:02, 396.53it/s]
Adding requests:  45%|████▌     | 931/2048 [00:02<00:02, 403.85it/s]
Adding requests:  48%|████▊     | 973/2048 [00:02<00:02, 406.45it/s]
Adding requests:  50%|████▉     | 1016/2048 [00:02<00:02, 410.69it/s]
Adding requests:  52%|█████▏    | 1058/2048 [00:02<00:02, 410.65it/s]
Adding requests:  54%|█████▎    | 1100/2048 [00:02<00:02, 410.97it/s]
Adding requests:  56%|█████▌    | 1142/2048 [00:02<00:02, 400.21it/s]
Adding requests:  58%|█████▊    | 1186/2048 [00:02<00:02, 410.87it/s]
Adding requests:  60%|██████    | 1229/2048 [00:03<00:01, 413.27it/s]
Adding requests:  62%|██████▏   | 1271/2048 [00:03<00:01, 410.07it/s]
Adding requests:  64%|██████▍   | 1313/2048 [00:03<00:01, 405.30it/s]
Adding requests:  66%|██████▌   | 1355/2048 [00:03<00:01, 408.13it/s]
Adding requests:  68%|██████▊   | 1398/2048 [00:03<00:01, 412.59it/s]
Adding requests:  70%|███████   | 1440/2048 [00:03<00:01, 407.49it/s]
Adding requests:  72%|███████▏  | 1482/2048 [00:03<00:01, 410.53it/s]
Adding requests:  74%|███████▍  | 1524/2048 [00:03<00:01, 413.25it/s]
Adding requests:  77%|███████▋  | 1567/2048 [00:03<00:01, 416.36it/s]
Adding requests:  79%|███████▊  | 1611/2048 [00:04<00:01, 419.09it/s]
Adding requests:  81%|████████  | 1653/2048 [00:04<00:00, 417.25it/s]
Adding requests:  83%|████████▎ | 1695/2048 [00:04<00:00, 414.12it/s]
Adding requests:  85%|████████▍ | 1737/2048 [00:04<00:00, 414.94it/s]
Adding requests:  87%|████████▋ | 1779/2048 [00:04<00:00, 411.27it/s]
Adding requests:  89%|████████▉ | 1821/2048 [00:04<00:00, 413.61it/s]
Adding requests:  91%|█████████ | 1863/2048 [00:04<00:00, 405.45it/s]
Adding requests:  93%|█████████▎| 1905/2048 [00:04<00:00, 395.64it/s]
Adding requests:  95%|█████████▍| 1945/2048 [00:04<00:00, 396.84it/s]
Adding requests:  97%|█████████▋| 1986/2048 [00:04<00:00, 399.59it/s]
Adding requests:  99%|█████████▉| 2028/2048 [00:05<00:00, 404.85it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 402.75it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:00<00:00, 2288.89it/s, est. speed input: 2344329.86 toks/s, output: 2289.05 toks/s]
Processed prompts:  23%|██▎       | 471/2048 [00:04<00:17, 91.31it/s, est. speed input: 109746.69 toks/s, output: 107.17 toks/s]    
Processed prompts:  28%|██▊       | 569/2048 [00:06<00:19, 77.04it/s, est. speed input: 93486.48 toks/s, output: 91.30 toks/s]  
Processed prompts:  31%|███       | 625/2048 [00:07<00:19, 73.63it/s, est. speed input: 89491.37 toks/s, output: 87.39 toks/s]
Processed prompts:  32%|███▏      | 661/2048 [00:08<00:21, 65.63it/s, est. speed input: 83865.96 toks/s, output: 81.90 toks/s]
Processed prompts:  33%|███▎      | 686/2048 [00:08<00:20, 67.21it/s, est. speed input: 83848.59 toks/s, output: 81.88 toks/s]
Processed prompts:  34%|███▍      | 705/2048 [00:08<00:20, 66.57it/s, est. speed input: 83132.34 toks/s, output: 81.18 toks/s]
Processed prompts:  35%|███▌      | 720/2048 [00:08<00:20, 63.89it/s, est. speed input: 82003.37 toks/s, output: 80.08 toks/s]
Processed prompts:  36%|███▌      | 732/2048 [00:09<00:22, 59.51it/s, est. speed input: 80623.70 toks/s, output: 78.73 toks/s]
Processed prompts:  36%|███▌      | 742/2048 [00:09<00:24, 54.07it/s, est. speed input: 79117.10 toks/s, output: 77.26 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:09<00:25, 50.72it/s, est. speed input: 77908.17 toks/s, output: 76.08 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:10<00:25, 51.06it/s, est. speed input: 77171.67 toks/s, output: 75.36 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:10<00:24, 51.35it/s, est. speed input: 76479.63 toks/s, output: 74.69 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:10<00:24, 51.54it/s, est. speed input: 75822.43 toks/s, output: 74.05 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:13<01:19, 15.46it/s, est. speed input: 60998.93 toks/s, output: 59.57 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:14<01:02, 19.34it/s, est. speed input: 60833.40 toks/s, output: 59.41 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:14<00:50, 23.65it/s, est. speed input: 60674.28 toks/s, output: 59.25 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:14<00:41, 28.16it/s, est. speed input: 60522.37 toks/s, output: 59.10 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:14<00:35, 32.58it/s, est. speed input: 60375.91 toks/s, output: 58.96 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:15<00:31, 36.65it/s, est. speed input: 60234.19 toks/s, output: 58.82 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:15<00:28, 40.19it/s, est. speed input: 60098.76 toks/s, output: 58.69 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:15<00:25, 43.13it/s, est. speed input: 59967.63 toks/s, output: 58.56 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:16<00:24, 45.46it/s, est. speed input: 59841.43 toks/s, output: 58.44 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:16<00:22, 47.25it/s, est. speed input: 59719.44 toks/s, output: 58.32 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:16<00:21, 49.27it/s, est. speed input: 59655.54 toks/s, output: 58.26 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:17<00:21, 50.07it/s, est. speed input: 59540.97 toks/s, output: 58.15 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:17<00:20, 50.65it/s, est. speed input: 59431.20 toks/s, output: 58.04 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:17<00:20, 51.05it/s, est. speed input: 59323.77 toks/s, output: 57.93 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:18<00:19, 51.34it/s, est. speed input: 59220.84 toks/s, output: 57.83 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:18<00:19, 51.56it/s, est. speed input: 59122.34 toks/s, output: 57.74 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:18<00:18, 51.71it/s, est. speed input: 59026.16 toks/s, output: 57.64 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:18<00:18, 51.81it/s, est. speed input: 58933.02 toks/s, output: 57.55 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:19<00:18, 51.90it/s, est. speed input: 58844.10 toks/s, output: 57.46 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:19<00:17, 51.94it/s, est. speed input: 58756.82 toks/s, output: 57.38 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:19<00:17, 51.98it/s, est. speed input: 58672.62 toks/s, output: 57.30 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:20<00:16, 52.81it/s, est. speed input: 58636.55 toks/s, output: 57.26 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:20<00:16, 52.59it/s, est. speed input: 58556.78 toks/s, output: 57.18 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:20<00:16, 52.41it/s, est. speed input: 58477.84 toks/s, output: 57.11 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:21<00:16, 52.26it/s, est. speed input: 58400.39 toks/s, output: 57.03 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:21<00:15, 52.17it/s, est. speed input: 58325.51 toks/s, output: 56.96 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:21<00:15, 52.09it/s, est. speed input: 58252.11 toks/s, output: 56.89 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:22<00:15, 52.03it/s, est. speed input: 58180.75 toks/s, output: 56.82 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:22<00:15, 52.01it/s, est. speed input: 58111.66 toks/s, output: 56.75 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:22<00:14, 52.00it/s, est. speed input: 58044.87 toks/s, output: 56.68 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:22<00:14, 51.98it/s, est. speed input: 57979.77 toks/s, output: 56.62 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:23<00:14, 51.98it/s, est. speed input: 57916.57 toks/s, output: 56.56 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:23<00:13, 51.96it/s, est. speed input: 57854.54 toks/s, output: 56.50 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:23<00:13, 51.95it/s, est. speed input: 57793.81 toks/s, output: 56.44 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:24<00:13, 51.95it/s, est. speed input: 57735.53 toks/s, output: 56.38 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:24<00:12, 51.93it/s, est. speed input: 57677.43 toks/s, output: 56.33 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:24<00:12, 51.92it/s, est. speed input: 57621.10 toks/s, output: 56.27 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:25<00:12, 51.93it/s, est. speed input: 57566.50 toks/s, output: 56.22 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:25<00:11, 51.93it/s, est. speed input: 57513.35 toks/s, output: 56.17 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:25<00:11, 51.93it/s, est. speed input: 57461.50 toks/s, output: 56.11 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:26<00:11, 51.94it/s, est. speed input: 57410.98 toks/s, output: 56.07 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:26<00:11, 51.93it/s, est. speed input: 57361.21 toks/s, output: 56.02 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:26<00:10, 51.94it/s, est. speed input: 57313.16 toks/s, output: 55.97 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:26<00:10, 51.93it/s, est. speed input: 57265.73 toks/s, output: 55.92 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:27<00:10, 51.94it/s, est. speed input: 57219.95 toks/s, output: 55.88 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:27<00:09, 51.94it/s, est. speed input: 57174.71 toks/s, output: 55.83 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:27<00:09, 51.95it/s, est. speed input: 57131.01 toks/s, output: 55.79 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:28<00:09, 51.94it/s, est. speed input: 57087.78 toks/s, output: 55.75 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:28<00:08, 51.94it/s, est. speed input: 57045.46 toks/s, output: 55.71 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:28<00:08, 51.98it/s, est. speed input: 57005.64 toks/s, output: 55.67 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:29<00:08, 52.00it/s, est. speed input: 56966.60 toks/s, output: 55.63 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:29<00:07, 52.03it/s, est. speed input: 56928.62 toks/s, output: 55.59 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:29<00:07, 52.05it/s, est. speed input: 56891.63 toks/s, output: 55.56 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:30<00:07, 52.06it/s, est. speed input: 56855.23 toks/s, output: 55.52 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:30<00:07, 52.07it/s, est. speed input: 56819.65 toks/s, output: 55.49 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:30<00:06, 52.08it/s, est. speed input: 56784.88 toks/s, output: 55.45 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:30<00:06, 52.07it/s, est. speed input: 56750.03 toks/s, output: 55.42 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:31<00:06, 52.08it/s, est. speed input: 56716.79 toks/s, output: 55.39 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:31<00:05, 52.08it/s, est. speed input: 56683.67 toks/s, output: 55.36 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:31<00:05, 52.09it/s, est. speed input: 56651.57 toks/s, output: 55.32 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:32<00:05, 52.10it/s, est. speed input: 56620.44 toks/s, output: 55.29 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:32<00:04, 52.11it/s, est. speed input: 56589.75 toks/s, output: 55.26 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:32<00:04, 52.10it/s, est. speed input: 56559.30 toks/s, output: 55.23 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:33<00:04, 52.10it/s, est. speed input: 56529.39 toks/s, output: 55.20 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:33<00:03, 52.09it/s, est. speed input: 56499.91 toks/s, output: 55.18 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:33<00:03, 52.09it/s, est. speed input: 56471.05 toks/s, output: 55.15 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:33<00:03, 52.90it/s, est. speed input: 56468.90 toks/s, output: 55.15 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:34<00:03, 52.66it/s, est. speed input: 56440.90 toks/s, output: 55.12 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:34<00:02, 52.48it/s, est. speed input: 56413.34 toks/s, output: 55.09 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:34<00:02, 52.36it/s, est. speed input: 56386.24 toks/s, output: 55.06 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:35<00:02, 52.29it/s, est. speed input: 56360.04 toks/s, output: 55.04 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:35<00:01, 52.23it/s, est. speed input: 56333.84 toks/s, output: 55.01 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:35<00:01, 52.19it/s, est. speed input: 56308.22 toks/s, output: 54.99 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:36<00:01, 52.15it/s, est. speed input: 56282.82 toks/s, output: 54.96 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:36<00:00, 52.14it/s, est. speed input: 56258.13 toks/s, output: 54.94 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:36<00:00, 52.13it/s, est. speed input: 56233.81 toks/s, output: 54.92 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:37<00:00, 53.06it/s, est. speed input: 56237.37 toks/s, output: 54.92 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:37<00:00, 53.06it/s, est. speed input: 56623.95 toks/s, output: 55.30 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:37<00:00, 55.30it/s, est. speed input: 56623.95 toks/s, output: 55.30 toks/s]
[rank0]:[W128 12:16:22.173754208 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 101.6s

测试结果:
  Requests/s:   51.81
  Tokens/s:     53104.73
  Total Reqs:   2048
  Elapsed:      39.53s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     53052.92

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:16:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:16:56 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=47691) WARNING 01-28 12:17:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=47691) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=47691) WARNING 01-28 12:17:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.54 requests/s, 52831.44 total tokens/s, 51.54 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 12:16:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:16:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:16:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:16:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:16:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:16:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:16:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:16:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:16:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:16:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:16:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:16:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:16:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:16:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:17:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:17:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:17:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:17:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:17:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:17:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:17:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:17:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:17:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:17:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:17:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:17:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:17:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:17:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=47691) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=47691) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=47691) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=47691) 
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=47691) [2026-01-28 12:17:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=47691) [rank0]:W0128 12:17:23.318000 47691 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=47691) [rank0]:W0128 12:17:23.418000 47691 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=47691) [rank0]:W0128 12:17:24.691000 47691 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=47691) [rank0]:W0128 12:17:24.852000 47691 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=47691) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.26it/s]
(EngineCore_DP0 pid=47691) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.21it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.78it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 37/4096 [00:00<00:11, 367.55it/s]
Adding requests:   2%|▏         | 76/4096 [00:00<00:10, 379.75it/s]
Adding requests:   3%|▎         | 115/4096 [00:00<00:10, 382.43it/s]
Adding requests:   4%|▍         | 154/4096 [00:00<00:10, 380.38it/s]
Adding requests:   5%|▍         | 193/4096 [00:00<00:10, 381.07it/s]
Adding requests:   6%|▌         | 233/4096 [00:00<00:10, 385.93it/s]
Adding requests:   7%|▋         | 274/4096 [00:00<00:09, 390.72it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:09, 389.27it/s]
Adding requests:   9%|▊         | 353/4096 [00:00<00:09, 388.71it/s]
Adding requests:  10%|▉         | 393/4096 [00:01<00:09, 389.90it/s]
Adding requests:  11%|█         | 433/4096 [00:01<00:09, 392.12it/s]
Adding requests:  12%|█▏        | 473/4096 [00:01<00:09, 388.87it/s]
Adding requests:  12%|█▎        | 512/4096 [00:01<00:09, 386.77it/s]
Adding requests:  13%|█▎        | 551/4096 [00:01<00:09, 384.22it/s]
Adding requests:  14%|█▍        | 592/4096 [00:01<00:08, 391.67it/s]
Adding requests:  15%|█▌        | 633/4096 [00:01<00:08, 394.75it/s]
Adding requests:  16%|█▋        | 675/4096 [00:01<00:08, 401.25it/s]
Adding requests:  18%|█▊        | 717/4096 [00:01<00:08, 404.73it/s]
Adding requests:  19%|█▊        | 759/4096 [00:01<00:08, 406.01it/s]
Adding requests:  20%|█▉        | 800/4096 [00:02<00:08, 400.83it/s]
Adding requests:  21%|██        | 841/4096 [00:02<00:08, 389.10it/s]
Adding requests:  22%|██▏       | 883/4096 [00:02<00:08, 396.15it/s]
Adding requests:  23%|██▎       | 923/4096 [00:02<00:08, 396.53it/s]
Adding requests:  24%|██▎       | 963/4096 [00:02<00:07, 395.63it/s]
Adding requests:  24%|██▍       | 1003/4096 [00:02<00:07, 393.73it/s]
Adding requests:  26%|██▌       | 1045/4096 [00:02<00:07, 400.17it/s]
Adding requests:  27%|██▋       | 1086/4096 [00:02<00:07, 387.33it/s]
Adding requests:  27%|██▋       | 1126/4096 [00:02<00:07, 390.70it/s]
Adding requests:  29%|██▊       | 1169/4096 [00:02<00:07, 401.39it/s]
Adding requests:  30%|██▉       | 1212/4096 [00:03<00:07, 409.35it/s]
Adding requests:  31%|███       | 1254/4096 [00:03<00:07, 402.25it/s]
Adding requests:  32%|███▏      | 1295/4096 [00:03<00:06, 402.17it/s]
Adding requests:  33%|███▎      | 1338/4096 [00:03<00:06, 406.45it/s]
Adding requests:  34%|███▎      | 1380/4096 [00:03<00:06, 408.76it/s]
Adding requests:  35%|███▍      | 1422/4096 [00:03<00:06, 411.61it/s]
Adding requests:  36%|███▌      | 1465/4096 [00:03<00:06, 416.81it/s]
Adding requests:  37%|███▋      | 1507/4096 [00:03<00:06, 417.41it/s]
Adding requests:  38%|███▊      | 1549/4096 [00:03<00:06, 418.15it/s]
Adding requests:  39%|███▉      | 1591/4096 [00:03<00:06, 415.67it/s]
Adding requests:  40%|███▉      | 1633/4096 [00:04<00:05, 414.87it/s]
Adding requests:  41%|████      | 1675/4096 [00:04<00:05, 407.25it/s]
Adding requests:  42%|████▏     | 1717/4096 [00:04<00:05, 409.86it/s]
Adding requests:  43%|████▎     | 1759/4096 [00:04<00:05, 410.78it/s]
Adding requests:  44%|████▍     | 1801/4096 [00:04<00:05, 411.23it/s]
Adding requests:  45%|████▍     | 1843/4096 [00:04<00:05, 410.06it/s]
Adding requests:  46%|████▌     | 1885/4096 [00:04<00:05, 408.28it/s]
Adding requests:  47%|████▋     | 1927/4096 [00:04<00:05, 408.74it/s]
Adding requests:  48%|████▊     | 1968/4096 [00:04<00:05, 404.06it/s]
Adding requests:  49%|████▉     | 2009/4096 [00:05<00:05, 402.86it/s]
Adding requests:  50%|█████     | 2050/4096 [00:05<00:05, 402.08it/s]
Adding requests:  51%|█████     | 2091/4096 [00:05<00:05, 397.82it/s]
Adding requests:  52%|█████▏    | 2131/4096 [00:05<00:04, 394.50it/s]
Adding requests:  53%|█████▎    | 2171/4096 [00:05<00:04, 393.93it/s]
Adding requests:  54%|█████▍    | 2211/4096 [00:05<00:04, 393.88it/s]
Adding requests:  55%|█████▌    | 2253/4096 [00:05<00:04, 401.27it/s]
Adding requests:  56%|█████▌    | 2294/4096 [00:05<00:04, 387.93it/s]
Adding requests:  57%|█████▋    | 2335/4096 [00:05<00:04, 391.42it/s]
Adding requests:  58%|█████▊    | 2377/4096 [00:05<00:04, 396.70it/s]
Adding requests:  59%|█████▉    | 2419/4096 [00:06<00:04, 401.86it/s]
Adding requests:  60%|██████    | 2461/4096 [00:06<00:04, 405.62it/s]
Adding requests:  61%|██████    | 2502/4096 [00:06<00:03, 406.53it/s]
Adding requests:  62%|██████▏   | 2544/4096 [00:06<00:03, 408.25it/s]
Adding requests:  63%|██████▎   | 2585/4096 [00:06<00:03, 408.75it/s]
Adding requests:  64%|██████▍   | 2626/4096 [00:06<00:03, 407.41it/s]
Adding requests:  65%|██████▌   | 2669/4096 [00:06<00:03, 412.08it/s]
Adding requests:  66%|██████▌   | 2711/4096 [00:06<00:03, 407.30it/s]
Adding requests:  67%|██████▋   | 2752/4096 [00:06<00:03, 406.44it/s]
Adding requests:  68%|██████▊   | 2793/4096 [00:06<00:03, 406.82it/s]
Adding requests:  69%|██████▉   | 2834/4096 [00:07<00:03, 406.11it/s]
Adding requests:  70%|███████   | 2875/4096 [00:07<00:03, 406.58it/s]
Adding requests:  71%|███████   | 2916/4096 [00:07<00:02, 402.55it/s]
Adding requests:  72%|███████▏  | 2957/4096 [00:07<00:02, 398.81it/s]
Adding requests:  73%|███████▎  | 2998/4096 [00:07<00:02, 400.95it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:07<00:02, 402.00it/s]
Adding requests:  75%|███████▌  | 3080/4096 [00:07<00:02, 393.80it/s]
Adding requests:  76%|███████▌  | 3122/4096 [00:07<00:02, 398.53it/s]
Adding requests:  77%|███████▋  | 3162/4096 [00:07<00:02, 396.79it/s]
Adding requests:  78%|███████▊  | 3202/4096 [00:08<00:02, 349.57it/s]
Adding requests:  79%|███████▉  | 3238/4096 [00:08<00:02, 335.29it/s]
Adding requests:  80%|████████  | 3278/4096 [00:08<00:02, 352.11it/s]
Adding requests:  81%|████████  | 3314/4096 [00:10<00:17, 43.76it/s] 
Adding requests:  82%|████████▏ | 3356/4096 [00:11<00:12, 61.24it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:11<00:08, 82.88it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:11<00:06, 109.58it/s]
Adding requests:  85%|████████▍ | 3477/4096 [00:11<00:04, 138.72it/s]
Adding requests:  86%|████████▌ | 3519/4096 [00:11<00:03, 174.84it/s]
Adding requests:  87%|████████▋ | 3560/4096 [00:11<00:02, 210.80it/s]
Adding requests:  88%|████████▊ | 3601/4096 [00:11<00:02, 246.28it/s]
Adding requests:  89%|████████▉ | 3641/4096 [00:11<00:01, 271.07it/s]
Adding requests:  90%|████████▉ | 3682/4096 [00:11<00:01, 301.34it/s]
Adding requests:  91%|█████████ | 3721/4096 [00:11<00:01, 322.16it/s]
Adding requests:  92%|█████████▏| 3762/4096 [00:12<00:00, 344.05it/s]
Adding requests:  93%|█████████▎| 3804/4096 [00:12<00:00, 362.77it/s]
Adding requests:  94%|█████████▍| 3846/4096 [00:12<00:00, 378.22it/s]
Adding requests:  95%|█████████▍| 3888/4096 [00:12<00:00, 387.17it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:12<00:00, 396.00it/s]
Adding requests:  97%|█████████▋| 3972/4096 [00:12<00:00, 402.22it/s]
Adding requests:  98%|█████████▊| 4014/4096 [00:12<00:00, 403.62it/s]
Adding requests:  99%|█████████▉| 4056/4096 [00:12<00:00, 399.81it/s]
Adding requests: 100%|██████████| 4096/4096 [00:12<00:00, 318.22it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:00<00:03, 1175.60it/s, est. speed input: 1203875.10 toks/s, output: 1175.62 toks/s]
Processed prompts:  15%|█▌        | 632/4096 [00:02<00:15, 221.31it/s, est. speed input: 282588.20 toks/s, output: 275.96 toks/s]   
Processed prompts:  17%|█▋        | 685/4096 [00:03<00:24, 141.01it/s, est. speed input: 198897.47 toks/s, output: 194.24 toks/s]
Processed prompts:  17%|█▋        | 716/4096 [00:04<00:28, 118.83it/s, est. speed input: 176821.70 toks/s, output: 172.68 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:04<00:34, 97.39it/s, est. speed input: 158616.90 toks/s, output: 154.90 toks/s] 
Processed prompts:  19%|█▉        | 770/4096 [00:05<00:39, 84.95it/s, est. speed input: 146367.88 toks/s, output: 142.94 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:06<00:43, 75.68it/s, est. speed input: 136711.41 toks/s, output: 133.51 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:06<00:47, 68.81it/s, est. speed input: 128851.73 toks/s, output: 125.83 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:07<00:50, 63.83it/s, est. speed input: 122339.02 toks/s, output: 119.47 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:07<00:53, 60.22it/s, est. speed input: 116840.26 toks/s, output: 114.10 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:08<00:54, 57.66it/s, est. speed input: 112154.77 toks/s, output: 109.53 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:09<00:55, 56.10it/s, est. speed input: 108220.89 toks/s, output: 105.68 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:09<00:56, 54.73it/s, est. speed input: 104678.03 toks/s, output: 102.22 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:10<00:57, 53.77it/s, est. speed input: 101560.24 toks/s, output: 99.18 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:10<00:57, 53.09it/s, est. speed input: 98793.70 toks/s, output: 96.48 toks/s] 
Processed prompts:  27%|██▋       | 1090/4096 [00:11<00:57, 52.62it/s, est. speed input: 96324.15 toks/s, output: 94.07 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:12<00:56, 52.33it/s, est. speed input: 94119.64 toks/s, output: 91.91 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:12<00:56, 52.51it/s, est. speed input: 92237.44 toks/s, output: 90.08 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:13<00:55, 52.25it/s, est. speed input: 90422.45 toks/s, output: 88.30 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:14<00:55, 52.07it/s, est. speed input: 88766.26 toks/s, output: 86.69 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:14<00:54, 51.98it/s, est. speed input: 87259.41 toks/s, output: 85.21 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:15<00:54, 51.83it/s, est. speed input: 85855.90 toks/s, output: 83.84 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:15<00:53, 51.77it/s, est. speed input: 84570.11 toks/s, output: 82.59 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:16<00:53, 51.72it/s, est. speed input: 83380.53 toks/s, output: 81.43 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:17<00:52, 51.69it/s, est. speed input: 82276.69 toks/s, output: 80.35 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:17<00:51, 51.67it/s, est. speed input: 81251.28 toks/s, output: 79.35 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:18<00:51, 51.65it/s, est. speed input: 80293.84 toks/s, output: 78.41 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:19<00:50, 51.63it/s, est. speed input: 79397.78 toks/s, output: 77.54 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:19<00:50, 51.63it/s, est. speed input: 78559.14 toks/s, output: 76.72 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:20<00:49, 51.66it/s, est. speed input: 77777.48 toks/s, output: 75.95 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:20<00:48, 51.59it/s, est. speed input: 77029.11 toks/s, output: 75.22 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:21<00:48, 51.59it/s, est. speed input: 76329.77 toks/s, output: 74.54 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:22<00:47, 51.58it/s, est. speed input: 75670.14 toks/s, output: 73.90 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:22<00:47, 51.57it/s, est. speed input: 75045.27 toks/s, output: 73.29 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:23<00:46, 51.56it/s, est. speed input: 74453.00 toks/s, output: 72.71 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:23<00:45, 51.56it/s, est. speed input: 73891.78 toks/s, output: 72.16 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:24<00:45, 51.49it/s, est. speed input: 73351.98 toks/s, output: 71.63 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:25<00:44, 51.46it/s, est. speed input: 72839.83 toks/s, output: 71.13 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:25<00:44, 51.49it/s, est. speed input: 72358.05 toks/s, output: 70.66 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:26<00:43, 51.89it/s, est. speed input: 71941.14 toks/s, output: 70.25 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:27<00:42, 51.79it/s, est. speed input: 71502.17 toks/s, output: 69.83 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:27<00:42, 51.73it/s, est. speed input: 71083.25 toks/s, output: 69.42 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:28<00:41, 51.68it/s, est. speed input: 70682.36 toks/s, output: 69.03 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:28<00:40, 51.64it/s, est. speed input: 70298.34 toks/s, output: 68.65 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:29<00:40, 51.61it/s, est. speed input: 69930.22 toks/s, output: 68.29 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:30<00:39, 51.59it/s, est. speed input: 69577.79 toks/s, output: 67.95 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:33<01:28, 22.82it/s, est. speed input: 63836.43 toks/s, output: 62.34 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:34<01:12, 27.40it/s, est. speed input: 63634.34 toks/s, output: 62.14 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:34<01:01, 31.88it/s, est. speed input: 63439.95 toks/s, output: 61.95 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:35<00:53, 36.00it/s, est. speed input: 63252.42 toks/s, output: 61.77 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:35<00:47, 39.58it/s, est. speed input: 63071.20 toks/s, output: 61.59 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:36<00:43, 42.54it/s, est. speed input: 62896.07 toks/s, output: 61.42 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:37<00:40, 45.18it/s, est. speed input: 62752.28 toks/s, output: 61.28 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:37<00:38, 46.92it/s, est. speed input: 62588.48 toks/s, output: 61.12 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:38<00:36, 48.21it/s, est. speed input: 62429.39 toks/s, output: 60.97 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:38<00:35, 49.18it/s, est. speed input: 62276.34 toks/s, output: 60.82 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:39<00:34, 49.79it/s, est. speed input: 62122.53 toks/s, output: 60.67 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:40<00:33, 50.27it/s, est. speed input: 61976.35 toks/s, output: 60.52 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:40<00:32, 50.62it/s, est. speed input: 61834.83 toks/s, output: 60.39 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:41<00:31, 51.25it/s, est. speed input: 61720.88 toks/s, output: 60.27 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:42<00:30, 51.31it/s, est. speed input: 61587.53 toks/s, output: 60.14 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:42<00:29, 51.35it/s, est. speed input: 61457.66 toks/s, output: 60.02 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:43<00:29, 51.39it/s, est. speed input: 61331.88 toks/s, output: 59.89 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:43<00:28, 51.42it/s, est. speed input: 61210.08 toks/s, output: 59.78 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:44<00:27, 51.43it/s, est. speed input: 61091.48 toks/s, output: 59.66 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:45<00:27, 51.45it/s, est. speed input: 60976.17 toks/s, output: 59.55 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:45<00:26, 51.46it/s, est. speed input: 60864.16 toks/s, output: 59.44 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:46<00:26, 51.48it/s, est. speed input: 60755.56 toks/s, output: 59.33 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:47<00:25, 51.52it/s, est. speed input: 60651.75 toks/s, output: 59.23 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:47<00:24, 51.57it/s, est. speed input: 60551.14 toks/s, output: 59.13 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:48<00:24, 51.60it/s, est. speed input: 60453.01 toks/s, output: 59.04 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:48<00:23, 51.61it/s, est. speed input: 60357.25 toks/s, output: 58.94 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:49<00:22, 51.63it/s, est. speed input: 60263.95 toks/s, output: 58.85 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:50<00:22, 51.64it/s, est. speed input: 60173.13 toks/s, output: 58.76 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:50<00:21, 51.65it/s, est. speed input: 60084.30 toks/s, output: 58.68 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:51<00:21, 51.65it/s, est. speed input: 59997.85 toks/s, output: 58.59 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:51<00:20, 51.66it/s, est. speed input: 59913.47 toks/s, output: 58.51 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:52<00:19, 51.66it/s, est. speed input: 59830.85 toks/s, output: 58.43 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:53<00:19, 51.66it/s, est. speed input: 59750.15 toks/s, output: 58.35 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:53<00:18, 51.66it/s, est. speed input: 59671.30 toks/s, output: 58.27 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:54<00:17, 51.66it/s, est. speed input: 59594.33 toks/s, output: 58.20 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:55<00:17, 51.65it/s, est. speed input: 59518.74 toks/s, output: 58.12 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:55<00:16, 51.65it/s, est. speed input: 59444.94 toks/s, output: 58.05 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:56<00:16, 51.66it/s, est. speed input: 59373.24 toks/s, output: 57.98 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:56<00:15, 51.66it/s, est. speed input: 59302.79 toks/s, output: 57.91 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:57<00:14, 51.66it/s, est. speed input: 59233.99 toks/s, output: 57.85 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:58<00:14, 51.70it/s, est. speed input: 59168.09 toks/s, output: 57.78 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:58<00:13, 51.64it/s, est. speed input: 59100.13 toks/s, output: 57.71 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:59<00:12, 51.64it/s, est. speed input: 59035.27 toks/s, output: 57.65 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:00<00:12, 51.64it/s, est. speed input: 58971.76 toks/s, output: 57.59 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:00<00:11, 51.64it/s, est. speed input: 58909.69 toks/s, output: 57.53 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:01<00:11, 51.64it/s, est. speed input: 58848.66 toks/s, output: 57.47 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:01<00:10, 51.65it/s, est. speed input: 58789.42 toks/s, output: 57.41 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:02<00:09, 51.60it/s, est. speed input: 58729.08 toks/s, output: 57.35 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:03<00:09, 51.60it/s, est. speed input: 58671.14 toks/s, output: 57.30 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:03<00:08, 51.60it/s, est. speed input: 58614.49 toks/s, output: 57.24 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:04<00:08, 51.60it/s, est. speed input: 58558.94 toks/s, output: 57.19 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:04<00:07, 52.00it/s, est. speed input: 58518.50 toks/s, output: 57.15 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:08<00:15, 22.85it/s, est. speed input: 56225.97 toks/s, output: 54.91 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:08<00:11, 27.44it/s, est. speed input: 56195.75 toks/s, output: 54.88 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:09<00:08, 31.92it/s, est. speed input: 56165.94 toks/s, output: 54.85 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:10<00:07, 36.05it/s, est. speed input: 56136.71 toks/s, output: 54.82 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:10<00:05, 39.64it/s, est. speed input: 56108.04 toks/s, output: 54.79 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:11<00:04, 42.60it/s, est. speed input: 56079.81 toks/s, output: 54.77 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:11<00:03, 44.96it/s, est. speed input: 56052.08 toks/s, output: 54.74 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:12<00:02, 46.76it/s, est. speed input: 56024.58 toks/s, output: 54.71 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:13<00:01, 48.12it/s, est. speed input: 55997.74 toks/s, output: 54.69 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:13<00:01, 49.46it/s, est. speed input: 55982.64 toks/s, output: 54.67 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:14<00:00, 50.57it/s, est. speed input: 55971.76 toks/s, output: 54.66 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:14<00:00, 50.57it/s, est. speed input: 56384.30 toks/s, output: 55.06 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:14<00:00, 55.06it/s, est. speed input: 56384.30 toks/s, output: 55.06 toks/s]
[rank0]:[W128 12:19:00.402755839 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 158.3s

测试结果:
  Requests/s:   51.54
  Tokens/s:     52831.44
  Total Reqs:   4096
  Elapsed:      79.47s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     52779.89


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,16.6503,8541.5963,7.6876
1024,1024,1,128,128,17.1315,17559.8317,7.4716
2048,1024,2,256,128,30.5875,31352.2335,8.3694
4096,1024,4,512,128,52.5825,53897.0265,9.7371
8192,1024,8,1024,128,51.4131,52698.3975,19.9171
16384,1024,16,2048,128,51.8095,53104.7312,39.5294
32768,1024,32,4096,128,51.5429,52831.4360,79.4678

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:19:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:19:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=48749) WARNING 01-28 12:19:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=48749) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=48749) WARNING 01-28 12:19:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.04 requests/s, 8227.81 total tokens/s, 16.04 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 12:19:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:19:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:19:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:19:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:19:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:19:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:19:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:19:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:19:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:19:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:19:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:19:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:19:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:19:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:19:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:19:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:19:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=48749) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=48749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.13s/it]
(EngineCore_DP0 pid=48749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.13s/it]
(EngineCore_DP0 pid=48749) 
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=48749) [2026-01-28 12:19:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=48749) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]
(EngineCore_DP0 pid=48749) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  51%|█████     | 65/128 [00:00<00:00, 645.60it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 655.78it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.24it/s, est. speed input: 3195.93 toks/s, output: 6.24 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:10, 11.55it/s, est. speed input: 5449.48 toks/s, output: 10.64 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:08, 13.70it/s, est. speed input: 6365.92 toks/s, output: 12.43 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:08, 14.83it/s, est. speed input: 6867.68 toks/s, output: 13.41 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:07, 15.46it/s, est. speed input: 7176.32 toks/s, output: 14.02 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:07, 15.77it/s, est. speed input: 7368.82 toks/s, output: 14.39 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:07, 16.01it/s, est. speed input: 7517.34 toks/s, output: 14.68 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:01<00:06, 16.16it/s, est. speed input: 7628.70 toks/s, output: 14.90 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:06, 16.34it/s, est. speed input: 7729.29 toks/s, output: 15.10 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:01<00:06, 16.43it/s, est. speed input: 7805.50 toks/s, output: 15.24 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 16.44it/s, est. speed input: 7860.59 toks/s, output: 15.35 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:06, 16.46it/s, est. speed input: 7908.32 toks/s, output: 15.45 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:06, 16.39it/s, est. speed input: 7939.04 toks/s, output: 15.51 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:06, 16.44it/s, est. speed input: 7976.64 toks/s, output: 15.58 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:06, 16.50it/s, est. speed input: 8011.61 toks/s, output: 15.65 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 16.49it/s, est. speed input: 8037.27 toks/s, output: 15.70 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:02<00:05, 16.54it/s, est. speed input: 8065.82 toks/s, output: 15.75 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:02<00:05, 16.64it/s, est. speed input: 8096.14 toks/s, output: 15.81 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.57it/s, est. speed input: 8111.90 toks/s, output: 15.84 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 16.62it/s, est. speed input: 8134.57 toks/s, output: 15.89 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 16.66it/s, est. speed input: 8155.12 toks/s, output: 15.93 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:05, 16.72it/s, est. speed input: 8175.85 toks/s, output: 15.97 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 16.66it/s, est. speed input: 8188.10 toks/s, output: 15.99 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 16.61it/s, est. speed input: 8198.98 toks/s, output: 16.01 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:03<00:04, 16.55it/s, est. speed input: 8207.17 toks/s, output: 16.03 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:03<00:04, 16.55it/s, est. speed input: 8217.33 toks/s, output: 16.05 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:03<00:04, 16.60it/s, est. speed input: 8229.92 toks/s, output: 16.07 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.67it/s, est. speed input: 8243.14 toks/s, output: 16.10 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.71it/s, est. speed input: 8255.17 toks/s, output: 16.12 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.73it/s, est. speed input: 8266.31 toks/s, output: 16.15 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:04, 16.75it/s, est. speed input: 8276.72 toks/s, output: 16.17 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.74it/s, est. speed input: 8285.29 toks/s, output: 16.18 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:04<00:03, 16.73it/s, est. speed input: 8293.49 toks/s, output: 16.20 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:04<00:03, 16.65it/s, est. speed input: 8297.41 toks/s, output: 16.21 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:04<00:03, 16.63it/s, est. speed input: 8302.97 toks/s, output: 16.22 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.60it/s, est. speed input: 8307.41 toks/s, output: 16.23 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.57it/s, est. speed input: 8311.00 toks/s, output: 16.23 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.61it/s, est. speed input: 8317.34 toks/s, output: 16.24 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.66it/s, est. speed input: 8324.07 toks/s, output: 16.26 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.59it/s, est. speed input: 8326.57 toks/s, output: 16.26 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.60it/s, est. speed input: 8331.10 toks/s, output: 16.27 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:05<00:02, 16.56it/s, est. speed input: 8333.24 toks/s, output: 16.28 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:05<00:02, 16.60it/s, est. speed input: 8338.26 toks/s, output: 16.29 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:05<00:02, 16.68it/s, est. speed input: 8344.93 toks/s, output: 16.30 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.74it/s, est. speed input: 8351.45 toks/s, output: 16.31 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.77it/s, est. speed input: 8357.34 toks/s, output: 16.32 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.80it/s, est. speed input: 8363.12 toks/s, output: 16.33 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.83it/s, est. speed input: 8368.98 toks/s, output: 16.35 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.85it/s, est. speed input: 8374.57 toks/s, output: 16.36 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:06<00:01, 16.82it/s, est. speed input: 8378.66 toks/s, output: 16.36 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:06<00:01, 16.81it/s, est. speed input: 8382.68 toks/s, output: 16.37 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:06<00:01, 16.82it/s, est. speed input: 8387.45 toks/s, output: 16.38 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.85it/s, est. speed input: 8392.32 toks/s, output: 16.39 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.87it/s, est. speed input: 8397.38 toks/s, output: 16.40 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.86it/s, est. speed input: 8401.34 toks/s, output: 16.41 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.88it/s, est. speed input: 8405.90 toks/s, output: 16.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.77it/s, est. speed input: 8406.94 toks/s, output: 16.42 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:07<00:00, 16.67it/s, est. speed input: 8406.97 toks/s, output: 16.42 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:07<00:00, 16.59it/s, est. speed input: 8407.04 toks/s, output: 16.42 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:07<00:00, 16.56it/s, est. speed input: 8407.47 toks/s, output: 16.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 16.63it/s, est. speed input: 8410.52 toks/s, output: 16.43 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.71it/s, est. speed input: 8414.39 toks/s, output: 16.43 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.61it/s, est. speed input: 8414.04 toks/s, output: 16.43 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.69it/s, est. speed input: 8417.58 toks/s, output: 16.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.69it/s, est. speed input: 8419.34 toks/s, output: 16.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.44it/s, est. speed input: 8419.34 toks/s, output: 16.44 toks/s]
[rank0]:[W128 12:19:57.629382068 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.6s

测试结果:
  Requests/s:   16.04
  Tokens/s:     8227.81
  Total Reqs:   128
  Elapsed:      7.98s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     8211.77

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:20:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:20:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=49397) WARNING 01-28 12:20:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=49397) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=49397) WARNING 01-28 12:20:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.39 requests/s, 16796.54 total tokens/s, 16.39 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 12:20:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:20:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:20:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:20:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:20:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:20:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:20:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:20:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:20:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:20:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:20:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:20:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:20:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:20:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:20:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:20:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:20:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=49397) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=49397) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=49397) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=49397) 
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=49397) [2026-01-28 12:20:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=49397) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.04it/s]
(EngineCore_DP0 pid=49397) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 35/128 [00:00<00:00, 347.79it/s]
Adding requests:  55%|█████▌    | 71/128 [00:00<00:00, 351.89it/s]
Adding requests:  84%|████████▎ | 107/128 [00:00<00:00, 342.42it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 346.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 36.18it/s, est. speed input: 37055.67 toks/s, output: 36.18 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 22.37it/s, est. speed input: 24462.29 toks/s, output: 23.89 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:05, 20.06it/s, est. speed input: 22209.81 toks/s, output: 21.69 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:05, 18.99it/s, est. speed input: 21110.79 toks/s, output: 20.62 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:06, 18.48it/s, est. speed input: 20608.43 toks/s, output: 20.12 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:06, 18.09it/s, est. speed input: 20229.14 toks/s, output: 19.75 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:06, 17.82it/s, est. speed input: 19938.23 toks/s, output: 19.47 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:01<00:05, 17.61it/s, est. speed input: 19699.01 toks/s, output: 19.24 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:05, 17.42it/s, est. speed input: 19489.01 toks/s, output: 19.03 toks/s]
Processed prompts:  21%|██        | 27/128 [00:01<00:05, 17.29it/s, est. speed input: 19317.78 toks/s, output: 18.86 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:05, 17.14it/s, est. speed input: 19153.05 toks/s, output: 18.70 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:05, 17.08it/s, est. speed input: 19026.20 toks/s, output: 18.58 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:05, 17.09it/s, est. speed input: 18926.57 toks/s, output: 18.48 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:05, 17.02it/s, est. speed input: 18824.34 toks/s, output: 18.38 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:02<00:05, 16.98it/s, est. speed input: 18733.86 toks/s, output: 18.29 toks/s]
Processed prompts:  30%|███       | 39/128 [00:02<00:05, 17.03it/s, est. speed input: 18668.90 toks/s, output: 18.23 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:02<00:05, 17.06it/s, est. speed input: 18611.59 toks/s, output: 18.18 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:02<00:04, 17.03it/s, est. speed input: 18550.14 toks/s, output: 18.12 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:02<00:04, 17.02it/s, est. speed input: 18495.84 toks/s, output: 18.06 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:02<00:04, 17.00it/s, est. speed input: 18445.07 toks/s, output: 18.01 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:02<00:04, 17.00it/s, est. speed input: 18399.88 toks/s, output: 17.97 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:02<00:04, 16.94it/s, est. speed input: 18349.64 toks/s, output: 17.92 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:04, 16.93it/s, est. speed input: 18308.30 toks/s, output: 17.88 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:03<00:04, 16.94it/s, est. speed input: 18272.57 toks/s, output: 17.84 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:03<00:04, 16.94it/s, est. speed input: 18238.15 toks/s, output: 17.81 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:03<00:04, 16.96it/s, est. speed input: 18209.38 toks/s, output: 17.78 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:03<00:03, 16.96it/s, est. speed input: 18180.84 toks/s, output: 17.75 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:03<00:03, 16.84it/s, est. speed input: 18139.59 toks/s, output: 17.71 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:03<00:03, 16.81it/s, est. speed input: 18106.93 toks/s, output: 17.68 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:03<00:03, 16.81it/s, est. speed input: 18078.81 toks/s, output: 17.65 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:03<00:03, 16.84it/s, est. speed input: 18055.43 toks/s, output: 17.63 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:04<00:03, 16.79it/s, est. speed input: 18026.60 toks/s, output: 17.60 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:04<00:03, 16.86it/s, est. speed input: 18009.46 toks/s, output: 17.59 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:04<00:03, 16.88it/s, est. speed input: 17991.21 toks/s, output: 17.57 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:04<00:03, 16.72it/s, est. speed input: 17956.74 toks/s, output: 17.54 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:04<00:02, 16.67it/s, est. speed input: 17929.33 toks/s, output: 17.51 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:04<00:02, 16.64it/s, est. speed input: 17904.56 toks/s, output: 17.48 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:04<00:02, 16.55it/s, est. speed input: 17874.81 toks/s, output: 17.46 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:04<00:02, 16.62it/s, est. speed input: 17857.70 toks/s, output: 17.44 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:02, 16.57it/s, est. speed input: 17833.17 toks/s, output: 17.42 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:05<00:02, 16.56it/s, est. speed input: 17812.21 toks/s, output: 17.39 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:05<00:02, 16.59it/s, est. speed input: 17795.18 toks/s, output: 17.38 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:05<00:02, 16.63it/s, est. speed input: 17780.10 toks/s, output: 17.36 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:05<00:01, 16.59it/s, est. speed input: 17760.68 toks/s, output: 17.34 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:05<00:01, 16.53it/s, est. speed input: 17739.38 toks/s, output: 17.32 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:05<00:01, 16.45it/s, est. speed input: 17716.18 toks/s, output: 17.30 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:05<00:01, 16.52it/s, est. speed input: 17702.88 toks/s, output: 17.29 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:05<00:01, 16.50it/s, est. speed input: 17685.76 toks/s, output: 17.27 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:06<00:01, 16.61it/s, est. speed input: 17677.76 toks/s, output: 17.26 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:06<00:01, 16.76it/s, est. speed input: 17674.99 toks/s, output: 17.26 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:06<00:01, 16.83it/s, est. speed input: 17669.63 toks/s, output: 17.26 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:06<00:01, 16.91it/s, est. speed input: 17667.10 toks/s, output: 17.25 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:06<00:00, 16.97it/s, est. speed input: 17664.42 toks/s, output: 17.25 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:06<00:00, 16.99it/s, est. speed input: 17660.45 toks/s, output: 17.25 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:06<00:00, 16.99it/s, est. speed input: 17656.01 toks/s, output: 17.24 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:06<00:00, 17.05it/s, est. speed input: 17655.00 toks/s, output: 17.24 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:07<00:00, 17.03it/s, est. speed input: 17650.64 toks/s, output: 17.24 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:07<00:00, 16.89it/s, est. speed input: 17639.16 toks/s, output: 17.23 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:07<00:00, 16.86it/s, est. speed input: 17631.75 toks/s, output: 17.22 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:07<00:00, 16.79it/s, est. speed input: 17621.71 toks/s, output: 17.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 16.79it/s, est. speed input: 17617.15 toks/s, output: 17.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:07<00:00, 17.20it/s, est. speed input: 17617.15 toks/s, output: 17.20 toks/s]
[rank0]:[W128 12:20:53.173345890 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.0s

测试结果:
  Requests/s:   16.39
  Tokens/s:     16796.54
  Total Reqs:   128
  Elapsed:      7.81s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     16780.15

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:21:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:21:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50008) WARNING 01-28 12:21:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50008) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50008) WARNING 01-28 12:21:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.95 requests/s, 32745.15 total tokens/s, 31.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 12:21:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:21:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:21:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:21:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:21:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:21:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:21:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:21:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:21:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:21:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:21:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:21:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:21:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:21:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:21:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:21:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:21:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=50008) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=50008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]
(EngineCore_DP0 pid=50008) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.46s/it]
(EngineCore_DP0 pid=50008) 
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=50008) [2026-01-28 12:21:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=50008) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  7.86it/s]
(EngineCore_DP0 pid=50008) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.95it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 34/256 [00:00<00:00, 333.82it/s]
Adding requests:  28%|██▊       | 71/256 [00:00<00:00, 351.46it/s]
Adding requests:  42%|████▏     | 107/256 [00:00<00:00, 351.11it/s]
Adding requests:  56%|█████▌    | 143/256 [00:00<00:00, 349.70it/s]
Adding requests:  70%|██████▉   | 178/256 [00:00<00:00, 341.03it/s]
Adding requests:  83%|████████▎ | 213/256 [00:00<00:00, 340.97it/s]
Adding requests:  98%|█████████▊| 252/256 [00:00<00:00, 355.00it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 349.50it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 157.70it/s, est. speed input: 161510.78 toks/s, output: 157.71 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:00<00:04, 52.36it/s, est. speed input: 60336.02 toks/s, output: 58.92 toks/s]   
Processed prompts:  18%|█▊        | 45/256 [00:00<00:04, 46.92it/s, est. speed input: 54252.17 toks/s, output: 52.98 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:05, 40.75it/s, est. speed input: 48900.06 toks/s, output: 47.75 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 40.99it/s, est. speed input: 48317.55 toks/s, output: 47.18 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 36.99it/s, est. speed input: 45721.38 toks/s, output: 44.65 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 38.09it/s, est. speed input: 45503.95 toks/s, output: 44.44 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:05, 34.89it/s, est. speed input: 43737.88 toks/s, output: 42.71 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:05, 34.66it/s, est. speed input: 43148.37 toks/s, output: 42.14 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:05, 34.46it/s, est. speed input: 42626.93 toks/s, output: 41.63 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:02<00:05, 34.34it/s, est. speed input: 42176.07 toks/s, output: 41.19 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:04, 34.13it/s, est. speed input: 41746.07 toks/s, output: 40.77 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:04, 33.98it/s, est. speed input: 41361.47 toks/s, output: 40.39 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 33.92it/s, est. speed input: 41026.46 toks/s, output: 40.06 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 33.87it/s, est. speed input: 40722.54 toks/s, output: 39.77 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 33.82it/s, est. speed input: 40442.67 toks/s, output: 39.49 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:04, 33.83it/s, est. speed input: 40194.58 toks/s, output: 39.25 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:04, 33.81it/s, est. speed input: 39961.88 toks/s, output: 39.02 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:04, 33.66it/s, est. speed input: 39727.10 toks/s, output: 38.80 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:03<00:04, 33.39it/s, est. speed input: 39484.47 toks/s, output: 38.56 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:03<00:03, 33.03it/s, est. speed input: 39235.30 toks/s, output: 38.32 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:03, 32.53it/s, est. speed input: 38968.59 toks/s, output: 38.05 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:03, 32.49it/s, est. speed input: 38762.98 toks/s, output: 37.85 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 32.59it/s, est. speed input: 38589.41 toks/s, output: 37.68 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:03, 32.53it/s, est. speed input: 38410.16 toks/s, output: 37.51 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:03, 32.45it/s, est. speed input: 38236.96 toks/s, output: 37.34 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:03, 32.56it/s, est. speed input: 38095.89 toks/s, output: 37.20 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:04<00:03, 32.66it/s, est. speed input: 37964.95 toks/s, output: 37.07 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:04<00:03, 32.69it/s, est. speed input: 37837.01 toks/s, output: 36.95 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:04<00:02, 32.76it/s, est. speed input: 37721.44 toks/s, output: 36.84 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 32.77it/s, est. speed input: 37609.00 toks/s, output: 36.73 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 32.80it/s, est. speed input: 37503.94 toks/s, output: 36.62 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 32.91it/s, est. speed input: 37412.78 toks/s, output: 36.54 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 32.87it/s, est. speed input: 37316.08 toks/s, output: 36.44 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:02, 32.96it/s, est. speed input: 37234.11 toks/s, output: 36.36 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:05<00:02, 32.80it/s, est. speed input: 37136.34 toks/s, output: 36.27 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:05<00:02, 32.78it/s, est. speed input: 37050.89 toks/s, output: 36.18 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:05<00:01, 32.81it/s, est. speed input: 36973.67 toks/s, output: 36.11 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:05<00:01, 32.63it/s, est. speed input: 36883.09 toks/s, output: 36.02 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 32.58it/s, est. speed input: 36802.62 toks/s, output: 35.94 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 32.61it/s, est. speed input: 36731.15 toks/s, output: 35.87 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 32.55it/s, est. speed input: 36655.04 toks/s, output: 35.80 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 32.57it/s, est. speed input: 36588.18 toks/s, output: 35.73 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:06<00:01, 32.58it/s, est. speed input: 36522.91 toks/s, output: 35.67 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:06<00:01, 32.58it/s, est. speed input: 36460.39 toks/s, output: 35.61 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:06<00:00, 32.81it/s, est. speed input: 36416.65 toks/s, output: 35.56 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:06<00:00, 32.71it/s, est. speed input: 36355.68 toks/s, output: 35.50 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:08<00:05,  4.66it/s, est. speed input: 26408.11 toks/s, output: 25.79 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:09<00:03,  6.28it/s, est. speed input: 26502.16 toks/s, output: 25.88 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:09<00:01,  8.29it/s, est. speed input: 26595.00 toks/s, output: 25.97 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:09<00:01, 10.68it/s, est. speed input: 26687.30 toks/s, output: 26.06 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:09<00:00, 13.39it/s, est. speed input: 26774.86 toks/s, output: 26.15 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:09<00:00, 16.23it/s, est. speed input: 26854.83 toks/s, output: 26.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 19.13it/s, est. speed input: 26939.02 toks/s, output: 26.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 19.13it/s, est. speed input: 26939.02 toks/s, output: 26.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 26.31it/s, est. speed input: 26939.02 toks/s, output: 26.31 toks/s]
[rank0]:[W128 12:21:51.444071275 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.6s

测试结果:
  Requests/s:   31.95
  Tokens/s:     32745.15
  Total Reqs:   256
  Elapsed:      8.01s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     32713.20

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:22:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:22:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50659) WARNING 01-28 12:22:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=50659) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50659) WARNING 01-28 12:22:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.00 requests/s, 52274.16 total tokens/s, 51.00 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 12:22:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:22:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:22:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:22:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:22:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:22:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:22:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:22:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:22:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:22:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:22:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:22:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:22:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:22:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:22:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:22:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:22:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=50659) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=50659) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=50659) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=50659) 
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=50659) [2026-01-28 12:22:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=50659) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.18it/s]
(EngineCore_DP0 pid=50659) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.54it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 34/512 [00:00<00:01, 339.11it/s]
Adding requests:  14%|█▍        | 71/512 [00:00<00:01, 355.75it/s]
Adding requests:  21%|██▏       | 110/512 [00:00<00:01, 368.70it/s]
Adding requests:  29%|██▊       | 147/512 [00:00<00:00, 366.67it/s]
Adding requests:  36%|███▌      | 185/512 [00:00<00:00, 368.07it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 373.84it/s]
Adding requests:  51%|█████▏    | 263/512 [00:00<00:00, 378.00it/s]
Adding requests:  59%|█████▉    | 301/512 [00:00<00:00, 374.66it/s]
Adding requests:  66%|██████▋   | 340/512 [00:00<00:00, 377.72it/s]
Adding requests:  74%|███████▍  | 379/512 [00:01<00:00, 381.03it/s]
Adding requests:  82%|████████▏ | 418/512 [00:01<00:00, 382.83it/s]
Adding requests:  89%|████████▉ | 457/512 [00:01<00:00, 380.45it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 380.24it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 375.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:00, 506.38it/s, est. speed input: 518611.80 toks/s, output: 506.40 toks/s]
Processed prompts:  22%|██▏       | 113/512 [00:01<00:04, 94.81it/s, est. speed input: 112077.05 toks/s, output: 109.45 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:01<00:05, 74.22it/s, est. speed input: 90432.81 toks/s, output: 88.31 toks/s]  
Processed prompts:  30%|███       | 154/512 [00:01<00:05, 68.57it/s, est. speed input: 84476.71 toks/s, output: 82.50 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:02<00:05, 65.10it/s, est. speed input: 81161.10 toks/s, output: 79.26 toks/s]
Processed prompts:  34%|███▍      | 176/512 [00:02<00:05, 65.23it/s, est. speed input: 80229.70 toks/s, output: 78.35 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:02<00:05, 64.07it/s, est. speed input: 79000.94 toks/s, output: 77.15 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:02<00:05, 61.46it/s, est. speed input: 77473.53 toks/s, output: 75.66 toks/s]
Processed prompts:  39%|███▉      | 200/512 [00:02<00:05, 57.70it/s, est. speed input: 75744.78 toks/s, output: 73.97 toks/s]
Processed prompts:  40%|████      | 207/512 [00:02<00:05, 54.67it/s, est. speed input: 74203.67 toks/s, output: 72.46 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:03<00:05, 52.27it/s, est. speed input: 72810.81 toks/s, output: 71.10 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:03<00:05, 52.25it/s, est. speed input: 71872.93 toks/s, output: 70.19 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:03<00:05, 52.23it/s, est. speed input: 71020.86 toks/s, output: 69.36 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:03<00:05, 52.22it/s, est. speed input: 70244.48 toks/s, output: 68.60 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:03<00:05, 52.22it/s, est. speed input: 69535.76 toks/s, output: 67.91 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:03<00:04, 52.24it/s, est. speed input: 68887.70 toks/s, output: 67.27 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:03<00:04, 52.28it/s, est. speed input: 68294.28 toks/s, output: 66.69 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:04<00:04, 52.27it/s, est. speed input: 67739.22 toks/s, output: 66.15 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:04<00:04, 52.27it/s, est. speed input: 67225.92 toks/s, output: 65.65 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:04<00:04, 52.17it/s, est. speed input: 66732.15 toks/s, output: 65.17 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:04<00:04, 52.11it/s, est. speed input: 66274.84 toks/s, output: 64.72 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:04<00:04, 52.16it/s, est. speed input: 65859.02 toks/s, output: 64.32 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:04<00:03, 52.17it/s, est. speed input: 65466.60 toks/s, output: 63.93 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:05<00:03, 52.12it/s, est. speed input: 65091.59 toks/s, output: 63.57 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:05<00:03, 52.11it/s, est. speed input: 64741.16 toks/s, output: 63.22 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:05<00:03, 52.14it/s, est. speed input: 64416.24 toks/s, output: 62.91 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:05<00:03, 52.15it/s, est. speed input: 64106.83 toks/s, output: 62.60 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:05<00:03, 52.18it/s, est. speed input: 63817.78 toks/s, output: 62.32 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:05<00:02, 52.19it/s, est. speed input: 63543.43 toks/s, output: 62.05 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:05<00:02, 52.21it/s, est. speed input: 63283.65 toks/s, output: 61.80 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:06<00:02, 52.19it/s, est. speed input: 63033.75 toks/s, output: 61.56 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:06<00:02, 52.19it/s, est. speed input: 62797.80 toks/s, output: 61.33 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:06<00:02, 52.16it/s, est. speed input: 62570.71 toks/s, output: 61.10 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:06<00:02, 52.07it/s, est. speed input: 62347.62 toks/s, output: 60.89 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:06<00:02, 52.11it/s, est. speed input: 62143.85 toks/s, output: 60.69 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:06<00:01, 52.14it/s, est. speed input: 61948.92 toks/s, output: 60.50 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:06<00:01, 52.12it/s, est. speed input: 61759.95 toks/s, output: 60.31 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:07<00:01, 52.03it/s, est. speed input: 61571.83 toks/s, output: 60.13 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:07<00:01, 51.91it/s, est. speed input: 61387.52 toks/s, output: 59.95 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:07<00:01, 51.96it/s, est. speed input: 61221.91 toks/s, output: 59.79 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:07<00:01, 51.97it/s, est. speed input: 61060.02 toks/s, output: 59.63 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:07<00:00, 52.03it/s, est. speed input: 60909.32 toks/s, output: 59.48 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:07<00:00, 52.09it/s, est. speed input: 60765.65 toks/s, output: 59.34 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:08<00:00, 52.08it/s, est. speed input: 60623.51 toks/s, output: 59.20 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:08<00:00, 52.11it/s, est. speed input: 60490.06 toks/s, output: 59.07 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:08<00:00, 52.13it/s, est. speed input: 60360.70 toks/s, output: 58.95 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:08<00:00, 52.13it/s, est. speed input: 60234.93 toks/s, output: 58.82 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:08<00:00, 53.69it/s, est. speed input: 60217.24 toks/s, output: 58.81 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 53.69it/s, est. speed input: 60452.06 toks/s, output: 59.04 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 59.03it/s, est. speed input: 60452.06 toks/s, output: 59.04 toks/s]
[rank0]:[W128 12:22:49.208734631 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.3s

测试结果:
  Requests/s:   51.00
  Tokens/s:     52274.16
  Total Reqs:   512
  Elapsed:      10.04s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     52223.16

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:23:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:23:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=51275) WARNING 01-28 12:23:14 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=51275) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=51275) WARNING 01-28 12:23:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.52 requests/s, 52807.92 total tokens/s, 51.52 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 12:23:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:23:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:23:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:23:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:23:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:23:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:23:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:23:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:23:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:23:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:23:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:23:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:23:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:23:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:23:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:23:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:23:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=51275) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=51275) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=51275) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=51275) 
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=51275) [2026-01-28 12:23:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=51275) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  6.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.78it/s]
(EngineCore_DP0 pid=51275) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.56it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 35/1024 [00:00<00:02, 349.42it/s]
Adding requests:   7%|▋         | 73/1024 [00:00<00:02, 365.60it/s]
Adding requests:  11%|█         | 112/1024 [00:00<00:02, 375.41it/s]
Adding requests:  15%|█▍        | 150/1024 [00:00<00:02, 376.15it/s]
Adding requests:  18%|█▊        | 188/1024 [00:00<00:02, 374.09it/s]
Adding requests:  22%|██▏       | 227/1024 [00:00<00:02, 379.24it/s]
Adding requests:  26%|██▌       | 266/1024 [00:00<00:01, 382.02it/s]
Adding requests:  30%|██▉       | 305/1024 [00:00<00:01, 375.08it/s]
Adding requests:  33%|███▎      | 343/1024 [00:00<00:01, 372.41it/s]
Adding requests:  37%|███▋      | 383/1024 [00:01<00:01, 376.92it/s]
Adding requests:  41%|████▏     | 423/1024 [00:01<00:01, 381.69it/s]
Adding requests:  45%|████▌     | 463/1024 [00:01<00:01, 385.23it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 388.91it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:01, 384.08it/s]
Adding requests:  57%|█████▋    | 584/1024 [00:01<00:01, 392.80it/s]
Adding requests:  61%|██████    | 624/1024 [00:01<00:01, 389.33it/s]
Adding requests:  65%|██████▍   | 665/1024 [00:01<00:00, 394.49it/s]
Adding requests:  69%|██████▉   | 708/1024 [00:01<00:00, 403.73it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:01<00:00, 401.87it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:02<00:00, 402.47it/s]
Adding requests:  81%|████████  | 831/1024 [00:02<00:00, 397.47it/s]
Adding requests:  85%|████████▌ | 873/1024 [00:02<00:00, 402.02it/s]
Adding requests:  89%|████████▉ | 915/1024 [00:02<00:00, 406.11it/s]
Adding requests:  93%|█████████▎| 956/1024 [00:02<00:00, 404.28it/s]
Adding requests:  97%|█████████▋| 997/1024 [00:02<00:00, 404.60it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 390.78it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:00<00:00, 1189.54it/s, est. speed input: 1218424.77 toks/s, output: 1189.64 toks/s]
Processed prompts:  24%|██▎       | 241/1024 [00:02<00:08, 93.10it/s, est. speed input: 110849.45 toks/s, output: 108.25 toks/s]    
Processed prompts:  29%|██▊       | 294/1024 [00:03<00:09, 75.36it/s, est. speed input: 91472.13 toks/s, output: 89.33 toks/s]  
Processed prompts:  32%|███▏      | 325/1024 [00:03<00:10, 69.27it/s, est. speed input: 85352.79 toks/s, output: 83.35 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:04<00:10, 64.34it/s, est. speed input: 81345.91 toks/s, output: 79.44 toks/s]
Processed prompts:  35%|███▌      | 361/1024 [00:04<00:09, 67.47it/s, est. speed input: 82004.76 toks/s, output: 80.08 toks/s]
Processed prompts:  37%|███▋      | 375/1024 [00:04<00:10, 63.04it/s, est. speed input: 79806.10 toks/s, output: 77.94 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:11, 56.89it/s, est. speed input: 77260.96 toks/s, output: 75.45 toks/s]
Processed prompts:  39%|███▊      | 395/1024 [00:05<00:10, 57.20it/s, est. speed input: 76775.77 toks/s, output: 74.98 toks/s]
Processed prompts:  39%|███▉      | 403/1024 [00:05<00:11, 56.41it/s, est. speed input: 76128.40 toks/s, output: 74.34 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:11, 54.41it/s, est. speed input: 75336.28 toks/s, output: 73.57 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:05<00:11, 54.04it/s, est. speed input: 74769.39 toks/s, output: 73.02 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:05<00:11, 53.70it/s, est. speed input: 74228.90 toks/s, output: 72.49 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:11, 53.44it/s, est. speed input: 73716.99 toks/s, output: 71.99 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:10, 53.23it/s, est. speed input: 73230.01 toks/s, output: 71.51 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:10, 53.05it/s, est. speed input: 72764.46 toks/s, output: 71.06 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:10, 52.91it/s, est. speed input: 72319.87 toks/s, output: 70.62 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:10, 52.76it/s, est. speed input: 71890.06 toks/s, output: 70.20 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:10, 52.67it/s, est. speed input: 71481.14 toks/s, output: 69.81 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:06<00:10, 52.61it/s, est. speed input: 71091.63 toks/s, output: 69.43 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:10, 52.58it/s, est. speed input: 70719.60 toks/s, output: 69.06 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:10, 52.55it/s, est. speed input: 70362.58 toks/s, output: 68.71 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:09, 52.50it/s, est. speed input: 70017.14 toks/s, output: 68.38 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:09, 52.48it/s, est. speed input: 69687.69 toks/s, output: 68.05 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:09, 52.46it/s, est. speed input: 69370.57 toks/s, output: 67.74 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:09, 52.46it/s, est. speed input: 69066.67 toks/s, output: 67.45 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:09, 52.46it/s, est. speed input: 68774.11 toks/s, output: 67.16 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:09, 52.43it/s, est. speed input: 68490.23 toks/s, output: 66.88 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:08, 52.42it/s, est. speed input: 68218.27 toks/s, output: 66.62 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:08, 52.43it/s, est. speed input: 67957.27 toks/s, output: 66.36 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 52.39it/s, est. speed input: 67701.33 toks/s, output: 66.11 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:08, 52.40it/s, est. speed input: 67457.70 toks/s, output: 65.88 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:08<00:08, 52.43it/s, est. speed input: 67224.05 toks/s, output: 65.65 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:08, 52.45it/s, est. speed input: 66997.63 toks/s, output: 65.43 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:08, 52.45it/s, est. speed input: 66778.26 toks/s, output: 65.21 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:07, 52.41it/s, est. speed input: 66562.71 toks/s, output: 65.00 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:07, 52.38it/s, est. speed input: 66354.23 toks/s, output: 64.80 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 52.42it/s, est. speed input: 66156.72 toks/s, output: 64.61 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:09<00:07, 52.43it/s, est. speed input: 65963.54 toks/s, output: 64.42 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:09<00:07, 52.44it/s, est. speed input: 65776.89 toks/s, output: 64.24 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:07, 52.48it/s, est. speed input: 65597.53 toks/s, output: 64.06 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:06, 52.49it/s, est. speed input: 65422.52 toks/s, output: 63.89 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:06, 52.49it/s, est. speed input: 65252.74 toks/s, output: 63.72 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 52.47it/s, est. speed input: 65086.23 toks/s, output: 63.56 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 52.44it/s, est. speed input: 64923.41 toks/s, output: 63.40 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:10<00:06, 52.40it/s, est. speed input: 64764.08 toks/s, output: 63.25 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:06, 52.41it/s, est. speed input: 64611.17 toks/s, output: 63.10 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:06, 52.41it/s, est. speed input: 64462.44 toks/s, output: 62.95 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:05, 52.39it/s, est. speed input: 64316.36 toks/s, output: 62.81 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 52.38it/s, est. speed input: 64174.22 toks/s, output: 62.67 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 52.38it/s, est. speed input: 64036.67 toks/s, output: 62.54 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:11<00:05, 52.39it/s, est. speed input: 63902.83 toks/s, output: 62.41 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:11<00:05, 52.38it/s, est. speed input: 63771.59 toks/s, output: 62.28 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:05, 52.39it/s, est. speed input: 63644.26 toks/s, output: 62.15 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:05, 52.35it/s, est. speed input: 63518.33 toks/s, output: 62.03 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:04, 52.36it/s, est. speed input: 63396.68 toks/s, output: 61.91 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 52.35it/s, est. speed input: 63277.50 toks/s, output: 61.79 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:12<00:04, 52.34it/s, est. speed input: 63161.23 toks/s, output: 61.68 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:12<00:04, 52.38it/s, est. speed input: 63049.62 toks/s, output: 61.57 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:04, 52.38it/s, est. speed input: 62939.71 toks/s, output: 61.46 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:04, 52.39it/s, est. speed input: 62832.28 toks/s, output: 61.36 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:03, 52.36it/s, est. speed input: 62726.09 toks/s, output: 61.26 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 52.35it/s, est. speed input: 62622.56 toks/s, output: 61.15 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 52.04it/s, est. speed input: 62507.74 toks/s, output: 61.04 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:13<00:03, 51.64it/s, est. speed input: 62387.25 toks/s, output: 60.92 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:13<00:03, 51.37it/s, est. speed input: 62269.83 toks/s, output: 60.81 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:03, 51.19it/s, est. speed input: 62155.03 toks/s, output: 60.70 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:03, 51.04it/s, est. speed input: 62041.96 toks/s, output: 60.59 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 50.94it/s, est. speed input: 61931.71 toks/s, output: 60.48 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 50.91it/s, est. speed input: 61825.48 toks/s, output: 60.38 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:14<00:02, 50.88it/s, est. speed input: 61720.85 toks/s, output: 60.27 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:14<00:02, 50.87it/s, est. speed input: 61618.89 toks/s, output: 60.17 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:15<00:02, 50.88it/s, est. speed input: 61520.04 toks/s, output: 60.08 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:15<00:02, 50.88it/s, est. speed input: 61422.96 toks/s, output: 59.98 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:02, 50.87it/s, est. speed input: 61327.28 toks/s, output: 59.89 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 50.84it/s, est. speed input: 61232.61 toks/s, output: 59.80 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:15<00:01, 50.85it/s, est. speed input: 61141.03 toks/s, output: 59.71 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:15<00:01, 50.85it/s, est. speed input: 61051.01 toks/s, output: 59.62 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:16<00:01, 50.85it/s, est. speed input: 60963.09 toks/s, output: 59.53 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:16<00:01, 50.83it/s, est. speed input: 60875.94 toks/s, output: 59.45 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:01, 50.80it/s, est. speed input: 60789.63 toks/s, output: 59.36 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 50.77it/s, est. speed input: 60704.60 toks/s, output: 59.28 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 52.17it/s, est. speed input: 60673.63 toks/s, output: 59.25 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:16<00:00, 51.76it/s, est. speed input: 60592.64 toks/s, output: 59.17 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:16<00:00, 51.49it/s, est. speed input: 60513.95 toks/s, output: 59.10 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:17<00:00, 51.32it/s, est. speed input: 60437.02 toks/s, output: 59.02 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:17<00:00, 52.86it/s, est. speed input: 60418.93 toks/s, output: 59.00 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 52.86it/s, est. speed input: 60774.18 toks/s, output: 59.35 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 59.35it/s, est. speed input: 60774.18 toks/s, output: 59.35 toks/s]
[rank0]:[W128 12:24:03.754809807 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 74.0s

测试结果:
  Requests/s:   51.52
  Tokens/s:     52807.92
  Total Reqs:   1024
  Elapsed:      19.88s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     52756.40

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:24:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:24:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=52018) WARNING 01-28 12:24:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=52018) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=52018) WARNING 01-28 12:24:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.82 requests/s, 52091.29 total tokens/s, 50.82 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 12:24:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:24:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:24:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:24:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:24:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:24:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:24:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:24:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:24:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:24:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:24:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:24:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:24:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:24:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:24:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:24:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:24:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=52018) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=52018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=52018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=52018) 
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=52018) [2026-01-28 12:24:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=52018) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  8.27it/s]
(EngineCore_DP0 pid=52018) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.15it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  8.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  8.80it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 37/2048 [00:00<00:05, 365.72it/s]
Adding requests:   4%|▎         | 76/2048 [00:00<00:05, 378.07it/s]
Adding requests:   6%|▌         | 115/2048 [00:00<00:05, 382.57it/s]
Adding requests:   8%|▊         | 154/2048 [00:00<00:04, 382.83it/s]
Adding requests:   9%|▉         | 193/2048 [00:00<00:04, 380.39it/s]
Adding requests:  11%|█▏        | 232/2048 [00:00<00:04, 381.80it/s]
Adding requests:  13%|█▎        | 271/2048 [00:00<00:04, 381.93it/s]
Adding requests:  15%|█▌        | 310/2048 [00:00<00:04, 380.74it/s]
Adding requests:  17%|█▋        | 349/2048 [00:00<00:04, 381.60it/s]
Adding requests:  19%|█▉        | 389/2048 [00:01<00:04, 386.03it/s]
Adding requests:  21%|██        | 429/2048 [00:01<00:04, 390.04it/s]
Adding requests:  23%|██▎       | 469/2048 [00:01<00:04, 388.04it/s]
Adding requests:  25%|██▍       | 508/2048 [00:01<00:03, 386.25it/s]
Adding requests:  27%|██▋       | 547/2048 [00:01<00:03, 380.30it/s]
Adding requests:  29%|██▊       | 586/2048 [00:01<00:03, 373.31it/s]
Adding requests:  31%|███       | 626/2048 [00:01<00:03, 378.22it/s]
Adding requests:  33%|███▎      | 666/2048 [00:01<00:03, 382.04it/s]
Adding requests:  35%|███▍      | 708/2048 [00:01<00:03, 390.90it/s]
Adding requests:  37%|███▋      | 748/2048 [00:01<00:03, 381.48it/s]
Adding requests:  39%|███▊      | 789/2048 [00:02<00:03, 387.07it/s]
Adding requests:  40%|████      | 828/2048 [00:02<00:03, 384.14it/s]
Adding requests:  42%|████▏     | 869/2048 [00:02<00:03, 389.85it/s]
Adding requests:  44%|████▍     | 911/2048 [00:02<00:02, 398.27it/s]
Adding requests:  46%|████▋     | 951/2048 [00:02<00:02, 397.16it/s]
Adding requests:  48%|████▊     | 991/2048 [00:02<00:02, 395.42it/s]
Adding requests:  50%|█████     | 1032/2048 [00:02<00:02, 397.20it/s]
Adding requests:  52%|█████▏    | 1072/2048 [00:02<00:02, 385.36it/s]
Adding requests:  54%|█████▍    | 1112/2048 [00:02<00:02, 388.91it/s]
Adding requests:  56%|█████▌    | 1151/2048 [00:02<00:02, 388.22it/s]
Adding requests:  58%|█████▊    | 1191/2048 [00:03<00:02, 389.67it/s]
Adding requests:  60%|██████    | 1230/2048 [00:03<00:02, 387.86it/s]
Adding requests:  62%|██████▏   | 1269/2048 [00:03<00:02, 382.82it/s]
Adding requests:  64%|██████▍   | 1309/2048 [00:03<00:01, 386.87it/s]
Adding requests:  66%|██████▌   | 1351/2048 [00:03<00:01, 395.15it/s]
Adding requests:  68%|██████▊   | 1393/2048 [00:03<00:01, 399.90it/s]
Adding requests:  70%|███████   | 1434/2048 [00:03<00:01, 396.25it/s]
Adding requests:  72%|███████▏  | 1475/2048 [00:03<00:01, 397.89it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:03<00:01, 403.90it/s]
Adding requests:  76%|███████▌  | 1558/2048 [00:04<00:01, 403.97it/s]
Adding requests:  78%|███████▊  | 1602/2048 [00:04<00:01, 412.71it/s]
Adding requests:  80%|████████  | 1644/2048 [00:04<00:00, 405.56it/s]
Adding requests:  82%|████████▏ | 1685/2048 [00:04<00:00, 405.43it/s]
Adding requests:  84%|████████▍ | 1727/2048 [00:04<00:00, 408.52it/s]
Adding requests:  86%|████████▋ | 1768/2048 [00:04<00:00, 406.47it/s]
Adding requests:  88%|████████▊ | 1809/2048 [00:04<00:00, 406.80it/s]
Adding requests:  90%|█████████ | 1851/2048 [00:04<00:00, 410.12it/s]
Adding requests:  92%|█████████▏| 1893/2048 [00:04<00:00, 406.17it/s]
Adding requests:  94%|█████████▍| 1934/2048 [00:04<00:00, 394.37it/s]
Adding requests:  96%|█████████▋| 1976/2048 [00:05<00:00, 399.89it/s]
Adding requests:  99%|█████████▊| 2018/2048 [00:05<00:00, 404.79it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 392.87it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:00<00:01, 1178.01it/s, est. speed input: 1206429.16 toks/s, output: 1178.06 toks/s]
Processed prompts:  18%|█▊        | 376/2048 [00:02<00:12, 129.16it/s, est. speed input: 161942.96 toks/s, output: 158.15 toks/s]   
Processed prompts:  21%|██        | 428/2048 [00:03<00:15, 102.34it/s, est. speed input: 132415.46 toks/s, output: 129.31 toks/s]
Processed prompts:  22%|██▏       | 459/2048 [00:03<00:17, 88.56it/s, est. speed input: 119368.17 toks/s, output: 116.57 toks/s] 
Processed prompts:  23%|██▎       | 480/2048 [00:04<00:18, 85.31it/s, est. speed input: 115704.28 toks/s, output: 112.99 toks/s]
Processed prompts:  24%|██▍       | 496/2048 [00:04<00:19, 79.17it/s, est. speed input: 111413.71 toks/s, output: 108.80 toks/s]
Processed prompts:  25%|██▍       | 509/2048 [00:04<00:21, 71.51it/s, est. speed input: 107053.10 toks/s, output: 104.54 toks/s]
Processed prompts:  25%|██▌       | 519/2048 [00:05<00:24, 62.54it/s, est. speed input: 102597.73 toks/s, output: 100.19 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:05<00:27, 55.89it/s, est. speed input: 98846.81 toks/s, output: 96.53 toks/s]  
Processed prompts:  27%|██▋       | 546/2048 [00:05<00:27, 54.76it/s, est. speed input: 96379.71 toks/s, output: 94.12 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:06<00:27, 53.88it/s, est. speed input: 94162.69 toks/s, output: 91.96 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:06<00:27, 53.22it/s, est. speed input: 92159.21 toks/s, output: 90.00 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:06<00:27, 52.74it/s, est. speed input: 90341.81 toks/s, output: 88.22 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:07<00:27, 52.38it/s, est. speed input: 88683.62 toks/s, output: 86.60 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:07<00:27, 52.12it/s, est. speed input: 87163.18 toks/s, output: 85.12 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:07<00:27, 51.95it/s, est. speed input: 85771.52 toks/s, output: 83.76 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:07<00:26, 51.81it/s, est. speed input: 84483.66 toks/s, output: 82.50 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:08<00:26, 51.71it/s, est. speed input: 83291.03 toks/s, output: 81.34 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:08<00:26, 51.65it/s, est. speed input: 82186.85 toks/s, output: 80.26 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:08<00:26, 51.60it/s, est. speed input: 81158.99 toks/s, output: 79.26 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:09<00:25, 51.56it/s, est. speed input: 80199.03 toks/s, output: 78.32 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:09<00:25, 51.48it/s, est. speed input: 79293.62 toks/s, output: 77.44 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:09<00:25, 50.61it/s, est. speed input: 78311.83 toks/s, output: 76.48 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:12<01:29, 14.25it/s, est. speed input: 61277.21 toks/s, output: 59.84 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:13<01:09, 18.09it/s, est. speed input: 60992.45 toks/s, output: 59.56 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:13<00:55, 22.28it/s, est. speed input: 60719.02 toks/s, output: 59.30 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:13<00:46, 26.61it/s, est. speed input: 60461.89 toks/s, output: 59.04 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:14<00:39, 30.79it/s, est. speed input: 60214.86 toks/s, output: 58.80 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:14<00:34, 34.60it/s, est. speed input: 59979.60 toks/s, output: 58.57 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:14<00:31, 37.88it/s, est. speed input: 59754.15 toks/s, output: 58.35 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:15<00:28, 40.57it/s, est. speed input: 59537.65 toks/s, output: 58.14 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:15<00:26, 42.69it/s, est. speed input: 59331.31 toks/s, output: 57.94 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:15<00:25, 44.31it/s, est. speed input: 59132.51 toks/s, output: 57.75 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:16<00:24, 45.52it/s, est. speed input: 58941.81 toks/s, output: 57.56 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:16<00:23, 46.41it/s, est. speed input: 58759.10 toks/s, output: 57.38 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:16<00:23, 47.05it/s, est. speed input: 58583.45 toks/s, output: 57.21 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:17<00:22, 48.13it/s, est. speed input: 58463.76 toks/s, output: 57.09 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:17<00:21, 48.27it/s, est. speed input: 58299.97 toks/s, output: 56.93 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:17<00:21, 48.37it/s, est. speed input: 58141.91 toks/s, output: 56.78 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:18<00:21, 48.44it/s, est. speed input: 57989.97 toks/s, output: 56.63 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:18<00:20, 48.50it/s, est. speed input: 57843.74 toks/s, output: 56.49 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:18<00:20, 48.61it/s, est. speed input: 57707.70 toks/s, output: 56.36 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:19<00:19, 49.08it/s, est. speed input: 57602.92 toks/s, output: 56.25 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:19<00:19, 49.42it/s, est. speed input: 57501.71 toks/s, output: 56.15 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:19<00:18, 49.65it/s, est. speed input: 57402.91 toks/s, output: 56.06 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:20<00:18, 49.83it/s, est. speed input: 57308.47 toks/s, output: 55.97 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:20<00:18, 50.01it/s, est. speed input: 57220.21 toks/s, output: 55.88 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:20<00:17, 50.91it/s, est. speed input: 57179.29 toks/s, output: 55.84 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:20<00:17, 50.84it/s, est. speed input: 57099.85 toks/s, output: 55.76 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:21<00:16, 50.80it/s, est. speed input: 57023.31 toks/s, output: 55.69 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:21<00:16, 50.77it/s, est. speed input: 56948.75 toks/s, output: 55.61 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:21<00:16, 50.75it/s, est. speed input: 56876.58 toks/s, output: 55.54 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:22<00:16, 50.73it/s, est. speed input: 56805.85 toks/s, output: 55.47 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:22<00:15, 50.72it/s, est. speed input: 56737.22 toks/s, output: 55.41 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:22<00:15, 50.69it/s, est. speed input: 56669.70 toks/s, output: 55.34 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:23<00:15, 50.69it/s, est. speed input: 56604.57 toks/s, output: 55.28 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:23<00:14, 50.68it/s, est. speed input: 56541.34 toks/s, output: 55.22 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:23<00:14, 50.68it/s, est. speed input: 56479.88 toks/s, output: 55.16 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:24<00:14, 50.69it/s, est. speed input: 56420.23 toks/s, output: 55.10 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:24<00:13, 50.69it/s, est. speed input: 56361.96 toks/s, output: 55.04 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:24<00:13, 50.69it/s, est. speed input: 56305.16 toks/s, output: 54.99 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:25<00:13, 50.68it/s, est. speed input: 56249.48 toks/s, output: 54.93 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:25<00:12, 50.67it/s, est. speed input: 56194.98 toks/s, output: 54.88 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:25<00:12, 50.67it/s, est. speed input: 56142.10 toks/s, output: 54.83 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:26<00:12, 50.67it/s, est. speed input: 56090.52 toks/s, output: 54.78 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:26<00:11, 50.67it/s, est. speed input: 56039.84 toks/s, output: 54.73 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:26<00:11, 50.67it/s, est. speed input: 55990.89 toks/s, output: 54.68 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:26<00:11, 50.69it/s, est. speed input: 55943.67 toks/s, output: 54.63 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:27<00:11, 50.68it/s, est. speed input: 55896.66 toks/s, output: 54.59 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:27<00:10, 50.69it/s, est. speed input: 55851.05 toks/s, output: 54.54 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:27<00:10, 50.68it/s, est. speed input: 55806.17 toks/s, output: 54.50 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:28<00:10, 50.73it/s, est. speed input: 55764.64 toks/s, output: 54.46 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:28<00:09, 51.12it/s, est. speed input: 55737.97 toks/s, output: 54.43 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:28<00:09, 51.42it/s, est. speed input: 55712.89 toks/s, output: 54.41 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:29<00:08, 51.58it/s, est. speed input: 55686.42 toks/s, output: 54.38 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:29<00:08, 51.73it/s, est. speed input: 55661.83 toks/s, output: 54.36 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:29<00:08, 51.82it/s, est. speed input: 55637.61 toks/s, output: 54.33 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:30<00:07, 51.89it/s, est. speed input: 55613.54 toks/s, output: 54.31 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:30<00:07, 51.94it/s, est. speed input: 55590.08 toks/s, output: 54.29 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:30<00:07, 51.96it/s, est. speed input: 55566.78 toks/s, output: 54.26 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:31<00:07, 51.98it/s, est. speed input: 55543.93 toks/s, output: 54.24 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:31<00:06, 51.98it/s, est. speed input: 55521.27 toks/s, output: 54.22 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:31<00:06, 51.98it/s, est. speed input: 55499.03 toks/s, output: 54.20 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:31<00:06, 52.00it/s, est. speed input: 55477.69 toks/s, output: 54.18 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:32<00:05, 52.01it/s, est. speed input: 55456.74 toks/s, output: 54.16 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:32<00:05, 52.00it/s, est. speed input: 55435.79 toks/s, output: 54.14 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:32<00:05, 52.02it/s, est. speed input: 55415.75 toks/s, output: 54.12 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:33<00:04, 52.02it/s, est. speed input: 55395.86 toks/s, output: 54.10 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:33<00:04, 52.05it/s, est. speed input: 55377.41 toks/s, output: 54.08 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:33<00:04, 52.01it/s, est. speed input: 55357.14 toks/s, output: 54.06 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:34<00:03, 52.01it/s, est. speed input: 55338.17 toks/s, output: 54.04 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:34<00:03, 52.02it/s, est. speed input: 55319.81 toks/s, output: 54.02 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:34<00:03, 52.75it/s, est. speed input: 55324.37 toks/s, output: 54.03 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:34<00:03, 52.53it/s, est. speed input: 55306.22 toks/s, output: 54.01 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:35<00:02, 52.38it/s, est. speed input: 55288.62 toks/s, output: 53.99 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:35<00:02, 52.28it/s, est. speed input: 55271.37 toks/s, output: 53.98 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:35<00:02, 52.20it/s, est. speed input: 55254.33 toks/s, output: 53.96 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:36<00:01, 52.14it/s, est. speed input: 55237.12 toks/s, output: 53.94 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:36<00:01, 51.92it/s, est. speed input: 55215.01 toks/s, output: 53.92 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:36<00:01, 51.77it/s, est. speed input: 55193.62 toks/s, output: 53.90 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:37<00:00, 51.67it/s, est. speed input: 55172.38 toks/s, output: 53.88 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:37<00:00, 51.60it/s, est. speed input: 55151.67 toks/s, output: 53.86 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:37<00:00, 52.40it/s, est. speed input: 55155.69 toks/s, output: 53.86 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:37<00:00, 52.40it/s, est. speed input: 55534.82 toks/s, output: 54.23 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:37<00:00, 54.23it/s, est. speed input: 55534.82 toks/s, output: 54.23 toks/s]
[rank0]:[W128 12:25:46.695582609 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 103.4s

测试结果:
  Requests/s:   50.82
  Tokens/s:     52091.29
  Total Reqs:   2048
  Elapsed:      40.30s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     52040.47

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:26:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:26:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=53210) WARNING 01-28 12:26:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=53210) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=53210) WARNING 01-28 12:26:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.73 requests/s, 51995.61 total tokens/s, 50.73 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 12:26:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:26:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:26:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:26:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:26:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:26:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:26:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:26:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:26:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:26:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:26:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:26:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:26:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:26:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:26:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:26:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:26:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=53210) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=53210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=53210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.40it/s]
(EngineCore_DP0 pid=53210) 
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=53210) [2026-01-28 12:26:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:48.286000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:48.385000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:49.723000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) [rank0]:W0128 12:26:49.877000 53210 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=53210) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.35it/s]
(EngineCore_DP0 pid=53210) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  8.68it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 36/4096 [00:00<00:11, 355.40it/s]
Adding requests:   2%|▏         | 75/4096 [00:00<00:10, 370.76it/s]
Adding requests:   3%|▎         | 114/4096 [00:00<00:10, 374.99it/s]
Adding requests:   4%|▎         | 152/4096 [00:00<00:10, 375.52it/s]
Adding requests:   5%|▍         | 190/4096 [00:00<00:10, 375.96it/s]
Adding requests:   6%|▌         | 230/4096 [00:00<00:10, 382.59it/s]
Adding requests:   7%|▋         | 269/4096 [00:00<00:10, 378.34it/s]
Adding requests:   7%|▋         | 307/4096 [00:00<00:10, 375.47it/s]
Adding requests:   8%|▊         | 345/4096 [00:00<00:09, 376.29it/s]
Adding requests:   9%|▉         | 383/4096 [00:01<00:09, 375.88it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:09, 377.94it/s]
Adding requests:  11%|█▏        | 461/4096 [00:01<00:09, 379.76it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:09, 381.53it/s]
Adding requests:  13%|█▎        | 539/4096 [00:01<00:09, 376.92it/s]
Adding requests:  14%|█▍        | 579/4096 [00:01<00:09, 383.65it/s]
Adding requests:  15%|█▌        | 618/4096 [00:01<00:09, 384.30it/s]
Adding requests:  16%|█▌        | 658/4096 [00:01<00:08, 387.92it/s]
Adding requests:  17%|█▋        | 700/4096 [00:01<00:08, 396.37it/s]
Adding requests:  18%|█▊        | 740/4096 [00:01<00:08, 395.53it/s]
Adding requests:  19%|█▉        | 780/4096 [00:02<00:08, 389.89it/s]
Adding requests:  20%|██        | 820/4096 [00:02<00:08, 385.09it/s]
Adding requests:  21%|██        | 859/4096 [00:02<00:08, 385.58it/s]
Adding requests:  22%|██▏       | 900/4096 [00:02<00:08, 391.87it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:08, 393.91it/s]
Adding requests:  24%|██▍       | 980/4096 [00:02<00:07, 394.13it/s]
Adding requests:  25%|██▍       | 1021/4096 [00:02<00:07, 398.30it/s]
Adding requests:  26%|██▌       | 1062/4096 [00:02<00:07, 401.00it/s]
Adding requests:  27%|██▋       | 1103/4096 [00:02<00:07, 397.68it/s]
Adding requests:  28%|██▊       | 1143/4096 [00:02<00:07, 393.63it/s]
Adding requests:  29%|██▉       | 1186/4096 [00:03<00:07, 390.23it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:03<00:07, 393.35it/s]
Adding requests:  31%|███       | 1267/4096 [00:03<00:07, 393.12it/s]
Adding requests:  32%|███▏      | 1307/4096 [00:03<00:07, 394.71it/s]
Adding requests:  33%|███▎      | 1347/4096 [00:03<00:07, 392.32it/s]
Adding requests:  34%|███▍      | 1387/4096 [00:03<00:06, 391.65it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:06, 391.23it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:03<00:06, 392.52it/s]
Adding requests:  37%|███▋      | 1508/4096 [00:03<00:06, 396.60it/s]
Adding requests:  38%|███▊      | 1549/4096 [00:03<00:06, 399.60it/s]
Adding requests:  39%|███▉      | 1589/4096 [00:04<00:06, 396.95it/s]
Adding requests:  40%|███▉      | 1629/4096 [00:04<00:06, 396.31it/s]
Adding requests:  41%|████      | 1669/4096 [00:04<00:06, 389.14it/s]
Adding requests:  42%|████▏     | 1708/4096 [00:04<00:06, 389.24it/s]
Adding requests:  43%|████▎     | 1747/4096 [00:04<00:06, 387.43it/s]
Adding requests:  44%|████▎     | 1786/4096 [00:04<00:05, 386.75it/s]
Adding requests:  45%|████▍     | 1825/4096 [00:04<00:05, 385.89it/s]
Adding requests:  46%|████▌     | 1864/4096 [00:04<00:05, 379.50it/s]
Adding requests:  46%|████▋     | 1904/4096 [00:04<00:05, 384.20it/s]
Adding requests:  47%|████▋     | 1943/4096 [00:05<00:05, 384.49it/s]
Adding requests:  48%|████▊     | 1982/4096 [00:05<00:05, 383.24it/s]
Adding requests:  49%|████▉     | 2021/4096 [00:05<00:05, 381.17it/s]
Adding requests:  50%|█████     | 2061/4096 [00:05<00:05, 385.40it/s]
Adding requests:  51%|█████▏    | 2100/4096 [00:05<00:05, 382.20it/s]
Adding requests:  52%|█████▏    | 2139/4096 [00:05<00:05, 376.33it/s]
Adding requests:  53%|█████▎    | 2177/4096 [00:05<00:05, 374.70it/s]
Adding requests:  54%|█████▍    | 2215/4096 [00:05<00:05, 373.75it/s]
Adding requests:  55%|█████▌    | 2253/4096 [00:05<00:04, 375.33it/s]
Adding requests:  56%|█████▌    | 2291/4096 [00:05<00:04, 373.25it/s]
Adding requests:  57%|█████▋    | 2329/4096 [00:08<00:42, 41.36it/s] 
Adding requests:  58%|█████▊    | 2365/4096 [00:08<00:31, 55.46it/s]
Adding requests:  59%|█████▊    | 2402/4096 [00:08<00:22, 74.18it/s]
Adding requests:  59%|█████▉    | 2436/4096 [00:09<00:17, 94.81it/s]
Adding requests:  60%|██████    | 2474/4096 [00:09<00:13, 123.37it/s]
Adding requests:  61%|██████▏   | 2512/4096 [00:09<00:10, 155.37it/s]
Adding requests:  62%|██████▏   | 2549/4096 [00:09<00:08, 187.70it/s]
Adding requests:  63%|██████▎   | 2588/4096 [00:09<00:06, 223.05it/s]
Adding requests:  64%|██████▍   | 2626/4096 [00:09<00:05, 253.85it/s]
Adding requests:  65%|██████▌   | 2666/4096 [00:09<00:05, 285.10it/s]
Adding requests:  66%|██████▌   | 2704/4096 [00:09<00:04, 306.11it/s]
Adding requests:  67%|██████▋   | 2743/4096 [00:09<00:04, 326.64it/s]
Adding requests:  68%|██████▊   | 2781/4096 [00:10<00:03, 337.92it/s]
Adding requests:  69%|██████▉   | 2819/4096 [00:10<00:03, 346.64it/s]
Adding requests:  70%|██████▉   | 2857/4096 [00:10<00:03, 352.22it/s]
Adding requests:  71%|███████   | 2896/4096 [00:10<00:03, 362.31it/s]
Adding requests:  72%|███████▏  | 2934/4096 [00:10<00:03, 365.27it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:10<00:03, 370.18it/s]
Adding requests:  74%|███████▎  | 3012/4096 [00:10<00:02, 374.52it/s]
Adding requests:  74%|███████▍  | 3051/4096 [00:10<00:02, 376.04it/s]
Adding requests:  75%|███████▌  | 3089/4096 [00:10<00:02, 374.80it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:10<00:02, 373.80it/s]
Adding requests:  77%|███████▋  | 3165/4096 [00:11<00:02, 374.20it/s]
Adding requests:  78%|███████▊  | 3203/4096 [00:11<00:02, 373.92it/s]
Adding requests:  79%|███████▉  | 3242/4096 [00:11<00:02, 378.44it/s]
Adding requests:  80%|████████  | 3281/4096 [00:11<00:02, 381.01it/s]
Adding requests:  81%|████████  | 3320/4096 [00:11<00:02, 380.24it/s]
Adding requests:  82%|████████▏ | 3359/4096 [00:11<00:01, 380.36it/s]
Adding requests:  83%|████████▎ | 3398/4096 [00:11<00:01, 378.21it/s]
Adding requests:  84%|████████▍ | 3438/4096 [00:11<00:01, 380.45it/s]
Adding requests:  85%|████████▍ | 3477/4096 [00:11<00:01, 374.58it/s]
Adding requests:  86%|████████▌ | 3515/4096 [00:11<00:01, 375.20it/s]
Adding requests:  87%|████████▋ | 3553/4096 [00:12<00:01, 366.90it/s]
Adding requests:  88%|████████▊ | 3590/4096 [00:12<00:01, 365.62it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:12<00:01, 362.76it/s]
Adding requests:  90%|████████▉ | 3666/4096 [00:12<00:01, 369.88it/s]
Adding requests:  91%|█████████ | 3707/4096 [00:12<00:01, 379.28it/s]
Adding requests:  91%|█████████▏| 3746/4096 [00:12<00:00, 381.98it/s]
Adding requests:  92%|█████████▏| 3785/4096 [00:12<00:00, 377.04it/s]
Adding requests:  93%|█████████▎| 3825/4096 [00:12<00:00, 381.48it/s]
Adding requests:  94%|█████████▍| 3865/4096 [00:12<00:00, 385.25it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:12<00:00, 384.20it/s]
Adding requests:  96%|█████████▋| 3944/4096 [00:13<00:00, 388.11it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:13<00:00, 390.47it/s]
Adding requests:  98%|█████████▊| 4024/4096 [00:13<00:00, 392.97it/s]
Adding requests:  99%|█████████▉| 4064/4096 [00:13<00:00, 389.75it/s]
Adding requests: 100%|██████████| 4096/4096 [00:13<00:00, 304.13it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:00<00:03, 970.80it/s, est. speed input: 994155.25 toks/s, output: 970.81 toks/s]
Processed prompts:  15%|█▍        | 612/4096 [00:02<00:17, 204.09it/s, est. speed input: 260909.29 toks/s, output: 254.79 toks/s]
Processed prompts:  16%|█▌        | 656/4096 [00:03<00:20, 166.63it/s, est. speed input: 222506.99 toks/s, output: 217.29 toks/s]
Processed prompts:  17%|█▋        | 683/4096 [00:03<00:25, 131.37it/s, est. speed input: 192380.75 toks/s, output: 187.87 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:04<00:32, 104.00it/s, est. speed input: 170015.55 toks/s, output: 166.03 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:04<00:37, 88.70it/s, est. speed input: 155211.87 toks/s, output: 151.57 toks/s] 
Processed prompts:  19%|█▉        | 770/4096 [00:05<00:42, 77.83it/s, est. speed input: 143737.48 toks/s, output: 140.37 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:06<00:46, 70.12it/s, est. speed input: 134576.07 toks/s, output: 131.42 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:06<00:50, 64.68it/s, est. speed input: 127096.13 toks/s, output: 124.12 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:07<00:53, 60.85it/s, est. speed input: 120872.35 toks/s, output: 118.04 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:07<00:54, 58.17it/s, est. speed input: 115617.51 toks/s, output: 112.91 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:08<00:56, 56.28it/s, est. speed input: 111116.44 toks/s, output: 108.51 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:09<00:57, 54.96it/s, est. speed input: 107220.09 toks/s, output: 104.71 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:09<00:57, 54.10it/s, est. speed input: 103842.27 toks/s, output: 101.41 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:10<00:57, 53.51it/s, est. speed input: 100867.57 toks/s, output: 98.50 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:11<00:57, 53.09it/s, est. speed input: 98224.40 toks/s, output: 95.92 toks/s] 
Processed prompts:  27%|██▋       | 1090/4096 [00:11<00:56, 52.80it/s, est. speed input: 95857.78 toks/s, output: 93.61 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:12<00:56, 52.59it/s, est. speed input: 93729.19 toks/s, output: 91.53 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:12<00:55, 52.83it/s, est. speed input: 91909.68 toks/s, output: 89.76 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:13<00:55, 52.62it/s, est. speed input: 90155.08 toks/s, output: 88.04 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:14<00:54, 52.47it/s, est. speed input: 88551.53 toks/s, output: 86.48 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:14<00:54, 52.37it/s, est. speed input: 87082.23 toks/s, output: 85.04 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:15<00:53, 52.29it/s, est. speed input: 85730.46 toks/s, output: 83.72 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:15<00:53, 52.23it/s, est. speed input: 84481.19 toks/s, output: 82.50 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:16<00:52, 52.19it/s, est. speed input: 83324.62 toks/s, output: 81.37 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:17<00:52, 52.10it/s, est. speed input: 82239.88 toks/s, output: 80.31 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:17<00:51, 51.70it/s, est. speed input: 81169.07 toks/s, output: 79.27 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:18<00:51, 51.38it/s, est. speed input: 80163.75 toks/s, output: 78.28 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:19<00:51, 51.15it/s, est. speed input: 79224.51 toks/s, output: 77.37 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:19<00:50, 51.00it/s, est. speed input: 78345.78 toks/s, output: 76.51 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:20<00:50, 50.89it/s, est. speed input: 77522.00 toks/s, output: 75.70 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:20<00:49, 50.81it/s, est. speed input: 76747.42 toks/s, output: 74.95 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:21<00:49, 50.76it/s, est. speed input: 76018.00 toks/s, output: 74.24 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:22<00:48, 50.72it/s, est. speed input: 75330.55 toks/s, output: 73.56 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:22<00:47, 50.69it/s, est. speed input: 74680.83 toks/s, output: 72.93 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:23<00:47, 50.67it/s, est. speed input: 74065.56 toks/s, output: 72.33 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:24<00:46, 50.67it/s, est. speed input: 73483.80 toks/s, output: 71.76 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:24<00:46, 50.65it/s, est. speed input: 72930.17 toks/s, output: 71.22 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:25<00:45, 50.61it/s, est. speed input: 72401.42 toks/s, output: 70.70 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:26<00:45, 49.94it/s, est. speed input: 71822.53 toks/s, output: 70.14 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:26<00:44, 49.87it/s, est. speed input: 71317.74 toks/s, output: 69.65 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:27<00:44, 49.40it/s, est. speed input: 70789.51 toks/s, output: 69.13 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:30<01:40, 21.68it/s, est. speed input: 64007.78 toks/s, output: 62.51 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:31<01:22, 25.98it/s, est. speed input: 63704.09 toks/s, output: 62.21 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:32<01:09, 30.18it/s, est. speed input: 63413.79 toks/s, output: 61.93 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:32<01:01, 34.03it/s, est. speed input: 63134.39 toks/s, output: 61.65 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:33<00:54, 37.64it/s, est. speed input: 62906.86 toks/s, output: 61.43 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:34<00:49, 40.71it/s, est. speed input: 62693.32 toks/s, output: 61.22 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:34<00:45, 43.18it/s, est. speed input: 62487.66 toks/s, output: 61.02 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:35<00:43, 45.10it/s, est. speed input: 62289.62 toks/s, output: 60.83 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:35<00:41, 46.54it/s, est. speed input: 62098.51 toks/s, output: 60.64 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:36<00:39, 47.95it/s, est. speed input: 61941.21 toks/s, output: 60.49 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:37<00:37, 49.13it/s, est. speed input: 61799.69 toks/s, output: 60.35 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:37<00:36, 50.34it/s, est. speed input: 61686.98 toks/s, output: 60.24 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:38<00:35, 50.86it/s, est. speed input: 61553.68 toks/s, output: 60.11 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:38<00:34, 51.27it/s, est. speed input: 61427.45 toks/s, output: 59.99 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:39<00:33, 51.49it/s, est. speed input: 61300.26 toks/s, output: 59.86 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:40<00:32, 51.72it/s, est. speed input: 61181.76 toks/s, output: 59.75 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:40<00:32, 51.78it/s, est. speed input: 61060.95 toks/s, output: 59.63 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:41<00:31, 51.89it/s, est. speed input: 60947.13 toks/s, output: 59.52 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:42<00:30, 52.33it/s, est. speed input: 60857.96 toks/s, output: 59.43 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:42<00:29, 52.28it/s, est. speed input: 60750.49 toks/s, output: 59.33 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:43<00:29, 52.22it/s, est. speed input: 60645.53 toks/s, output: 59.22 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:43<00:28, 52.19it/s, est. speed input: 60543.44 toks/s, output: 59.12 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:44<00:28, 52.17it/s, est. speed input: 60444.63 toks/s, output: 59.03 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:45<00:27, 52.16it/s, est. speed input: 60348.49 toks/s, output: 58.93 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:45<00:26, 52.14it/s, est. speed input: 60254.48 toks/s, output: 58.84 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:46<00:26, 52.17it/s, est. speed input: 60165.26 toks/s, output: 58.76 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:46<00:25, 52.10it/s, est. speed input: 60073.61 toks/s, output: 58.67 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:47<00:25, 52.12it/s, est. speed input: 59987.47 toks/s, output: 58.58 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:48<00:24, 52.11it/s, est. speed input: 59902.99 toks/s, output: 58.50 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:48<00:23, 52.11it/s, est. speed input: 59820.66 toks/s, output: 58.42 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:49<00:23, 52.11it/s, est. speed input: 59740.49 toks/s, output: 58.34 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:50<00:22, 52.11it/s, est. speed input: 59662.02 toks/s, output: 58.26 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:50<00:22, 52.11it/s, est. speed input: 59585.52 toks/s, output: 58.19 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:51<00:21, 52.11it/s, est. speed input: 59510.96 toks/s, output: 58.12 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:51<00:20, 52.11it/s, est. speed input: 59438.07 toks/s, output: 58.04 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:52<00:20, 51.82it/s, est. speed input: 59354.05 toks/s, output: 57.96 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:53<00:19, 51.42it/s, est. speed input: 59263.19 toks/s, output: 57.87 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:53<00:19, 51.16it/s, est. speed input: 59174.94 toks/s, output: 57.79 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:54<00:18, 50.97it/s, est. speed input: 59088.42 toks/s, output: 57.70 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:55<00:18, 50.84it/s, est. speed input: 59004.06 toks/s, output: 57.62 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:55<00:17, 50.75it/s, est. speed input: 58921.67 toks/s, output: 57.54 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:56<00:17, 50.69it/s, est. speed input: 58841.04 toks/s, output: 57.46 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:56<00:16, 50.64it/s, est. speed input: 58762.08 toks/s, output: 57.38 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:57<00:15, 50.62it/s, est. speed input: 58685.19 toks/s, output: 57.31 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:58<00:15, 50.60it/s, est. speed input: 58609.91 toks/s, output: 57.24 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:58<00:14, 50.58it/s, est. speed input: 58536.19 toks/s, output: 57.16 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:59<00:13, 50.57it/s, est. speed input: 58464.09 toks/s, output: 57.09 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:00<00:13, 50.56it/s, est. speed input: 58393.24 toks/s, output: 57.02 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:00<00:12, 49.84it/s, est. speed input: 58294.87 toks/s, output: 56.93 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:01<00:12, 49.31it/s, est. speed input: 58197.06 toks/s, output: 56.83 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:02<00:11, 48.94it/s, est. speed input: 58101.12 toks/s, output: 56.74 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:02<00:11, 48.68it/s, est. speed input: 58007.27 toks/s, output: 56.65 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:06<00:23, 21.40it/s, est. speed input: 55479.16 toks/s, output: 54.18 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:06<00:18, 25.69it/s, est. speed input: 55418.04 toks/s, output: 54.12 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:07<00:14, 30.10it/s, est. speed input: 55380.63 toks/s, output: 54.08 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:08<00:12, 34.22it/s, est. speed input: 55344.27 toks/s, output: 54.05 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:08<00:10, 38.06it/s, est. speed input: 55320.96 toks/s, output: 54.02 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:09<00:08, 41.04it/s, est. speed input: 55285.53 toks/s, output: 53.99 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:10<00:07, 43.44it/s, est. speed input: 55251.29 toks/s, output: 53.96 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:10<00:06, 45.29it/s, est. speed input: 55217.25 toks/s, output: 53.92 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:11<00:05, 46.96it/s, est. speed input: 55194.54 toks/s, output: 53.90 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:11<00:04, 48.40it/s, est. speed input: 55178.95 toks/s, output: 53.89 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:12<00:03, 49.45it/s, est. speed input: 55163.54 toks/s, output: 53.87 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:13<00:03, 50.22it/s, est. speed input: 55148.42 toks/s, output: 53.86 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:13<00:02, 50.82it/s, est. speed input: 55134.81 toks/s, output: 53.84 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:14<00:01, 51.17it/s, est. speed input: 55119.29 toks/s, output: 53.83 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:14<00:01, 51.82it/s, est. speed input: 55115.94 toks/s, output: 53.82 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:15<00:00, 52.41it/s, est. speed input: 55116.08 toks/s, output: 53.82 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:15<00:00, 52.41it/s, est. speed input: 55522.40 toks/s, output: 54.22 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:15<00:00, 54.22it/s, est. speed input: 55522.40 toks/s, output: 54.22 toks/s]
[rank0]:[W128 12:28:27.964629058 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 160.9s

测试结果:
  Requests/s:   50.73
  Tokens/s:     51995.61
  Total Reqs:   4096
  Elapsed:      80.75s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     51944.89


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,16.0386,8227.8102,7.9807
1024,1024,1,128,128,16.3869,16796.5378,7.8111
2048,1024,2,256,128,31.9465,32745.1489,8.0134
4096,1024,4,512,128,50.9992,52274.1594,10.0394
8192,1024,8,1024,128,51.5199,52807.9236,19.8758
16384,1024,16,2048,128,50.8208,52091.2944,40.2985
32768,1024,32,4096,128,50.7274,51995.6129,80.7453

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_114255.log
[SUCCESS] bitnet1.58-2b-fp8 Prefill 完成 (2742.1s)

[INFO] Prefill 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 4: 完整 Prefill Benchmark - SUCCESS
Duration: 5254.9 seconds (87.6 minutes)
----------------------------------------------------------------------


======================================================================
TASK 5: 完整 Decode Benchmark
Started: 2026-01-28 12:28:31
======================================================================


------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-int8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 4090                   ││
│ GPU (short):      RTX4090                                   │
│ Memory:           24.0 GB                                    │
│ CC:               cc89 (Ampere)                              ││
│ SM Code:          sm_89                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           581.80                                    │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-int8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_122835.log


============================================================
  BitNet-2B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:28:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:28:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=54440) WARNING 01-28 12:28:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=54440) WARNING 01-28 12:29:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 25.69 requests/s, 6988.61 total tokens/s, 6577.51 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:28:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:28:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:28:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:28:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:28:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:28:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:28:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:28:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:28:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:28:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:28:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:28:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:28:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:28:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:28:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:28:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:28:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:28:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:28:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=54440) [2026-01-28 12:28:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=54440) [2026-01-28 12:28:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=54440) [2026-01-28 12:28:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=54440) [2026-01-28 12:28:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=54440) [2026-01-28 12:28:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=54440) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=54440) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
(EngineCore_DP0 pid=54440) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.43it/s]
(EngineCore_DP0 pid=54440) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=54440) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  7.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.11it/s]
(EngineCore_DP0 pid=54440) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.96it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  7.83it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.07it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.17it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.37it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2926.21it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:32,  2.43s/it, est. speed input: 6.59 toks/s, output: 105.43 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.43s/it, est. speed input: 414.98 toks/s, output: 6639.62 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 25.93it/s, est. speed input: 414.98 toks/s, output: 6639.62 toks/s]
[rank0]:[W128 12:29:24.874578327 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.8s

测试结果:
  Requests/s:   25.69
  Tokens/s:     6988.61
  Total Reqs:   64
  Elapsed:      2.49s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6577.51

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:29:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:29:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=54983) WARNING 01-28 12:29:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=54983) WARNING 01-28 12:29:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.79 requests/s, 10550.84 total tokens/s, 9930.21 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:29:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:29:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:29:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:29:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:29:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:29:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:29:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:29:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:29:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:29:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:29:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:29:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:29:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:29:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:29:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:29:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:29:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:29:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:29:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=54983) [2026-01-28 12:29:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=54983) [2026-01-28 12:29:44] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=54983) [2026-01-28 12:29:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=54983) [2026-01-28 12:29:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=54983) [2026-01-28 12:29:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=54983) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=54983) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=54983) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=54983) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=54983) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:04,  7.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:03,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:01<00:02,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:02<00:01,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.11it/s]
(EngineCore_DP0 pid=54983) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.89it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.75it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:02,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.08it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  8.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3354.12it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<06:42,  3.17s/it, est. speed input: 5.05 toks/s, output: 80.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  3.17s/it, est. speed input: 628.19 toks/s, output: 10050.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.26it/s, est. speed input: 628.19 toks/s, output: 10050.96 toks/s]
[rank0]:[W128 12:30:17.145388839 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.0s

测试结果:
  Requests/s:   38.79
  Tokens/s:     10550.84
  Total Reqs:   128
  Elapsed:      3.30s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      9930.21

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:30:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:30:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=55514) WARNING 01-28 12:30:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=55514) WARNING 01-28 12:30:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.86 requests/s, 14648.67 total tokens/s, 13786.98 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:30:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:30:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:30:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:30:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:30:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:30:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:30:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:30:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:30:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:30:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:30:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:30:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:30:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:30:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:30:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:30:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:30:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:30:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:30:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=55514) [2026-01-28 12:30:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=55514) [2026-01-28 12:30:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=55514) [2026-01-28 12:30:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=55514) [2026-01-28 12:30:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=55514) [2026-01-28 12:30:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=55514) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=55514) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.81it/s]
(EngineCore_DP0 pid=55514) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.81it/s]
(EngineCore_DP0 pid=55514) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=55514) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:04<00:00,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.31it/s]
(EngineCore_DP0 pid=55514) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.18it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.99it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:01,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  7.74it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  8.08it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:04<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.59it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.50it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3251.79it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:04<17:48,  4.19s/it, est. speed input: 3.82 toks/s, output: 61.12 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:04<00:07, 24.64it/s, est. speed input: 279.80 toks/s, output: 4476.70 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:04<00:01, 61.10it/s, est. speed input: 578.58 toks/s, output: 9257.17 toks/s]
Processed prompts:  86%|████████▌ | 219/256 [00:04<00:00, 92.84it/s, est. speed input: 777.04 toks/s, output: 12432.59 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 92.84it/s, est. speed input: 876.48 toks/s, output: 14023.68 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 54.78it/s, est. speed input: 876.48 toks/s, output: 14023.68 toks/s]
[rank0]:[W128 12:31:11.190008331 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.9s

测试结果:
  Requests/s:   53.86
  Tokens/s:     14648.67
  Total Reqs:   256
  Elapsed:      4.75s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      13786.98

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:31:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:31:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=56071) WARNING 01-28 12:31:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=56071) WARNING 01-28 12:31:39 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.58 requests/s, 13214.12 total tokens/s, 12436.82 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:31:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:31:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:31:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:31:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:31:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:31:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:31:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:31:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:31:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:31:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:31:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:31:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:31:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:31:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:31:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:31:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:31:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:31:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:31:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=56071) [2026-01-28 12:31:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=56071) [2026-01-28 12:31:30] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=56071) [2026-01-28 12:31:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=56071) [2026-01-28 12:31:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=56071) [2026-01-28 12:31:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=56071) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=56071) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=56071) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=56071) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=56071) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:04,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:03,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:02,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:05<00:01,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:05<00:01,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:01,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:06<00:00,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:06<00:00,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  7.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.02it/s]
(EngineCore_DP0 pid=56071) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:09,  5.53it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:07,  6.21it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:07,  6.65it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:06,  7.08it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:06,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  7.60it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:05,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:05,  7.91it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:05,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:02<00:04,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:03,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:03,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:03<00:03,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:02,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:04<00:02,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:04<00:02,  8.39it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:04<00:01,  8.27it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  8.21it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:05<00:01,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:05<00:01,  8.27it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:05<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:05<00:00,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:06<00:00,  8.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  8.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  8.19it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  68%|██████▊   | 346/512 [00:00<00:00, 3456.85it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3528.35it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:08<1:09:27,  8.16s/it, est. speed input: 1.96 toks/s, output: 31.39 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:08<01:25,  5.61it/s, est. speed input: 63.81 toks/s, output: 1021.01 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:08<00:21, 19.33it/s, est. speed input: 173.77 toks/s, output: 2780.28 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:08<00:08, 42.71it/s, est. speed input: 309.24 toks/s, output: 4947.80 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:08<00:03, 75.67it/s, est. speed input: 450.73 toks/s, output: 7211.70 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:08<00:01, 127.86it/s, est. speed input: 621.31 toks/s, output: 9940.89 toks/s]
Processed prompts:  80%|████████  | 412/512 [00:08<00:00, 174.88it/s, est. speed input: 748.06 toks/s, output: 11968.92 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [00:08<00:00, 211.54it/s, est. speed input: 855.39 toks/s, output: 13686.22 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 211.54it/s, est. speed input: 788.27 toks/s, output: 12612.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 49.27it/s, est. speed input: 788.27 toks/s, output: 12612.30 toks/s] 
[rank0]:[W128 12:32:21.982449562 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.5s

测试结果:
  Requests/s:   48.58
  Tokens/s:     13214.12
  Total Reqs:   512
  Elapsed:      10.54s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      12436.82


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,25.6934,6988.6051,2.4909
128,16,128,128,256,256,38.7899,10550.8446,3.2998
256,16,256,256,256,256,53.8554,14648.6712,4.7535
512,16,512,512,256,256,48.5813,13214.1179,10.5390

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:32:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:32:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=56719) WARNING 01-28 12:32:40 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=56719) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=56719) WARNING 01-28 12:32:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 26.88 requests/s, 7312.04 total tokens/s, 6881.92 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:32:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:32:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:32:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:32:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:32:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:32:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:32:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:32:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:32:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:32:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:32:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:32:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:32:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:32:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:32:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:32:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:32:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:32:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:32:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=56719) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=56719) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=56719) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.55it/s]
(EngineCore_DP0 pid=56719) 
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=56719) [2026-01-28 12:32:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=56719) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  5.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:02,  6.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.66it/s]
(EngineCore_DP0 pid=56719) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.53it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  9.19it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.31it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00,  9.40it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.19it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3144.97it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:26,  2.33s/it, est. speed input: 6.87 toks/s, output: 109.95 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.33s/it, est. speed input: 434.08 toks/s, output: 6945.27 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 27.13it/s, est. speed input: 434.08 toks/s, output: 6945.27 toks/s]
[rank0]:[W128 12:33:15.219045287 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.9s

测试结果:
  Requests/s:   26.88
  Tokens/s:     7312.04
  Total Reqs:   64
  Elapsed:      2.38s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6881.92

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:33:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:33:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=57289) WARNING 01-28 12:33:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=57289) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=57289) WARNING 01-28 12:33:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.31 requests/s, 12323.72 total tokens/s, 11598.79 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:33:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:33:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:33:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:33:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:33:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:33:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:33:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:33:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:33:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:33:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:33:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:33:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:33:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:33:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:33:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:33:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:33:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:33:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:33:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=57289) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=57289) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.03s/it]
(EngineCore_DP0 pid=57289) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.03s/it]
(EngineCore_DP0 pid=57289) 
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=57289) [2026-01-28 12:33:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=57289) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:11,  3.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:03,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.90it/s]
(EngineCore_DP0 pid=57289) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.14it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.85it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3338.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:43,  2.70s/it, est. speed input: 5.92 toks/s, output: 94.67 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.70s/it, est. speed input: 735.27 toks/s, output: 11764.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.95it/s, est. speed input: 735.27 toks/s, output: 11764.26 toks/s]
[rank0]:[W128 12:34:05.574072584 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.9s

测试结果:
  Requests/s:   45.31
  Tokens/s:     12323.72
  Total Reqs:   128
  Elapsed:      2.83s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      11598.79

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:34:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:34:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=57841) WARNING 01-28 12:34:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=57841) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=57841) WARNING 01-28 12:34:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 63.99 requests/s, 17405.07 total tokens/s, 16381.25 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:34:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:34:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:34:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:34:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:34:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:34:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:34:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:34:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:34:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:34:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:34:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:34:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:34:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:34:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:34:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:34:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:34:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:34:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:34:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=57841) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=57841) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=57841) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=57841) 
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=57841) [2026-01-28 12:34:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=57841) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:02,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:01,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:02<00:01,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.66it/s]
(EngineCore_DP0 pid=57841) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.44it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  8.32it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  9.05it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  9.05it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  8.97it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3337.63it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<14:58,  3.52s/it, est. speed input: 4.54 toks/s, output: 72.69 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:03<00:04, 37.93it/s, est. speed input: 431.67 toks/s, output: 6906.73 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:03<00:00, 81.65it/s, est. speed input: 793.05 toks/s, output: 12688.85 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:03<00:00, 117.30it/s, est. speed input: 1036.01 toks/s, output: 16576.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 117.30it/s, est. speed input: 1044.23 toks/s, output: 16707.60 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 65.26it/s, est. speed input: 1044.23 toks/s, output: 16707.60 toks/s] 
[rank0]:[W128 12:35:00.936614824 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.4s

测试结果:
  Requests/s:   63.99
  Tokens/s:     17405.07
  Total Reqs:   256
  Elapsed:      4.00s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      16381.25

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:35:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:35:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=58459) WARNING 01-28 12:35:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=58459) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=58459) WARNING 01-28 12:35:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 65.10 requests/s, 17708.09 total tokens/s, 16666.44 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:35:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:35:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:35:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:35:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:35:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:35:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:35:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:35:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:35:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:35:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:35:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:35:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:35:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:35:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:35:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:35:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:35:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:35:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:35:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=58459) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=58459) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=58459) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=58459) 
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=58459) [2026-01-28 12:35:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=58459) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:04,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:03,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:02,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:01,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.57it/s]
(EngineCore_DP0 pid=58459) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:10,  4.98it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:08,  5.68it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:07,  6.50it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.34it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  7.85it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:05,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:04,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  9.01it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:03,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:03,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:02,  9.05it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:05<00:22,  1.13it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:05<00:16,  1.54it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:05<00:11,  2.05it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:05<00:08,  2.67it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:05<00:06,  3.38it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:06<00:05,  4.14it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:06<00:04,  4.94it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:06<00:03,  5.74it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:06<00:02,  6.42it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:06<00:02,  7.01it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:06<00:02,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:06<00:01,  7.88it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:06<00:01,  8.09it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:06<00:01,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:07<00:01,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:07<00:01,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:07<00:01,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:07<00:01,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:07<00:00,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:07<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:07<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:07<00:00,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:07<00:00,  9.17it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:08<00:00,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  9.27it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:08<00:00,  9.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  9.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  6.09it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 3508.74it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3554.99it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<51:00,  5.99s/it, est. speed input: 2.67 toks/s, output: 42.74 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:06<00:31, 14.48it/s, est. speed input: 164.50 toks/s, output: 2631.97 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:06<00:09, 39.24it/s, est. speed input: 364.82 toks/s, output: 5837.10 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:02, 87.66it/s, est. speed input: 651.09 toks/s, output: 10417.34 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:06<00:01, 140.62it/s, est. speed input: 888.92 toks/s, output: 14222.63 toks/s]
Processed prompts:  86%|████████▌ | 439/512 [00:06<00:00, 191.31it/s, est. speed input: 1072.57 toks/s, output: 17161.05 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 191.31it/s, est. speed input: 1061.29 toks/s, output: 16980.57 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 66.33it/s, est. speed input: 1061.29 toks/s, output: 16980.57 toks/s] 
[rank0]:[W128 12:36:08.987122775 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 67.2s

测试结果:
  Requests/s:   65.10
  Tokens/s:     17708.09
  Total Reqs:   512
  Elapsed:      7.86s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      16666.44


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,26.8825,7312.0441,2.3807
128,16,128,128,256,256,45.3078,12323.7158,2.8251
256,16,256,256,256,256,63.9892,17405.0730,4.0007
512,16,512,512,256,256,65.1033,17708.0882,7.8644

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:36:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:36:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=59098) WARNING 01-28 12:36:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=59098) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=59098) WARNING 01-28 12:36:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 25.25 requests/s, 6868.26 total tokens/s, 6464.25 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:36:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:36:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:36:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:36:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:36:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:36:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:36:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:36:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:36:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:36:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:36:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:36:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:36:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:36:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:36:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:36:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:36:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:36:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:36:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=59098) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=59098) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=59098) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]
(EngineCore_DP0 pid=59098) 
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=59098) [2026-01-28 12:36:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=59098) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:07,  2.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:07,  2.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:04,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  4.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  5.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  6.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.79it/s]
(EngineCore_DP0 pid=59098) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.41it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.12it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  9.23it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.30it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00,  9.38it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.11it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3094.25it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:36,  2.48s/it, est. speed input: 6.46 toks/s, output: 103.33 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.48s/it, est. speed input: 407.57 toks/s, output: 6521.07 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 25.47it/s, est. speed input: 407.57 toks/s, output: 6521.07 toks/s]
[rank0]:[W128 12:36:59.849282062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.4s

测试结果:
  Requests/s:   25.25
  Tokens/s:     6868.26
  Total Reqs:   64
  Elapsed:      2.53s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6464.25

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:37:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:37:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=59699) WARNING 01-28 12:37:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=59699) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=59699) WARNING 01-28 12:37:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.85 requests/s, 11111.04 total tokens/s, 10457.45 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:37:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:37:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:37:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:37:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:37:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:37:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:37:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:37:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:37:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:37:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:37:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:37:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:37:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:37:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:37:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:37:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:37:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:37:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:37:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=59699) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=59699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=59699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=59699) 
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=59699) [2026-01-28 12:37:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=59699) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:10,  3.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  6.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:03,  7.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:02,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.99it/s]
(EngineCore_DP0 pid=59699) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.21it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.29it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  9.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.96it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3236.01it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<06:21,  3.00s/it, est. speed input: 5.33 toks/s, output: 85.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  3.00s/it, est. speed input: 662.25 toks/s, output: 10596.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.39it/s, est. speed input: 662.25 toks/s, output: 10596.03 toks/s]
[rank0]:[W128 12:37:52.687921509 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.0s

测试结果:
  Requests/s:   40.85
  Tokens/s:     11111.04
  Total Reqs:   128
  Elapsed:      3.13s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10457.45

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:38:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:38:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=60252) WARNING 01-28 12:38:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=60252) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=60252) WARNING 01-28 12:38:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.80 requests/s, 16537.74 total tokens/s, 15564.93 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:38:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:38:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:38:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:38:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:38:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:38:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:38:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:38:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:38:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:38:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:38:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:38:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:38:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:38:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:38:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:38:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:38:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:38:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=60252) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=60252) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.67it/s]
(EngineCore_DP0 pid=60252) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.67it/s]
(EngineCore_DP0 pid=60252) 
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=60252) [2026-01-28 12:38:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=60252) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:02,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.57it/s]
(EngineCore_DP0 pid=60252) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.32it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  8.25it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  9.01it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  9.06it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:01,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  9.18it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  9.23it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  8.87it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3422.82it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<15:46,  3.71s/it, est. speed input: 4.31 toks/s, output: 68.94 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 32.09it/s, est. speed input: 364.76 toks/s, output: 5836.15 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 73.12it/s, est. speed input: 702.56 toks/s, output: 11241.00 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:04<00:00, 110.17it/s, est. speed input: 940.91 toks/s, output: 15054.59 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 110.17it/s, est. speed input: 990.75 toks/s, output: 15851.96 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 61.92it/s, est. speed input: 990.75 toks/s, output: 15851.96 toks/s] 
[rank0]:[W128 12:38:45.107352094 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.0s

测试结果:
  Requests/s:   60.80
  Tokens/s:     16537.74
  Total Reqs:   256
  Elapsed:      4.21s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15564.93

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:38:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:38:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=60803) WARNING 01-28 12:39:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=60803) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=60803) WARNING 01-28 12:39:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.10 requests/s, 16620.07 total tokens/s, 15642.42 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:38:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:38:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:38:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:38:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:38:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:38:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:38:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:38:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:38:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:38:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:39:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:39:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:39:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:39:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:39:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:39:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:39:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:39:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:39:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:39:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:39:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:39:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:39:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:39:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=60803) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=60803) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.66it/s]
(EngineCore_DP0 pid=60803) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.66it/s]
(EngineCore_DP0 pid=60803) 
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9891840 bytes
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6594560 bytes
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35610624 bytes
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=60803) [2026-01-28 12:39:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=60803) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:03,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:02,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.41it/s]
(EngineCore_DP0 pid=60803) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  6.33it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:06,  7.06it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.54it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.82it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  8.02it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  8.46it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:05,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:04,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.23it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:04,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:03,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:03,  7.99it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:03,  8.23it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:02,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  8.15it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  8.23it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:02,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:04<00:01,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:04<00:01,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:01,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:05<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:05<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  9.16it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  9.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  8.57it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  68%|██████▊   | 347/512 [00:00<00:00, 3463.44it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3509.36it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<54:14,  6.37s/it, est. speed input: 2.51 toks/s, output: 40.19 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:06<00:32, 13.63it/s, est. speed input: 154.82 toks/s, output: 2477.10 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:06<00:10, 36.66it/s, est. speed input: 341.03 toks/s, output: 5456.40 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:06<00:03, 76.68it/s, est. speed input: 576.36 toks/s, output: 9221.69 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:01, 125.70it/s, est. speed input: 793.14 toks/s, output: 12690.29 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:06<00:00, 174.76it/s, est. speed input: 966.16 toks/s, output: 15458.54 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [00:07<00:00, 207.89it/s, est. speed input: 1110.67 toks/s, output: 17770.72 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 207.89it/s, est. speed input: 995.20 toks/s, output: 15923.19 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 62.20it/s, est. speed input: 995.20 toks/s, output: 15923.19 toks/s] 
[rank0]:[W128 12:39:53.628053024 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 67.7s

测试结果:
  Requests/s:   61.10
  Tokens/s:     16620.07
  Total Reqs:   512
  Elapsed:      8.38s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      15642.42


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,25.2510,6868.2612,2.5346
128,16,128,128,256,256,40.8494,11111.0418,3.1335
256,16,256,256,256,256,60.8005,16537.7434,4.2105
512,16,512,512,256,256,61.1032,16620.0673,8.3793

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:40:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:40:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=61458) WARNING 01-28 12:40:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=61458) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=61458) WARNING 01-28 12:40:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 24.69 requests/s, 6714.66 total tokens/s, 6319.68 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:40:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:40:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:40:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:40:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:40:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:40:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:40:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:40:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:40:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:40:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:40:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:40:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:40:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:40:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:40:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:40:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:40:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:40:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=61458) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=61458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=61458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]
(EngineCore_DP0 pid=61458) 
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=61458) [2026-01-28 12:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=61458) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:02,  6.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:01,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.43it/s]
(EngineCore_DP0 pid=61458) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.17it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.17it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.85it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3122.76it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:39,  2.54s/it, est. speed input: 6.31 toks/s, output: 100.96 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.54s/it, est. speed input: 398.34 toks/s, output: 6373.41 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 24.89it/s, est. speed input: 398.34 toks/s, output: 6373.41 toks/s]
[rank0]:[W128 12:40:47.468126754 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.8s

测试结果:
  Requests/s:   24.69
  Tokens/s:     6714.66
  Total Reqs:   64
  Elapsed:      2.59s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6319.68

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:40:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:40:56 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=62028) WARNING 01-28 12:41:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=62028) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=62028) WARNING 01-28 12:41:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.64 requests/s, 11597.71 total tokens/s, 10915.49 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:40:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:40:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:40:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:40:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:40:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:40:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:40:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:40:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:40:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:40:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:41:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:41:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:41:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:41:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:41:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:41:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:41:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:41:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:41:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=62028) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=62028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=62028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=62028) 
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=62028) [2026-01-28 12:41:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=62028) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  6.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:03,  7.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:01,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.03it/s]
(EngineCore_DP0 pid=62028) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.29it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:00,  9.01it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  9.15it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  9.19it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  9.27it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.32it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:01<00:00,  9.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.04it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3327.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<06:04,  2.87s/it, est. speed input: 5.57 toks/s, output: 89.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.87s/it, est. speed input: 691.40 toks/s, output: 11062.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.21it/s, est. speed input: 691.40 toks/s, output: 11062.45 toks/s]
[rank0]:[W128 12:41:37.647435753 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.3s

测试结果:
  Requests/s:   42.64
  Tokens/s:     11597.71
  Total Reqs:   128
  Elapsed:      3.00s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10915.49

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:41:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:41:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=62576) WARNING 01-28 12:42:03 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=62576) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=62576) WARNING 01-28 12:42:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.06 requests/s, 16607.40 total tokens/s, 15630.49 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:41:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:41:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:41:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:41:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:41:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:41:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:41:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:41:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:41:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:41:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:41:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:41:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:41:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:41:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:41:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:41:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:41:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:41:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:41:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=62576) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=62576) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.30it/s]
(EngineCore_DP0 pid=62576) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.30it/s]
(EngineCore_DP0 pid=62576) 
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=62576) [2026-01-28 12:42:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=62576) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.56it/s]
(EngineCore_DP0 pid=62576) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.27it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  8.14it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.45it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  9.05it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:01,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  8.86it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3407.30it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<15:39,  3.68s/it, est. speed input: 4.34 toks/s, output: 69.48 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 32.34it/s, est. speed input: 367.64 toks/s, output: 5882.11 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 73.64it/s, est. speed input: 707.78 toks/s, output: 11324.52 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:04<00:00, 110.37it/s, est. speed input: 946.05 toks/s, output: 15136.69 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 110.37it/s, est. speed input: 995.10 toks/s, output: 15921.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 62.19it/s, est. speed input: 995.10 toks/s, output: 15921.62 toks/s] 
[rank0]:[W128 12:42:40.196177258 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 62.9s

测试结果:
  Requests/s:   61.06
  Tokens/s:     16607.40
  Total Reqs:   256
  Elapsed:      4.19s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15630.49

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:42:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:42:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=63407) WARNING 01-28 12:42:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=63407) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=63407) WARNING 01-28 12:43:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.76 requests/s, 16800.02 total tokens/s, 15811.78 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:42:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:42:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:42:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:42:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:42:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:42:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:42:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:42:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:42:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:42:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:42:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:42:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:42:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:42:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:42:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:42:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:42:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:42:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:42:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=63407) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=63407) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.52it/s]
(EngineCore_DP0 pid=63407) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.52it/s]
(EngineCore_DP0 pid=63407) 
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 39813120 bytes
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=63407) [2026-01-28 12:43:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=63407) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:04,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:03,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:02,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.50it/s]
(EngineCore_DP0 pid=63407) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:02<02:27,  2.96s/it]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:03<01:03,  1.29s/it]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:03<00:36,  1.32it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:03<00:23,  1.99it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:03<00:16,  2.77it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:03<00:12,  3.63it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:03<00:09,  4.46it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:03<00:08,  5.24it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:03<00:07,  5.97it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:04<00:06,  6.62it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:04<00:05,  7.11it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:04<00:05,  7.55it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:04<00:04,  7.90it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:04<00:04,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:04<00:04,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:04<00:04,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:04<00:03,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:04<00:03,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:05<00:03,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:05<00:03,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:05<00:03,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:05<00:03,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:05<00:03,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:05<00:03,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:05<00:02,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:05<00:02,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:05<00:02,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:06<00:02,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:06<00:02,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:06<00:02,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:06<00:02,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:06<00:02,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:06<00:02,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:06<00:01,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:06<00:01,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:06<00:01,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:07<00:01,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:07<00:01,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:07<00:01,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:07<00:01,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:07<00:01,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:07<00:01,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:07<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:07<00:00,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:08<00:00,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:08<00:00,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:08<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:08<00:00,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:08<00:00,  8.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  5.88it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  67%|██████▋   | 341/512 [00:00<00:00, 3401.54it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3440.94it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<53:26,  6.28s/it, est. speed input: 2.55 toks/s, output: 40.79 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:06<00:32, 13.84it/s, est. speed input: 157.16 toks/s, output: 2514.61 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:06<00:09, 37.21it/s, est. speed input: 346.16 toks/s, output: 5538.52 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:06<00:03, 77.83it/s, est. speed input: 585.04 toks/s, output: 9360.65 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:06<00:01, 132.32it/s, est. speed input: 827.41 toks/s, output: 13238.46 toks/s]
Processed prompts:  84%|████████▎ | 428/512 [00:06<00:00, 181.02it/s, est. speed input: 1001.91 toks/s, output: 16030.56 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:08<00:00, 111.13it/s, est. speed input: 995.35 toks/s, output: 15925.57 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 111.13it/s, est. speed input: 1006.49 toks/s, output: 16103.87 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 62.90it/s, est. speed input: 1006.49 toks/s, output: 16103.87 toks/s] 
[rank0]:[W128 12:43:48.727357256 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.0s

测试结果:
  Requests/s:   61.76
  Tokens/s:     16800.02
  Total Reqs:   512
  Elapsed:      8.29s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      15811.78


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,24.6863,6714.6648,2.5925
128,16,128,128,256,256,42.6387,11597.7130,3.0020
256,16,256,256,256,256,61.0566,16607.3993,4.1928
512,16,512,512,256,256,61.7648,16800.0168,8.2895

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:43:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:43:57 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=64487) WARNING 01-28 12:44:04 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=64487) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=64487) WARNING 01-28 12:44:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 24.37 requests/s, 6628.37 total tokens/s, 6238.47 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:43:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:43:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:43:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:43:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:43:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:43:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:43:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:43:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:43:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:43:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:43:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:43:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:43:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:43:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:44:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:44:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:44:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:44:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:44:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:44:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:44:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:44:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:44:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=64487) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=64487) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=64487) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=64487) 
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=64487) [2026-01-28 12:44:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=64487) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:07,  2.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:06,  2.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:04,  3.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  4.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  5.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  9.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  9.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.90it/s]
(EngineCore_DP0 pid=64487) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.26it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  9.05it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  9.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.94it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3160.18it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:41,  2.57s/it, est. speed input: 6.23 toks/s, output: 99.62 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.57s/it, est. speed input: 393.15 toks/s, output: 6290.41 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 24.57it/s, est. speed input: 393.15 toks/s, output: 6290.41 toks/s]
[rank0]:[W128 12:44:39.513230261 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.5s

测试结果:
  Requests/s:   24.37
  Tokens/s:     6628.37
  Total Reqs:   64
  Elapsed:      2.63s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6238.47

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:44:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:44:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=65412) WARNING 01-28 12:44:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=65412) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=65412) WARNING 01-28 12:45:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.43 requests/s, 11268.61 total tokens/s, 10605.75 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:44:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:44:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:44:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:44:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:44:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:44:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:44:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:44:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:44:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:44:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:44:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:44:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:44:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:44:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:44:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:44:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:44:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:44:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:44:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=65412) [2026-01-28 12:44:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=65412) [2026-01-28 12:44:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=65412) [2026-01-28 12:44:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=65412) [2026-01-28 12:44:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=65412) [2026-01-28 12:44:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=65412) [2026-01-28 12:44:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=65412) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=65412) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=65412) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=65412) 
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=65412) [2026-01-28 12:45:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=65412) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:04,  7.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:04<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.82it/s]
(EngineCore_DP0 pid=65412) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.25it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  9.21it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.35it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:01<00:00,  9.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.03it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3338.23it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<06:16,  2.96s/it, est. speed input: 5.40 toks/s, output: 86.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  2.96s/it, est. speed input: 671.48 toks/s, output: 10743.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.97it/s, est. speed input: 671.48 toks/s, output: 10743.72 toks/s]
[rank0]:[W128 12:45:33.879407412 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.9s

测试结果:
  Requests/s:   41.43
  Tokens/s:     11268.61
  Total Reqs:   128
  Elapsed:      3.09s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10605.75

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:45:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:45:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=66299) WARNING 01-28 12:45:50 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=66299) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=66299) WARNING 01-28 12:46:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.93 requests/s, 15485.80 total tokens/s, 14574.87 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:45:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:45:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:45:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:45:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:45:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:45:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:45:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:45:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:45:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:45:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:45:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:45:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:45:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:45:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:45:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:45:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:45:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:45:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:45:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=66299) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=66299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=66299) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.41it/s]
(EngineCore_DP0 pid=66299) 
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=66299) [2026-01-28 12:45:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=66299) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:02,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:02<00:01,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.65it/s]
(EngineCore_DP0 pid=66299) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.02it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  8.01it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:01,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  7.97it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  7.81it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  8.74it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.65it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3295.76it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:06<28:30,  6.71s/it, est. speed input: 2.39 toks/s, output: 38.16 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:06<00:09, 18.05it/s, est. speed input: 204.23 toks/s, output: 3267.63 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:06<00:02, 41.97it/s, est. speed input: 397.42 toks/s, output: 6358.73 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:07<00:00, 64.03it/s, est. speed input: 530.62 toks/s, output: 8489.85 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 64.03it/s, est. speed input: 571.51 toks/s, output: 9144.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 35.72it/s, est. speed input: 571.51 toks/s, output: 9144.14 toks/s]
[rank0]:[W128 12:46:30.861721637 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.6s

测试结果:
  Requests/s:   56.93
  Tokens/s:     15485.80
  Total Reqs:   256
  Elapsed:      4.50s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      14574.87

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:46:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:46:40 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=67214) WARNING 01-28 12:46:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=67214) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=67214) WARNING 01-28 12:46:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 57.25 requests/s, 15572.01 total tokens/s, 14656.01 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:46:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:46:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:46:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:46:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:46:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:46:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:46:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:46:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:46:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:46:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:46:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 12:46:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 12:46:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 12:46:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 12:46:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:46:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:46:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:46:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:46:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=67214) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=67214) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=67214) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=67214) 
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7864320 bytes
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42467328 bytes
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=67214) [2026-01-28 12:46:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 21299200 bytes
(EngineCore_DP0 pid=67214) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  8.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.70it/s]
(EngineCore_DP0 pid=67214) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  6.61it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:06,  7.56it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.98it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:05,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:05,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:04,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:03,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:03,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  9.03it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  9.13it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  9.18it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  9.21it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  9.32it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:02,  9.34it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:02,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02,  9.12it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:02<00:02,  9.08it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  9.07it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  9.15it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  9.17it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:01,  9.23it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:03<00:01,  9.22it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:03<00:01,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  9.20it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.95it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:01,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:04<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:04<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  8.85it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  67%|██████▋   | 342/512 [00:00<00:00, 3412.62it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3492.61it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<57:20,  6.73s/it, est. speed input: 2.38 toks/s, output: 38.02 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:06<01:10,  6.79it/s, est. speed input: 77.26 toks/s, output: 1236.18 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:06<00:12, 30.47it/s, est. speed input: 268.49 toks/s, output: 4295.77 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:07<00:04, 70.71it/s, est. speed input: 505.63 toks/s, output: 8090.13 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:07<00:01, 120.37it/s, est. speed input: 726.57 toks/s, output: 11624.98 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:07<00:00, 166.62it/s, est. speed input: 889.43 toks/s, output: 14230.88 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [00:07<00:00, 206.60it/s, est. speed input: 1027.12 toks/s, output: 16433.97 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 206.60it/s, est. speed input: 931.44 toks/s, output: 14903.02 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 58.21it/s, est. speed input: 931.44 toks/s, output: 14903.02 toks/s] 
[rank0]:[W128 12:47:36.033041707 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.7s

测试结果:
  Requests/s:   57.25
  Tokens/s:     15572.01
  Total Reqs:   512
  Elapsed:      8.94s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      14656.01


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,24.3690,6628.3719,2.6263
128,16,128,128,256,256,41.4287,11268.6094,3.0896
256,16,256,256,256,256,56.9331,15485.8027,4.4965
512,16,512,512,256,256,57.2500,15572.0107,8.9432

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_122835.log
[SUCCESS] bitnet1.58-2b-int8 Decode 完成 (1151.3s)

------------------------------------------------------------
  Decode Benchmark: bitnet1.58-2b-fp8
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512


============================================================
  SlideSparse vLLM Throughput Benchmark
============================================================


┌─────────────────────────────────────────────────────────────┐
│                    Hardware Information                      │
├─────────────────────────────────────────────────────────────┤
│ GPU:              NVIDIA GeForce RTX 4090                   ││
│ GPU (short):      RTX4090                                   │
│ Memory:           24.0 GB                                    │
│ CC:               cc89 (Ampere)                              ││
│ SM Code:          sm_89                                     │
├─────────────────────────────────────────────────────────────┤
│ CUDA Runtime:     12.9                                      │
│ CUDA Driver:      13.0                                      │
│ Driver:           581.80                                    │
│ PyTorch:          2.9.0+cu129                               │
├─────────────────────────────────────────────────────────────┤
│ Triton:           ✓ supported                               ││
│ FP8 Support:      ✓                                         │
│ INT8 Support:     ✓                                         │
└─────────────────────────────────────────────────────────────┘

测试配置:
  模型:             ['bitnet1.58-2b-fp8']
  Backends:         ['cublaslt', 'cusparselt']
  Sparsities:       ['2_4', '2_6', '2_8', '2_10']
  Stages:           ['decode']
  M_prefill:        [64, 128, 256, 512]
  M_decode:         [64, 128, 256, 512]
  GPU 内存利用率:   0.8

输出目录结构:
  throughput_benchmark_results/{stage}/{hw_folder}/{backend}/[{sparsity}/]
============================================================
[INFO] 日志文件: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_124747.log


============================================================
  BitNet-2B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:47:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:47:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=68413) WARNING 01-28 12:48:01 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=68413) WARNING 01-28 12:48:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 27.73 requests/s, 7541.67 total tokens/s, 7098.04 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:47:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:47:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:47:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:47:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:47:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:47:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:47:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:47:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:47:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:47:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:47:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:47:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:47:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:47:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:48:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:48:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:48:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:48:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:48:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:48:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:48:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:48:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:48:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=68413) [2026-01-28 12:48:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=68413) [2026-01-28 12:48:01] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=68413) [2026-01-28 12:48:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=68413) [2026-01-28 12:48:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=68413) [2026-01-28 12:48:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=68413) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=68413) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.89it/s]
(EngineCore_DP0 pid=68413) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.89it/s]
(EngineCore_DP0 pid=68413) 
(EngineCore_DP0 pid=68413) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:01,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:01<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:01<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.22it/s]
(EngineCore_DP0 pid=68413) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.05it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  7.94it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.27it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.46it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.58it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3174.12it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:21,  2.25s/it, est. speed input: 7.11 toks/s, output: 113.74 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.25s/it, est. speed input: 447.83 toks/s, output: 7165.30 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 27.99it/s, est. speed input: 447.83 toks/s, output: 7165.30 toks/s]
[rank0]:[W128 12:48:35.619953705 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.2s

测试结果:
  Requests/s:   27.73
  Tokens/s:     7541.67
  Total Reqs:   64
  Elapsed:      2.31s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      7098.04

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:48:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:48:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=69270) WARNING 01-28 12:48:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=69270) WARNING 01-28 12:49:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.62 requests/s, 9959.80 total tokens/s, 9373.93 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:48:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:48:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:48:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:48:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:48:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:48:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:48:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:48:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:48:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:48:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:48:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:48:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:48:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:48:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:48:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:48:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:48:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:48:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:48:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=69270) [2026-01-28 12:48:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=69270) [2026-01-28 12:48:55] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=69270) [2026-01-28 12:48:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=69270) [2026-01-28 12:48:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=69270) [2026-01-28 12:48:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=69270) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=69270) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=69270) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=69270) 
(EngineCore_DP0 pid=69270) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:04,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:03,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:01<00:02,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:02<00:01,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.02it/s]
(EngineCore_DP0 pid=69270) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.45it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.13it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:02,  7.43it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  7.64it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  7.84it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  7.95it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:01,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:01,  7.75it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  7.81it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  7.90it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  7.91it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  7.98it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  8.02it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  8.03it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  7.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  7.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  7.87it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3157.49it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<07:06,  3.36s/it, est. speed input: 4.77 toks/s, output: 76.27 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  3.36s/it, est. speed input: 592.99 toks/s, output: 9487.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.06it/s, est. speed input: 592.99 toks/s, output: 9487.90 toks/s]
[rank0]:[W128 12:49:29.840935486 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.8s

测试结果:
  Requests/s:   36.62
  Tokens/s:     9959.80
  Total Reqs:   128
  Elapsed:      3.50s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      9373.93

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:49:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:49:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=70145) WARNING 01-28 12:49:45 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=70145) WARNING 01-28 12:49:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.47 requests/s, 15360.55 total tokens/s, 14456.99 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:49:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:49:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:49:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:49:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:49:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:49:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:49:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:49:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:49:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:49:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:49:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:49:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:49:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:49:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:49:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:49:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:49:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:49:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:49:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=70145) [2026-01-28 12:49:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=70145) [2026-01-28 12:49:45] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=70145) [2026-01-28 12:49:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=70145) [2026-01-28 12:49:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=70145) [2026-01-28 12:49:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=70145) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=70145) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=70145) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=70145) 
(EngineCore_DP0 pid=70145) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:04<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.31it/s]
(EngineCore_DP0 pid=70145) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.87it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.72it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:04,  7.98it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.15it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:03,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.46it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.39it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:02<00:02,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:01,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  7.95it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  7.85it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:04<00:00,  7.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.38it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3442.35it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<16:57,  3.99s/it, est. speed input: 4.01 toks/s, output: 64.13 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:04<00:05, 29.84it/s, est. speed input: 339.26 toks/s, output: 5428.17 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:01, 65.46it/s, est. speed input: 631.91 toks/s, output: 10110.59 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:04<00:00, 98.85it/s, est. speed input: 841.58 toks/s, output: 13465.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 98.85it/s, est. speed input: 918.94 toks/s, output: 14703.07 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 57.43it/s, est. speed input: 918.94 toks/s, output: 14703.07 toks/s]
[rank0]:[W128 12:50:22.592205205 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.6s

测试结果:
  Requests/s:   56.47
  Tokens/s:     15360.55
  Total Reqs:   256
  Elapsed:      4.53s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      14456.99

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:50:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:50:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=71045) WARNING 01-28 12:50:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=71045) WARNING 01-28 12:50:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.05 requests/s, 15245.07 total tokens/s, 14348.30 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:50:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:50:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:50:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:50:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:50:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:50:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:50:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:50:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:50:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:50:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:50:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:50:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:50:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:50:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:50:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:50:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:50:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:50:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:50:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=71045) [2026-01-28 12:50:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=71045) [2026-01-28 12:50:42] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=71045) [2026-01-28 12:50:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=71045) [2026-01-28 12:50:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=71045) [2026-01-28 12:50:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=71045) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=71045) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=71045) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=71045) 
(EngineCore_DP0 pid=71045) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  7.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:05,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:05,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:04,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:04,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:04,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:04,  7.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:05<00:29,  1.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:05<00:21,  1.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:05<00:15,  1.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:05<00:11,  2.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:06<00:08,  3.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:06<00:06,  3.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:06<00:05,  4.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:06<00:04,  5.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:06<00:03,  5.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:06<00:03,  6.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:06<00:02,  6.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:06<00:02,  7.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:07<00:02,  7.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:07<00:02,  7.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:07<00:02,  7.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:07<00:01,  7.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:07<00:01,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:07<00:01,  7.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:07<00:01,  7.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:08<00:01,  7.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:08<00:01,  7.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:08<00:01,  7.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:08<00:01,  7.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:08<00:00,  7.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:08<00:00,  7.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:08<00:00,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:09<00:00,  7.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:09<00:00,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:09<00:00,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:09<00:00,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  7.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  5.34it/s]
(EngineCore_DP0 pid=71045) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  6.24it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:07,  6.99it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.21it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.34it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:06,  7.57it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  7.81it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  7.95it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:05,  7.98it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:05,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:05,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.39it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:04,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:04,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:04,  7.72it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  8.00it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  8.21it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:03,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:03<00:03,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:02,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  8.45it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:04<00:02,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:04<00:02,  8.46it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:04<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  7.82it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:05<00:01,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:05<00:01,  8.39it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:05<00:00,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:05<00:00,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  7.92it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:06<00:00,  7.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  7.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  8.17it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  68%|██████▊   | 349/512 [00:00<00:00, 3486.22it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3556.98it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<58:55,  6.92s/it, est. speed input: 2.31 toks/s, output: 37.00 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:07<01:12,  6.61it/s, est. speed input: 75.18 toks/s, output: 1202.85 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:07<00:13, 29.71it/s, est. speed input: 261.53 toks/s, output: 4184.52 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:07<00:04, 69.04it/s, est. speed input: 492.79 toks/s, output: 7884.57 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:07<00:01, 112.10it/s, est. speed input: 683.26 toks/s, output: 10932.06 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:07<00:00, 160.97it/s, est. speed input: 852.96 toks/s, output: 13647.34 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:07<00:00, 202.34it/s, est. speed input: 989.16 toks/s, output: 15826.48 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 202.34it/s, est. speed input: 911.29 toks/s, output: 14580.57 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 56.95it/s, est. speed input: 911.29 toks/s, output: 14580.57 toks/s] 
[rank0]:[W128 12:51:32.567542713 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.7s

测试结果:
  Requests/s:   56.05
  Tokens/s:     15245.07
  Total Reqs:   512
  Elapsed:      9.14s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      14348.30


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cublaslt/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,27.7267,7541.6716,2.3082
128,16,128,128,256,256,36.6169,9959.8048,3.4957
256,16,256,256,256,256,56.4726,15360.5487,4.5332
512,16,512,512,256,256,56.0481,15245.0734,9.1350

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:51:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:51:42 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=72131) WARNING 01-28 12:51:51 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=72131) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=72131) WARNING 01-28 12:52:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 24.88 requests/s, 6766.77 total tokens/s, 6368.73 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:51:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:51:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:51:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:51:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:51:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:51:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:51:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:51:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:51:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:51:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:51:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:51:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:51:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:51:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:51:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:51:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:51:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:51:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:51:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:52] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=72131) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=72131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=72131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=72131) 
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=72131) [2026-01-28 12:51:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=72131) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  3.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  5.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:02,  6.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:01,  6.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:01,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  7.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.11it/s]
(EngineCore_DP0 pid=72131) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.13it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.15it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:03<00:11,  1.39s/it]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:03<00:06,  1.13it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:03<00:03,  1.64it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:03<00:02,  2.28it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:03<00:01,  3.02it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:03<00:00,  3.84it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:03<00:00,  4.66it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:03<00:00,  5.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:04<00:00,  6.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:04<00:00,  2.71it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3104.63it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:38,  2.52s/it, est. speed input: 6.36 toks/s, output: 101.76 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.52s/it, est. speed input: 401.47 toks/s, output: 6423.59 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 25.09it/s, est. speed input: 401.47 toks/s, output: 6423.59 toks/s]
[rank0]:[W128 12:52:27.699779251 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.8s

测试结果:
  Requests/s:   24.88
  Tokens/s:     6766.77
  Total Reqs:   64
  Elapsed:      2.57s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6368.73

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:52:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:52:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=73067) WARNING 01-28 12:52:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=73067) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=73067) WARNING 01-28 12:52:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.15 requests/s, 11193.17 total tokens/s, 10534.75 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:52:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:52:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:52:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:52:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:52:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:52:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:52:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:52:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:52:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:52:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:52:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:52:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:52:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:52:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:52:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:52:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:52:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:52:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:52:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=73067) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=73067) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=73067) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=73067) 
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=73067) [2026-01-28 12:52:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=73067) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:11,  3.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:03,  7.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.81it/s]
(EngineCore_DP0 pid=73067) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.21it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.70it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3269.18it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<06:18,  2.98s/it, est. speed input: 5.37 toks/s, output: 85.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  2.98s/it, est. speed input: 667.15 toks/s, output: 10674.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.69it/s, est. speed input: 667.15 toks/s, output: 10674.31 toks/s]
[rank0]:[W128 12:53:18.168976702 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.0s

测试结果:
  Requests/s:   41.15
  Tokens/s:     11193.17
  Total Reqs:   128
  Elapsed:      3.11s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10534.75

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:53:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:53:28 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=73961) WARNING 01-28 12:53:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=73961) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=73961) WARNING 01-28 12:53:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.43 requests/s, 16708.37 total tokens/s, 15725.52 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:53:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:53:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:53:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:53:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:53:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:53:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:53:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:53:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:53:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:53:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:53:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:53:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:53:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:53:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:53:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:53:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:53:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:53:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:53:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=73961) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=73961) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=73961) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=73961) 
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=73961) [2026-01-28 12:53:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=73961) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:03,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  8.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:02,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.53it/s]
(EngineCore_DP0 pid=73961) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.18it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  8.05it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.84it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:05<00:11,  1.13it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:05<00:07,  1.53it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:05<00:05,  2.03it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:05<00:03,  2.63it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:05<00:02,  3.33it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:05<00:01,  4.10it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:05<00:01,  4.87it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:05<00:01,  5.59it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:06<00:00,  6.24it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:06<00:00,  6.87it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:06<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:06<00:00,  7.86it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:06<00:00,  8.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  8.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.31it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3320.23it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<15:32,  3.66s/it, est. speed input: 4.38 toks/s, output: 70.02 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:03<00:05, 32.57it/s, est. speed input: 370.35 toks/s, output: 5925.54 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 74.11it/s, est. speed input: 712.73 toks/s, output: 11403.64 toks/s]
Processed prompts:  93%|█████████▎| 237/256 [00:03<00:00, 110.90it/s, est. speed input: 950.16 toks/s, output: 15202.61 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 110.90it/s, est. speed input: 1001.73 toks/s, output: 16027.69 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 62.61it/s, est. speed input: 1001.73 toks/s, output: 16027.69 toks/s] 
[rank0]:[W128 12:54:14.245125049 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.2s

测试结果:
  Requests/s:   61.43
  Tokens/s:     16708.37
  Total Reqs:   256
  Elapsed:      4.17s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      15725.52

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:54:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:54:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=74849) WARNING 01-28 12:54:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=74849) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=74849) WARNING 01-28 12:54:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 59.72 requests/s, 16242.72 total tokens/s, 15287.26 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:54:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:54:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:54:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:54:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:54:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:54:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:54:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:54:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:54:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:54:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:54:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:54:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:54:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:54:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:54:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:54:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:54:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:54:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:54:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:31] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:31] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=74849) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=74849) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=74849) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=74849) 
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4915200 bytes
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26542080 bytes
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=74849) [2026-01-28 12:54:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13271040 bytes
(EngineCore_DP0 pid=74849) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:04,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:04,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:03,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:03,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:04<00:02,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:02,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:02,  7.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:05<00:01,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:05<00:01,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:01,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:06<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:06<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  7.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.13it/s]
(EngineCore_DP0 pid=74849) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:09,  5.10it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:07,  6.53it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.27it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.69it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  7.88it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  8.00it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:05,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:05,  8.21it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.39it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:04,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:04,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:03,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:03<00:03,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:03,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:02,  8.38it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:04<00:02,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:04<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.39it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:05<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:05<00:00,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:05<00:00,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  8.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  8.37it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  65%|██████▍   | 331/512 [00:00<00:00, 3302.32it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3323.03it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:09<1:17:21,  9.08s/it, est. speed input: 1.76 toks/s, output: 28.18 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:09<00:46,  9.64it/s, est. speed input: 109.26 toks/s, output: 1748.10 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:09<00:14, 26.17it/s, est. speed input: 241.74 toks/s, output: 3867.86 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:09<00:04, 55.41it/s, est. speed input: 410.39 toks/s, output: 6566.19 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:09<00:01, 95.90it/s, est. speed input: 583.25 toks/s, output: 9332.05 toks/s]
Processed prompts:  84%|████████▎ | 428/512 [00:09<00:00, 133.88it/s, est. speed input: 709.31 toks/s, output: 11348.90 toks/s]
Processed prompts:  99%|█████████▊| 505/512 [00:11<00:00, 94.65it/s, est. speed input: 734.54 toks/s, output: 11752.68 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 94.65it/s, est. speed input: 744.34 toks/s, output: 11909.41 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 46.52it/s, est. speed input: 744.34 toks/s, output: 11909.41 toks/s]
[rank0]:[W128 12:55:23.849899561 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.8s

测试结果:
  Requests/s:   59.72
  Tokens/s:     16242.72
  Total Reqs:   512
  Elapsed:      8.57s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      15287.26


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,24.8778,6766.7704,2.5726
128,16,128,128,256,256,41.1514,11193.1715,3.1105
256,16,256,256,256,256,61.4278,16708.3651,4.1675
512,16,512,512,256,256,59.7159,16242.7189,8.5739

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:55:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:55:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=75936) WARNING 01-28 12:55:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=75936) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=75936) WARNING 01-28 12:55:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 23.86 requests/s, 6489.90 total tokens/s, 6108.15 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:55:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:55:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:55:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:55:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:55:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:55:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:55:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:55:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:55:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:55:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:55:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:55:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:55:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:55:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:55:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:55:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:55:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:55:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:55:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=75936) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=75936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=75936) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=75936) 
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=75936) [2026-01-28 12:55:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=75936) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:06,  2.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:06,  2.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:04,  3.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:03,  4.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  5.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:01,  6.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.84it/s]
(EngineCore_DP0 pid=75936) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.14it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  8.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.68it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3048.64it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:45,  2.62s/it, est. speed input: 6.10 toks/s, output: 97.65 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.62s/it, est. speed input: 384.97 toks/s, output: 6159.51 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 24.06it/s, est. speed input: 384.97 toks/s, output: 6159.51 toks/s]
[rank0]:[W128 12:56:14.851316008 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.6s

测试结果:
  Requests/s:   23.86
  Tokens/s:     6489.90
  Total Reqs:   64
  Elapsed:      2.68s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      6108.15

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:56:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:56:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=76862) WARNING 01-28 12:56:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=76862) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=76862) WARNING 01-28 12:56:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.02 requests/s, 10340.96 total tokens/s, 9732.67 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 12:56:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:56:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:56:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:56:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:56:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:56:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:56:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:56:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:56:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:56:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:56:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:56:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:56:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:56:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:56:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:56:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:56:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:56:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:56:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=76862) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=76862) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=76862) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=76862) 
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=76862) [2026-01-28 12:56:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=76862) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:08,  3.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:08,  3.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  5.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:04,  6.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  7.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.84it/s]
(EngineCore_DP0 pid=76862) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.00it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.86it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.23it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:04<00:06,  1.08it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:04<00:04,  1.48it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:04<00:02,  1.97it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:04<00:01,  2.58it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:04<00:00,  3.29it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:04<00:00,  4.06it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:04<00:00,  4.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:04<00:00,  5.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:04<00:00,  3.91it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3233.19it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<06:50,  3.23s/it, est. speed input: 4.95 toks/s, output: 79.27 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00,  3.23s/it, est. speed input: 615.82 toks/s, output: 9853.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.49it/s, est. speed input: 615.82 toks/s, output: 9853.11 toks/s]
[rank0]:[W128 12:57:08.363038317 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.8s

测试结果:
  Requests/s:   38.02
  Tokens/s:     10340.96
  Total Reqs:   128
  Elapsed:      3.37s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      9732.67

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:57:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:57:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=77749) WARNING 01-28 12:57:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=77749) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=77749) WARNING 01-28 12:57:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.61 requests/s, 15124.76 total tokens/s, 14235.07 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 12:57:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:57:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:57:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:57:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:57:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:57:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:57:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:57:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:57:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:57:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:57:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:57:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:57:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:57:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:57:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:57:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:57:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:57:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:57:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W128 12:57:32.566998102 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=77749) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=77749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=77749) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.44it/s]
(EngineCore_DP0 pid=77749) 
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=77749) [2026-01-28 12:57:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=77749) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:02,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:04<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.36it/s]
(EngineCore_DP0 pid=77749) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.11it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  8.11it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.06it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  7.97it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.18it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:01,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  8.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.60it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3121.16it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:06<28:16,  6.65s/it, est. speed input: 2.41 toks/s, output: 38.49 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:06<00:09, 18.19it/s, est. speed input: 205.87 toks/s, output: 3293.93 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:06<00:02, 40.65it/s, est. speed input: 387.06 toks/s, output: 6192.96 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:06<00:00, 63.03it/s, est. speed input: 520.69 toks/s, output: 8331.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 63.03it/s, est. speed input: 575.19 toks/s, output: 9203.05 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 35.95it/s, est. speed input: 575.19 toks/s, output: 9203.05 toks/s]
[rank0]:[W128 12:58:13.091451891 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 64.9s

测试结果:
  Requests/s:   55.61
  Tokens/s:     15124.76
  Total Reqs:   256
  Elapsed:      4.60s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      14235.07

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:58:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:58:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=78772) WARNING 01-28 12:58:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=78772) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=78772) WARNING 01-28 12:58:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.71 requests/s, 15424.53 total tokens/s, 14517.20 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 12:58:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:58:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:58:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:58:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:58:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:58:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:58:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:58:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:58:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:58:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:58:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:58:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:58:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:58:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:58:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:58:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:58:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:58:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:58:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=78772) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=78772) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=78772) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.60it/s]
(EngineCore_DP0 pid=78772) 
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9891840 bytes
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6594560 bytes
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35610624 bytes
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=78772) [2026-01-28 12:58:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=78772) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  8.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:04,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:02,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:05<00:01,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:06<00:00,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.29it/s]
(EngineCore_DP0 pid=78772) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  6.24it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:06,  7.12it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.51it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:05,  7.92it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  8.46it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:05,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:04,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:03,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:03,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:03,  8.93it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:02,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:02,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:03<00:01,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:04<00:01,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.10it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:01,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:05<00:00,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:05<00:00,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  9.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  8.60it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  67%|██████▋   | 344/512 [00:00<00:00, 3434.66it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3478.87it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<57:29,  6.75s/it, est. speed input: 2.37 toks/s, output: 37.92 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:06<01:10,  6.77it/s, est. speed input: 77.03 toks/s, output: 1232.52 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:06<00:12, 30.45it/s, est. speed input: 268.01 toks/s, output: 4288.21 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:07<00:04, 70.67it/s, est. speed input: 504.79 toks/s, output: 8076.64 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:07<00:01, 114.65it/s, est. speed input: 699.72 toks/s, output: 11195.45 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:07<00:00, 164.08it/s, est. speed input: 872.93 toks/s, output: 13966.87 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:07<00:00, 204.91it/s, est. speed input: 1011.21 toks/s, output: 16179.38 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 204.91it/s, est. speed input: 922.52 toks/s, output: 14760.38 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 57.66it/s, est. speed input: 922.52 toks/s, output: 14760.38 toks/s] 
[rank0]:[W128 12:59:19.091852571 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.0s

测试结果:
  Requests/s:   56.71
  Tokens/s:     15424.53
  Total Reqs:   512
  Elapsed:      9.03s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      14517.20


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,23.8599,6489.9043,2.6823
128,16,128,128,256,256,38.0183,10340.9648,3.3668
256,16,256,256,256,256,55.6057,15124.7626,4.6038
512,16,512,512,256,256,56.7078,15424.5295,9.0287

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 12:59:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 12:59:32 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=79865) WARNING 01-28 12:59:39 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=79865) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=79865) WARNING 01-28 12:59:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 22.91 requests/s, 6232.10 total tokens/s, 5865.51 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 12:59:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:59:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:59:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:59:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:59:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:59:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:59:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:59:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:59:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 12:59:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 12:59:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 12:59:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 12:59:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 12:59:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 12:59:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 12:59:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 12:59:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 12:59:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 12:59:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=79865) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=79865) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=79865) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=79865) 
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=79865) [2026-01-28 12:59:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=79865) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:04,  3.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  5.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:02,  6.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:02,  6.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  7.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:01<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.44it/s]
(EngineCore_DP0 pid=79865) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.97it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  7.93it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.77it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  8.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.64it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3012.88it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:51,  2.73s/it, est. speed input: 5.86 toks/s, output: 93.78 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.73s/it, est. speed input: 369.60 toks/s, output: 5913.58 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 23.10it/s, est. speed input: 369.60 toks/s, output: 5913.58 toks/s]
[rank0]:[W128 13:00:14.731105877 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.9s

测试结果:
  Requests/s:   22.91
  Tokens/s:     6232.10
  Total Reqs:   64
  Elapsed:      2.79s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      5865.51

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:00:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 13:00:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=81009) WARNING 01-28 13:00:33 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=81009) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=81009) WARNING 01-28 13:00:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.07 requests/s, 10627.04 total tokens/s, 10001.92 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 13:00:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:00:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:00:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:00:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:00:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:00:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:00:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:00:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:00:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:00:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:00:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:00:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:00:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:00:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:00:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:00:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:00:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:00:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:00:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=81009) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=81009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=81009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]
(EngineCore_DP0 pid=81009) 
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=81009) [2026-01-28 13:00:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=81009) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:08,  3.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:08,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  5.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:04,  6.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  7.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:02,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.93it/s]
(EngineCore_DP0 pid=81009) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.13it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.94it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  9.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.76it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3331.56it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:06<12:50,  6.07s/it, est. speed input: 2.64 toks/s, output: 42.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00,  6.07s/it, est. speed input: 332.24 toks/s, output: 5315.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:06<00:00, 20.76it/s, est. speed input: 332.24 toks/s, output: 5315.81 toks/s]
[rank0]:[W128 13:01:08.498530199 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.0s

测试结果:
  Requests/s:   39.07
  Tokens/s:     10627.04
  Total Reqs:   128
  Elapsed:      3.28s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10001.92

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:01:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 13:01:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=81941) WARNING 01-28 13:01:24 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=81941) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=81941) WARNING 01-28 13:01:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.53 requests/s, 14833.32 total tokens/s, 13960.77 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 13:01:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:01:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:01:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:01:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:01:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:01:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:01:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:01:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:01:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:01:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:01:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:01:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:01:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:01:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:01:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:01:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:01:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:01:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:01:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=81941) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=81941) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=81941) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=81941) 
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=81941) [2026-01-28 13:01:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=81941) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:03,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:03<00:00,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:04<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.33it/s]
(EngineCore_DP0 pid=81941) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.01it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.87it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.45it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:01,  7.79it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:01,  7.67it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  7.61it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  7.90it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:04<00:00,  8.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  8.41it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3263.51it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:04<17:27,  4.11s/it, est. speed input: 3.90 toks/s, output: 62.34 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:04<00:05, 29.01it/s, est. speed input: 329.85 toks/s, output: 5277.53 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:04<00:01, 60.54it/s, est. speed input: 588.87 toks/s, output: 9421.81 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:04<00:00, 91.72it/s, est. speed input: 783.58 toks/s, output: 12537.32 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 91.72it/s, est. speed input: 887.67 toks/s, output: 14202.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 55.48it/s, est. speed input: 887.67 toks/s, output: 14202.67 toks/s]
[rank0]:[W128 13:02:02.623187656 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.7s

测试结果:
  Requests/s:   54.53
  Tokens/s:     14833.32
  Total Reqs:   256
  Elapsed:      4.69s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      13960.77

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:02:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 13:02:13 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=82860) WARNING 01-28 13:02:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=82860) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=82860) WARNING 01-28 13:02:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.01 requests/s, 15235.37 total tokens/s, 14339.17 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 13:02:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:02:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:02:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:02:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:02:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:02:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:02:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:02:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:02:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:02:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:02:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:02:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:02:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:02:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:02:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:02:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:02:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:02:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:02:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=82860) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=82860) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.53it/s]
(EngineCore_DP0 pid=82860) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.53it/s]
(EngineCore_DP0 pid=82860) 
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7372800 bytes
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 39813120 bytes
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=82860) [2026-01-28 13:02:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 19906560 bytes
(EngineCore_DP0 pid=82860) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:04,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:03,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:06<00:18,  1.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:06<00:12,  1.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:06<00:09,  1.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:06<00:06,  2.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:06<00:05,  3.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:07<00:03,  3.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:07<00:03,  4.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:07<00:02,  5.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:07<00:01,  6.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:07<00:01,  6.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:07<00:01,  6.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:07<00:01,  7.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:07<00:01,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:08<00:00,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:08<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:08<00:00,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:08<00:00,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:08<00:00,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:08<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:08<00:00,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  5.75it/s]
(EngineCore_DP0 pid=82860) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  6.21it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:06,  7.02it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.32it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  7.70it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  7.99it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:05,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:05,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:04,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:03,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:03,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:03<00:03,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:02,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:02,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:04<00:01,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:04<00:01,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:05<00:01,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:05<00:00,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:05<00:00,  8.45it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:06<00:00,  8.37it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  70%|███████   | 359/512 [00:00<00:00, 3586.92it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3609.10it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<57:42,  6.78s/it, est. speed input: 2.36 toks/s, output: 37.78 toks/s]
Processed prompts:   7%|▋         | 37/512 [00:06<01:02,  7.58it/s, est. speed input: 86.10 toks/s, output: 1377.55 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:07<00:13, 29.99it/s, est. speed input: 267.05 toks/s, output: 4272.81 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:07<00:04, 70.09it/s, est. speed input: 503.00 toks/s, output: 8047.98 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:07<00:01, 113.82it/s, est. speed input: 697.07 toks/s, output: 11153.08 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:07<00:00, 158.74it/s, est. speed input: 854.91 toks/s, output: 13678.59 toks/s]
Processed prompts:  90%|█████████ | 463/512 [00:07<00:00, 201.65it/s, est. speed input: 991.26 toks/s, output: 15860.16 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 201.65it/s, est. speed input: 910.49 toks/s, output: 14567.77 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:08<00:00, 56.90it/s, est. speed input: 910.49 toks/s, output: 14567.77 toks/s] 
[rank0]:[W128 13:03:12.910916523 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.3s

测试结果:
  Requests/s:   56.01
  Tokens/s:     15235.37
  Total Reqs:   512
  Elapsed:      9.14s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      14339.17


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,22.9121,6232.1009,2.7933
128,16,128,128,256,256,39.0700,10627.0382,3.2762
256,16,256,256,256,256,54.5343,14833.3197,4.6943
512,16,512,512,256,256,56.0124,15235.3677,9.1408

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:03:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 13:03:22 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=83962) WARNING 01-28 13:03:31 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=83962) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=83962) WARNING 01-28 13:03:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 22.23 requests/s, 6045.76 total tokens/s, 5690.13 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-28 13:03:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:03:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:03:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:03:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:03:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:03:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:03:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:03:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:03:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:03:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:03:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:03:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:03:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:03:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:03:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:03:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:03:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:03:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:03:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=83962) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=83962) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=83962) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.37it/s]
(EngineCore_DP0 pid=83962) 
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=83962) [2026-01-28 13:03:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=83962) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:06,  2.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:06,  2.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:04,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:03,  4.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:02,  5.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  6.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  6.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:01,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:01<00:00,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.75it/s]
(EngineCore_DP0 pid=83962) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.90it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.18it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.27it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.27it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  8.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  8.51it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2996.00it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:57,  2.81s/it, est. speed input: 5.69 toks/s, output: 90.96 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.81s/it, est. speed input: 358.48 toks/s, output: 5735.65 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 22.40it/s, est. speed input: 358.48 toks/s, output: 5735.65 toks/s]
[rank0]:[W128 13:04:07.288559428 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.0s

测试结果:
  Requests/s:   22.23
  Tokens/s:     6045.76
  Total Reqs:   64
  Elapsed:      2.88s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      5690.13

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:04:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 13:04:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=84893) WARNING 01-28 13:04:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=84893) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=84893) WARNING 01-28 13:04:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.03 requests/s, 10616.13 total tokens/s, 9991.65 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-28 13:04:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:04:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:04:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:04:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:04:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:04:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:04:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:04:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:04:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:04:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:04:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:04:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:04:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:04:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:04:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:04:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:04:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:04:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:04:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=84893) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=84893) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=84893) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=84893) 
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=84893) [2026-01-28 13:04:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=84893) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:08,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:08,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:05,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:04,  6.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  7.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:02,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:02,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:02,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:02,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:01,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  8.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:03<00:00,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  8.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.04it/s]
(EngineCore_DP0 pid=84893) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.10it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  8.07it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  8.87it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:01<00:00,  8.89it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  9.02it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:01<00:00,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  9.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  8.78it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3279.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<06:38,  3.14s/it, est. speed input: 5.10 toks/s, output: 81.63 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 55.13it/s, est. speed input: 627.35 toks/s, output: 10037.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 55.13it/s, est. speed input: 632.25 toks/s, output: 10116.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.51it/s, est. speed input: 632.25 toks/s, output: 10116.04 toks/s]
[rank0]:[W128 13:04:58.531568289 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.0s

测试结果:
  Requests/s:   39.03
  Tokens/s:     10616.13
  Total Reqs:   128
  Elapsed:      3.28s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      9991.65

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:05:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 13:05:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=85771) WARNING 01-28 13:05:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=85771) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=85771) WARNING 01-28 13:05:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 54.05 requests/s, 14700.64 total tokens/s, 13835.90 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-28 13:05:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:05:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:05:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:05:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:05:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:05:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:05:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:05:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:05:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:05:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:05:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:05:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:05:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:05:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:05:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:05:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:05:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:05:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:05:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=85771) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=85771) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=85771) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.39it/s]
(EngineCore_DP0 pid=85771) 
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=85771) [2026-01-28 13:05:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=85771) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  8.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:03,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:03,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:02,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:02,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:02,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:02,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:02,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  8.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:03<00:01,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:03<00:01,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:00,  8.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:04<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:04<00:00,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:04<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.22it/s]
(EngineCore_DP0 pid=85771) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.98it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.86it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:01,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  8.23it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  7.90it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  7.66it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  7.81it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:06<00:01,  1.03it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:06<00:00,  1.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  1.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.02it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 3183.14it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:04<17:37,  4.15s/it, est. speed input: 3.86 toks/s, output: 61.73 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:04<00:07, 24.87it/s, est. speed input: 282.47 toks/s, output: 4519.47 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:04<00:01, 58.65it/s, est. speed input: 559.10 toks/s, output: 8945.55 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:04<00:00, 90.67it/s, est. speed input: 757.71 toks/s, output: 12123.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 90.67it/s, est. speed input: 879.98 toks/s, output: 14079.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 55.00it/s, est. speed input: 879.98 toks/s, output: 14079.62 toks/s]
[rank0]:[W128 13:05:56.754603294 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.7s

测试结果:
  Requests/s:   54.05
  Tokens/s:     14700.64
  Total Reqs:   256
  Elapsed:      4.74s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      13835.90

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:06:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 13:06:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=86788) WARNING 01-28 13:06:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=86788) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=86788) WARNING 01-28 13:06:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.42 requests/s, 14529.96 total tokens/s, 13675.25 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-28 13:06:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:06:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:06:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:06:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:06:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:06:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:06:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:06:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:06:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:06:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:06:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:06:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:06:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:06:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:06:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:06:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:06:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:06:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:06:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=86788) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=86788) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=86788) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=86788) 
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11796480 bytes
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42467328 bytes
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=86788) [2026-01-28 13:06:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21299200 bytes
(EngineCore_DP0 pid=86788) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:03,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:02,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:01,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:06<00:00,  8.50it/s]
(EngineCore_DP0 pid=86788) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  6.46it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:06,  7.34it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  7.70it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:05,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:00<00:04,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:04,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:04,  8.83it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:01<00:04,  8.94it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:01<00:04,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:01<00:04,  8.91it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:04,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:01<00:04,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:01<00:03,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:01<00:03,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:03,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:02<00:03,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:02<00:03,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:02<00:03,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:02<00:03,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:02<00:03,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:03,  8.45it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:02<00:03,  8.49it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:02,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:03<00:02,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:03<00:02,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:02,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:02,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:02,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:03<00:01,  8.58it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:04<00:01,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:04<00:01,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  8.69it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:04<00:01,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:04<00:01,  8.62it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:01,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:05<00:00,  8.70it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:05<00:00,  8.68it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:05<00:00,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:05<00:00,  8.27it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  8.03it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:05<00:00,  8.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  8.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  8.53it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  69%|██████▉   | 353/512 [00:00<00:00, 3527.37it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3565.99it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:09<1:24:13,  9.89s/it, est. speed input: 1.62 toks/s, output: 25.89 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:09<01:42,  4.65it/s, est. speed input: 52.84 toks/s, output: 845.42 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:10<00:18, 21.16it/s, est. speed input: 184.88 toks/s, output: 2958.14 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:10<00:05, 49.82it/s, est. speed input: 350.03 toks/s, output: 5600.49 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:10<00:02, 82.13it/s, est. speed input: 487.32 toks/s, output: 7797.03 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:10<00:01, 116.97it/s, est. speed input: 600.40 toks/s, output: 9606.32 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:10<00:00, 153.50it/s, est. speed input: 701.16 toks/s, output: 11218.50 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 153.50it/s, est. speed input: 673.20 toks/s, output: 10771.20 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 42.07it/s, est. speed input: 673.20 toks/s, output: 10771.20 toks/s] 
[rank0]:[W128 13:07:05.489182691 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.2s

测试结果:
  Requests/s:   53.42
  Tokens/s:     14529.96
  Total Reqs:   512
  Elapsed:      9.58s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      13675.25


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,22.2271,6045.7649,2.8794
128,16,128,128,256,256,39.0299,10616.1297,3.2795
256,16,256,256,256,256,54.0465,14700.6394,4.7367
512,16,512,512,256,256,53.4190,14529.9579,9.5846

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================

[INFO] 日志已保存: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/logs/benchmark_20260128_124747.log
[SUCCESS] bitnet1.58-2b-fp8 Decode 完成 (1166.1s)

[INFO] Decode 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 5: 完整 Decode Benchmark - SUCCESS
Duration: 2317.4 seconds (38.6 minutes)
----------------------------------------------------------------------


======================================================================
TASK 6: Kernel: cuBLASLt
Started: 2026-01-28 13:07:08
======================================================================


------------------------------------------------------------
  cuBLASLt Kernel: BitNet-2B [fp8e4m3]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype fp8e4m3 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cublaslt --model BitNet-2B

============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 8, 有效: 8
    NK 2/4: (2560, 2560)
      → 算法数: 8, 有效: 8
    NK 3/4: (13824, 2560)
      → 算法数: 8, 有效: 8
    NK 4/4: (2560, 6912)
      → 算法数: 8, 有效: 8

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp8e4m3']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[SUCCESS] cuBLASLt Kernel [fp8e4m3] 测试完成 (21.7s)

------------------------------------------------------------
  cuBLASLt Kernel: BitNet-2B [int8]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype int8 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cublaslt --model BitNet-2B

============================================================
cuBLASLt Dense GEMM 算法搜索
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cublaslt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuBLASLt 可用

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 2, 有效: 2
    NK 2/4: (2560, 2560)
      → 算法数: 2, 有效: 2
    NK 3/4: (13824, 2560)
      → 算法数: 2, 有效: 2
    NK 4/4: (2560, 6912)
      → 算法数: 2, 有效: 2

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['int8']
Backend: cublaslt
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuBLASLt Dense GEMM Search
[cuBLASLt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuBLASLt:   /root/vllmbench/slidesparse/benchmark_kernel/cuBLASLt/alg_search_results
[SUCCESS] cuBLASLt Kernel [int8] 测试完成 (16.7s)

[INFO] cuBLASLt Kernel 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 6: Kernel: cuBLASLt - SUCCESS
Duration: 38.4 seconds (0.6 minutes)
----------------------------------------------------------------------


======================================================================
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10)
Started: 2026-01-28 13:07:46
======================================================================


------------------------------------------------------------
  cuSPARSELt 高稀疏 Kernel: BitNet-2B [fp8e4m3]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype fp8e4m3 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_4
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 2560)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 2560)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 6912)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_6
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 3424)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 3424)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 9216)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_8
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 3840)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 3840)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 10368)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_10
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4096)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4096)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11072)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp8e4m3']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 高稀疏 [fp8e4m3] 测试完成 (50.7s)

------------------------------------------------------------
  cuSPARSELt 高稀疏 Kernel: BitNet-2B [int8]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype int8 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_4,2_6,2_8,2_10

[INFO] Sparsity=2_4, K_factor=1.000
[INFO] NK list adjusted: [(3840, 2560), (2560, 2560), (13824, 2560), (2560, 6912)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_4
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 2560)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 2560)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 2560)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 6912)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_4.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_4.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_6, K_factor=1.333
[INFO] NK list adjusted: [(3840, 3424), (2560, 3424), (13824, 3424), (2560, 9216)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_6
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3424)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 3424)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 3424)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 9216)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_6.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_6.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_8, K_factor=1.500
[INFO] NK list adjusted: [(3840, 3840), (2560, 3840), (13824, 3840), (2560, 10368)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_8
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 3840)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 3840)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 3840)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 10368)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_8.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_8.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_10, K_factor=1.600
[INFO] NK list adjusted: [(3840, 4096), (2560, 4096), (13824, 4096), (2560, 11072)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_10
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4096)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4096)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4096)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11072)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_10.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_10.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['int8']
Backend: cusparselt
Sparsity: ['2_4', '2_6', '2_8', '2_10']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_4)
[INFO] K_factor = 1.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_4

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_6)
[INFO] K_factor = 1.333
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_6

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_8)
[INFO] K_factor = 1.500
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_8

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_10)
[INFO] K_factor = 1.600
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_10

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 高稀疏 [int8] 测试完成 (53.4s)

[INFO] cuSPARSELt 高稀疏 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS
Duration: 104.1 seconds (1.7 minutes)
----------------------------------------------------------------------


======================================================================
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf)
Started: 2026-01-28 13:09:31
======================================================================


------------------------------------------------------------
  cuSPARSELt 低稀疏 Kernel: BitNet-2B [fp8e4m3]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype fp8e4m3 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_12
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4288)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4288)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11520)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_14
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4416)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4416)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11872)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_16
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4480)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4480)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 12096)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: fp8e4m3 -> fp8e4m3 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 5120)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 5120)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 13824)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/FP8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['fp8e4m3']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=FP8E4M3
============================================================

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[fp8e4m3] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype fp8e4m3 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 低稀疏 [fp8e4m3] 测试完成 (55.4s)

------------------------------------------------------------
  cuSPARSELt 低稀疏 Kernel: BitNet-2B [int8]
------------------------------------------------------------
[INFO] 执行: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/benchmark_entry.py --dtype int8 --warmup 25 --repeat 50 --m_list 64,128,256,512,1024,2048,4096,8192,16384 --backend cusparselt --model BitNet-2B --sparsity 2_12,2_14,2_16,2_inf

[INFO] Sparsity=2_12, K_factor=1.667
[INFO] NK list adjusted: [(3840, 4288), (2560, 4288), (13824, 4288), (2560, 11520)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_12
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4288)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4288)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4288)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11520)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_12.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_12.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_14, K_factor=1.714
[INFO] NK list adjusted: [(3840, 4416), (2560, 4416), (13824, 4416), (2560, 11872)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_14
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4416)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4416)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4416)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 11872)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_14.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_14.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_16, K_factor=1.750
[INFO] NK list adjusted: [(3840, 4480), (2560, 4480), (13824, 4480), (2560, 12096)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_16
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 4480)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 4480)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 4480)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 12096)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_16.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_16.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
[INFO] Sparsity=2_inf, K_factor=2.000
[INFO] NK list adjusted: [(3840, 5120), (2560, 5120), (13824, 5120), (2560, 13824)]
============================================================
cuSPARSELt Sparse GEMM 算法搜索 (2:4 稀疏)
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
dtype: int8 -> int8 (same input/output)
Sparsity: 2_inf
Segment-K 测试: 关闭
API 搜索对比: 开启
warmup=25, repeat=50

[1/4] 编译 CUDA 扩展...
✓ Using existing: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
[2/4] 加载 CUDA 扩展...
✓ cuSPARSELt 可用
✓ Segment-K 支持: 否

[3/4] 开始算法搜索...
      NK 组合: 4 个, M 列表: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]

    NK 1/4: (3840, 5120)
      → 算法数: 4, 有效: 4
    NK 2/4: (2560, 5120)
      → 算法数: 4, 有效: 4
    NK 3/4: (13824, 5120)
      → 算法数: 4, 有效: 4
    NK 4/4: (2560, 13824)
      → 算法数: 4, 有效: 4

    搜索统计: 总计=36, 成功=36, 失败=0, 错误=0
    成功率: 100.0%

[4/4] 保存结果...
    CSV: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_inf.csv
    JSON: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8/alg_search_BitNet-2B-INT8_2_inf.json

✓ 完成! 结果已保存到: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results/RTX4090_cc89_py312_cu129_x86_64/INT8
============================================================
============================================================
SlideSparse Kernel Benchmark
============================================================
GPU: NVIDIA GeForce RTX 4090 (cc89)
Mode: MODEL
Model: BitNet-2B-INT8
M_list: [64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384]
dtype: ['int8']
Backend: cusparselt
Sparsity: ['2_12', '2_14', '2_16', '2_inf']
warmup=25, repeat=50
============================================================

============================================================
Benchmark dtype=INT8
============================================================

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_12)
[INFO] K_factor = 1.667
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_12

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_14)
[INFO] K_factor = 1.714
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_14

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_16)
[INFO] K_factor = 1.750
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_16

[int8] Running cuSPARSELt Sparse GEMM Search (sparsity=2_inf)
[INFO] K_factor = 2.000
[cuSPARSELt] Running: /usr/bin/python3 /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search.py --dtype int8 --model BitNet-2B-INT8 --warmup 25 --repeat 50 --out_dir /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results --m_list 64,128,256,512,1024,2048,4096,8192,16384 --sparsity 2_inf

============================================================
Benchmark Complete!
============================================================

Results saved to:
  - cuSPARSELt: /root/vllmbench/slidesparse/benchmark_kernel/cuSPARSELt/alg_search_results
[SUCCESS] cuSPARSELt 低稀疏 [int8] 测试完成 (57.2s)

[INFO] cuSPARSELt 低稀疏 统计: 成功 2, 失败 0

----------------------------------------------------------------------
TASK 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS
Duration: 112.6 seconds (1.9 minutes)
----------------------------------------------------------------------



============================================================
  最终总结
============================================================


  Task 1: 基础模型准备 (下载 + 量化) - SUCCESS (99.9s)
  Task 2: SlideSparse 转换 (prune + slide) - SKIPPED
  Task 3: 离线调优 (粗调优 + 细调优) - SKIPPED
  Task 4: 完整 Prefill Benchmark - SUCCESS (5254.9s)
  Task 5: 完整 Decode Benchmark - SUCCESS (2317.4s)
  Task 6: Kernel: cuBLASLt - SUCCESS (38.4s)
  Task 7: Kernel: cuSPARSELt 高稀疏 (2_4~2_10) - SUCCESS (104.1s)
  Task 8: Kernel: cuSPARSELt 低稀疏 (2_12~2_inf) - SUCCESS (112.6s)

  总计: 6 成功, 0 失败, 2 跳过
  总耗时: 7927.3 秒 (2.20 小时)

[INFO] 日志文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260128_105916.log
[INFO] 状态文件: /root/vllmbench/slidesparse/tools/bitnet_bench_20260128_105916_status.json
