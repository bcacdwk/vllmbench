# SlideSparse Phase 3: FP8 GEMM é›†æˆæŠ€æœ¯åˆ†æ

> æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ vLLM ä¸­ FP8 GEMM çš„å®ç°æ¶æ„ï¼ŒåŒ…æ‹¬ compressed-tensors æ ¼å¼ã€CUTLASS å†…æ ¸å®ç°ä»¥åŠ cuBLASLt æ›¿æ¢æ–¹æ¡ˆã€‚

---

## ç›®å½•

1. [Compressed-Tensors è½¬å‘æ¶æ„åˆ†æ](#1-compressed-tensors-è½¬å‘æ¶æ„åˆ†æ)
2. [å½“å‰ CUTLASS FP8 GEMM å®ç°è¯¦è§£](#2-å½“å‰-cutlass-fp8-gemm-å®ç°è¯¦è§£)
3. [cuBLASLt æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹](#3-cublaslt-æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹)
4. [å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹](#4-å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹)

---

## 1. Compressed-Tensors è½¬å‘æ¶æ„åˆ†æ

### 1.1 ä¸ºä»€ä¹ˆä½¿ç”¨ Compressed-Tensors è€Œä¸æ˜¯åŸç”Ÿ FP8/INT8

**æ ¸å¿ƒåŸå› ï¼šCompressed-Tensors æ˜¯ä¸€ä¸ªå…ƒæ ¼å¼ï¼ˆMeta-Formatï¼‰**

HuggingFace ä¸Šçš„é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ RedHat çš„ W8A8ã€FP8-dynamic æ¨¡å‹ï¼‰ä½¿ç”¨ `compressed-tensors` ä½œä¸ºé‡åŒ–é…ç½®æ ¼å¼ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„é‡åŒ–å®ç°ï¼Œè€Œæ˜¯ä¸€ä¸ª**é…ç½®è§£æå±‚**ï¼Œå®ƒä¼šï¼š

1. **è¯»å–æ¨¡å‹çš„ `config.json`** ä¸­çš„é‡åŒ–é…ç½®
2. **è‡ªåŠ¨æ£€æµ‹é‡åŒ–ç±»å‹**ï¼ˆFP8ã€INT8ã€W4A16 ç­‰ï¼‰
3. **é€‰æ‹©å¯¹åº”çš„ Scheme**ï¼ˆ`CompressedTensorsW8A8Fp8`ã€`CompressedTensorsW8A8Int8` ç­‰ï¼‰

```python
# vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
class CompressedTensorsConfig(QuantizationConfig):
    def get_scheme(self, layer, layer_name):
        # æ ¹æ® layer ç±»å‹å’Œé…ç½®é€‰æ‹© scheme
        scheme = self._get_scheme_from_parts(...)
        
        # âœ… æˆ‘ä»¬çš„ cuBLASLt åŒ…è£…ç‚¹
        scheme = wrap_scheme_with_cublaslt(scheme)
        return scheme
```

**å…¸å‹çš„æ¨¡å‹é…ç½®ç¤ºä¾‹**ï¼ˆ`config.json`ï¼‰ï¼š
```json
{
  "quantization_config": {
    "quant_method": "compressed-tensors",
    "config_groups": {
      "group_0": {
        "weights": {
          "num_bits": 8,
          "type": "float",
          "strategy": "channel"
        },
        "input_activations": {
          "num_bits": 8,
          "type": "float",
          "strategy": "token"
        }
      }
    }
  }
}
```

### 1.2 å½“å‰ SlideSparse cuBLASLt è½¬å‘æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç”¨æˆ·åŠ è½½ FP8 é‡åŒ–æ¨¡å‹                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CompressedTensorsConfig.get_scheme()                   â”‚
â”‚                                                                     â”‚
â”‚   1. è§£æ config_groups é…ç½®                                         â”‚
â”‚   2. è°ƒç”¨ _get_scheme_from_parts() â†’ CompressedTensorsW8A8Fp8       â”‚
â”‚   3. âœ… wrap_scheme_with_cublaslt(scheme) åŒ…è£…                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CuBLASLtSchemeWrapper                            â”‚
â”‚                                                                     â”‚
â”‚   - _original_scheme: CompressedTensorsW8A8Fp8                      â”‚
â”‚   - create_weights()      â†’ å§”æ‰˜ç»™åŸå§‹ scheme                        â”‚
â”‚   - process_weights_after_loading() â†’ å§”æ‰˜ç»™åŸå§‹ scheme              â”‚
â”‚   - apply_weights()       â†’ è°ƒç”¨ CuBLASLtFp8LinearOp                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CuBLASLtFp8LinearOp.apply()                      â”‚
â”‚                                                                     â”‚
â”‚   å½“å‰å®ç°ï¼ˆUSE_REAL_CUBLASLT=Falseï¼‰:                               â”‚
â”‚   - è°ƒç”¨ vLLM åŸç”Ÿ Fp8LinearOpï¼ˆä½¿ç”¨ CUTLASSï¼‰                       â”‚
â”‚                                                                     â”‚
â”‚   ç›®æ ‡å®ç°ï¼ˆUSE_REAL_CUBLASLT=Trueï¼‰:                                â”‚
â”‚   - è°ƒç”¨ cuBLASLt FP8 matmul API                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 è½¬å‘æ¶æ„éªŒè¯

**å½“å‰æ¶æ„å·²ç¡®è®¤æ­£ç¡®ï¼š**

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| ç¯å¢ƒå˜é‡æ£€æµ‹ | âœ… | `VLLM_USE_CUBLASLT=1` æˆ– `SLIDESPARSE_USE_CUBLASLT=1` |
| Scheme åŒ…è£… | âœ… | `wrap_scheme_with_cublaslt()` åœ¨ `get_scheme()` ä¸­è°ƒç”¨ |
| æƒé‡åˆ›å»º | âœ… | å§”æ‰˜ç»™åŸå§‹ schemeï¼Œä¿æŒå…¼å®¹æ€§ |
| æƒé‡åŠ è½½ | âœ… | å§”æ‰˜ç»™åŸå§‹ schemeï¼Œsafetensor æ ¼å¼æ­£å¸¸åŠ è½½ |
| æ¨ç†è·¯å¾„ | âœ… | `apply_weights()` æ­£ç¡®è°ƒç”¨ `CuBLASLtFp8LinearOp` |

---

## 2. å½“å‰ CUTLASS FP8 GEMM å®ç°è¯¦è§£

### 2.1 å®Œæ•´è°ƒç”¨é“¾

```
CompressedTensorsW8A8Fp8.apply_weights()
    â”‚
    â”œâ”€â†’ QuantFP8.apply()  [è¾“å…¥é‡åŒ–ï¼Œå¯é€‰]
    â”‚       â””â”€â†’ per-token / per-tensor é‡åŒ–
    â”‚
    â””â”€â†’ Fp8LinearOp.apply()
            â”‚
            â”œâ”€â†’ cutlass_w8a8_scaled_mm()  [CUDA 12.0+, SM90+]
            â”‚       â””â”€â†’ ops.cutlass_scaled_mm()
            â”‚               â””â”€â†’ cutlass_scaled_mm_sm90_fp8()
            â”‚
            â”œâ”€â†’ ops.scaled_fp8_quant()  [Flash-attention è·¯å¾„]
            â”‚
            â””â”€â†’ torch._scaled_mm()  [Fallback]
```

### 2.2 è¾“å…¥é‡åŒ–è¿‡ç¨‹ï¼ˆQuantFP8ï¼‰

```python
# vllm/model_executor/layers/quantization/input_quant_fp8.py
class QuantFP8(CustomOp):
    """FP8 è¾“å…¥é‡åŒ–ï¼Œæ”¯æŒä¸‰ç§ç­–ç•¥"""
    
    # é‡åŒ–å…¬å¼: x_fp8 = x / scale
    # scale è®¡ç®—: scale = max(|x|) / fp8_max
    
    def __init__(self, quant_config):
        self.strategy = quant_config.input_strategy
        # "tensor" - æ•´ä¸ª tensor å…±äº«ä¸€ä¸ª scale
        # "token"  - æ¯è¡Œï¼ˆtokenï¼‰ä¸€ä¸ª scale  
        # "group"  - æ¯ group_size ä¸ªå…ƒç´ ä¸€ä¸ª scale
```

**é‡åŒ–ç­–ç•¥è¯´æ˜ï¼š**

| ç­–ç•¥ | scale å½¢çŠ¶ | è¯´æ˜ |
|------|-----------|------|
| `per-tensor` | `[1]` | æ•´ä¸ªè¾“å…¥å…±äº«ä¸€ä¸ª scaleï¼Œç²¾åº¦æœ€ä½ä½†æœ€å¿« |
| `per-token` | `[M, 1]` | æ¯è¡Œä¸€ä¸ª scaleï¼ŒLLM æ¨ç†çš„å…¸å‹é…ç½® |
| `per-channel` | `[1, K]` | æ¯åˆ—ä¸€ä¸ª scaleï¼Œæƒé‡é‡åŒ–å¸¸ç”¨ |

### 2.3 GEMM Layout è®¾è®¡

**vLLM CUTLASS FP8 GEMM Layout:**

```
é—®é¢˜å®šä¹‰: C[M,N] = A[M,K] Ã— B[K,N]

å®é™…å­˜å‚¨ï¼ˆCUTLASS å†…éƒ¨ï¼‰:
    A: RowMajor    [M, K]  - æ¯è¡Œè¿ç»­å­˜å‚¨
    B: ColumnMajor [K, N]  - ç­‰ä»·äº [N, K]^Tï¼Œæ¯åˆ—è¿ç»­å­˜å‚¨  
    C: RowMajor    [M, N]
    D: RowMajor    [M, N]

åœ¨ vLLM ä¸­:
    input (x):   [batch, hidden_dim] = [M, K]  RowMajor
    weight (w):  [out_features, in_features] = [N, K]  å®é™…å­˜å‚¨
                 ä¼ ç»™ CUTLASS æ—¶ä½œä¸º ColumnMajor [K, N]
    output:      [batch, out_features] = [M, N]  RowMajor
```

**swap_ab æœºåˆ¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ï¼š**

```cpp
// csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh

// å½“ M å¾ˆå°æ—¶ï¼ˆdecode é˜¶æ®µï¼‰ï¼Œäº¤æ¢ A å’Œ B ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
template <bool swap_ab = false>
void cutlass_scaled_mm_sm90_fp8_dispatch(...) {
    // swap_ab=true æ—¶:
    //   å®é™…è®¡ç®—: D^T = B^T Ã— A^T
    //   ç­‰ä»·äº:   D   = A Ã— B
    //   ä½†åˆ©ç”¨äº† B çš„æ›´å¥½çš„å†…å­˜è®¿é—®æ¨¡å¼
}

// é€‰æ‹©é€»è¾‘
if (M <= 64) {
    // å° M åœºæ™¯ï¼ˆdecodeï¼‰ï¼Œä½¿ç”¨ swap_ab
    cutlass_scaled_mm_sm90_fp8_dispatch<true>(...);
} else {
    // å¤§ M åœºæ™¯ï¼ˆprefillï¼‰ï¼Œä¸ä½¿ç”¨ swap_ab
    cutlass_scaled_mm_sm90_fp8_dispatch<false>(...);
}
```

### 2.4 Epilogueï¼ˆèåˆåå¤„ç†ï¼‰è¯¦è§£

**CUTLASS 3.x Epilogue è®¡ç®—å…¬å¼ï¼š**

```
D = scale_a * (scale_b * Accumulator) + bias

å…·ä½“å±•å¼€ï¼ˆScaledEpilogueBiasï¼‰:
    Compute0: tmp = scale_b * Accum      (é€å…ƒç´ æˆ–é€è¡Œ)
    Compute1: D = scale_a * tmp + bias   (é€å…ƒç´ æˆ–é€åˆ—)
```

**Epilogue ç±»å‹å®šä¹‰ï¼š**

```cpp
// csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp

// åŸºç¡€ Epilogueï¼ˆæ—  biasï¼‰
struct ScaledEpilogue {
    // scale_a: ColOrScalarLoad - æ¯åˆ—ä¸€ä¸ª scale æˆ–å…¨å±€ scalar
    // scale_b: RowOrScalarLoad - æ¯è¡Œä¸€ä¸ª scale æˆ–å…¨å±€ scalar
    
    using EVTCompute = Sm90EVT<
        Compute1,           // D = scale_a * tmp
        ScaleA,             // ColOrScalarLoad
        Sm90EVT<
            Compute0,       // tmp = scale_b * Accum
            ScaleB,         // RowOrScalarLoad
            Accum           // ç´¯åŠ å™¨è¾“å‡º
        >
    >;
};

// å¸¦ Bias çš„ Epilogue
struct ScaledEpilogueBias {
    // bias: RowLoad - æ¯è¡Œä¸€ä¸ª biasï¼ˆå¹¿æ’­åˆ°æ‰€æœ‰åˆ—ï¼‰
    
    using EVTCompute = Sm90EVT<
        Compute1,           // D = scale_a * tmp + bias
        ScaleA,             // ColOrScalarLoad
        Sm90EVT<
            Compute0,       // tmp = scale_b * Accum
            ScaleB,         // RowOrScalarLoad
            Accum
        >,
        Bias                // RowLoad
    >;
};

// swap_ab åœºæ™¯çš„ Biasï¼ˆåˆ—å¹¿æ’­ï¼‰
struct ScaledEpilogueColumnBias {
    // å½“ swap_ab=true æ—¶ï¼Œbias éœ€è¦åˆ—æ–¹å‘åŠ è½½
    using Bias = ColLoad<float>;
};
```

**Scale åŠ è½½æ¨¡å¼ï¼š**

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| `ScalarLoad` | å…¨å±€å•ä¸€ scale | per-tensor é‡åŒ– |
| `RowLoad` | æ¯è¡Œä¸€ä¸ª scale | per-token (activation) |
| `ColLoad` | æ¯åˆ—ä¸€ä¸ª scale | per-channel (weight) |
| `RowOrScalarLoad` | è¿è¡Œæ—¶é€‰æ‹© | å…¼å®¹ä¸¤ç§æ¨¡å¼ |
| `ColOrScalarLoad` | è¿è¡Œæ—¶é€‰æ‹© | å…¼å®¹ä¸¤ç§æ¨¡å¼ |

### 2.5 Kernel é€‰æ‹©ç­–ç•¥

```cpp
// csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh

template <typename OutType, bool swap_ab, bool with_bias>
void cutlass_gemm_sm90_fp8_dispatch(int M, int N, int K, ...) {
    
    // æ ¹æ®é—®é¢˜è§„æ¨¡é€‰æ‹©æœ€ä¼˜ kernel é…ç½®
    if (M <= 16) {
        // æå° Mï¼šä½¿ç”¨ M16N128K128 tile
        using TileShape = Shape<_16, _128, _128>;
        using ClusterShape = Shape<_1, _2, _1>;
        
    } else if (M <= 64) {
        // å° Mï¼šä½¿ç”¨ M64N128K128 tile
        using TileShape = Shape<_64, _128, _128>;
        using ClusterShape = Shape<_1, _2, _1>;
        
    } else if (M <= 128) {
        // ä¸­ç­‰ Mï¼šä½¿ç”¨ M128N128K128 tile
        using TileShape = Shape<_128, _128, _128>;
        using ClusterShape = Shape<_1, _1, _1>;
        
    } else if (N >= 8192) {
        // å¤§ Nï¼ˆå®½çŸ©é˜µï¼‰ï¼šä½¿ç”¨ä¸“é—¨é…ç½®
        using TileShape = Shape<_128, _256, _64>;
        using ClusterShape = Shape<_2, _1, _1>;
        
    } else {
        // é»˜è®¤é…ç½®
        using TileShape = Shape<_128, _128, _128>;
        using ClusterShape = Shape<_2, _1, _1>;
    }
}
```

---

## 3. cuBLASLt æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹

### 3.1 cuBLASLt FP8 GEMM æ ¸å¿ƒæ¦‚å¿µ

**åŸºæœ¬è®¡ç®—å…¬å¼ï¼ˆTensorwide Scalingï¼‰ï¼š**

```
D = scaleD * (Î± * scaleA * scaleB * op(A) Ã— op(B) + Î² * scaleC * C)
```

**vLLM åœºæ™¯ç®€åŒ–ï¼ˆæ—  scaleC/scaleDï¼ŒÎ²=0ï¼‰ï¼š**

```
D = Î± * scaleA * scaleB * op(A) Ã— op(B) + bias
```

### 3.2 Layout å¯¹åº”å…³ç³»

| vLLM/CUTLASS | cuBLASLt | è¯´æ˜ |
|--------------|----------|------|
| A: RowMajor | A: ColumnMajor + CUBLAS_OP_T | è½¬ç½®åç­‰ä»· |
| B: ColumnMajor | B: ColumnMajor + CUBLAS_OP_N | ç›´æ¥å¯¹åº” |
| C/D: RowMajor | C/D: ColumnMajor + è½¬ç½® | éœ€è¦é¢å¤–å¤„ç† |

**æ¨èåšæ³•ï¼šä½¿ç”¨ TN æ ¼å¼**

```cpp
// cuBLASLt æœ€ä¼˜é…ç½®ï¼ˆAda/Hopperï¼‰
CUBLAS_OP_T  // A è½¬ç½®
CUBLAS_OP_N  // B ä¸è½¬ç½®
```

### 3.3 Scale å¤„ç†æ–¹å¼å¯¹æ¯”

**CUTLASS æ–¹å¼ï¼š**
- `scale_a`ï¼šper-tokenï¼ˆåˆ—å‘é‡ï¼‰æˆ– scalar
- `scale_b`ï¼šper-channelï¼ˆè¡Œå‘é‡ï¼‰æˆ– scalar
- èåˆåœ¨ Epilogue ä¸­è®¡ç®—

**cuBLASLt æ–¹å¼ï¼š**
- `CUBLASLT_MATMUL_DESC_A_SCALE_POINTER`ï¼šæŒ‡å‘ scaleA
- `CUBLASLT_MATMUL_DESC_B_SCALE_POINTER`ï¼šæŒ‡å‘ scaleB
- æ”¯æŒä¸¤ç§ Scale Modeï¼š
  - `CUBLASLT_MATMUL_MATRIX_SCALE_SCALAR_32F`ï¼šper-tensorï¼ˆé»˜è®¤ï¼‰
  - `CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F`ï¼šper-row/colï¼ˆSM90+ï¼‰

### 3.4 Bias å¤„ç†

**cuBLASLt Epilogue é€‰é¡¹ï¼š**

```cpp
// è®¾ç½® epilogue ç±»å‹
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_EPILOGUE,
    &epilogue,  // CUBLASLT_EPILOGUE_BIAS
    sizeof(epilogue)
);

// è®¾ç½® bias æŒ‡é’ˆ
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_BIAS_POINTER,
    &bias_ptr,
    sizeof(bias_ptr)
);

// è®¾ç½® bias æ•°æ®ç±»å‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸è¾“å‡ºç›¸åŒï¼‰
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE,
    &bias_type,  // CUDA_R_32F for FP8 kernels
    sizeof(bias_type)
);
```

**é‡è¦é™åˆ¶ï¼š**
- Bias å‘é‡é•¿åº¦å¿…é¡»ç­‰äºè¾“å‡ºçŸ©é˜µè¡Œæ•°ï¼ˆMï¼‰
- Bias è¢«å¹¿æ’­åˆ°æ‰€æœ‰åˆ—
- FP8 kernel çš„ bias ç±»å‹é€šå¸¸æ˜¯ `CUDA_R_16BF` æˆ– `CUDA_R_32F`

### 3.5 å®Œæ•´ cuBLASLt FP8 GEMM å®ç°æ¡†æ¶

```cpp
// ä¼ªä»£ç ç¤ºä¾‹
cublasStatus_t cublaslt_fp8_gemm(
    int M, int N, int K,
    const void* A,        // FP8 input [M, K]
    const void* B,        // FP8 weight [K, N] (stored as [N, K]^T)
    void* D,              // Output [M, N]
    const float* scale_a, // per-token scale [M] or scalar
    const float* scale_b, // per-channel scale [N] or scalar
    const float* bias,    // optional bias [M]
    bool is_scale_a_scalar,
    bool is_scale_b_scalar,
    cudaStream_t stream
) {
    cublasLtHandle_t handle;
    cublasLtCreate(&handle);
    
    // 1. åˆ›å»ºçŸ©é˜µä¹˜æ³•æè¿°ç¬¦
    cublasLtMatmulDesc_t matmulDesc;
    cublasComputeType_t computeType = CUBLAS_COMPUTE_32F;
    cudaDataType_t scaleType = CUDA_R_32F;
    cublasLtMatmulDescCreate(&matmulDesc, computeType, scaleType);
    
    // 2. è®¾ç½®è½¬ç½®æ“ä½œï¼ˆTN æ ¼å¼ï¼‰
    cublasOperation_t opA = CUBLAS_OP_T;
    cublasOperation_t opB = CUBLAS_OP_N;
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSA, &opA, sizeof(opA));
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSB, &opB, sizeof(opB));
    
    // 3. è®¾ç½® Scale æŒ‡é’ˆ
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &scale_a, sizeof(scale_a));
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &scale_b, sizeof(scale_b));
    
    // 4. è®¾ç½® Scale Modeï¼ˆper-tensor vs per-row/colï¼‰
    if (!is_scale_a_scalar || !is_scale_b_scalar) {
        // Outer vector scalingï¼ˆSM90+ onlyï¼‰
        int32_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F;
        if (!is_scale_a_scalar) {
            cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode));
        }
        if (!is_scale_b_scalar) {
            cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode));
        }
    }
    
    // 5. è®¾ç½® Biasï¼ˆå¦‚æœæœ‰ï¼‰
    if (bias != nullptr) {
        cublasLtEpilogue_t epilogue = CUBLASLT_EPILOGUE_BIAS;
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_EPILOGUE, &epilogue, sizeof(epilogue));
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_BIAS_POINTER, &bias, sizeof(bias));
        
        cudaDataType_t biasType = CUDA_R_32F;  // æˆ– CUDA_R_16BF
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE, &biasType, sizeof(biasType));
    }
    
    // 6. åˆ›å»ºçŸ©é˜µå¸ƒå±€
    cublasLtMatrixLayout_t Adesc, Bdesc, Ddesc;
    
    // A: [K, M] ColumnMajor (å› ä¸º opA=T, å®é™…è¯»å– [M, K] RowMajor)
    cublasLtMatrixLayoutCreate(&Adesc, CUDA_R_8F_E4M3, K, M, K);
    
    // B: [K, N] ColumnMajor
    cublasLtMatrixLayoutCreate(&Bdesc, CUDA_R_8F_E4M3, K, N, K);
    
    // D: [M, N] (éœ€è¦æ ¹æ®è¾“å‡ºç±»å‹è®¾ç½®)
    cublasLtMatrixLayoutCreate(&Ddesc, CUDA_R_16BF, M, N, M);
    
    // 7. è·å–ç®—æ³•å¯å‘å¼
    cublasLtMatmulPreference_t preference;
    cublasLtMatmulPreferenceCreate(&preference);
    
    size_t workspaceSize = 64 * 1024 * 1024;  // 64 MB
    cublasLtMatmulPreferenceSetAttribute(preference, CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES, &workspaceSize, sizeof(workspaceSize));
    
    cublasLtMatmulHeuristicResult_t heuristicResult;
    int returnedResults = 0;
    cublasLtMatmulAlgoGetHeuristic(handle, matmulDesc, Adesc, Bdesc, Ddesc, Ddesc, preference, 1, &heuristicResult, &returnedResults);
    
    // 8. æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
    float alpha = 1.0f;
    float beta = 0.0f;
    void* workspace = nullptr;
    cudaMalloc(&workspace, heuristicResult.workspaceSize);
    
    cublasLtMatmul(
        handle, matmulDesc,
        &alpha,
        A, Adesc,
        B, Bdesc,
        &beta,
        D, Ddesc,  // C = D for in-place
        D, Ddesc,
        &heuristicResult.algo,
        workspace, heuristicResult.workspaceSize,
        stream
    );
    
    // æ¸…ç†
    cudaFree(workspace);
    cublasLtMatmulPreferenceDestroy(preference);
    cublasLtMatrixLayoutDestroy(Adesc);
    cublasLtMatrixLayoutDestroy(Bdesc);
    cublasLtMatrixLayoutDestroy(Ddesc);
    cublasLtMatmulDescDestroy(matmulDesc);
    cublasLtDestroy(handle);
    
    return CUBLAS_STATUS_SUCCESS;
}
```

### 3.6 å…³é”®æ³¨æ„äº‹é¡¹

#### 3.6.1 æ•°æ®ç±»å‹æ”¯æŒ

| Atype | Btype | Ctype | Dtype | æ”¯æŒ |
|-------|-------|-------|-------|------|
| E4M3 | E4M3 | BF16 | BF16 | âœ… |
| E4M3 | E4M3 | FP16 | FP16 | âœ… |
| E4M3 | E4M3 | FP32 | FP32 | âœ… |
| E5M2 | E4M3 | BF16 | BF16 | âœ… |
| E4M3 | E5M2 | BF16 | BF16 | âœ… |

#### 3.6.2 Scale Mode é™åˆ¶ï¼ˆOuter Vector Scalingï¼‰

```cpp
// SM90 (Hopper) ç‹¬æœ‰åŠŸèƒ½
CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F

// é™åˆ¶:
// 1. ä»…æ”¯æŒ SM90+
// 2. scaleD ä¸æ”¯æŒï¼ˆè¾“å‡ºå¿…é¡»æ˜¯ FP16/BF16/FP32ï¼‰
// 3. scaleA é•¿åº¦ = M, scaleB é•¿åº¦ = N
```

#### 3.6.3 å¯¹é½è¦æ±‚

- æ‰€æœ‰çŸ©é˜µæŒ‡é’ˆå¿…é¡» 16 å­—èŠ‚å¯¹é½
- ç»´åº¦ M, K æœ€å¥½æ˜¯ 16 çš„å€æ•°
- workspace å¿…é¡» 256 å­—èŠ‚å¯¹é½

#### 3.6.4 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å¤ç”¨ Handle å’Œ Descriptor**ï¼šåˆ›å»ºå¼€é”€å¤§ï¼Œåº”å…¨å±€å¤ç”¨
2. **ç¼“å­˜ Heuristic ç»“æœ**ï¼šç›¸åŒé—®é¢˜è§„æ¨¡å¯å¤ç”¨ç®—æ³•é€‰æ‹©
3. **Workspace é¢„åˆ†é…**ï¼šé¿å…è¿è¡Œæ—¶åˆ†é…
4. **ä½¿ç”¨ Fast Accumulation**ï¼š`CUBLASLT_MATMUL_DESC_FAST_ACCUM = 1`

---

## 4. å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹

### 4.1 ä¿®æ”¹ CuBLASLtFp8LinearOp

```python
# slidesparse/core/cublaslt_linear_method.py

class CuBLASLtFp8LinearOp:
    USE_REAL_CUBLASLT = True  # å¯ç”¨çœŸå® cuBLASLt
    
    def __init__(self):
        # åˆå§‹åŒ– cuBLASLt handleï¼ˆå•ä¾‹ï¼‰
        self._handle = self._get_or_create_handle()
        
    @classmethod
    def _get_or_create_handle(cls):
        # å…¨å±€ handle ç¼“å­˜
        if not hasattr(cls, '_global_handle'):
            cls._global_handle = cublaslt_create_handle()
        return cls._global_handle
    
    def apply(
        self,
        x: torch.Tensor,           # [M, K] FP8
        weight: torch.Tensor,      # [N, K] FP8
        x_scale: torch.Tensor,     # [M] or [1]
        weight_scale: torch.Tensor,# [N] or [1]
        bias: Optional[torch.Tensor] = None,
        output_dtype: torch.dtype = torch.bfloat16,
    ) -> torch.Tensor:
        
        M, K = x.shape
        N = weight.shape[0]
        
        # ç¡®å®š scale mode
        is_x_scale_scalar = (x_scale.numel() == 1)
        is_w_scale_scalar = (weight_scale.numel() == 1)
        
        # è°ƒç”¨ cuBLASLt kernel
        output = cublaslt_fp8_gemm(
            self._handle,
            x.data_ptr(),
            weight.data_ptr(),
            x_scale.data_ptr(),
            weight_scale.data_ptr(),
            bias.data_ptr() if bias is not None else None,
            M, N, K,
            is_x_scale_scalar,
            is_w_scale_scalar,
            output_dtype,
            torch.cuda.current_stream().cuda_stream,
        )
        
        return output
```

### 4.2 CUDA ç»‘å®šå®ç°

éœ€è¦åœ¨ `csrc/` ç›®å½•ä¸‹æ·»åŠ  cuBLASLt wrapperï¼š

```cpp
// csrc/quantization/cublaslt_fp8_gemm.cu

#include <cublasLt.h>
#include <torch/extension.h>

// å…¨å±€ handle ç®¡ç†
class CublasLtHandlePool {
public:
    static cublasLtHandle_t get() {
        static thread_local cublasLtHandle_t handle = nullptr;
        if (handle == nullptr) {
            cublasLtCreate(&handle);
        }
        return handle;
    }
};

torch::Tensor cublaslt_fp8_gemm(
    torch::Tensor A,        // [M, K] FP8
    torch::Tensor B,        // [N, K] FP8 (stored transposed)
    torch::Tensor scale_a,  // [M] or [1]
    torch::Tensor scale_b,  // [N] or [1]
    c10::optional<torch::Tensor> bias,
    bool is_scale_a_scalar,
    bool is_scale_b_scalar,
    torch::Dtype output_dtype
) {
    // å®ç°å‚è§ 3.5 èŠ‚æ¡†æ¶
}

// Python ç»‘å®š
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("cublaslt_fp8_gemm", &cublaslt_fp8_gemm, "cuBLASLt FP8 GEMM");
}
```

### 4.3 CMake é›†æˆ

```cmake
# cmake/cublaslt_extension.cmake

find_package(CUDAToolkit REQUIRED)

add_library(cublaslt_fp8_gemm SHARED
    csrc/quantization/cublaslt_fp8_gemm.cu
)

target_link_libraries(cublaslt_fp8_gemm
    CUDA::cublasLt
    ${TORCH_LIBRARIES}
)
```

---

## 5. æµ‹è¯•ä¸éªŒè¯è®¡åˆ’

### 5.1 æ­£ç¡®æ€§æµ‹è¯•

```python
def test_cublaslt_fp8_gemm_correctness():
    """å¯¹æ¯” cuBLASLt ä¸ CUTLASS ç»“æœ"""
    M, N, K = 1024, 4096, 4096
    
    # éšæœºç”Ÿæˆ FP8 è¾“å…¥
    x = torch.randn(M, K, device='cuda').to(torch.float8_e4m3fn)
    w = torch.randn(N, K, device='cuda').to(torch.float8_e4m3fn)
    scale_x = torch.rand(M, device='cuda')
    scale_w = torch.rand(N, device='cuda')
    bias = torch.randn(N, device='cuda', dtype=torch.bfloat16)
    
    # CUTLASS å‚è€ƒå®ç°
    ref = cutlass_scaled_mm(x, w, scale_x, scale_w, bias)
    
    # cuBLASLt å®ç°
    out = cublaslt_fp8_gemm(x, w, scale_x, scale_w, bias)
    
    # éªŒè¯ï¼ˆFP8 è®¡ç®—å…è®¸ä¸€å®šè¯¯å·®ï¼‰
    assert torch.allclose(out, ref, rtol=1e-2, atol=1e-2)
```

### 5.2 æ€§èƒ½æµ‹è¯•

```python
def benchmark_cublaslt_vs_cutlass():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    configs = [
        (1, 4096, 4096),     # decode
        (32, 4096, 4096),    # small batch
        (128, 4096, 4096),   # medium batch
        (1024, 4096, 4096),  # large batch
        (4096, 4096, 4096),  # prefill
    ]
    
    for M, N, K in configs:
        # é¢„çƒ­
        for _ in range(10):
            cutlass_run(M, N, K)
            cublaslt_run(M, N, K)
        
        # æµ‹é‡
        cutlass_time = benchmark(cutlass_run, M, N, K, iters=100)
        cublaslt_time = benchmark(cublaslt_run, M, N, K, iters=100)
        
        print(f"[{M}, {N}, {K}] CUTLASS: {cutlass_time:.2f}ms, cuBLASLt: {cublaslt_time:.2f}ms")
```

---

## 6. æ€»ç»“

### 6.1 å½“å‰çŠ¶æ€

| ç»„ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| ç¯å¢ƒå˜é‡åˆ‡æ¢ | âœ… å®Œæˆ | `VLLM_USE_CUBLASLT=1` |
| Scheme åŒ…è£…æ¶æ„ | âœ… å®Œæˆ | `CuBLASLtSchemeWrapper` |
| æƒé‡åŠ è½½å…¼å®¹ | âœ… å®Œæˆ | å§”æ‰˜ç»™åŸå§‹ scheme |
| cuBLASLt çœŸå®å®ç° | ğŸ”„ å¾…å¼€å‘ | æœ¬æ–‡æ¡£æä¾›æ¡†æ¶ |

### 6.2 å¼€å‘ä¼˜å…ˆçº§

1. **é«˜ä¼˜å…ˆçº§**ï¼šå®ç°åŸºç¡€ cuBLASLt FP8 GEMMï¼ˆper-tensor scaleï¼‰
2. **ä¸­ä¼˜å…ˆçº§**ï¼šæ”¯æŒ per-token/per-channel scaleï¼ˆOuter Vector Scalingï¼‰
3. **ä½ä¼˜å…ˆçº§**ï¼šBias èåˆã€GELU/ReLU èåˆ

### 6.3 é£é™©ä¸æŒ‘æˆ˜

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|---------|
| Layout ä¸åŒ¹é… | ç»“æœé”™è¯¯ | ä»”ç»†éªŒè¯è½¬ç½®é€»è¾‘ |
| Scale mode é™åˆ¶ | SM89 ä¸æ”¯æŒ outer vector | å›é€€åˆ° per-tensor |
| æ€§èƒ½å›é€€ | å° M åœºæ™¯å¯èƒ½æ›´æ…¢ | æ ¹æ®è§„æ¨¡åŠ¨æ€é€‰æ‹©åç«¯ |
| cuBLAS ç‰ˆæœ¬å…¼å®¹ | API å·®å¼‚ | ç‰ˆæœ¬æ£€æµ‹ + æ¡ä»¶ç¼–è¯‘ |

---

## 7. å½“å‰å¤–æŒ‚æ–¹æ³•çš„å§”æ‰˜è¯¦ç»†åˆ†æ

æœ¬ç« è¯¦ç»†è¯´æ˜å½“å‰ `CuBLASLtFp8LinearMethod` / `CuBLASLtSchemeWrapper` ä¸­å„æ­¥éª¤çš„å§”æ‰˜æƒ…å†µã€‚

### 7.1 FP8 å§”æ‰˜é“¾è·¯æ€»è§ˆ

| æ­¥éª¤ | å½“å‰çŠ¶æ€ | å¤–æŒ‚å‡½æ•° | è½¬å‘ç›®æ ‡ | vLLM å…·ä½“ä½ç½® |
|------|----------|----------|----------|---------------|
| **æƒé‡åŠ è½½** | âœ… å§”æ‰˜ | `create_weights()` | `original_scheme.create_weights()` | `compressed_tensors_w8a8_fp8.py:84-130` |
| **æƒé‡å¤„ç†** | âœ… å§”æ‰˜ | `process_weights_after_loading()` | `original_scheme.process_weights_after_loading()` | `compressed_tensors_w8a8_fp8.py:132-172` |
| **è¾“å…¥åŠ è½½** | N/A | - | ç”± PyTorch è‡ªåŠ¨å¤„ç† | - |
| **è¾“å…¥é‡åŒ–** | âœ… å§”æ‰˜ | `apply()` â†’ `_fp8_linear_op.apply()` | `Fp8LinearOp.quant_fp8()` | `w8a8_utils.py:462-467` â†’ `QuantFP8` |
| **GEMM+åé‡åŒ–** | âœ… å§”æ‰˜ | `apply()` â†’ `_fp8_linear_op.apply()` | `cutlass_w8a8_scaled_mm()` | `w8a8_utils.py:150-165` |
| **è¾“å‡ºè¿”å›** | âœ… å§”æ‰˜ | `apply()` è¿”å› | `_fp8_linear_op.apply()` è¿”å› | BF16 tensor |

### 7.2 FP8 å„æ­¥éª¤è¯¦ç»†è¯´æ˜

#### 7.2.1 æƒé‡åŠ è½½ (`create_weights`)

```
CuBLASLtFp8LinearMethod.create_weights()
    â”‚
    â””â”€â†’ self.original_scheme.create_weights()  # å®Œå…¨å§”æ‰˜
            â”‚
            â””â”€â†’ CompressedTensorsW8A8Fp8.create_weights()
                    â”‚
                    â”œâ”€â†’ create_fp8_weight_parameter()      # åˆ›å»º FP8 æƒé‡ [N, K]
                    â”œâ”€â†’ create_fp8_scale_parameter()       # åˆ›å»º weight_scale
                    â””â”€â†’ create_fp8_input_scale() (å¯é€‰)    # é™æ€é‡åŒ–æ—¶åˆ›å»º input_scale
```

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ç»™åŸå§‹ schemeï¼Œä¸åšä»»ä½•ä¿®æ”¹ã€‚
**åç»­è®¡åˆ’**ï¼šå¦‚éœ€è‡ªå®šä¹‰æƒé‡æ ¼å¼ï¼Œéœ€è¦åœ¨æ­¤å¤„ä»‹å…¥ã€‚

#### 7.2.2 æƒé‡å¤„ç† (`process_weights_after_loading`)

```
CuBLASLtFp8LinearMethod.process_weights_after_loading()
    â”‚
    â””â”€â†’ self.original_scheme.process_weights_after_loading()  # å®Œå…¨å§”æ‰˜
            â”‚
            â””â”€â†’ CompressedTensorsW8A8Fp8.process_weights_after_loading()
                    â”‚
                    â”œâ”€â†’ process_fp8_weight_tensor_strategy()   # per-tensor
                    â”œâ”€â†’ process_fp8_weight_channel_strategy()  # per-channel â† Qwen ä½¿ç”¨
                    â””â”€â†’ process_fp8_weight_block_strategy()    # block
                    â”‚
                    â””â”€â†’ weight = weight.t()  # å…³é”®ï¼šæƒé‡è½¬ç½®ä¸º [K, N]
```

**å…³é”®å¤„ç†**ï¼š
- æ ¹æ®ç­–ç•¥å¤„ç† weight_scaleï¼ˆper-tensor/per-channel/blockï¼‰
- **æƒé‡è½¬ç½®**ï¼šä» `[N, K]` è½¬ä¸º `[K, N]`
- å°† weight å’Œ weight_scale è½¬ä¸º `torch.nn.Parameter`

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ã€‚
**æ³¨æ„**ï¼šcuBLASLt éœ€è¦ç‰¹å®š layoutï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹æ­¤å¤„ã€‚

#### 7.2.3 è¾“å…¥é‡åŒ–

```
CuBLASLtFp8LinearOp.apply()
    â”‚
    â””â”€â†’ self._fp8_linear_op.apply()  # å§”æ‰˜ç»™ vLLM çš„ Fp8LinearOp
            â”‚
            â””â”€â†’ Fp8LinearOp.apply() [w8a8_utils.py:440-490]
                    â”‚
                    â””â”€â†’ self.quant_fp8(input_2d, input_scale, input_scale_ub)
                            â”‚
                            â””â”€â†’ QuantFP8.__call__() [input_quant_fp8.py]
                                    â”‚
                                    â””â”€â†’ ops.scaled_fp8_quant(input, scale)
                                            â”‚
                                            â””â”€â†’ è¿”å› (qinput, x_scale)
                                                     FP8      FP32
```

**é‡åŒ–å…¬å¼**ï¼š`qinput = input / scale`ï¼Œå…¶ä¸­ `scale = max(|input|) / fp8_max`

**Scale å½¢çŠ¶ï¼ˆQwen FP8 é…ç½®ï¼‰**ï¼š
- `x_scale`: `[M, 1]` (per-token)
- `weight_scale`: `[N, 1]` (per-channel)

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ç»™ `QuantFP8`ã€‚
**åç»­è®¡åˆ’**ï¼šå¦‚éœ€è‡ªå®šä¹‰é‡åŒ–ï¼Œåœ¨ `CuBLASLtFp8LinearOp.apply()` ä¸­ç›´æ¥è°ƒç”¨è‡ªå·±çš„é‡åŒ–å‡½æ•°ã€‚

#### 7.2.4 GEMM + åé‡åŒ–

```
Fp8LinearOp.apply() [w8a8_utils.py:480-490]
    â”‚
    â”œâ”€â†’ dispatch_w8a8_scaled_mm(preferred_backend, ...)  # é€‰æ‹©åç«¯
    â”‚       â”‚
    â”‚       â””â”€â†’ è¿”å› cutlass_w8a8_scaled_mm (CUDA + SM90)
    â”‚
    â””â”€â†’ cutlass_w8a8_scaled_mm() [w8a8_utils.py:150-165]
            â”‚
            â””â”€â†’ ops.cutlass_scaled_mm(qinput, weight, scale_a, scale_b, bias)
                    â”‚
                    â””â”€â†’ C++ è°ƒç”¨: cutlass_scaled_mm_sm90_fp8()
                            â”‚
                            â””â”€â†’ output = scale_a * (scale_b * (qinput @ weight.T)) + bias
```

**GEMM å‚æ•°**ï¼š
| å‚æ•° | å½¢çŠ¶ | è¯´æ˜ |
|------|------|------|
| `qinput` | `[M, K]` FP8 | é‡åŒ–åçš„è¾“å…¥ |
| `weight` | `[K, N]` FP8 | è½¬ç½®åçš„æƒé‡ |
| `scale_a` | `[M, 1]` FP32 | è¾“å…¥ scale (per-token) |
| `scale_b` | `[N, 1]` FP32 | æƒé‡ scale (per-channel) |
| `bias` | `[N]` BF16 | å¯é€‰åç½® |
| `output` | `[M, N]` BF16 | è¾“å‡º |

**å½“å‰çŠ¶æ€**ï¼šé€šè¿‡ `_fp8_linear_op.apply()` é—´æ¥å§”æ‰˜ç»™ cutlassã€‚
**æ›¿æ¢ç‚¹**ï¼šè¿™é‡Œæ˜¯ cuBLASLt æ›¿æ¢çš„å…³é”®ä½ç½®ã€‚

### 7.3 INT8 å§”æ‰˜é“¾è·¯æ€»è§ˆ

| æ­¥éª¤ | å½“å‰çŠ¶æ€ | å¤–æŒ‚å‡½æ•° | è½¬å‘ç›®æ ‡ | vLLM å…·ä½“ä½ç½® |
|------|----------|----------|----------|---------------|
| **æƒé‡åŠ è½½** | âŒ æœªæ”¯æŒ | - | - | `compressed_tensors_w8a8_int8.py:43-96` |
| **æƒé‡å¤„ç†** | âŒ æœªæ”¯æŒ | - | - | `cutlass.py:34-109` (kernel.process_weights) |
| **è¾“å…¥é‡åŒ–** | âŒ æœªæ”¯æŒ | - | `ops.scaled_int8_quant()` | `cutlass.py:127-129` |
| **GEMM+åé‡åŒ–** | âŒ æœªæ”¯æŒ | - | `ops.cutlass_scaled_mm()` | `cutlass.py:144-147` |

### 7.4 INT8 è¯¦ç»†è¯´æ˜

å½“å‰ `wrap_scheme_with_cublaslt()` **ä¸æ”¯æŒ INT8**ï¼Œä»…æ£€æµ‹ `W8A8Fp8`ï¼š

```python
# cublaslt_linear_method.py:287
if "W8A8Fp8" in scheme_name:
    return CuBLASLtFp8LinearMethod(original_scheme)
else:
    # INT8 ä¼šèµ°è¿™é‡Œï¼Œè¿”å›åŸå§‹ scheme
    return original_scheme
```

**INT8 çš„æ¶æ„å·®å¼‚**ï¼š

1. **Scheme ç±»**ï¼š`CompressedTensorsW8A8Int8`
2. **Kernel é€‰æ‹©**ï¼šä½¿ç”¨ `ScaledMMLinearKernel` æ¶æ„
   - `CutlassScaledMMLinearKernel` (CUDA)
   - `TorchScaledMMLinearKernel` (fallback)
3. **é‡åŒ–å‡½æ•°**ï¼š`ops.scaled_int8_quant()` vs FP8 çš„ `ops.scaled_fp8_quant()`
4. **éå¯¹ç§°æ”¯æŒ**ï¼šINT8 æ”¯æŒ asymmetric quantization (AZP)

**INT8 å…³é”®å‡½æ•°ä½ç½®**ï¼š

| å‡½æ•° | æ–‡ä»¶ | è¯´æ˜ |
|------|------|------|
| `create_weights` | `compressed_tensors_w8a8_int8.py:43-96` | åˆ›å»º INT8 æƒé‡å’Œ scale |
| `process_weights_after_loading` | `cutlass.py:34-109` | æƒé‡è½¬ç½®ã€scale å¤„ç†ã€AZP è®¡ç®— |
| `apply_weights` | `cutlass.py:115-147` | INT8 é‡åŒ– + GEMM |
| `scaled_int8_quant` | `_custom_ops` | INT8 åŠ¨æ€/é™æ€é‡åŒ– |
| `cutlass_scaled_mm_azp` | `_custom_ops` | å¸¦ AZP çš„ INT8 GEMM |

### 7.5 åç»­æ¥ç®¡è®¡åˆ’

#### FP8 æ¥ç®¡æ­¥éª¤

1. **ä¿æŒå§”æ‰˜**ï¼š`create_weights()`, `process_weights_after_loading()`
2. **è‡ªä¸»å®ç°**ï¼š
   - åœ¨ `CuBLASLtFp8LinearOp.apply()` ä¸­ï¼š
     - ç›´æ¥è°ƒç”¨è‡ªå®šä¹‰çš„ FP8 é‡åŒ–å‡½æ•°ï¼ˆæˆ–å¤ç”¨ `QuantFP8`ï¼‰
     - è°ƒç”¨ cuBLASLt FP8 GEMM kernel
     - è¿”å› BF16 è¾“å‡º

```python
# CuBLASLtFp8LinearOp._apply_cublaslt() çš„ç›®æ ‡å®ç°
def _apply_cublaslt(self, input, weight, weight_scale, ...):
    # 1. è¾“å…¥é‡åŒ–ï¼ˆå¯å¤ç”¨ QuantFP8 æˆ–è‡ªå®ç°ï¼‰
    qinput, x_scale = self.quant_fp8(input, input_scale)
    
    # 2. cuBLASLt GEMMï¼ˆæ›¿æ¢ cutlass_scaled_mmï¼‰
    output = cublaslt_fp8_gemm(
        qinput, weight, x_scale, weight_scale, bias
    )
    
    # 3. è¿”å› BF16 è¾“å‡º
    return output
```

#### INT8 æ”¯æŒè®¡åˆ’

1. åˆ›å»º `CuBLASLtInt8LinearMethod` ç±»
2. åœ¨ `wrap_scheme_with_cublaslt()` ä¸­æ·»åŠ  INT8 æ£€æµ‹
3. å®ç° INT8 ç‰ˆæœ¬çš„ `apply_weights()`

---

## 8. æµ‹è¯•æ¡†æ¶è®¾è®¡

### 8.1 æµ‹è¯•è„šæœ¬æ¶æ„

æµ‹è¯•è„šæœ¬ `test_cublaslt_00_kernel.py` è®¾è®¡ä¸ºå¯ç‹¬ç«‹è¿è¡Œï¼Œç›´æ¥æµ‹è¯• GEMM kernel çš„æ­£ç¡®æ€§å’Œæ€§èƒ½ã€‚

```
æµ‹è¯•æµç¨‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ç”Ÿæˆæµ‹è¯•æ•°æ®                                             â”‚
â”‚     - input_bf16 [M, K] - BF16 æ ¼å¼                         â”‚
â”‚     - weight_fp8 [N, K] - FP8 æ ¼å¼ï¼ˆè½¬ç½®å [K, N]ï¼‰           â”‚
â”‚     - weight_scale [N, 1] - per-channel                     â”‚
â”‚     - bias [N] - å¯é€‰                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. è°ƒç”¨è¢«æµ‹ kernelï¼ˆé€šè¿‡ CuBLASLtFp8LinearOpï¼‰               â”‚
â”‚     - å†…éƒ¨ä¼šè¿›è¡Œ BF16 â†’ FP8 é‡åŒ–ï¼ˆper-token dynamicï¼‰        â”‚
â”‚     - æ‰§è¡Œ FP8 GEMMï¼ˆå½“å‰æ˜¯ cutlassï¼Œå°†æ›¿æ¢ä¸º cuBLASLtï¼‰      â”‚
â”‚     - è¿”å› BF16 è¾“å‡º                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. è®¡ç®—å‚è€ƒç»“æœï¼ˆæ¨¡æ‹Ÿå®Œæ•´ FP8 GEMM æµç¨‹ï¼‰                    â”‚
â”‚     - è¾“å…¥é‡åŒ–ï¼šinput_bf16 â†’ input_fp8, x_scale             â”‚
â”‚     - FP8 çŸ©é˜µä¹˜ï¼šinput_fp8 @ weight_fp8_t                   â”‚
â”‚     - åé‡åŒ–ï¼šresult * x_scale * weight_scale               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. æ¯”è¾ƒç»“æœå’Œæµ‹é‡æ€§èƒ½                                       â”‚
â”‚     - æ­£ç¡®æ€§ï¼štorch.allclose(output, reference)             â”‚
â”‚     - æ€§èƒ½ï¼šæµ‹é‡ååé‡ (TFLOPS)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 æµ‹è¯•æ¥å£è®¾è®¡

ä¸ºæ”¯æŒç‹¬ç«‹æµ‹è¯•ï¼Œåœ¨ `CuBLASLtFp8LinearOp` ä¸­å·²æ·»åŠ ä¸“ç”¨æ¥å£ï¼š

```python
class CuBLASLtFp8LinearOp:
    def apply_for_test(
        self,
        input: torch.Tensor,           # [M, K] BF16ï¼Œä¼šè¢«é‡åŒ–
        weight: torch.Tensor,          # [K, N] FP8ï¼Œå·²é‡åŒ–å·²è½¬ç½®ï¼Œcolumn-major
        weight_scale: torch.Tensor,    # [N, 1] FP32
        out_dtype: torch.dtype = torch.bfloat16,
        input_scale: torch.Tensor | None = None,  # None = dynamic quant
        bias: torch.Tensor | None = None,
    ) -> torch.Tensor:
        """
        ä¸“ç”¨äºæµ‹è¯•çš„æ¥å£ï¼Œè·³è¿‡ layer å¯¹è±¡ä¾èµ–
        """
        ...
```

### 8.3 æµ‹è¯•è„šæœ¬ä½¿ç”¨

```bash
# è¿è¡Œå•ä¸ªè§„æ¨¡æµ‹è¯•
python slidesparse/test/test_cublaslt_00_kernel.py --m 256 --n 896 --k 896

# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python slidesparse/test/test_cublaslt_00_kernel.py --all
```

### 8.4 æµ‹è¯•ç»“æœï¼ˆRTX 5080, SM 12.0ï¼‰

| M | N | K | Bias | Time(ms) | TFLOPS | Status |
|---:|---:|---:|:---:|---:|---:|:---:|
| 1 | 896 | 896 | No | 0.050 | 0.03 | âœ… |
| 1 | 4864 | 896 | No | 0.049 | 0.18 | âœ… |
| 32 | 896 | 896 | No | 0.049 | 1.05 | âœ… |
| 32 | 4864 | 896 | No | 0.049 | 5.68 | âœ… |
| 128 | 4864 | 896 | No | 0.049 | 22.76 | âœ… |
| 128 | 896 | 4864 | Yes | 0.105 | 10.66 | âœ… |
| 512 | 4864 | 896 | No | 0.061 | 73.64 | âœ… |
| 1024 | 4864 | 896 | No | 0.090 | 98.87 | âœ… |
| 2048 | 2048 | 2048 | No | 0.181 | 95.14 | âœ… |
| 4096 | 4096 | 4096 | No | 1.232 | 111.52 | âœ… |

**æµ‹è¯•ç¯å¢ƒè¯´æ˜**ï¼š
- å½“å‰åº•å±‚ kernel ä¸º CUTLASS `cutlass_scaled_mm`
- æ›¿æ¢ä¸º cuBLASLt åï¼Œéœ€é‡æ–°è¿è¡Œæµ‹è¯•éªŒè¯æ­£ç¡®æ€§å’Œæ€§èƒ½å·®å¼‚
- è¯¯å·®å®¹é™ï¼šrtol=0.05, atol=0.05ï¼ˆFP8 é‡åŒ–è¯¯å·®æ­£å¸¸èŒƒå›´ï¼‰

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.2*  
*æ›´æ–°æ—¥æœŸï¼š2025-01*  
*ä½œè€…ï¼šSlideSparse Team*
