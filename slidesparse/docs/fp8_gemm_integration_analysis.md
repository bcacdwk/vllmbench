# SlideSparse Phase 3-6: FP8 GEMM é›†æˆæŠ€æœ¯åˆ†æ

> æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ vLLM ä¸­ FP8 GEMM çš„å®ç°æ¶æ„ï¼ŒåŒ…æ‹¬ compressed-tensors æ ¼å¼ã€CUTLASS/cuBLASLt/cuSPARSELt å†…æ ¸å®ç°ä»¥åŠ SlideSparse é›†æˆæ–¹æ¡ˆã€‚

---

## ç›®å½•

1. [Compressed-Tensors è½¬å‘æ¶æ„åˆ†æ](#1-compressed-tensors-è½¬å‘æ¶æ„åˆ†æ)
2. [å½“å‰ CUTLASS FP8 GEMM å®ç°è¯¦è§£](#2-å½“å‰-cutlass-fp8-gemm-å®ç°è¯¦è§£)
3. [cuBLASLt æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹](#3-cublaslt-æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹)
4. [å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹](#4-å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹)
5. [æµ‹è¯•ä¸éªŒè¯è®¡åˆ’](#5-æµ‹è¯•ä¸éªŒè¯è®¡åˆ’)
6. [æ€»ç»“](#6-æ€»ç»“)
7. [å½“å‰å¤–æŒ‚æ–¹æ³•çš„å§”æ‰˜è¯¦ç»†åˆ†æ](#7-å½“å‰å¤–æŒ‚æ–¹æ³•çš„å§”æ‰˜è¯¦ç»†åˆ†æ)
8. [æµ‹è¯•æ¡†æ¶è®¾è®¡](#8-æµ‹è¯•æ¡†æ¶è®¾è®¡)
9. [vLLM GEMM åç«¯åˆ†å‘æœºåˆ¶æ·±åº¦åˆ†æ](#9-vllm-gemm-åç«¯åˆ†å‘æœºåˆ¶æ·±åº¦åˆ†æ)
10. [æ€»ç»“ä¸ä¸‹ä¸€æ­¥](#10-æ€»ç»“ä¸ä¸‹ä¸€æ­¥)
11. [cuBLASLt é›†æˆå…³é”®é—®é¢˜åˆ†æ](#11-cublaslt-é›†æˆå…³é”®é—®é¢˜åˆ†æ)
12. [cuSPARSELt FP8 Linear æ–¹æ³•å¼€å‘è§„åˆ’](#12-cusparselt-fp8-linear-æ–¹æ³•å¼€å‘è§„åˆ’)

---

## 1. Compressed-Tensors æ¶æ„åˆ†æä¸ SlideSparse é›†æˆ

### 1.1 Compressed-Tensors æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

#### 1.1.1 æ ¸å¿ƒå®šä½ï¼šå…ƒæ ¼å¼ï¼ˆMeta-Formatï¼‰è€Œéå…·ä½“é‡åŒ–æ–¹æ³•

**Compressed-Tensors ä¸æ˜¯ä¸€ç§å…·ä½“çš„é‡åŒ–æ–¹æ³•ï¼Œè€Œæ˜¯ä¸€ä¸ªã€Œé…ç½®è§£æå±‚ã€æˆ–ã€Œé‡åŒ–åˆ†å‘æ¡†æ¶ã€ã€‚**

HuggingFace ä¸Šå¤§é‡çš„é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ RedHatã€NeuralMagicã€AMD å‘å¸ƒçš„æ¨¡å‹ï¼‰ä½¿ç”¨ `compressed-tensors` ä½œä¸ºç»Ÿä¸€çš„é‡åŒ–é…ç½®æ ¼å¼ã€‚å®ƒè§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯ï¼š

| é—®é¢˜ | Compressed-Tensors çš„è§£å†³æ–¹æ¡ˆ |
|------|------------------------------|
| é‡åŒ–æ ¼å¼ç¢ç‰‡åŒ– | ç»Ÿä¸€çš„é…ç½®æ ¼å¼ï¼Œå­˜å‚¨åœ¨ `config.json` |
| ä¸åŒå±‚ä¸åŒç²¾åº¦ | é€šè¿‡ `config_groups` + `targets` æ”¯æŒæ··åˆç²¾åº¦ |
| ç¨€ç–æ€§ä¸é‡åŒ–ç»„åˆ | åŒæ—¶æ”¯æŒ 2:4 sparsity + é‡åŒ– |
| è‡ªåŠ¨æ£€æµ‹ä¸åˆ†å‘ | æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„ Scheme |

#### 1.1.2 é…ç½®æ ¼å¼ç¤ºä¾‹

æ¨¡å‹çš„ `config.json` ä¸­çš„ `quantization_config` å­—æ®µï¼š

```json
{
  "quantization_config": {
    "quant_method": "compressed-tensors",
    "format": "float-quantized",
    "config_groups": {
      "group_0": {
        "weights": {
          "num_bits": 8,
          "type": "float",           // FP8 é‡åŒ–
          "strategy": "channel",     // per-channel æƒé‡é‡åŒ–
          "symmetric": true
        },
        "input_activations": {
          "num_bits": 8,
          "type": "float",           // FP8 æ¿€æ´»é‡åŒ–
          "strategy": "token",       // per-token åŠ¨æ€é‡åŒ–
          "dynamic": true
        },
        "targets": ["Linear"]        // åº”ç”¨äºæ‰€æœ‰ Linear å±‚
      }
    },
    "ignore": ["lm_head"]            // å¿½ç•¥æŸäº›å±‚
  }
}
```

#### 1.1.3 æ”¯æŒçš„é‡åŒ–æ–¹æ¡ˆï¼ˆSchemeï¼‰

Compressed-Tensors æ ¹æ®é…ç½®è‡ªåŠ¨åˆ†å‘åˆ°ä»¥ä¸‹å…·ä½“å®ç°ï¼š

| Scheme ç±»å | Weight | Activation | è¯´æ˜ |
|-------------|--------|------------|------|
| `CompressedTensorsW8A8Fp8` | FP8 8-bit | FP8 8-bit | FP8 å¯¹ç§°é‡åŒ– |
| `CompressedTensorsW8A8Int8` | INT8 8-bit | INT8 8-bit | INT8 å¯¹ç§°/éå¯¹ç§° |
| `CompressedTensorsW8A16Fp8` | FP8 8-bit | FP16/BF16 | ä»…æƒé‡é‡åŒ– |
| `CompressedTensorsWNA16` | INT 4/8-bit | FP16/BF16 | GPTQ-like |
| `CompressedTensors24` | - | - | 2:4 ç¨€ç– (å¯é€‰é‡åŒ–) |
| `CompressedTensorsW4A16Sparse24` | INT4 | FP16 | 2:4 ç¨€ç– + INT4 |
| `CompressedTensorsW4A4Fp4` | FP4 | FP4 | NVFP4 æ ¼å¼ |

### 1.2 vLLM å¯¹ Compressed-Tensors çš„å¤„ç†æ¡†æ¶

#### 1.2.1 ä¸¤å±‚æ¶æ„è®¾è®¡

vLLM é‡‡ç”¨**ä¸¤å±‚æ¶æ„**å¤„ç† compressed-tensorsï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç¬¬ä¸€å±‚: Config + LinearMethodï¼ˆèƒ¶æ°´å±‚ï¼‰                    â”‚
â”‚                      è´Ÿè´£é…ç½®è§£æå’Œè°ƒç”¨å§”æ‰˜ï¼Œä¸æ¶‰åŠè®¡ç®—                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  CompressedTensorsConfig (é…ç½®å­˜å‚¨ä¸åˆ†å‘)                                     â”‚
â”‚    â”œâ”€â”€ from_config()          : è§£æ config.json â†’ target_scheme_map        â”‚
â”‚    â”œâ”€â”€ get_quant_method()     : è¿”å› CompressedTensorsLinearMethod          â”‚
â”‚    â”œâ”€â”€ get_scheme()           : â˜… æ ¸å¿ƒåˆ†å‘é€»è¾‘ï¼Œé€‰æ‹©å…·ä½“ Scheme              â”‚
â”‚    â””â”€â”€ _get_scheme_from_parts(): æ ¹æ® weight_quant/input_quant åˆ›å»º Scheme  â”‚
â”‚                                                                              â”‚
â”‚  CompressedTensorsLinearMethod (èƒ¶æ°´å±‚ï¼Œå®ç° LinearMethodBase æ¥å£)           â”‚
â”‚    â”œâ”€â”€ create_weights()                â†’ å§”æ‰˜ layer.scheme.create_weights() â”‚
â”‚    â”œâ”€â”€ process_weights_after_loading() â†’ å§”æ‰˜ layer.scheme.process_...()    â”‚
â”‚    â””â”€â”€ apply()                         â†’ å§”æ‰˜ layer.scheme.apply_weights()  â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ æ‰€æœ‰æ–¹æ³•éƒ½å§”æ‰˜ç»™ scheme
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç¬¬äºŒå±‚: Schemeï¼ˆå…·ä½“é‡åŒ–å®ç°ï¼‰                              â”‚
â”‚                     è´Ÿè´£æƒé‡ç®¡ç†å’Œå®é™…è®¡ç®—ï¼ŒåŒ…å« kernel é€‰æ‹©                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  CompressedTensorsScheme (æŠ½è±¡åŸºç±»)                                           â”‚
â”‚    â”œâ”€â”€ get_min_capability()          : GPU æœ€ä½ç®—åŠ›è¦æ±‚                       â”‚
â”‚    â”œâ”€â”€ create_weights()              : åˆ›å»ºé‡åŒ–å‚æ•° (weight, scale, zp ç­‰)   â”‚
â”‚    â”œâ”€â”€ process_weights_after_loading(): æƒé‡åå¤„ç† (è½¬ç½®ã€é‡æ’ã€æ‰“åŒ…ç­‰)       â”‚
â”‚    â””â”€â”€ apply_weights()               : â˜… æ‰§è¡Œçº¿æ€§è®¡ç®— (å«é‡åŒ–/åé‡åŒ–)         â”‚
â”‚                                                                              â”‚
â”‚  å…·ä½“å®ç°ç¤ºä¾‹ - CompressedTensorsW8A8Fp8:                                     â”‚
â”‚    â”œâ”€â”€ __init__()              : åˆå§‹åŒ– Fp8LinearOp (å°è£… CUTLASS kernel)     â”‚
â”‚    â”œâ”€â”€ create_weights()        : åˆ›å»º weight(FP8), weight_scale, input_scale â”‚
â”‚    â”œâ”€â”€ process_weights_after_loading(): å¤„ç† scale åˆå¹¶ã€weight è½¬ç½®          â”‚
â”‚    â””â”€â”€ apply_weights()         : è°ƒç”¨ self.fp8_linear.apply() æ‰§è¡Œ GEMM      â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.2.2 åè¯è§£é‡Šä¸åŒºåˆ†

| åè¯ | å±‚çº§ | èŒè´£ | ç¤ºä¾‹ |
|------|------|------|------|
| **Config** | ç¬¬ä¸€å±‚ | é…ç½®è§£æã€å­˜å‚¨ã€åˆ†å‘ | `CompressedTensorsConfig` |
| **LinearMethod** | ç¬¬ä¸€å±‚ | vLLM æ¥å£é€‚é…ï¼Œçº¯å§”æ‰˜ | `CompressedTensorsLinearMethod` |
| **Scheme** | ç¬¬äºŒå±‚ | å…·ä½“é‡åŒ–æ–¹æ¡ˆå®ç° | `CompressedTensorsW8A8Fp8` |
| **LinearOp** | ç¬¬äºŒå±‚å†…éƒ¨ | Kernel è°ƒç”¨å°è£… | `Fp8LinearOp`, `W8A8BlockFp8LinearOp` |

#### 1.2.3 å…³é”®æ–¹æ³•çš„ä½œç”¨

**`create_weights()` çš„ä½œç”¨**ï¼š
- åœ¨æ¨¡å‹åˆå§‹åŒ–é˜¶æ®µè°ƒç”¨
- åˆ›å»ºè¯¥é‡åŒ–æ–¹æ¡ˆéœ€è¦çš„æ‰€æœ‰å‚æ•°ï¼ˆweight, scale, zero_point ç­‰ï¼‰
- å°†å‚æ•°æ³¨å†Œåˆ° layer ä¸Šï¼Œä¾›åç»­ weight_loader åŠ è½½

```python
# CompressedTensorsW8A8Fp8.create_weights() ç¤ºä¾‹
def create_weights(self, layer, ...):
    # åˆ›å»º FP8 æƒé‡
    weight = create_fp8_weight_parameter(output_size, input_size, weight_loader)
    layer.register_parameter("weight", weight)
    
    # åˆ›å»ºæƒé‡ scale
    weight_scale = create_fp8_scale_parameter(...)
    layer.register_parameter("weight_scale", weight_scale)
    
    # åˆ›å»ºè¾“å…¥ scaleï¼ˆé™æ€é‡åŒ–æ—¶ï¼‰
    if self.is_static_input_scheme:
        input_scale = create_fp8_input_scale(...)
        layer.register_parameter("input_scale", input_scale)
```

**`process_weights_after_loading()` çš„ä½œç”¨**ï¼š
- åœ¨æƒé‡åŠ è½½å®Œæˆåè°ƒç”¨
- æ‰§è¡Œæƒé‡åå¤„ç†ï¼šè½¬ç½®ã€scale åˆå¹¶ã€æ ¼å¼è½¬æ¢ç­‰
- ä¸ºæ¨ç†é˜¶æ®µåšå‡†å¤‡

```python
# CompressedTensorsW8A8Fp8.process_weights_after_loading() ç¤ºä¾‹
def process_weights_after_loading(self, layer):
    # å¤„ç†æƒé‡å’Œ scale
    weight, weight_scale, input_scale = process_fp8_weight_tensor_strategy(...)
    
    # è½¬ç½®æƒé‡ [out, in] -> [in, out].T
    weight = weight.t()
    
    # æ›´æ–°ä¸º Parameter
    layer.weight = Parameter(weight.data, requires_grad=False)
    layer.weight_scale = Parameter(weight_scale.data, requires_grad=False)
```

**`apply_weights()` çš„ä½œç”¨**ï¼š
- åœ¨æ¨ç†é˜¶æ®µçš„æ¯æ¬¡å‰å‘ä¼ æ’­è°ƒç”¨
- æ‰§è¡Œå®é™…çš„çº¿æ€§è®¡ç®—ï¼ˆå«é‡åŒ–/åé‡åŒ–ï¼‰
- å†…éƒ¨è°ƒç”¨ LinearOp é€‰æ‹©å…·ä½“çš„ kernel

```python
# CompressedTensorsW8A8Fp8.apply_weights() ç¤ºä¾‹
def apply_weights(self, layer, x, bias=None):
    # é€šè¿‡ Fp8LinearOp è°ƒç”¨ CUTLASS kernel
    return self.fp8_linear.apply(
        input=x,
        weight=layer.weight,
        weight_scale=layer.weight_scale,
        input_scale=layer.input_scale,
        bias=bias,
    )
```

#### 1.2.4 åˆ†å‘é€»è¾‘è¯¦è§£

`CompressedTensorsConfig.get_scheme()` æ˜¯æ ¸å¿ƒåˆ†å‘é€»è¾‘ï¼š

```python
def get_scheme(self, layer, layer_name):
    # 1. ä»é…ç½®ä¸­æå–é‡åŒ–å‚æ•°
    scheme_dict = self.get_scheme_dict(layer, layer_name)
    weight_quant = scheme_dict.get("weights")        # QuantizationArgs
    input_quant = scheme_dict.get("input_activations")  # QuantizationArgs | None
    
    # 2. æ£€æŸ¥æ˜¯å¦æ”¯æŒ 2:4 ç¨€ç–
    if self.supports_cutlass_24(weight_quant, input_quant, sparsity_scheme):
        return CompressedTensors24(...)
    
    # 3. æ ¹æ® weight_quant å’Œ input_quant é€‰æ‹© Scheme
    return self._get_scheme_from_parts(weight_quant, input_quant, format)
```

`_get_scheme_from_parts()` çš„åˆ†å‘è§„åˆ™ï¼š

```python
def _get_scheme_from_parts(self, weight_quant, input_quant, format):
    # FP8 W8A8
    if self._is_fp8_w8a8(weight_quant, input_quant):
        return CompressedTensorsW8A8Fp8(weight_quant, is_static=...)
    
    # INT8 W8A8
    if self._is_static_tensor_w8a8(weight_quant, input_quant):
        return CompressedTensorsW8A8Int8(strategy=..., is_static=True)
    
    if self._is_dynamic_token_w8a8(weight_quant, input_quant):
        return CompressedTensorsW8A8Int8(strategy=..., is_static=False)
    
    # WNA16 (ä»…æƒé‡é‡åŒ–)
    if self._is_wNa16_group_channel(weight_quant, input_quant):
        return CompressedTensorsWNA16(num_bits=..., group_size=...)
    
    # ... å…¶ä»–æ–¹æ¡ˆ
```

### 1.3 SlideSparse çš„ Hook æ–¹å¼

#### 1.3.1 Hook ç‚¹ä½ç½®

SlideSparse åœ¨ `get_scheme()` è¿”å›å‰è¿›è¡Œ hookï¼š

```python
# compressed_tensors.py ä¸­çš„ get_scheme() æœ«å°¾
def get_scheme(self, layer, layer_name):
    # ... é€‰æ‹© scheme çš„é€»è¾‘ ...
    scheme = self._get_scheme_from_parts(weight_quant, input_quant, format)
    
    # â˜… SlideSparse Hook ç‚¹
    if is_slidesparse_enabled() and scheme is not None:
        scheme = wrap_scheme_fp8(scheme)  # åŒ…è£…åŸå§‹ scheme
    
    return scheme
```

#### 1.3.2 wrap_scheme_fp8 çš„å·¥ä½œåŸç†

```python
def wrap_scheme_fp8(original_scheme):
    """
    åŒ…è£… W8A8Fp8 schemeï¼Œæ›¿æ¢ apply_weights ä¸º SlideSparse kernel
    """
    scheme_name = type(original_scheme).__name__
    
    # åªå¤„ç† W8A8Fp8
    if "W8A8Fp8" not in scheme_name:
        return original_scheme  # å…¶ä»– scheme ä¸å¤„ç†
    
    # è¿”å›åŒ…è£…åçš„ Method
    return SlideSparseFp8LinearMethod(original_scheme)
```

#### 1.3.3 SlideSparseFp8LinearMethod çš„å§”æ‰˜æ¨¡å¼

```python
class SlideSparseFp8LinearMethod:
    def __init__(self, original_scheme):
        self.original_scheme = original_scheme
        # åˆ›å»º SlideSparse Opï¼ˆæ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹© kernelï¼‰
        self.slidesparse_fp8_linear = SlideSparseFp8LinearOp(...)
    
    def create_weights(self, layer, ...):
        # â˜… å®Œå…¨å§”æ‰˜ç»™åŸå§‹ schemeï¼ˆä¿æŒæƒé‡æ ¼å¼å…¼å®¹ï¼‰
        return self.original_scheme.create_weights(layer, ...)
    
    def process_weights_after_loading(self, layer):
        # â˜… å®Œå…¨å§”æ‰˜ç»™åŸå§‹ scheme
        return self.original_scheme.process_weights_after_loading(layer)
    
    def apply_weights(self, layer, x, bias=None):
        # â˜… æ›¿æ¢ä¸º SlideSparse kernel
        return self.slidesparse_fp8_linear.apply(
            input=x,
            weight=layer.weight,
            weight_scale=layer.weight_scale,
            input_scale=layer.input_scale,
            bias=bias,
        )
```

#### 1.3.4 å®Œæ•´çš„è°ƒç”¨æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·åŠ è½½ FP8 é‡åŒ–æ¨¡å‹                                â”‚
â”‚                   vllm serve model --quantization compressed-tensors        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CompressedTensorsConfig.get_scheme()                     â”‚
â”‚                                                                             â”‚
â”‚   1. è§£æ config.json â†’ weight_quant: FP8/channel, input_quant: FP8/token  â”‚
â”‚   2. _get_scheme_from_parts() â†’ CompressedTensorsW8A8Fp8                   â”‚
â”‚   3. â˜… if is_slidesparse_enabled(): wrap_scheme_fp8(scheme)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ SlideSparse ç¦ç”¨                       â”‚ SlideSparse å¯ç”¨
                   â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CompressedTensorsW8A8Fp8    â”‚    â”‚        SlideSparseFp8LinearMethod        â”‚
â”‚                              â”‚    â”‚                                          â”‚
â”‚  apply_weights() â†’           â”‚    â”‚  create_weights()      â†’ å§”æ‰˜åŸå§‹ scheme â”‚
â”‚    Fp8LinearOp.apply()       â”‚    â”‚  process_weights_...() â†’ å§”æ‰˜åŸå§‹ scheme â”‚
â”‚      â†’ CUTLASS kernel        â”‚    â”‚  apply_weights()       â†’ SlideSparse Op  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚              SlideSparseFp8LinearOp.apply()             â”‚
                   â”‚                                                         â”‚
                   â”‚  æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹© kernel:                                â”‚
                   â”‚  â”œâ”€ USE_CUBLASLT=1   â†’ cuBLASLt_FP8_linear()           â”‚
                   â”‚  â”œâ”€ USE_CUSPARSELT=1 â†’ cuSPARSELt_FP8_linear()         â”‚
                   â”‚  â””â”€ é»˜è®¤              â†’ cutlass_FP8_linear() (fallback)â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.4 ç¯å¢ƒå˜é‡æ§åˆ¶

| ç¯å¢ƒå˜é‡ | é»˜è®¤å€¼ | ä½œç”¨ |
|---------|--------|------|
| `DISABLE_SLIDESPARSE=1` | æœªè®¾ç½® | å®Œå…¨ç¦ç”¨ SlideSparseï¼Œä½¿ç”¨ vLLM åŸç”Ÿè·¯å¾„ |
| `USE_CUBLASLT=1` | æœªè®¾ç½® | å¯ç”¨ cuBLASLt kernel |
| `USE_CUSPARSELT=1` | æœªè®¾ç½® | å¯ç”¨ cuSPARSELt 2:4 ç¨€ç– kernel |
| `INNER_DTYPE_FP32=1` | æœªè®¾ç½® | GEMM è¾“å‡ºä½¿ç”¨ FP32ï¼ˆä»… cuBLASLt/cuSPARSELtï¼‰ |

### 1.5 æ¶æ„éªŒè¯çŠ¶æ€

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| ç¯å¢ƒå˜é‡æ£€æµ‹ | âœ… | `is_slidesparse_enabled()` æ­£å¸¸å·¥ä½œ |
| Scheme åŒ…è£… | âœ… | `wrap_scheme_fp8()` åœ¨ `get_scheme()` ä¸­æ­£ç¡®è°ƒç”¨ |
| æƒé‡åˆ›å»º | âœ… | å§”æ‰˜ç»™åŸå§‹ schemeï¼Œä¿æŒ safetensor æ ¼å¼å…¼å®¹ |
| æƒé‡åŠ è½½ | âœ… | å§”æ‰˜ç»™åŸå§‹ schemeï¼Œscale/weight æ­£å¸¸åŠ è½½ |
| æ¨ç†è·¯å¾„ | âœ… | `apply_weights()` æ­£ç¡®è°ƒç”¨ `SlideSparseFp8LinearOp` |
| Kernel é€‰æ‹© | âœ… | æ ¹æ®ç¯å¢ƒå˜é‡æ­£ç¡®é€‰æ‹© cuBLASLt/cuSPARSELt/CUTLASS |

---

## 2. å½“å‰ CUTLASS FP8 GEMM å®ç°è¯¦è§£

### 2.1 å®Œæ•´è°ƒç”¨é“¾

```
CompressedTensorsW8A8Fp8.apply_weights()
    â”‚
    â”œâ”€â†’ QuantFP8.apply()  [è¾“å…¥é‡åŒ–ï¼Œå¯é€‰]
    â”‚       â””â”€â†’ per-token / per-tensor é‡åŒ–
    â”‚
    â””â”€â†’ Fp8LinearOp.apply()
            â”‚
            â”œâ”€â†’ cutlass_w8a8_scaled_mm()  [CUDA 12.0+, SM90+]
            â”‚       â””â”€â†’ ops.cutlass_scaled_mm()
            â”‚               â””â”€â†’ cutlass_scaled_mm_sm90_fp8()
            â”‚
            â”œâ”€â†’ ops.scaled_fp8_quant()  [Flash-attention è·¯å¾„]
            â”‚
            â””â”€â†’ torch._scaled_mm()  [Fallback]
```

### 2.2 è¾“å…¥é‡åŒ–è¿‡ç¨‹ï¼ˆQuantFP8ï¼‰

```python
# vllm/model_executor/layers/quantization/input_quant_fp8.py
class QuantFP8(CustomOp):
    """FP8 è¾“å…¥é‡åŒ–ï¼Œæ”¯æŒä¸‰ç§ç­–ç•¥"""
    
    # é‡åŒ–å…¬å¼: x_fp8 = x / scale
    # scale è®¡ç®—: scale = max(|x|) / fp8_max
    
    def __init__(self, quant_config):
        self.strategy = quant_config.input_strategy
        # "tensor" - æ•´ä¸ª tensor å…±äº«ä¸€ä¸ª scale
        # "token"  - æ¯è¡Œï¼ˆtokenï¼‰ä¸€ä¸ª scale  
        # "group"  - æ¯ group_size ä¸ªå…ƒç´ ä¸€ä¸ª scale
```

**é‡åŒ–ç­–ç•¥è¯´æ˜ï¼š**

| ç­–ç•¥ | scale å½¢çŠ¶ | è¯´æ˜ |
|------|-----------|------|
| `per-tensor` | `[1]` | æ•´ä¸ªè¾“å…¥å…±äº«ä¸€ä¸ª scaleï¼Œç²¾åº¦æœ€ä½ä½†æœ€å¿« |
| `per-token` | `[M, 1]` | æ¯è¡Œä¸€ä¸ª scaleï¼ŒLLM æ¨ç†çš„å…¸å‹é…ç½® |
| `per-channel` | `[1, K]` | æ¯åˆ—ä¸€ä¸ª scaleï¼Œæƒé‡é‡åŒ–å¸¸ç”¨ |

### 2.3 GEMM Layout è®¾è®¡

**vLLM CUTLASS FP8 GEMM Layout:**

```
é—®é¢˜å®šä¹‰: C[M,N] = A[M,K] Ã— B[K,N]

å®é™…å­˜å‚¨ï¼ˆCUTLASS å†…éƒ¨ï¼‰:
    A: RowMajor    [M, K]  - æ¯è¡Œè¿ç»­å­˜å‚¨
    B: ColumnMajor [K, N]  - ç­‰ä»·äº [N, K]^Tï¼Œæ¯åˆ—è¿ç»­å­˜å‚¨  
    C: RowMajor    [M, N]
    D: RowMajor    [M, N]

åœ¨ vLLM ä¸­:
    input (x):   [batch, hidden_dim] = [M, K]  RowMajor
    weight (w):  [out_features, in_features] = [N, K]  å®é™…å­˜å‚¨
                 ä¼ ç»™ CUTLASS æ—¶ä½œä¸º ColumnMajor [K, N]
    output:      [batch, out_features] = [M, N]  RowMajor
```

**swap_ab æœºåˆ¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ï¼š**

```cpp
// csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh

// å½“ M å¾ˆå°æ—¶ï¼ˆdecode é˜¶æ®µï¼‰ï¼Œäº¤æ¢ A å’Œ B ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
template <bool swap_ab = false>
void cutlass_scaled_mm_sm90_fp8_dispatch(...) {
    // swap_ab=true æ—¶:
    //   å®é™…è®¡ç®—: D^T = B^T Ã— A^T
    //   ç­‰ä»·äº:   D   = A Ã— B
    //   ä½†åˆ©ç”¨äº† B çš„æ›´å¥½çš„å†…å­˜è®¿é—®æ¨¡å¼
}

// é€‰æ‹©é€»è¾‘
if (M <= 64) {
    // å° M åœºæ™¯ï¼ˆdecodeï¼‰ï¼Œä½¿ç”¨ swap_ab
    cutlass_scaled_mm_sm90_fp8_dispatch<true>(...);
} else {
    // å¤§ M åœºæ™¯ï¼ˆprefillï¼‰ï¼Œä¸ä½¿ç”¨ swap_ab
    cutlass_scaled_mm_sm90_fp8_dispatch<false>(...);
}
```

### 2.4 Epilogueï¼ˆèåˆåå¤„ç†ï¼‰è¯¦è§£

**CUTLASS 3.x Epilogue è®¡ç®—å…¬å¼ï¼š**

```
D = scale_a * (scale_b * Accumulator) + bias

å…·ä½“å±•å¼€ï¼ˆScaledEpilogueBiasï¼‰:
    Compute0: tmp = scale_b * Accum      (é€å…ƒç´ æˆ–é€è¡Œ)
    Compute1: D = scale_a * tmp + bias   (é€å…ƒç´ æˆ–é€åˆ—)
```

**Epilogue ç±»å‹å®šä¹‰ï¼š**

```cpp
// csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp

// åŸºç¡€ Epilogueï¼ˆæ—  biasï¼‰
struct ScaledEpilogue {
    // scale_a: ColOrScalarLoad - æ¯åˆ—ä¸€ä¸ª scale æˆ–å…¨å±€ scalar
    // scale_b: RowOrScalarLoad - æ¯è¡Œä¸€ä¸ª scale æˆ–å…¨å±€ scalar
    
    using EVTCompute = Sm90EVT<
        Compute1,           // D = scale_a * tmp
        ScaleA,             // ColOrScalarLoad
        Sm90EVT<
            Compute0,       // tmp = scale_b * Accum
            ScaleB,         // RowOrScalarLoad
            Accum           // ç´¯åŠ å™¨è¾“å‡º
        >
    >;
};

// å¸¦ Bias çš„ Epilogue
struct ScaledEpilogueBias {
    // bias: RowLoad - æ¯è¡Œä¸€ä¸ª biasï¼ˆå¹¿æ’­åˆ°æ‰€æœ‰åˆ—ï¼‰
    
    using EVTCompute = Sm90EVT<
        Compute1,           // D = scale_a * tmp + bias
        ScaleA,             // ColOrScalarLoad
        Sm90EVT<
            Compute0,       // tmp = scale_b * Accum
            ScaleB,         // RowOrScalarLoad
            Accum
        >,
        Bias                // RowLoad
    >;
};

// swap_ab åœºæ™¯çš„ Biasï¼ˆåˆ—å¹¿æ’­ï¼‰
struct ScaledEpilogueColumnBias {
    // å½“ swap_ab=true æ—¶ï¼Œbias éœ€è¦åˆ—æ–¹å‘åŠ è½½
    using Bias = ColLoad<float>;
};
```

**Scale åŠ è½½æ¨¡å¼ï¼š**

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| `ScalarLoad` | å…¨å±€å•ä¸€ scale | per-tensor é‡åŒ– |
| `RowLoad` | æ¯è¡Œä¸€ä¸ª scale | per-token (activation) |
| `ColLoad` | æ¯åˆ—ä¸€ä¸ª scale | per-channel (weight) |
| `RowOrScalarLoad` | è¿è¡Œæ—¶é€‰æ‹© | å…¼å®¹ä¸¤ç§æ¨¡å¼ |
| `ColOrScalarLoad` | è¿è¡Œæ—¶é€‰æ‹© | å…¼å®¹ä¸¤ç§æ¨¡å¼ |

### 2.5 Kernel é€‰æ‹©ç­–ç•¥

```cpp
// csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh

template <typename OutType, bool swap_ab, bool with_bias>
void cutlass_gemm_sm90_fp8_dispatch(int M, int N, int K, ...) {
    
    // æ ¹æ®é—®é¢˜è§„æ¨¡é€‰æ‹©æœ€ä¼˜ kernel é…ç½®
    if (M <= 16) {
        // æå° Mï¼šä½¿ç”¨ M16N128K128 tile
        using TileShape = Shape<_16, _128, _128>;
        using ClusterShape = Shape<_1, _2, _1>;
        
    } else if (M <= 64) {
        // å° Mï¼šä½¿ç”¨ M64N128K128 tile
        using TileShape = Shape<_64, _128, _128>;
        using ClusterShape = Shape<_1, _2, _1>;
        
    } else if (M <= 128) {
        // ä¸­ç­‰ Mï¼šä½¿ç”¨ M128N128K128 tile
        using TileShape = Shape<_128, _128, _128>;
        using ClusterShape = Shape<_1, _1, _1>;
        
    } else if (N >= 8192) {
        // å¤§ Nï¼ˆå®½çŸ©é˜µï¼‰ï¼šä½¿ç”¨ä¸“é—¨é…ç½®
        using TileShape = Shape<_128, _256, _64>;
        using ClusterShape = Shape<_2, _1, _1>;
        
    } else {
        // é»˜è®¤é…ç½®
        using TileShape = Shape<_128, _128, _128>;
        using ClusterShape = Shape<_2, _1, _1>;
    }
}
```

---

## 3. cuBLASLt æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹

### 3.1 cuBLASLt FP8 GEMM æ ¸å¿ƒæ¦‚å¿µ

**åŸºæœ¬è®¡ç®—å…¬å¼ï¼ˆTensorwide Scalingï¼‰ï¼š**

```
D = scaleD * (Î± * scaleA * scaleB * op(A) Ã— op(B) + Î² * scaleC * C)
```

**vLLM åœºæ™¯ç®€åŒ–ï¼ˆæ—  scaleC/scaleDï¼ŒÎ²=0ï¼‰ï¼š**

```
D = Î± * scaleA * scaleB * op(A) Ã— op(B) + bias
```

### 3.2 Layout å¯¹åº”å…³ç³»

| vLLM/CUTLASS | cuBLASLt | è¯´æ˜ |
|--------------|----------|------|
| A: RowMajor | A: ColumnMajor + CUBLAS_OP_T | è½¬ç½®åç­‰ä»· |
| B: ColumnMajor | B: ColumnMajor + CUBLAS_OP_N | ç›´æ¥å¯¹åº” |
| C/D: RowMajor | C/D: ColumnMajor + è½¬ç½® | éœ€è¦é¢å¤–å¤„ç† |

**æ¨èåšæ³•ï¼šä½¿ç”¨ TN æ ¼å¼**

```cpp
// cuBLASLt æœ€ä¼˜é…ç½®ï¼ˆAda/Hopperï¼‰
CUBLAS_OP_T  // A è½¬ç½®
CUBLAS_OP_N  // B ä¸è½¬ç½®
```

### 3.3 Scale å¤„ç†æ–¹å¼å¯¹æ¯”

**CUTLASS æ–¹å¼ï¼š**
- `scale_a`ï¼šper-tokenï¼ˆåˆ—å‘é‡ï¼‰æˆ– scalar
- `scale_b`ï¼šper-channelï¼ˆè¡Œå‘é‡ï¼‰æˆ– scalar
- èåˆåœ¨ Epilogue ä¸­è®¡ç®—

**cuBLASLt æ–¹å¼ï¼š**
- `CUBLASLT_MATMUL_DESC_A_SCALE_POINTER`ï¼šæŒ‡å‘ scaleA
- `CUBLASLT_MATMUL_DESC_B_SCALE_POINTER`ï¼šæŒ‡å‘ scaleB
- æ”¯æŒä¸¤ç§ Scale Modeï¼š
  - `CUBLASLT_MATMUL_MATRIX_SCALE_SCALAR_32F`ï¼šper-tensorï¼ˆé»˜è®¤ï¼‰
  - `CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F`ï¼šper-row/colï¼ˆSM90+ï¼‰

### 3.4 Bias å¤„ç†

**cuBLASLt Epilogue é€‰é¡¹ï¼š**

```cpp
// è®¾ç½® epilogue ç±»å‹
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_EPILOGUE,
    &epilogue,  // CUBLASLT_EPILOGUE_BIAS
    sizeof(epilogue)
);

// è®¾ç½® bias æŒ‡é’ˆ
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_BIAS_POINTER,
    &bias_ptr,
    sizeof(bias_ptr)
);

// è®¾ç½® bias æ•°æ®ç±»å‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸è¾“å‡ºç›¸åŒï¼‰
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE,
    &bias_type,  // CUDA_R_32F for FP8 kernels
    sizeof(bias_type)
);
```

**é‡è¦é™åˆ¶ï¼š**
- Bias å‘é‡é•¿åº¦å¿…é¡»ç­‰äºè¾“å‡ºçŸ©é˜µè¡Œæ•°ï¼ˆMï¼‰
- Bias è¢«å¹¿æ’­åˆ°æ‰€æœ‰åˆ—
- FP8 kernel çš„ bias ç±»å‹é€šå¸¸æ˜¯ `CUDA_R_16BF` æˆ– `CUDA_R_32F`

### 3.5 å®Œæ•´ cuBLASLt FP8 GEMM å®ç°æ¡†æ¶

```cpp
// ä¼ªä»£ç ç¤ºä¾‹
cublasStatus_t cublaslt_fp8_gemm(
    int M, int N, int K,
    const void* A,        // FP8 input [M, K]
    const void* B,        // FP8 weight [K, N] (stored as [N, K]^T)
    void* D,              // Output [M, N]
    const float* scale_a, // per-token scale [M] or scalar
    const float* scale_b, // per-channel scale [N] or scalar
    const float* bias,    // optional bias [M]
    bool is_scale_a_scalar,
    bool is_scale_b_scalar,
    cudaStream_t stream
) {
    cublasLtHandle_t handle;
    cublasLtCreate(&handle);
    
    // 1. åˆ›å»ºçŸ©é˜µä¹˜æ³•æè¿°ç¬¦
    cublasLtMatmulDesc_t matmulDesc;
    cublasComputeType_t computeType = CUBLAS_COMPUTE_32F;
    cudaDataType_t scaleType = CUDA_R_32F;
    cublasLtMatmulDescCreate(&matmulDesc, computeType, scaleType);
    
    // 2. è®¾ç½®è½¬ç½®æ“ä½œï¼ˆTN æ ¼å¼ï¼‰
    cublasOperation_t opA = CUBLAS_OP_T;
    cublasOperation_t opB = CUBLAS_OP_N;
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSA, &opA, sizeof(opA));
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSB, &opB, sizeof(opB));
    
    // 3. è®¾ç½® Scale æŒ‡é’ˆ
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &scale_a, sizeof(scale_a));
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &scale_b, sizeof(scale_b));
    
    // 4. è®¾ç½® Scale Modeï¼ˆper-tensor vs per-row/colï¼‰
    if (!is_scale_a_scalar || !is_scale_b_scalar) {
        // Outer vector scalingï¼ˆSM90+ onlyï¼‰
        int32_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F;
        if (!is_scale_a_scalar) {
            cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode));
        }
        if (!is_scale_b_scalar) {
            cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode));
        }
    }
    
    // 5. è®¾ç½® Biasï¼ˆå¦‚æœæœ‰ï¼‰
    if (bias != nullptr) {
        cublasLtEpilogue_t epilogue = CUBLASLT_EPILOGUE_BIAS;
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_EPILOGUE, &epilogue, sizeof(epilogue));
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_BIAS_POINTER, &bias, sizeof(bias));
        
        cudaDataType_t biasType = CUDA_R_32F;  // æˆ– CUDA_R_16BF
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE, &biasType, sizeof(biasType));
    }
    
    // 6. åˆ›å»ºçŸ©é˜µå¸ƒå±€
    cublasLtMatrixLayout_t Adesc, Bdesc, Ddesc;
    
    // A: [K, M] ColumnMajor (å› ä¸º opA=T, å®é™…è¯»å– [M, K] RowMajor)
    cublasLtMatrixLayoutCreate(&Adesc, CUDA_R_8F_E4M3, K, M, K);
    
    // B: [K, N] ColumnMajor
    cublasLtMatrixLayoutCreate(&Bdesc, CUDA_R_8F_E4M3, K, N, K);
    
    // D: [M, N] (éœ€è¦æ ¹æ®è¾“å‡ºç±»å‹è®¾ç½®)
    cublasLtMatrixLayoutCreate(&Ddesc, CUDA_R_16BF, M, N, M);
    
    // 7. è·å–ç®—æ³•å¯å‘å¼
    cublasLtMatmulPreference_t preference;
    cublasLtMatmulPreferenceCreate(&preference);
    
    size_t workspaceSize = 64 * 1024 * 1024;  // 64 MB
    cublasLtMatmulPreferenceSetAttribute(preference, CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES, &workspaceSize, sizeof(workspaceSize));
    
    cublasLtMatmulHeuristicResult_t heuristicResult;
    int returnedResults = 0;
    cublasLtMatmulAlgoGetHeuristic(handle, matmulDesc, Adesc, Bdesc, Ddesc, Ddesc, preference, 1, &heuristicResult, &returnedResults);
    
    // 8. æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
    float alpha = 1.0f;
    float beta = 0.0f;
    void* workspace = nullptr;
    cudaMalloc(&workspace, heuristicResult.workspaceSize);
    
    cublasLtMatmul(
        handle, matmulDesc,
        &alpha,
        A, Adesc,
        B, Bdesc,
        &beta,
        D, Ddesc,  // C = D for in-place
        D, Ddesc,
        &heuristicResult.algo,
        workspace, heuristicResult.workspaceSize,
        stream
    );
    
    // æ¸…ç†
    cudaFree(workspace);
    cublasLtMatmulPreferenceDestroy(preference);
    cublasLtMatrixLayoutDestroy(Adesc);
    cublasLtMatrixLayoutDestroy(Bdesc);
    cublasLtMatrixLayoutDestroy(Ddesc);
    cublasLtMatmulDescDestroy(matmulDesc);
    cublasLtDestroy(handle);
    
    return CUBLAS_STATUS_SUCCESS;
}
```

### 3.6 å…³é”®æ³¨æ„äº‹é¡¹

#### 3.6.1 æ•°æ®ç±»å‹æ”¯æŒ

| Atype | Btype | Ctype | Dtype | æ”¯æŒ |
|-------|-------|-------|-------|------|
| E4M3 | E4M3 | BF16 | BF16 | âœ… |
| E4M3 | E4M3 | FP16 | FP16 | âœ… |
| E4M3 | E4M3 | FP32 | FP32 | âœ… |
| E5M2 | E4M3 | BF16 | BF16 | âœ… |
| E4M3 | E5M2 | BF16 | BF16 | âœ… |

#### 3.6.2 Scale Mode é™åˆ¶ï¼ˆOuter Vector Scalingï¼‰

```cpp
// SM90 (Hopper) ç‹¬æœ‰åŠŸèƒ½
CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F

// é™åˆ¶:
// 1. ä»…æ”¯æŒ SM90+
// 2. scaleD ä¸æ”¯æŒï¼ˆè¾“å‡ºå¿…é¡»æ˜¯ FP16/BF16/FP32ï¼‰
// 3. scaleA é•¿åº¦ = M, scaleB é•¿åº¦ = N
```

#### 3.6.3 å¯¹é½è¦æ±‚

- æ‰€æœ‰çŸ©é˜µæŒ‡é’ˆå¿…é¡» 16 å­—èŠ‚å¯¹é½
- ç»´åº¦ M, K æœ€å¥½æ˜¯ 16 çš„å€æ•°
- workspace å¿…é¡» 256 å­—èŠ‚å¯¹é½

#### 3.6.4 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å¤ç”¨ Handle å’Œ Descriptor**ï¼šåˆ›å»ºå¼€é”€å¤§ï¼Œåº”å…¨å±€å¤ç”¨
2. **ç¼“å­˜ Heuristic ç»“æœ**ï¼šç›¸åŒé—®é¢˜è§„æ¨¡å¯å¤ç”¨ç®—æ³•é€‰æ‹©
3. **Workspace é¢„åˆ†é…**ï¼šé¿å…è¿è¡Œæ—¶åˆ†é…
4. **ä½¿ç”¨ Fast Accumulation**ï¼š`CUBLASLT_MATMUL_DESC_FAST_ACCUM = 1`

---

## 4. å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹

### 4.1 ä¿®æ”¹ CuBLASLtFp8LinearOp

```python
# slidesparse/core/cublaslt_linear_method.py

class CuBLASLtFp8LinearOp:
    USE_REAL_CUBLASLT = True  # å¯ç”¨çœŸå® cuBLASLt
    
    def __init__(self):
        # åˆå§‹åŒ– cuBLASLt handleï¼ˆå•ä¾‹ï¼‰
        self._handle = self._get_or_create_handle()
        
    @classmethod
    def _get_or_create_handle(cls):
        # å…¨å±€ handle ç¼“å­˜
        if not hasattr(cls, '_global_handle'):
            cls._global_handle = cublaslt_create_handle()
        return cls._global_handle
    
    def apply(
        self,
        x: torch.Tensor,           # [M, K] FP8
        weight: torch.Tensor,      # [N, K] FP8
        x_scale: torch.Tensor,     # [M] or [1]
        weight_scale: torch.Tensor,# [N] or [1]
        bias: Optional[torch.Tensor] = None,
        output_dtype: torch.dtype = torch.bfloat16,
    ) -> torch.Tensor:
        
        M, K = x.shape
        N = weight.shape[0]
        
        # ç¡®å®š scale mode
        is_x_scale_scalar = (x_scale.numel() == 1)
        is_w_scale_scalar = (weight_scale.numel() == 1)
        
        # è°ƒç”¨ cuBLASLt kernel
        output = cublaslt_fp8_gemm(
            self._handle,
            x.data_ptr(),
            weight.data_ptr(),
            x_scale.data_ptr(),
            weight_scale.data_ptr(),
            bias.data_ptr() if bias is not None else None,
            M, N, K,
            is_x_scale_scalar,
            is_w_scale_scalar,
            output_dtype,
            torch.cuda.current_stream().cuda_stream,
        )
        
        return output
```

### 4.2 CUDA ç»‘å®šå®ç°

éœ€è¦åœ¨ `csrc/` ç›®å½•ä¸‹æ·»åŠ  cuBLASLt wrapperï¼š

```cpp
// csrc/quantization/cublaslt_fp8_gemm.cu

#include <cublasLt.h>
#include <torch/extension.h>

// å…¨å±€ handle ç®¡ç†
class CublasLtHandlePool {
public:
    static cublasLtHandle_t get() {
        static thread_local cublasLtHandle_t handle = nullptr;
        if (handle == nullptr) {
            cublasLtCreate(&handle);
        }
        return handle;
    }
};

torch::Tensor cublaslt_fp8_gemm(
    torch::Tensor A,        // [M, K] FP8
    torch::Tensor B,        // [N, K] FP8 (stored transposed)
    torch::Tensor scale_a,  // [M] or [1]
    torch::Tensor scale_b,  // [N] or [1]
    c10::optional<torch::Tensor> bias,
    bool is_scale_a_scalar,
    bool is_scale_b_scalar,
    torch::Dtype output_dtype
) {
    // å®ç°å‚è§ 3.5 èŠ‚æ¡†æ¶
}

// Python ç»‘å®š
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("cublaslt_fp8_gemm", &cublaslt_fp8_gemm, "cuBLASLt FP8 GEMM");
}
```

### 4.3 CMake é›†æˆ

```cmake
# cmake/cublaslt_extension.cmake

find_package(CUDAToolkit REQUIRED)

add_library(cublaslt_fp8_gemm SHARED
    csrc/quantization/cublaslt_fp8_gemm.cu
)

target_link_libraries(cublaslt_fp8_gemm
    CUDA::cublasLt
    ${TORCH_LIBRARIES}
)
```

---

## 5. æµ‹è¯•ä¸éªŒè¯è®¡åˆ’

### 5.1 æ­£ç¡®æ€§æµ‹è¯•

```python
def test_cublaslt_fp8_gemm_correctness():
    """å¯¹æ¯” cuBLASLt ä¸ CUTLASS ç»“æœ"""
    M, N, K = 1024, 4096, 4096
    
    # éšæœºç”Ÿæˆ FP8 è¾“å…¥
    x = torch.randn(M, K, device='cuda').to(torch.float8_e4m3fn)
    w = torch.randn(N, K, device='cuda').to(torch.float8_e4m3fn)
    scale_x = torch.rand(M, device='cuda')
    scale_w = torch.rand(N, device='cuda')
    bias = torch.randn(N, device='cuda', dtype=torch.bfloat16)
    
    # CUTLASS å‚è€ƒå®ç°
    ref = cutlass_scaled_mm(x, w, scale_x, scale_w, bias)
    
    # cuBLASLt å®ç°
    out = cublaslt_fp8_gemm(x, w, scale_x, scale_w, bias)
    
    # éªŒè¯ï¼ˆFP8 è®¡ç®—å…è®¸ä¸€å®šè¯¯å·®ï¼‰
    assert torch.allclose(out, ref, rtol=1e-2, atol=1e-2)
```

### 5.2 æ€§èƒ½æµ‹è¯•

```python
def benchmark_cublaslt_vs_cutlass():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    configs = [
        (1, 4096, 4096),     # decode
        (32, 4096, 4096),    # small batch
        (128, 4096, 4096),   # medium batch
        (1024, 4096, 4096),  # large batch
        (4096, 4096, 4096),  # prefill
    ]
    
    for M, N, K in configs:
        # é¢„çƒ­
        for _ in range(10):
            cutlass_run(M, N, K)
            cublaslt_run(M, N, K)
        
        # æµ‹é‡
        cutlass_time = benchmark(cutlass_run, M, N, K, iters=100)
        cublaslt_time = benchmark(cublaslt_run, M, N, K, iters=100)
        
        print(f"[{M}, {N}, {K}] CUTLASS: {cutlass_time:.2f}ms, cuBLASLt: {cublaslt_time:.2f}ms")
```

---

## 6. æ€»ç»“

### 6.1 å½“å‰çŠ¶æ€

| ç»„ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| ç¯å¢ƒå˜é‡åˆ‡æ¢ | âœ… å®Œæˆ | `VLLM_USE_CUBLASLT=1` |
| Scheme åŒ…è£…æ¶æ„ | âœ… å®Œæˆ | `CuBLASLtSchemeWrapper` |
| æƒé‡åŠ è½½å…¼å®¹ | âœ… å®Œæˆ | å§”æ‰˜ç»™åŸå§‹ scheme |
| cuBLASLt çœŸå®å®ç° | ğŸ”„ å¾…å¼€å‘ | æœ¬æ–‡æ¡£æä¾›æ¡†æ¶ |

### 6.2 å¼€å‘ä¼˜å…ˆçº§

1. **é«˜ä¼˜å…ˆçº§**ï¼šå®ç°åŸºç¡€ cuBLASLt FP8 GEMMï¼ˆper-tensor scaleï¼‰
2. **ä¸­ä¼˜å…ˆçº§**ï¼šæ”¯æŒ per-token/per-channel scaleï¼ˆOuter Vector Scalingï¼‰
3. **ä½ä¼˜å…ˆçº§**ï¼šBias èåˆã€GELU/ReLU èåˆ

### 6.3 é£é™©ä¸æŒ‘æˆ˜

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|---------|
| Layout ä¸åŒ¹é… | ç»“æœé”™è¯¯ | ä»”ç»†éªŒè¯è½¬ç½®é€»è¾‘ |
| Scale mode é™åˆ¶ | SM89 ä¸æ”¯æŒ outer vector | å›é€€åˆ° per-tensor |
| æ€§èƒ½å›é€€ | å° M åœºæ™¯å¯èƒ½æ›´æ…¢ | æ ¹æ®è§„æ¨¡åŠ¨æ€é€‰æ‹©åç«¯ |
| cuBLAS ç‰ˆæœ¬å…¼å®¹ | API å·®å¼‚ | ç‰ˆæœ¬æ£€æµ‹ + æ¡ä»¶ç¼–è¯‘ |

---

## 7. å½“å‰å¤–æŒ‚æ–¹æ³•çš„å§”æ‰˜è¯¦ç»†åˆ†æ

æœ¬ç« è¯¦ç»†è¯´æ˜å½“å‰ `CuBLASLtFp8LinearMethod` / `CuBLASLtSchemeWrapper` ä¸­å„æ­¥éª¤çš„å§”æ‰˜æƒ…å†µã€‚

### 7.1 FP8 å§”æ‰˜é“¾è·¯æ€»è§ˆ

| æ­¥éª¤ | å½“å‰çŠ¶æ€ | å¤–æŒ‚å‡½æ•° | è½¬å‘ç›®æ ‡ | vLLM å…·ä½“ä½ç½® |
|------|----------|----------|----------|---------------|
| **æƒé‡åŠ è½½** | âœ… å§”æ‰˜ | `create_weights()` | `original_scheme.create_weights()` | `compressed_tensors_w8a8_fp8.py:84-130` |
| **æƒé‡å¤„ç†** | âœ… å§”æ‰˜ | `process_weights_after_loading()` | `original_scheme.process_weights_after_loading()` | `compressed_tensors_w8a8_fp8.py:132-172` |
| **è¾“å…¥åŠ è½½** | N/A | - | ç”± PyTorch è‡ªåŠ¨å¤„ç† | - |
| **è¾“å…¥é‡åŒ–** | âœ… å§”æ‰˜ | `apply()` â†’ `_fp8_linear_op.apply()` | `Fp8LinearOp.quant_fp8()` | `w8a8_utils.py:462-467` â†’ `QuantFP8` |
| **GEMM+åé‡åŒ–** | âœ… å§”æ‰˜ | `apply()` â†’ `_fp8_linear_op.apply()` | `cutlass_w8a8_scaled_mm()` | `w8a8_utils.py:150-165` |
| **è¾“å‡ºè¿”å›** | âœ… å§”æ‰˜ | `apply()` è¿”å› | `_fp8_linear_op.apply()` è¿”å› | BF16 tensor |

### 7.2 FP8 å„æ­¥éª¤è¯¦ç»†è¯´æ˜

#### 7.2.1 æƒé‡åŠ è½½ (`create_weights`)

```
CuBLASLtFp8LinearMethod.create_weights()
    â”‚
    â””â”€â†’ self.original_scheme.create_weights()  # å®Œå…¨å§”æ‰˜
            â”‚
            â””â”€â†’ CompressedTensorsW8A8Fp8.create_weights()
                    â”‚
                    â”œâ”€â†’ create_fp8_weight_parameter()      # åˆ›å»º FP8 æƒé‡ [N, K]
                    â”œâ”€â†’ create_fp8_scale_parameter()       # åˆ›å»º weight_scale
                    â””â”€â†’ create_fp8_input_scale() (å¯é€‰)    # é™æ€é‡åŒ–æ—¶åˆ›å»º input_scale
```

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ç»™åŸå§‹ schemeï¼Œä¸åšä»»ä½•ä¿®æ”¹ã€‚
**åç»­è®¡åˆ’**ï¼šå¦‚éœ€è‡ªå®šä¹‰æƒé‡æ ¼å¼ï¼Œéœ€è¦åœ¨æ­¤å¤„ä»‹å…¥ã€‚

#### 7.2.2 æƒé‡å¤„ç† (`process_weights_after_loading`)

```
CuBLASLtFp8LinearMethod.process_weights_after_loading()
    â”‚
    â””â”€â†’ self.original_scheme.process_weights_after_loading()  # å®Œå…¨å§”æ‰˜
            â”‚
            â””â”€â†’ CompressedTensorsW8A8Fp8.process_weights_after_loading()
                    â”‚
                    â”œâ”€â†’ process_fp8_weight_tensor_strategy()   # per-tensor
                    â”œâ”€â†’ process_fp8_weight_channel_strategy()  # per-channel â† Qwen ä½¿ç”¨
                    â””â”€â†’ process_fp8_weight_block_strategy()    # block
                    â”‚
                    â””â”€â†’ weight = weight.t()  # å…³é”®ï¼šæƒé‡è½¬ç½®ä¸º [K, N]
```

**å…³é”®å¤„ç†**ï¼š
- æ ¹æ®ç­–ç•¥å¤„ç† weight_scaleï¼ˆper-tensor/per-channel/blockï¼‰
- **æƒé‡è½¬ç½®**ï¼šä» `[N, K]` è½¬ä¸º `[K, N]`
- å°† weight å’Œ weight_scale è½¬ä¸º `torch.nn.Parameter`

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ã€‚
**æ³¨æ„**ï¼šcuBLASLt éœ€è¦ç‰¹å®š layoutï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹æ­¤å¤„ã€‚

#### 7.2.3 è¾“å…¥é‡åŒ–

```
CuBLASLtFp8LinearOp.apply()
    â”‚
    â””â”€â†’ self._fp8_linear_op.apply()  # å§”æ‰˜ç»™ vLLM çš„ Fp8LinearOp
            â”‚
            â””â”€â†’ Fp8LinearOp.apply() [w8a8_utils.py:440-490]
                    â”‚
                    â””â”€â†’ self.quant_fp8(input_2d, input_scale, input_scale_ub)
                            â”‚
                            â””â”€â†’ QuantFP8.__call__() [input_quant_fp8.py]
                                    â”‚
                                    â””â”€â†’ ops.scaled_fp8_quant(input, scale)
                                            â”‚
                                            â””â”€â†’ è¿”å› (qinput, x_scale)
                                                     FP8      FP32
```

**é‡åŒ–å…¬å¼**ï¼š`qinput = input / scale`ï¼Œå…¶ä¸­ `scale = max(|input|) / fp8_max`

**Scale å½¢çŠ¶ï¼ˆQwen FP8 é…ç½®ï¼‰**ï¼š
- `x_scale`: `[M, 1]` (per-token)
- `weight_scale`: `[N, 1]` (per-channel)

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ç»™ `QuantFP8`ã€‚
**åç»­è®¡åˆ’**ï¼šå¦‚éœ€è‡ªå®šä¹‰é‡åŒ–ï¼Œåœ¨ `CuBLASLtFp8LinearOp.apply()` ä¸­ç›´æ¥è°ƒç”¨è‡ªå·±çš„é‡åŒ–å‡½æ•°ã€‚

#### 7.2.4 GEMM + åé‡åŒ–

```
Fp8LinearOp.apply() [w8a8_utils.py:480-490]
    â”‚
    â”œâ”€â†’ dispatch_w8a8_scaled_mm(preferred_backend, ...)  # é€‰æ‹©åç«¯
    â”‚       â”‚
    â”‚       â””â”€â†’ è¿”å› cutlass_w8a8_scaled_mm (CUDA + SM90)
    â”‚
    â””â”€â†’ cutlass_w8a8_scaled_mm() [w8a8_utils.py:150-165]
            â”‚
            â””â”€â†’ ops.cutlass_scaled_mm(qinput, weight, scale_a, scale_b, bias)
                    â”‚
                    â””â”€â†’ C++ è°ƒç”¨: cutlass_scaled_mm_sm90_fp8()
                            â”‚
                            â””â”€â†’ output = scale_a * (scale_b * (qinput @ weight.T)) + bias
```

**GEMM å‚æ•°**ï¼š
| å‚æ•° | å½¢çŠ¶ | è¯´æ˜ |
|------|------|------|
| `qinput` | `[M, K]` FP8 | é‡åŒ–åçš„è¾“å…¥ |
| `weight` | `[K, N]` FP8 | è½¬ç½®åçš„æƒé‡ |
| `scale_a` | `[M, 1]` FP32 | è¾“å…¥ scale (per-token) |
| `scale_b` | `[N, 1]` FP32 | æƒé‡ scale (per-channel) |
| `bias` | `[N]` BF16 | å¯é€‰åç½® |
| `output` | `[M, N]` BF16 | è¾“å‡º |

**å½“å‰çŠ¶æ€**ï¼šé€šè¿‡ `_fp8_linear_op.apply()` é—´æ¥å§”æ‰˜ç»™ cutlassã€‚
**æ›¿æ¢ç‚¹**ï¼šè¿™é‡Œæ˜¯ cuBLASLt æ›¿æ¢çš„å…³é”®ä½ç½®ã€‚

### 7.3 INT8 å§”æ‰˜é“¾è·¯æ€»è§ˆ

| æ­¥éª¤ | å½“å‰çŠ¶æ€ | å¤–æŒ‚å‡½æ•° | è½¬å‘ç›®æ ‡ | vLLM å…·ä½“ä½ç½® |
|------|----------|----------|----------|---------------|
| **æƒé‡åŠ è½½** | âŒ æœªæ”¯æŒ | - | - | `compressed_tensors_w8a8_int8.py:43-96` |
| **æƒé‡å¤„ç†** | âŒ æœªæ”¯æŒ | - | - | `cutlass.py:34-109` (kernel.process_weights) |
| **è¾“å…¥é‡åŒ–** | âŒ æœªæ”¯æŒ | - | `ops.scaled_int8_quant()` | `cutlass.py:127-129` |
| **GEMM+åé‡åŒ–** | âŒ æœªæ”¯æŒ | - | `ops.cutlass_scaled_mm()` | `cutlass.py:144-147` |

### 7.4 INT8 è¯¦ç»†è¯´æ˜

å½“å‰ `wrap_scheme_with_cublaslt()` **ä¸æ”¯æŒ INT8**ï¼Œä»…æ£€æµ‹ `W8A8Fp8`ï¼š

```python
# cublaslt_linear_method.py:287
if "W8A8Fp8" in scheme_name:
    return CuBLASLtFp8LinearMethod(original_scheme)
else:
    # INT8 ä¼šèµ°è¿™é‡Œï¼Œè¿”å›åŸå§‹ scheme
    return original_scheme
```

**INT8 çš„æ¶æ„å·®å¼‚**ï¼š

1. **Scheme ç±»**ï¼š`CompressedTensorsW8A8Int8`
2. **Kernel é€‰æ‹©**ï¼šä½¿ç”¨ `ScaledMMLinearKernel` æ¶æ„
   - `CutlassScaledMMLinearKernel` (CUDA)
   - `TorchScaledMMLinearKernel` (fallback)
3. **é‡åŒ–å‡½æ•°**ï¼š`ops.scaled_int8_quant()` vs FP8 çš„ `ops.scaled_fp8_quant()`
4. **éå¯¹ç§°æ”¯æŒ**ï¼šINT8 æ”¯æŒ asymmetric quantization (AZP)

**INT8 å…³é”®å‡½æ•°ä½ç½®**ï¼š

| å‡½æ•° | æ–‡ä»¶ | è¯´æ˜ |
|------|------|------|
| `create_weights` | `compressed_tensors_w8a8_int8.py:43-96` | åˆ›å»º INT8 æƒé‡å’Œ scale |
| `process_weights_after_loading` | `cutlass.py:34-109` | æƒé‡è½¬ç½®ã€scale å¤„ç†ã€AZP è®¡ç®— |
| `apply_weights` | `cutlass.py:115-147` | INT8 é‡åŒ– + GEMM |
| `scaled_int8_quant` | `_custom_ops` | INT8 åŠ¨æ€/é™æ€é‡åŒ– |
| `cutlass_scaled_mm_azp` | `_custom_ops` | å¸¦ AZP çš„ INT8 GEMM |

### 7.5 åç»­æ¥ç®¡è®¡åˆ’

#### FP8 æ¥ç®¡æ­¥éª¤

1. **ä¿æŒå§”æ‰˜**ï¼š`create_weights()`, `process_weights_after_loading()`
2. **è‡ªä¸»å®ç°**ï¼š
   - åœ¨ `CuBLASLtFp8LinearOp.apply()` ä¸­ï¼š
     - ç›´æ¥è°ƒç”¨è‡ªå®šä¹‰çš„ FP8 é‡åŒ–å‡½æ•°ï¼ˆæˆ–å¤ç”¨ `QuantFP8`ï¼‰
     - è°ƒç”¨ cuBLASLt FP8 GEMM kernel
     - è¿”å› BF16 è¾“å‡º

```python
# CuBLASLtFp8LinearOp._apply_cublaslt() çš„ç›®æ ‡å®ç°
def _apply_cublaslt(self, input, weight, weight_scale, ...):
    # 1. è¾“å…¥é‡åŒ–ï¼ˆå¯å¤ç”¨ QuantFP8 æˆ–è‡ªå®ç°ï¼‰
    qinput, x_scale = self.quant_fp8(input, input_scale)
    
    # 2. cuBLASLt GEMMï¼ˆæ›¿æ¢ cutlass_scaled_mmï¼‰
    output = cublaslt_fp8_gemm(
        qinput, weight, x_scale, weight_scale, bias
    )
    
    # 3. è¿”å› BF16 è¾“å‡º
    return output
```

#### INT8 æ”¯æŒè®¡åˆ’

1. åˆ›å»º `CuBLASLtInt8LinearMethod` ç±»
2. åœ¨ `wrap_scheme_with_cublaslt()` ä¸­æ·»åŠ  INT8 æ£€æµ‹
3. å®ç° INT8 ç‰ˆæœ¬çš„ `apply_weights()`

---

## 8. æµ‹è¯•æ¡†æ¶è®¾è®¡

### 8.1 æµ‹è¯•è„šæœ¬æ¶æ„

æµ‹è¯•è„šæœ¬ `test_cublaslt_00_kernel.py` è®¾è®¡ä¸ºå¯ç‹¬ç«‹è¿è¡Œï¼Œç›´æ¥æµ‹è¯• GEMM kernel çš„æ­£ç¡®æ€§å’Œæ€§èƒ½ã€‚

```
æµ‹è¯•æµç¨‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ç”Ÿæˆæµ‹è¯•æ•°æ®                                             â”‚
â”‚     - input_bf16 [M, K] - BF16 æ ¼å¼                         â”‚
â”‚     - weight_fp8 [N, K] - FP8 æ ¼å¼ï¼ˆè½¬ç½®å [K, N]ï¼‰           â”‚
â”‚     - weight_scale [N, 1] - per-channel                     â”‚
â”‚     - bias [N] - å¯é€‰                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. è°ƒç”¨è¢«æµ‹ kernelï¼ˆé€šè¿‡ CuBLASLtFp8LinearOpï¼‰               â”‚
â”‚     - å†…éƒ¨ä¼šè¿›è¡Œ BF16 â†’ FP8 é‡åŒ–ï¼ˆper-token dynamicï¼‰        â”‚
â”‚     - æ‰§è¡Œ FP8 GEMMï¼ˆå½“å‰æ˜¯ cutlassï¼Œå°†æ›¿æ¢ä¸º cuBLASLtï¼‰      â”‚
â”‚     - è¿”å› BF16 è¾“å‡º                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. è®¡ç®—å‚è€ƒç»“æœï¼ˆæ¨¡æ‹Ÿå®Œæ•´ FP8 GEMM æµç¨‹ï¼‰                    â”‚
â”‚     - è¾“å…¥é‡åŒ–ï¼šinput_bf16 â†’ input_fp8, x_scale             â”‚
â”‚     - FP8 çŸ©é˜µä¹˜ï¼šinput_fp8 @ weight_fp8_t                   â”‚
â”‚     - åé‡åŒ–ï¼šresult * x_scale * weight_scale               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. æ¯”è¾ƒç»“æœå’Œæµ‹é‡æ€§èƒ½                                       â”‚
â”‚     - æ­£ç¡®æ€§ï¼štorch.allclose(output, reference)             â”‚
â”‚     - æ€§èƒ½ï¼šæµ‹é‡ååé‡ (TFLOPS)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 æµ‹è¯•æ¥å£è®¾è®¡

ä¸ºæ”¯æŒç‹¬ç«‹æµ‹è¯•ï¼Œåœ¨ `CuBLASLtFp8LinearOp` ä¸­å·²æ·»åŠ ä¸“ç”¨æ¥å£ï¼š

```python
class CuBLASLtFp8LinearOp:
    def apply_for_test(
        self,
        input: torch.Tensor,           # [M, K] BF16ï¼Œä¼šè¢«é‡åŒ–
        weight: torch.Tensor,          # [K, N] FP8ï¼Œå·²é‡åŒ–å·²è½¬ç½®ï¼Œcolumn-major
        weight_scale: torch.Tensor,    # [N, 1] FP32
        out_dtype: torch.dtype = torch.bfloat16,
        input_scale: torch.Tensor | None = None,  # None = dynamic quant
        bias: torch.Tensor | None = None,
    ) -> torch.Tensor:
        """
        ä¸“ç”¨äºæµ‹è¯•çš„æ¥å£ï¼Œè·³è¿‡ layer å¯¹è±¡ä¾èµ–
        """
        ...
```

### 8.3 æµ‹è¯•è„šæœ¬ä½¿ç”¨

```bash
# è¿è¡Œå•ä¸ªè§„æ¨¡æµ‹è¯•
python slidesparse/test/test_cublaslt_00_kernel.py --m 256 --n 896 --k 896

# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python slidesparse/test/test_cublaslt_00_kernel.py --all
```

### 8.4 æµ‹è¯•ç»“æœï¼ˆRTX 5080, SM 12.0ï¼‰

| M | N | K | Bias | Time(ms) | TFLOPS | Status |
|---:|---:|---:|:---:|---:|---:|:---:|
| 1 | 896 | 896 | No | 0.050 | 0.03 | âœ… |
| 1 | 4864 | 896 | No | 0.049 | 0.18 | âœ… |
| 32 | 896 | 896 | No | 0.049 | 1.05 | âœ… |
| 32 | 4864 | 896 | No | 0.049 | 5.68 | âœ… |
| 128 | 4864 | 896 | No | 0.049 | 22.76 | âœ… |
| 128 | 896 | 4864 | Yes | 0.105 | 10.66 | âœ… |
| 512 | 4864 | 896 | No | 0.061 | 73.64 | âœ… |
| 1024 | 4864 | 896 | No | 0.090 | 98.87 | âœ… |
| 2048 | 2048 | 2048 | No | 0.181 | 95.14 | âœ… |
| 4096 | 4096 | 4096 | No | 1.232 | 111.52 | âœ… |

**æµ‹è¯•ç¯å¢ƒè¯´æ˜**ï¼š
- å½“å‰åº•å±‚ kernel ä¸º CUTLASS `cutlass_scaled_mm`
- æ›¿æ¢ä¸º cuBLASLt åï¼Œéœ€é‡æ–°è¿è¡Œæµ‹è¯•éªŒè¯æ­£ç¡®æ€§å’Œæ€§èƒ½å·®å¼‚
- è¯¯å·®å®¹é™ï¼šrtol=0.05, atol=0.05ï¼ˆFP8 é‡åŒ–è¯¯å·®æ­£å¸¸èŒƒå›´ï¼‰

---

## 9. vLLM GEMM åç«¯åˆ†å‘æœºåˆ¶æ·±åº¦åˆ†æ

æœ¬ç« è¯¦ç»†è§£ç­”å…³äº FlashInferã€Paddingã€Triton å›é€€ç­‰å…³é”®é—®é¢˜ã€‚

### 9.1 FlashInfer æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™ä¼šè¢«è°ƒç”¨ï¼Ÿ

#### 9.1.1 FlashInfer ç®€ä»‹

**FlashInfer** æ˜¯ç”± NVIDIA å’Œç¤¾åŒºå¼€å‘çš„**é«˜æ€§èƒ½ Attention å’Œ GEMM å†…æ ¸åº“**ï¼Œä¸“é—¨é’ˆå¯¹ LLM æ¨ç†ä¼˜åŒ–ã€‚å®ƒæä¾›äº†ï¼š

1. **FlashAttention å˜ä½“**ï¼šé«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
2. **FP8 GEMM**ï¼šåŸºäº `bmm_fp8` çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•
3. **MoE ç›¸å…³ç®—å­**ï¼šFused MoEã€AlltoAll ç­‰

**å…³é”®ä»£ç ä½ç½®**ï¼š`vllm/utils/flashinfer.py`

```python
# flashinfer_scaled_fp8_mm çš„å®ç°
def flashinfer_scaled_fp8_mm(
    a: torch.Tensor,  # [M, K] FP8
    b: torch.Tensor,  # [K, N] FP8
    scale_a: torch.Tensor,  # scalar only!
    scale_b: torch.Tensor,  # scalar only!
    out_dtype: torch.dtype,
    bias: torch.Tensor | None = None,
) -> torch.Tensor:
    # âš ï¸ é‡è¦é™åˆ¶ï¼šåªæ”¯æŒ per-tensor scale
    assert scale_a.numel() == 1 and scale_b.numel() == 1
    
    output = bmm_fp8(
        a.unsqueeze(0),
        b.unsqueeze(0),
        scale_a,
        scale_b,
        out_dtype,
        "auto",
    ).view(a.shape[0], b.shape[1])
    
    if bias is not None:
        output = output + bias
    return output
```

#### 9.1.2 FlashInfer çš„å¯ç”¨æ¡ä»¶

åœ¨ `Fp8LinearOp.__init__()` ä¸­ï¼ˆ[w8a8_utils.py:405-416](vllm/model_executor/layers/quantization/utils/w8a8_utils.py#L405-L416)ï¼‰ï¼š

```python
class Fp8LinearOp:
    def __init__(self, ...):
        if current_platform.is_rocm():
            self.preferred_backend = "rocm"
        elif current_platform.is_cuda() and cutlass_fp8_supported():
            # å…³é”®æ¡ä»¶ï¼šCC >= 100 (Blackwell B100/B200) ä¸” flashinfer å¯ç”¨
            if has_flashinfer() and current_platform.has_device_capability(100):
                self.preferred_backend = "flashinfer"
            else:
                self.preferred_backend = "cutlass"
        else:
            self.preferred_backend = "torch"
```

**FlashInfer FP8 GEMM å¯ç”¨æ¡ä»¶**ï¼š
| æ¡ä»¶ | è¯´æ˜ |
|------|------|
| `has_flashinfer()` | FlashInfer Python åŒ…å·²å®‰è£… |
| `has_device_capability(100)` | **SM >= 100 (Blackwell)** |
| `per_tensor_weights and per_tensor_activations` | **å¿…é¡»æ˜¯ per-tensor é‡åŒ–** |

#### 9.1.3 ä¸ºä»€ä¹ˆ RTX 5080 ä¸èµ° FlashInferï¼Ÿ

**RTX 5080 æ˜¯ SM 12.0 (Blackwell Consumer)**ï¼Œæ»¡è¶³ CC >= 100 çš„æ¡ä»¶ã€‚ä½†æ˜¯ï¼Œè®©æˆ‘ä»¬çœ‹ `dispatch_w8a8_scaled_mm` çš„é€»è¾‘ï¼ˆ[w8a8_utils.py:363-378](vllm/model_executor/layers/quantization/utils/w8a8_utils.py#L363-L378)ï¼‰ï¼š

```python
def dispatch_w8a8_scaled_mm(
    preferred_backend: str, per_tensor_weights: bool, per_tensor_activations: bool
) -> Callable[..., torch.Tensor]:
    
    # æƒ…å†µ 1: per-tensor W å’Œ per-tensor A
    if per_tensor_weights and per_tensor_activations:
        if preferred_backend == "flashinfer":
            return flashinfer_w8a8_scaled_mm  # âœ… èµ° FlashInfer
        if preferred_backend == "cutlass":
            return cutlass_w8a8_scaled_mm
        ...
    
    # æƒ…å†µ 2: per-channel W æˆ– per-token Aï¼ˆæˆ‘ä»¬çš„ Qwen FP8 é…ç½®ï¼‰
    # cutlass_scaled_mm supports per tensor/channel W and per tensor/token A
    if preferred_backend == "cutlass" or preferred_backend == "flashinfer":
        return cutlass_w8a8_scaled_mm  # âš ï¸ å›é€€åˆ° CUTLASSï¼
```

**ç»“è®º**ï¼š
- **Qwen FP8 æ¨¡å‹ä½¿ç”¨ per-channel weight + per-token activation**
- FlashInfer çš„ `bmm_fp8` **åªæ”¯æŒ per-tensor scale**
- å› æ­¤å³ä½¿ preferred_backend="flashinfer"ï¼Œä¹Ÿä¼š**å›é€€åˆ° CUTLASS**

**ç®€å•æ€»ç»“**ï¼š

| æ˜¾å¡ | SM | preferred_backend | å®é™…ä½¿ç”¨ | åŸå›  |
|------|-----|-------------------|---------|------|
| H100 | 90 | cutlass | CUTLASS | SM < 100 |
| B100/B200 | 100 | flashinfer | CUTLASS | per-channel/per-token ä¸æ”¯æŒ |
| RTX 5080 | 120 | flashinfer | CUTLASS | per-channel/per-token ä¸æ”¯æŒ |
| ä»»æ„ | - | - | FlashInfer | ä»…å½“ per-tensor W + per-tensor A |

### 9.2 Padding æœºåˆ¶è¯¦è§£

#### 9.2.1 ä¸ºä»€ä¹ˆéœ€è¦ Paddingï¼Ÿ

CUTLASS å’Œ cuBLASLt çš„ FP8 GEMM å†…æ ¸å¯¹çŸ©é˜µç»´åº¦æœ‰å¯¹é½è¦æ±‚ï¼š
- **M, K, N æœ€å¥½æ˜¯ 16 çš„å€æ•°**
- ä¸å¯¹é½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™æˆ–è°ƒç”¨ fallback è·¯å¾„

#### 9.2.2 vLLM çš„ Padding ç­–ç•¥

åœ¨ `Fp8LinearOp.__init__()` ä¸­ï¼ˆ[w8a8_utils.py:420-430](vllm/model_executor/layers/quantization/utils/w8a8_utils.py#L420-L430)ï¼‰ï¼š

```python
class Fp8LinearOp:
    def __init__(self, ..., pad_output: bool | None = None):
        # pad_output çš„é»˜è®¤å€¼é€»è¾‘
        if pad_output is None:
            config = get_current_vllm_config().compilation_config
            pad_output = (
                # æ¡ä»¶1: æ²¡æœ‰ä½¿ç”¨ torch.compile
                config.mode < CompilationMode.VLLM_COMPILE
                # æ¡ä»¶2: ä½¿ç”¨ torch åç«¯ï¼ˆä¸æ˜¯ cutlass/flashinferï¼‰
                and self.preferred_backend == "torch"
            )
        
        # å¦‚æœéœ€è¦ paddingï¼Œpad åˆ° 17ï¼ˆè€Œä¸æ˜¯ 16ï¼‰
        # è¿™æ˜¯å› ä¸º torch._scaled_mm åœ¨ batch > 16 æ—¶æ€§èƒ½æ›´å¥½
        self.output_padding = 17 if pad_output else None
```

**å…³é”®ç»“è®º**ï¼š

| preferred_backend | torch.compile | æ˜¯å¦ Padding |
|-------------------|---------------|--------------|
| cutlass | å¦ | âŒ ä¸ Padding |
| cutlass | æ˜¯ | âŒ ä¸ Padding |
| flashinfer | å¦ | âŒ ä¸ Padding |
| flashinfer | æ˜¯ | âŒ ä¸ Padding |
| torch | å¦ | âœ… Padding åˆ° 17 |
| torch | æ˜¯ | âŒ ä¸ Paddingï¼ˆä¼šç ´ååŠ¨æ€ shapeï¼‰ |

#### 9.2.3 Padding åœ¨å“ªé‡Œå‘ç”Ÿï¼Ÿ

Padding å‘ç”Ÿåœ¨ `QuantFP8` çš„é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œ**ä¸æ˜¯åœ¨ GEMM é˜¶æ®µ**ï¼š

```python
# input_quant_fp8.py
class QuantFP8(CustomOp):
    def __init__(self, ..., num_token_padding: int | None = None):
        self.num_token_padding = num_token_padding
    
    def __call__(self, input, scale, scale_ub):
        if self.num_token_padding:
            # å¯¹è¾“å…¥è¿›è¡Œ padding
            input = pad_to(input, self.num_token_padding)
        return ops.scaled_fp8_quant(input, scale)
```

ç„¶ååœ¨ GEMM è¾“å‡ºæ—¶é€šè¿‡ `torch.narrow` æˆªå–æœ‰æ•ˆéƒ¨åˆ†ï¼š

```python
def torch_per_tensor_w8a8_scaled_mm(...):
    output = torch._scaled_mm(qinput, weight, ...)
    # æˆªå–æœ‰æ•ˆè¾“å‡ºï¼ˆå»é™¤ padding éƒ¨åˆ†ï¼‰
    return torch.narrow(output, 0, 0, qinput.shape[0]).view(*output_shape)
```

#### 9.2.4 å¦‚æœ M=31 ä¼šæ€æ ·ï¼Ÿ

**å¯¹äº CUTLASS/cuBLASLt åç«¯**ï¼ˆæˆ‘ä»¬çš„æƒ…å†µï¼‰ï¼š

1. **ä¸ä¼šè¿›è¡Œ Padding**ï¼š`output_padding = None`
2. **CUTLASS å¯ä»¥å¤„ç†ä»»æ„ M**ï¼šé€šè¿‡ tile masking å¤„ç†è¾¹ç•Œ
3. **æ€§èƒ½å¯èƒ½ç•¥æœ‰ä¸‹é™**ï¼šéå¯¹é½è®¿é—®ä¼šå¯¼è‡´éƒ¨åˆ† tile æµªè´¹

**å¯¹äº torch åç«¯**ï¼ˆfallbackï¼‰ï¼š
1. **ä¼š Padding åˆ° 17**ï¼šå¦‚æœ M < 17
2. **ç„¶å narrow å› M**ï¼šè¾“å‡ºæ—¶æˆªå–

### 9.3 Triton å›é€€æœºåˆ¶

#### 9.3.1 ä½•æ—¶ä¼šå›é€€åˆ° Tritonï¼Ÿ

åœ¨ `ops.cutlass_scaled_mm()` å‡½æ•°ä¸­ï¼ˆ[_custom_ops.py:863-875](vllm/_custom_ops.py#L863-L875)ï¼‰ï¼š

```python
def cutlass_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias=None):
    # ...
    
    # å…³é”®æ£€æŸ¥ï¼šb çš„ç»´åº¦æ˜¯å¦å¯¹ 16 å¯¹é½
    cutlass_compatible_b = b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0
    
    if current_platform.is_rocm() or not cutlass_compatible_b:
        # å›é€€åˆ° Triton å®ç°
        from vllm.model_executor.layers.quantization.compressed_tensors.triton_scaled_mm import (
            triton_scaled_mm,
        )
        out = triton_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias)
    else:
        # ä½¿ç”¨ CUTLASS
        out = torch.empty((a.shape[0], b.shape[1]), dtype=out_dtype, device=a.device)
        torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
    
    return out.view(*target_shape)
```

#### 9.3.2 è¿™é‡Œçš„ B æ˜¯ä»€ä¹ˆï¼Ÿ

**B æ˜¯æƒé‡çŸ©é˜µï¼ˆWeightï¼‰**ï¼Œä¸æ˜¯æ¿€æ´»ï¼ˆActivationï¼‰ï¼

```python
# åœ¨ cutlass_w8a8_scaled_mm ä¸­çš„è°ƒç”¨
ops.cutlass_scaled_mm(
    qinput,   # A: æ¿€æ´» [M, K]
    weight,   # B: æƒé‡ [K, N]ï¼ˆå·²è½¬ç½®ï¼‰
    ...
)
```

**æƒé‡åœ¨åŠ è½½æ—¶å·²ç»è½¬ç½®**ï¼Œæ‰€ä»¥ï¼š
- åŸå§‹æƒé‡ï¼š`[N, K]`ï¼ˆout_features, in_featuresï¼‰
- è½¬ç½®åæƒé‡ï¼š`[K, N]`
- `b.shape[0] = K`ï¼Œ`b.shape[1] = N`

#### 9.3.3 æƒé‡æ˜¯å¦å¯¹é½ï¼Ÿ

**é€šå¸¸æƒé‡æ˜¯å¯¹é½çš„**ï¼Œå› ä¸ºï¼š
- `K = hidden_dim`ï¼ˆå¦‚ 896, 4096 ç­‰ï¼‰é€šå¸¸æ˜¯ 16 çš„å€æ•°
- `N = out_features`ï¼ˆå¦‚ 896, 4864 ç­‰ï¼‰é€šå¸¸ä¹Ÿæ˜¯ 16 çš„å€æ•°

**ä½†å¦‚æœä¸å¯¹é½**ï¼ˆä¾‹å¦‚æŸäº›ç‰¹æ®Šæ¨¡å‹ï¼‰ï¼Œå°±ä¼šå›é€€åˆ° Tritonã€‚

#### 9.3.4 M ä¸å¯¹é½ä¼šå½±å“å—ï¼Ÿ

**M ä¸å¯¹é½ä¸ä¼šå¯¼è‡´å›é€€åˆ° Triton**ï¼

æ£€æŸ¥çš„æ˜¯ `b.shape`ï¼ˆæƒé‡çš„ K å’Œ Nï¼‰ï¼Œä¸æ˜¯ `a.shape`ï¼ˆæ¿€æ´»çš„ Mï¼‰ã€‚

CUTLASS å†…æ ¸é€šè¿‡ tile masking å¤„ç† M è¾¹ç•Œï¼Œæ‰€ä»¥ï¼š
- M=31 â†’ ä½¿ç”¨ CUTLASS
- M=1 â†’ ä½¿ç”¨ CUTLASS
- åªæœ‰ K æˆ– N ä¸å¯¹é½æ‰å›é€€åˆ° Triton

### 9.4 torch.ops._C.cutlass_scaled_mm çš„æ¥æº

#### 9.4.1 ç»‘å®šä½ç½®

`torch.ops._C.cutlass_scaled_mm` æ˜¯é€šè¿‡ PyTorch çš„ C++ æ‰©å±•æœºåˆ¶æ³¨å†Œçš„ã€‚

**å£°æ˜**ï¼ˆ[csrc/torch_bindings.cpp:436-439](csrc/torch_bindings.cpp#L436-L439)ï¼‰ï¼š
```cpp
ops.def(
    "cutlass_scaled_mm(Tensor! out, Tensor a,"
    " Tensor b, Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()");
ops.impl("cutlass_scaled_mm", torch::kCUDA, &cutlass_scaled_mm);
```

**å®ç°å…¥å£**ï¼ˆ[csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu:176-231](csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu#L176-L231)ï¼‰ï¼š
```cpp
void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
                       torch::Tensor const& b, torch::Tensor const& a_scales,
                       torch::Tensor const& b_scales,
                       std::optional<torch::Tensor> const& bias) {
    // æ ¹æ® SM ç‰ˆæœ¬åˆ†å‘åˆ°ä¸åŒå®ç°
    int32_t version_num = get_sm_version_num();
    
    if (version_num >= 120) {
        cutlass_scaled_mm_sm120(c, a, b, a_scales, b_scales, bias);  // Blackwell Consumer
    } else if (version_num >= 100) {
        cutlass_scaled_mm_sm100(c, a, b, a_scales, b_scales, bias);  // Blackwell Datacenter
    } else if (version_num >= 90) {
        cutlass_scaled_mm_sm90(c, a, b, a_scales, b_scales, bias);   // Hopper
    } else if (version_num == 89) {
        cutlass_scaled_mm_sm89(c, a, b, a_scales, b_scales, bias);   // Ada Lovelace
    } else if (version_num >= 80) {
        cutlass_scaled_mm_sm80(c, a, b, a_scales, b_scales, bias);   // Ampere
    }
    // ...
}
```

#### 9.4.2 CUTLASS æ˜¯ä»€ä¹ˆï¼Ÿ

**CUTLASS** (CUDA Templates for Linear Algebra Subroutines) æ˜¯ NVIDIA å¼€æºçš„**é«˜æ€§èƒ½ GEMM æ¨¡æ¿åº“**ï¼š

- GitHub: https://github.com/NVIDIA/cutlass
- æä¾›å„ç§æ•°æ®ç±»å‹çš„ GEMM å®ç°ï¼ˆFP8, INT8, FP16, BF16 ç­‰ï¼‰
- é’ˆå¯¹æ¯ä»£ GPU æ¶æ„ä¼˜åŒ–ï¼ˆSM75/80/89/90/100/120ï¼‰
- vLLM ä½¿ç”¨ CUTLASS ä½œä¸ºé»˜è®¤çš„ FP8 GEMM åç«¯

**ä½ å¯ä»¥æŠŠå®ƒç†è§£ä¸º**ï¼šä¸€ä¸ªå¼€æºçš„ã€é«˜æ€§èƒ½çš„ GEMM é»‘ç›’ï¼ŒvLLM å·²ç»å°è£…å¥½äº†ã€‚

### 9.5 å½“å‰ CuBLASLtFp8LinearOp.apply çš„å®Œæ•´æ€§åˆ†æ

#### 9.5.1 å½“å‰å®ç°å›é¡¾

å½“å‰çš„ `CuBLASLtFp8LinearOp.apply()` å·²ç»æ˜¯å®Œæ•´çš„ **quant + GEMM + dequant** æµç¨‹ï¼š

```python
def apply(self, input, weight, weight_scale, out_dtype, input_scale, input_scale_ub, bias):
    # 1. å±•å¹³è¾“å…¥
    input_2d = input.view(-1, input.shape[-1])
    output_shape = [*input.shape[:-1], weight.shape[1]]
    
    # 2. é‡åŒ–ï¼ˆä½¿ç”¨è‡ªå·±çš„ QuantFP8 å®ä¾‹ï¼‰
    if input.dtype != current_platform.fp8_dtype():
        qinput, x_scale = self.quant_fp8(input_2d, input_scale, input_scale_ub)
    else:
        qinput, x_scale = input_2d, input_scale
    
    # 3. GEMM + åé‡åŒ–ï¼ˆå½“å‰è°ƒç”¨ cutlassï¼Œåç»­æ›¿æ¢ä¸º cuBLASLtï¼‰
    return cublaslt_w8a8_scaled_mm(
        qinput=qinput,
        weight=weight,
        out_dtype=out_dtype,
        scale_a=x_scale,
        scale_b=weight_scale,
        bias=bias,
        output_shape=output_shape,
    )
```

#### 9.5.2 æ˜¯å¦æœ‰é—®é¢˜ï¼Ÿ

**å½“å‰å®ç°æ˜¯æ­£ç¡®çš„**ï¼Œä¸ vLLM åŸç”Ÿ `Fp8LinearOp.apply()` é€»è¾‘ä¸€è‡´ã€‚

**æ½œåœ¨é—®é¢˜å’Œæ³¨æ„äº‹é¡¹**ï¼š

| é—®é¢˜ | å½“å‰çŠ¶æ€ | è¯´æ˜ |
|------|----------|------|
| Padding | âœ… æ— é—®é¢˜ | æˆ‘ä»¬ä½¿ç”¨ `num_token_padding=None`ï¼Œä¸åš padding |
| Scale mode æ£€æµ‹ | âš ï¸ å¯æ”¹è¿› | å½“å‰ç›´æ¥è°ƒç”¨ CUTLASSï¼Œæ²¡æœ‰æ£€æµ‹ per-tensor/per-token |
| åç«¯åˆ†å‘ | âš ï¸ ç®€åŒ– | è·³è¿‡äº† `dispatch_w8a8_scaled_mm()`ï¼Œç›´æ¥ç”¨ cutlass |

#### 9.5.3 åç»­æ›¿æ¢ä½ç½®

**åªéœ€è¦ä¿®æ”¹ `cublaslt_w8a8_scaled_mm` å‡½æ•°**å³å¯å®Œæˆ cuBLASLt æ›¿æ¢ï¼š

```python
def cublaslt_w8a8_scaled_mm(*, qinput, weight, out_dtype, scale_a, scale_b, bias, output_shape):
    """
    å½“å‰å®ç°ï¼šè°ƒç”¨ cutlass_scaled_mmï¼ˆéªŒè¯æ¶æ„æ­£ç¡®æ€§ï¼‰
    åç»­å®ç°ï¼šæ›¿æ¢ä¸ºçœŸæ­£çš„ cuBLASLt kernel
    """
    # TODO: Phase 3 å®Œæˆåæ›¿æ¢ä¸ºçœŸæ­£çš„ cuBLASLt kernel
    # output = ops.cublaslt_scaled_mm(qinput, weight, scale_a, scale_b, bias)
    
    # å½“å‰ï¼šè°ƒç”¨ cutlass
    output = ops.cutlass_scaled_mm(
        qinput, weight, out_dtype=out_dtype, scale_a=scale_a, scale_b=scale_b, bias=bias
    )
    return output.view(*output_shape)
```

#### 9.5.4 æ›¿æ¢æ—¶éœ€è¦æ³¨æ„çš„ç‚¹

1. **Layout ä¸€è‡´æ€§**ï¼š
   - CUTLASS çš„ B æ˜¯ column-major `[K, N]`ï¼ˆstride: K=1, N=Kï¼‰
   - cuBLASLt ä¹Ÿä½¿ç”¨ column-majorï¼Œä½†å¯èƒ½éœ€è¦è°ƒæ•´ leading dimension

2. **Scale å¤„ç†**ï¼š
   - `scale_a`: per-token `[M, 1]` æˆ– per-tensor `[1]`
   - `scale_b`: per-channel `[N, 1]` æˆ– per-tensor `[1]`
   - cuBLASLt çš„ per-row/col scale éœ€è¦ `OUTER_VEC_32F` modeï¼ˆSM90+ onlyï¼‰

3. **è¾“å‡ºæ ¼å¼**ï¼š
   - ç¡®ä¿ cuBLASLt è¾“å‡ºä¸ CUTLASS ä¸€è‡´ï¼ˆBF16ï¼Œç›¸åŒ shapeï¼‰

4. **å¯¹é½è¦æ±‚**ï¼š
   - cuBLASLt å¯¹æŒ‡é’ˆå¯¹é½æœ‰æ›´ä¸¥æ ¼è¦æ±‚ï¼ˆ16 å­—èŠ‚ï¼‰
   - éœ€è¦æ£€æŸ¥ qinputã€weight æ˜¯å¦æ»¡è¶³

---

## 10. æ€»ç»“ä¸ä¸‹ä¸€æ­¥

### 10.1 å…³é”®å‘ç°

1. **FlashInfer FP8 GEMM åªæ”¯æŒ per-tensor scale**ï¼Œæˆ‘ä»¬çš„ Qwen FP8ï¼ˆper-channel W + per-token Aï¼‰ä¸ä¼šä½¿ç”¨å®ƒ
2. **Padding åªå¯¹ torch åç«¯ç”Ÿæ•ˆ**ï¼ŒCUTLASS ä¸éœ€è¦ padding
3. **Triton å›é€€åªçœ‹æƒé‡ç»´åº¦**ï¼ˆK å’Œ Nï¼‰ï¼ŒM ä¸å¯¹é½ä¸ä¼šè§¦å‘å›é€€
4. **å½“å‰ `CuBLASLtFp8LinearOp.apply()` å®ç°å®Œæ•´æ­£ç¡®**ï¼Œåªéœ€æ›¿æ¢ `cublaslt_w8a8_scaled_mm` å³å¯

### 10.2 ä¸‹ä¸€æ­¥æ“ä½œ

1. åœ¨ `csrc/` ç›®å½•å®ç°çœŸæ­£çš„ cuBLASLt FP8 GEMM wrapper
2. æ›¿æ¢ `cublaslt_w8a8_scaled_mm` ä¸­çš„ `ops.cutlass_scaled_mm` è°ƒç”¨
3. è¿è¡Œæµ‹è¯•éªŒè¯æ­£ç¡®æ€§å’Œæ€§èƒ½

---

## 11. cuBLASLt é›†æˆå…³é”®é—®é¢˜åˆ†æ

æœ¬ç« é’ˆå¯¹ cuBLASLt æ›¿æ¢ CUTLASS çš„å…³é”®æŠ€æœ¯é—®é¢˜è¿›è¡Œè¯¦ç»†åˆ†æã€‚

### 11.1 é—®é¢˜æ¦‚è§ˆä¸å®ç°è®¡åˆ’

| é—®é¢˜ç¼–å· | é—®é¢˜æè¿° | çŠ¶æ€ |
|---------|---------|------|
| Q1 | Layout åˆ†æï¼šCUTLASS çš„ A/W/Output å¸ƒå±€ | âœ… å·²åˆ†æ |
| Q2 | cuBLASLt T/N+C/C æ ¼å¼ä¸ vLLM çš„å¯¹æ¥ | âœ… å·²åˆ†æ |
| Q3 | Scale ç»´åº¦ä¸åé‡åŒ–æœºåˆ¶ | âœ… å·²åˆ†æ |
| Q4 | Bias å¹¿æ’­æ–¹å‘ | âœ… å·²åˆ†æ |
| Q5 | cuBLASLtMatmul API è°ƒç”¨è¦ç‚¹ | âœ… å·²åˆ†æ |

---

### 11.2 Q1: CUTLASS çš„ A/W/Output Layout åˆ†æ

#### 11.2.1 Safetensor åŸå§‹å­˜å‚¨æ ¼å¼

ä» checkpoint åˆ†æï¼ˆQwen2.5-0.5B-FP8ï¼‰ï¼š

```
Weight åŸå§‹æ ¼å¼:
    down_proj.weight:       [896, 4864]   FP8    â†’ [N, K] è¡Œä¸»åº
    down_proj.weight_scale: [896, 1]      BF16   â†’ [N, 1] per-channel
    gate_proj.weight:       [4864, 896]   FP8    â†’ [N, K] è¡Œä¸»åº
    gate_proj.weight_scale: [4864, 1]     BF16   â†’ [N, 1] per-channel

Bias æ ¼å¼ï¼ˆä»… QKV proj æœ‰ biasï¼‰:
    q_proj.bias: [896]  BF16 â†’ [N] 1D å‘é‡
    k_proj.bias: [128]  BF16 â†’ [N] 1D å‘é‡
```

**å…³é”®å‘ç°**ï¼š
- Weight: `[N, K]` è¡Œä¸»åºï¼ˆN=out_features, K=in_featuresï¼‰
- weight_scale: `[N, 1]` per-channelï¼ˆ**ä¸æ˜¯ `[1, K]`**ï¼Œä½ ä¹‹å‰çš„çŒœæµ‹éœ€è¦ä¿®æ­£ï¼‰
- Bias: `[N]` 1D å‘é‡

#### 11.2.2 vLLM æƒé‡å¤„ç†æµç¨‹

åœ¨ `compressed_tensors_w8a8_fp8.py` çš„ `process_weights_after_loading()` ä¸­ï¼š

```python
# ç¬¬ 145/151 è¡Œï¼šå…³é”®çš„è½¬ç½®æ“ä½œ
if self.strategy == QuantizationStrategy.TENSOR:
    weight, weight_scale, input_scale = process_fp8_weight_tensor_strategy(...)
    weight = weight.t()   # [N, K] â†’ [K, N]

elif self.strategy == QuantizationStrategy.CHANNEL:
    weight, weight_scale, input_scale = process_fp8_weight_channel_strategy(...)
    weight = weight.t()   # [N, K] â†’ [K, N]
```

**è½¬ç½®åçš„æƒé‡æ ¼å¼**ï¼š
- `weight`: `[K, N]`ï¼Œä½†åœ¨ PyTorch ä¸­ `.t()` å **stride å˜åŒ–**
- åŸå§‹ `[N, K]` è¡Œä¸»åºçš„ stride æ˜¯ `(K, 1)`
- `.t()` åå˜æˆ `[K, N]`ï¼Œstride æ˜¯ `(1, K)` â†’ **è¿™å°±æ˜¯åˆ—ä¸»åºï¼**

#### 11.2.3 CUTLASS æœŸæœ›çš„ Layout

ä» `scaled_mm_entry.cu` ç¬¬ 186-188 è¡Œçš„æ£€æŸ¥ï¼š

```cpp
// Check for strides and alignment
TORCH_CHECK(a.stride(1) == 1 && c.stride(1) == 1);  // Row-major
TORCH_CHECK(b.stride(0) == 1);                      // Column-major
```

ä» `scaled_mm.cuh` ç¬¬ 73-75 è¡Œçš„å®šä¹‰ï¼š

```cpp
ElementAB, cutlass::layout::RowMajor, AlignmentAB,     // A: RowMajor
ElementAB, cutlass::layout::ColumnMajor, AlignmentAB,  // B: ColumnMajor
```

**CUTLASS æœŸæœ›çš„è¾“å…¥**ï¼š

| çŸ©é˜µ | å½¢çŠ¶ | Layout | stride | è¯´æ˜ |
|------|------|--------|--------|------|
| A (input) | `[M, K]` | RowMajor | `(K, 1)` | æ¿€æ´»ï¼Œæ¯è¡Œè¿ç»­ |
| B (weight) | `[K, N]` | ColumnMajor | `(1, K)` | æƒé‡ï¼Œæ¯åˆ—è¿ç»­ |
| C (output) | `[M, N]` | RowMajor | `(N, 1)` | è¾“å‡ºï¼Œæ¯è¡Œè¿ç»­ |

**PyTorch è§†è§’**ï¼š
- `A [M, K]` è¡Œä¸»åº â†’ stride `(K, 1)` âœ…
- `B [K, N]` åˆ—ä¸»åº = `[N, K].t()` â†’ stride `(1, K)` âœ…
- `C [M, N]` è¡Œä¸»åº â†’ stride `(N, 1)` âœ…

#### 11.2.4 CUTLASS è®¡ç®—å…¬å¼ç¡®è®¤

```
CUTLASS è®¡ç®—: C[M,N] = A[M,K] Ã— B[K,N]
```

å…¶ä¸­ B æ˜¯åˆ—ä¸»åºå­˜å‚¨çš„ `[K, N]`ï¼Œç­‰ä»·äº PyTorch ä¸­ `weight.t()`ã€‚

---

### 11.3 Q2: cuBLASLt T/N+C/C æ ¼å¼å¯¹æ¥

#### 11.3.1 ä½ çš„éœ€æ±‚ç¡®è®¤

ä½ éœ€è¦ä½¿ç”¨ **W åœ¨å·¦ï¼ŒA åœ¨å³** çš„è®¡ç®—é¡ºåºï¼š
```
cuBLASLt è®¡ç®—: D = W Ã— A^T
```

å¹¶ä¸”è¦æ±‚ **T/N + C/C/C** æ ¼å¼ï¼ˆA è½¬ç½®ï¼ŒB ä¸è½¬ç½®ï¼Œå…¨éƒ¨åˆ—ä¸»åºï¼‰ã€‚

#### 11.3.2 Layout æ¨å¯¼

**è®¾å®š**ï¼š
- PyTorch ä¼ å…¥çš„ `A [M, K]` è¡Œä¸»åºï¼Œstride `(K, 1)`
- PyTorch ä¼ å…¥çš„ `W [K, N]` åˆ—ä¸»åºï¼ˆå³ `[N, K].t()`ï¼‰ï¼Œstride `(1, K)`

**cuBLASLt ç”¨åˆ—ä¸»åºè¯»å–è¡Œä¸»åº = éšå¼è½¬ç½®**ï¼š

| çŸ©é˜µ | PyTorch å­˜å‚¨ | cuBLASLt è¯»å–æ–¹å¼ | å®é™…è¯»åˆ°çš„ |
|------|-------------|------------------|-----------|
| A `[M, K]` row | å†…å­˜: `MÃ—K` è¿ç»­ | åˆ—ä¸»åºè¯» | `A^T [K, M]` |
| W `[K, N]` col | å†…å­˜: `KÃ—N` è¿ç»­ | åˆ—ä¸»åºè¯» | `W [K, N]` |

**è®¡ç®—è¿‡ç¨‹**ï¼š

```
cuBLASLt T/N é…ç½®:
    opA = CUBLAS_OP_T  â†’ å¯¹ "åˆ—ä¸»åºè¯»åˆ°çš„ A^T" å†è½¬ç½® â†’ å¾—åˆ° A [M, K]
    opB = CUBLAS_OP_N  â†’ å¯¹ "åˆ—ä¸»åºè¯»åˆ°çš„ W" ä¸è½¬ç½® â†’ å¾—åˆ° W [K, N]
    
ç­‰ç­‰ï¼Œè¿™å’Œä½ æƒ³è¦çš„ WÃ—A^T ä¸ä¸€æ ·ï¼
```

**é‡æ–°ç†è§£ä½ çš„éœ€æ±‚**ï¼š

ä½ è¯´çš„ "W åœ¨å·¦ï¼ŒA åœ¨å³" æ˜¯æŒ‡ cuBLASLt API çš„å‚æ•°é¡ºåºï¼Œè€Œä¸æ˜¯æ•°å­¦ä¸Šçš„çŸ©é˜µä¹˜æ³•é¡ºåºã€‚

è®©æˆ‘é‡æ–°æ¨å¯¼ï¼š

```
ä½ æƒ³è¦çš„æœ€ç»ˆè®¡ç®—ï¼ˆæ•°å­¦ä¸Šï¼‰: Output[M, N] = A[M, K] Ã— W^T[K, N]

ä½† vLLM ä¼ ç»™ä½ çš„ W å·²ç»æ˜¯ [K, N] åˆ—ä¸»åºäº†ï¼ˆå·²è½¬ç½®è¿‡ï¼‰ï¼

æ‰€ä»¥å®é™…è®¡ç®—: Output[M, N] = A[M, K] Ã— W_transposed[K, N]
```

#### 11.3.3 æ­£ç¡®çš„ cuBLASLt é…ç½®

**æ–¹æ¡ˆï¼šè®© cuBLASLt è®¡ç®— D = A Ã— B**

cuBLASLt é»˜è®¤æ˜¯åˆ—ä¸»åºï¼Œç”¨è¡Œä¸»åºæ•°æ®æ—¶éœ€è¦æŠ€å·§ï¼š

```
A: [M, K] è¡Œä¸»åºï¼Œstride (K, 1)
   â†’ cuBLASLt ç”¨åˆ—ä¸»åºè¯» â†’ è¯»æˆ [K, M]
   â†’ opA = T è½¬ç½®å›æ¥ â†’ å¾—åˆ° [M, K]

B: [K, N] åˆ—ä¸»åºï¼Œstride (1, K)  
   â†’ cuBLASLt ç”¨åˆ—ä¸»åºè¯» â†’ æ­£å¥½è¯»æˆ [K, N]
   â†’ opB = N ä¸è½¬ç½® â†’ å¾—åˆ° [K, N]

D: [M, N] è¡Œä¸»åº
   â†’ cuBLASLt ç”¨åˆ—ä¸»åºå†™ â†’ å†™æˆ [N, M]^T
   â†’ ä½†è¡Œä¸»åºçš„ [M, N] å­˜å‚¨ç­‰äºåˆ—ä¸»åºçš„ [N, M] âœ…
```

**ä½†æ˜¯ä½ è¦æ±‚ "W åœ¨å·¦ï¼ŒA åœ¨å³"ï¼**

è¿™éœ€è¦äº¤æ¢ A å’Œ B çš„ä½ç½®ï¼Œåˆ©ç”¨ `(AÃ—B)^T = B^T Ã— A^T` çš„æ€§è´¨ï¼š

```
cuBLASLt å‚æ•°é¡ºåº: D' = B' Ã— A'  ï¼ˆB'åœ¨å·¦ï¼ŒA'åœ¨å³ï¼‰

è®¾ï¼š
    B' = A^T = [K, M] ï¼ˆç”¨åˆ—ä¸»åºè¯»è¡Œä¸»åºçš„ A[M,K]ï¼‰
    A' = W   = [K, N] ï¼ˆåˆ—ä¸»åºçš„ Wï¼‰
    
é‚£ä¹ˆï¼š
    D' = A^T Ã— W = [K, M]^T Ã— [K, N]  ???  ç»´åº¦ä¸å¯¹ï¼
```

**æ­£ç¡®ç†è§£**ï¼šä½ æƒ³è¦çš„æ˜¯ cuBLASLt è®¡ç®— `D^T = W^T Ã— A^T`ï¼Œç„¶åç»“æœè‡ªåŠ¨å˜æˆ `D`ï¼š

```
å…³ç³»: D = A Ã— W  ç­‰ä»·äº  D^T = W^T Ã— A^T

æ‰€ä»¥:
    opA = T (å¯¹ cuBLAS çš„ç¬¬ä¸€ä¸ªçŸ©é˜µ W)
    opB = T (å¯¹ cuBLAS çš„ç¬¬äºŒä¸ªçŸ©é˜µ A)
    
ä½†è¿™æ˜¯ T/T é…ç½®ï¼Œä¸æ˜¯ä½ è¦çš„ T/Nï¼
```

#### 11.3.4 T/N + C/C/C é…ç½®çš„æ­£ç¡®ç”¨æ³•

è®©æˆ‘é‡æ–°ç†è§£ä½ çš„æ„å›¾ã€‚T/N æ„å‘³ç€ï¼š
- ç¬¬ä¸€ä¸ªçŸ©é˜µå‚æ•°ï¼šè½¬ç½®
- ç¬¬äºŒä¸ªçŸ©é˜µå‚æ•°ï¼šä¸è½¬ç½®

```
cublasLtMatmul çš„æ ‡å‡†è®¡ç®—: D = Î± Ã— op(A) Ã— op(B) + Î² Ã— C

è®¾:
    ç¬¬ä¸€ä¸ªçŸ©é˜µå‚æ•° = W_stored [N, K] è¡Œä¸»åº
    ç¬¬äºŒä¸ªçŸ©é˜µå‚æ•° = A_stored [M, K] è¡Œä¸»åº
    opA = T â†’ W_stored^T = [K, N]
    opB = N â†’ A_stored   = [M, K]  ä½†ç»´åº¦ä¸åŒ¹é…ï¼
```

**é—®é¢˜**ï¼š`op(A) Ã— op(B) = [K, N] Ã— [M, K]` ç»´åº¦ä¸å¯¹ï¼

**æ­£ç¡®æ–¹æ¡ˆ**ï¼šäº¤æ¢è¾“å…¥é¡ºåº

```
è®¾:
    ç¬¬ä¸€ä¸ªçŸ©é˜µå‚æ•° = W_stored [N, K] è¡Œä¸»åºï¼Œä¼ ç»™ cuBLAS æ—¶å‘Šè¯‰å®ƒæ˜¯ [K, N] åˆ—ä¸»åº
    ç¬¬äºŒä¸ªçŸ©é˜µå‚æ•° = A_stored [M, K] è¡Œä¸»åºï¼Œä¼ ç»™ cuBLAS æ—¶å‘Šè¯‰å®ƒæ˜¯ [K, M] åˆ—ä¸»åº
    opA = N â†’ [K, N]
    opB = T â†’ [K, M]^T = [M, K]  ç»´åº¦è¿˜æ˜¯ä¸å¯¹ï¼
```

**æœ€ç»ˆæ­£ç¡®é…ç½®ï¼ˆN/T + C/C/Cï¼‰**ï¼š

```
ç›®æ ‡: D[M, N] = A[M, K] Ã— W[K, N]

cuBLASLt é…ç½®ï¼ˆåˆ©ç”¨è¡Œä¸»åº = åˆ—ä¸»åºè½¬ç½®çš„ç‰¹æ€§ï¼‰:
    å®é™…ä¼ å…¥:
        A_ptr: æŒ‡å‘ W_stored çš„å†…å­˜ï¼ˆè¡Œä¸»åº [N, K]ï¼‰
        B_ptr: æŒ‡å‘ A_stored çš„å†…å­˜ï¼ˆè¡Œä¸»åº [M, K]ï¼‰
        C_ptr/D_ptr: è¾“å‡ºå†…å­˜
    
    å‘Šè¯‰ cuBLASLtï¼ˆåˆ—ä¸»åºè§†è§’ï¼‰:
        A: [K, N] åˆ—ä¸»åºï¼ˆå®é™…æ˜¯è¡Œä¸»åº [N, K] çš„å¦ä¸€ç§è§£è¯»ï¼‰
        B: [K, M] åˆ—ä¸»åºï¼ˆå®é™…æ˜¯è¡Œä¸»åº [M, K] çš„å¦ä¸€ç§è§£è¯»ï¼‰
        opA = N â†’ A ä¸å˜ï¼Œ[K, N]
        opB = T â†’ B è½¬ç½®ï¼Œ[K, M]^T = [M, K]
        
    è®¡ç®—: D = op(A) Ã— op(B) = [K, N] Ã— [M, K]  ç»´åº¦è¿˜æ˜¯ä¸å¯¹ï¼
```

**æˆ‘æ˜ç™½äº†ï¼ä½ éœ€è¦çš„æ˜¯ D^T çš„è®¡ç®—**ï¼š

```
ç›®æ ‡: D[M, N] = A[M, K] Ã— W[K, N]
ç­‰ä»·: D^T[N, M] = W^T[N, K] Ã— A^T[K, M]

cuBLASLt é…ç½®:
    ä¼ å…¥:
        A_ptr â†’ W_stored [N, K] è¡Œä¸»åº â†’ cuBLAS åˆ—ä¸»åºè¯»ä¸º [K, N]
        B_ptr â†’ A_stored [M, K] è¡Œä¸»åº â†’ cuBLAS åˆ—ä¸»åºè¯»ä¸º [K, M]
        
    opA = T â†’ [K, N]^T = [N, K]
    opB = N â†’ [K, M]
    
    è®¡ç®—: D' = op(A) Ã— op(B) = [N, K] Ã— [K, M] = [N, M] âœ…
    
    è¾“å‡º:
        D_ptr â†’ åˆ—ä¸»åºå†™ [N, M] â†’ è¡Œä¸»åºè¯»ä¸º [M, N] âœ…
```

**æœ€ç»ˆç­”æ¡ˆ**ï¼š

```cpp
// cuBLASLt T/N + Col/Col/Col é…ç½®
cublasOperation_t opA = CUBLAS_OP_T;   // å¯¹ Wï¼ˆç¬¬ä¸€ä¸ªå‚æ•°ï¼‰è½¬ç½®
cublasOperation_t opB = CUBLAS_OP_N;   // å¯¹ Aï¼ˆç¬¬äºŒä¸ªå‚æ•°ï¼‰ä¸è½¬ç½®

// çŸ©é˜µå¸ƒå±€
// W: è¡Œä¸»åº [N, K]ï¼Œä¼ ç»™ cuBLASLt å£°æ˜ä¸ºåˆ—ä¸»åº [K, N]ï¼Œlda = K
// A: è¡Œä¸»åº [M, K]ï¼Œä¼ ç»™ cuBLASLt å£°æ˜ä¸ºåˆ—ä¸»åº [K, M]ï¼Œldb = K
// D: åˆ—ä¸»åºå†™å‡º [N, M]ï¼Œç­‰äºè¡Œä¸»åº [M, N]ï¼Œldc = N

// è®¡ç®—: D' = W^T Ã— A = [K, N]^T Ã— [K, M] = [N, K] Ã— [K, M] = [N, M]
// è¾“å‡º: åˆ—ä¸»åº [N, M] = è¡Œä¸»åº [M, N] âœ…
```

---

### 11.4 Q3: Scale ç»´åº¦ä¸åé‡åŒ–æœºåˆ¶

#### 11.4.1 å®é™…çš„ Scale ç»´åº¦ï¼ˆä» checkpoint ç¡®è®¤ï¼‰

```
scale_a (input_scale):  per-token dynamic â†’ [M, 1] FP32
scale_b (weight_scale): per-channel       â†’ [N, 1] FP32 (ä¸æ˜¯ [1, K]!)
```

**ä½ ä¹‹å‰çš„çŒœæµ‹éœ€è¦ä¿®æ­£**ï¼šweight_scale æ˜¯ `[N, 1]` ä¸æ˜¯ `[1, K]`ã€‚

#### 11.4.2 CUTLASS çš„åé‡åŒ–å…¬å¼

ä» `scaled_mm_epilogues_c3x.hpp` åˆ†æï¼š

```cpp
// ScaledEpilogue çš„è®¡ç®—
using ScaleA = ColOrScalarLoad<float>;  // åˆ—æ–¹å‘åŠ è½½ â†’ [M, 1] å¹¿æ’­åˆ° [M, N]
using ScaleB = RowOrScalarLoad<float>;  // è¡Œæ–¹å‘åŠ è½½ â†’ [N, 1].T = [1, N] å¹¿æ’­åˆ° [M, N]

// EVTCompute0: tmp = ScaleB Ã— Accum
// EVTCompute1: D = ScaleA Ã— tmp

// å±•å¼€: D = ScaleA Ã— (ScaleB Ã— Accum)
//         = ScaleA[M,1] âŠ— (ScaleB[1,N] âŠ— Accum[M,N])
//         = (ScaleA[M,1] âŠ— ScaleB[1,N]) âŠ— Accum[M,N]  (å¹¿æ’­é€å…ƒç´ ä¹˜)
```

**CUTLASS åé‡åŒ–å…¬å¼**ï¼š

```
D[M,N] = scale_a[M,1] âŠ™ scale_b[1,N] âŠ™ (qA[M,K] Ã— qW[K,N])

å…¶ä¸­:
    qA: é‡åŒ–åçš„æ¿€æ´» [M, K] FP8
    qW: é‡åŒ–åçš„æƒé‡ [K, N] FP8  
    scale_a: [M, 1] å¹¿æ’­åˆ° [M, N]
    scale_b: [N, 1]^T = [1, N] å¹¿æ’­åˆ° [M, N]
    âŠ™: å¹¿æ’­é€å…ƒç´ ä¹˜æ³•
```

#### 11.4.3 cuBLASLt çš„ Outer Vector Scalingï¼ˆSM90+ï¼‰

ä»å®˜æ–¹æ–‡æ¡£ 3.1.4.3 èŠ‚ï¼š

```
Outer Vector Scaling for FP8 Data Types:

D_ij = Î± Ã— scale_A^i Ã— scale_B^j Ã— Î£(a_il Ã— b_lj) + Î² Ã— scale_C Ã— C_ij

å…¶ä¸­:
    scale_A: é•¿åº¦ä¸º M çš„å‘é‡ï¼Œæ¯è¡Œä¸€ä¸ª scale
    scale_B: é•¿åº¦ä¸º N çš„å‘é‡ï¼Œæ¯åˆ—ä¸€ä¸ª scale
```

**å¯ç”¨æ–¹æ³•**ï¼š

```cpp
// è®¾ç½® outer vector scaling mode
int32_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F;
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode));
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode));

// è®¾ç½® scale æŒ‡é’ˆ
float* scaleA = ...;  // é•¿åº¦ M
float* scaleB = ...;  // é•¿åº¦ N
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &scaleA, sizeof(scaleA));
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &scaleB, sizeof(scaleB));
```

#### 11.4.4 Scale é€‚é…é—®é¢˜

**CUTLASS çš„ scale è¯­ä¹‰**ï¼ˆAåœ¨å·¦ï¼‰ï¼š
- scale_a: å¯¹åº” A (input)ï¼Œç»´åº¦ [M, 1]
- scale_b: å¯¹åº” B (weight)ï¼Œç»´åº¦ [N, 1]

**cuBLASLt çš„ scale è¯­ä¹‰ï¼ˆWåœ¨å·¦ï¼ŒAåœ¨å³ï¼‰**ï¼š

ç”±äºæˆ‘ä»¬äº¤æ¢äº† A å’Œ B çš„ä½ç½®ï¼ˆW ä½œä¸º cuBLASLt çš„ç¬¬ä¸€ä¸ªå‚æ•°ï¼‰ï¼Œéœ€è¦ç›¸åº”è°ƒæ•´ï¼š

```
cuBLASLt è®¡ç®—: D'[N, M] = W'[N, K] Ã— A'[K, M]

å…¶ä¸­:
    W' = W^T (é€šè¿‡ opA=T å®ç°)
    A' = A^T (é€šè¿‡åˆ—ä¸»åºè¯»è¡Œä¸»åºå®ç°)

scale å¯¹åº”:
    cuBLASLt çš„ scale_A â†’ å¯¹åº” W â†’ ç»´åº¦ [N] (å› ä¸º op(W) çš„è¡Œæ•°æ˜¯ N)
    cuBLASLt çš„ scale_B â†’ å¯¹åº” A â†’ ç»´åº¦ [M] (å› ä¸º op(A) çš„åˆ—æ•°æ˜¯ M)
```

**å…³é”®é€‚é…**ï¼šéœ€è¦äº¤æ¢ä¼ å…¥ cuBLASLt çš„ scaleï¼š

```python
# vLLM ä¼ æ¥çš„:
#   scale_a: [M, 1] â†’ å¯¹åº” input
#   scale_b: [N, 1] â†’ å¯¹åº” weight

# ä¼ ç»™ cuBLASLt (Wåœ¨å·¦):
#   cublaslt_scale_A â†’ scale_b.squeeze() â†’ [N]  (weight scale)
#   cublaslt_scale_B â†’ scale_a.squeeze() â†’ [M]  (input scale)
```

---

### 11.5 Q4: Bias å¹¿æ’­æ–¹å‘

#### 11.5.1 Bias çš„å­˜å‚¨æ ¼å¼

ä» checkpoint ç¡®è®¤ï¼š
```
bias: [N] 1D å‘é‡ï¼ˆN = out_featuresï¼‰
```

#### 11.5.2 CUTLASS çš„ Bias å¤„ç†

ä» `scaled_mm_epilogues_c3x.hpp`ï¼š

```cpp
// ScaledEpilogueBias ä¸­
using Bias = RowLoad<ElementD>;  // è¡Œæ–¹å‘åŠ è½½

// è®¡ç®—å…¬å¼:
// D = ScaleA Ã— (ScaleB Ã— Accum) + Bias
// å…¶ä¸­ Bias æ˜¯ RowLoadï¼Œå¹¿æ’­åˆ°æ¯ä¸€è¡Œ
```

**Bias å¹¿æ’­æ–¹å‘**ï¼š`[1, N]` å¹¿æ’­åˆ° `[M, N]`ï¼Œå³**æ²¿ N ç»´åº¦ï¼ˆåˆ—æ–¹å‘ï¼‰å¹¿æ’­**ã€‚

æ¯ä¸€è¡Œï¼ˆæ¯ä¸ª tokenï¼‰åŠ ä¸Šç›¸åŒçš„ bias å‘é‡ã€‚

#### 11.5.3 cuBLASLt çš„ Bias å¤„ç†

```cpp
// è®¾ç½® epilogue ä¸º BIAS
cublasLtEpilogue_t epilogue = CUBLASLT_EPILOGUE_BIAS;
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_EPILOGUE, &epilogue, sizeof(epilogue));

// è®¾ç½® bias æŒ‡é’ˆå’Œç±»å‹
void* biasPtr = ...;  // [N] å‘é‡
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_BIAS_POINTER, &biasPtr, sizeof(biasPtr));

cudaDataType_t biasType = CUDA_R_16BF;  // BF16
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE, &biasType, sizeof(biasType));
```

**æ³¨æ„**ï¼šç”±äºæˆ‘ä»¬è®¡ç®—çš„æ˜¯ `D'[N, M]` ç„¶åå­˜å‚¨ä¸ºè¡Œä¸»åº `[M, N]`ï¼Œbias çš„å¹¿æ’­æ–¹å‘ä¹Ÿéœ€è¦è€ƒè™‘ã€‚

å®é™…ä¸Šï¼ŒcuBLASLt çš„ bias æ˜¯åŠ åœ¨è¾“å‡ºçŸ©é˜µçš„**åˆ—æ–¹å‘**ï¼ˆå› ä¸ºå®ƒç”¨åˆ—ä¸»åºï¼‰ã€‚
- è¾“å‡º `D'[N, M]` åˆ—ä¸»åº
- bias `[N]` åŠ åˆ°æ¯ä¸€åˆ—
- ç­‰ä»·äºè¡Œä¸»åº `D[M, N]` ä¸­ï¼Œbias `[N]` åŠ åˆ°æ¯ä¸€è¡Œ âœ…

---

### 11.6 Q5: cuBLASLtMatmul API è°ƒç”¨è¦ç‚¹

#### 11.6.1 å®Œæ•´çš„ API è°ƒç”¨æ¡†æ¶

```cpp
#include <cublasLt.h>

cublasStatus_t cublaslt_fp8_gemm_impl(
    cublasLtHandle_t handle,
    int M, int N, int K,
    const void* W_ptr,        // è¡Œä¸»åº [N, K] FP8
    const void* A_ptr,        // è¡Œä¸»åº [M, K] FP8
    void* D_ptr,              // è¡Œä¸»åº [M, N] BF16
    const float* scale_w,     // [N] weight scale
    const float* scale_a,     // [M] input scale
    const void* bias,         // [N] bias (å¯é€‰)
    cudaDataType_t biasType,
    cudaStream_t stream
) {
    // 1. åˆ›å»º matmul æè¿°ç¬¦
    cublasLtMatmulDesc_t matmulDesc;
    cublasComputeType_t computeType = CUBLAS_COMPUTE_32F;
    cublasLtMatmulDescCreate(&matmulDesc, computeType, CUDA_R_32F);
    
    // 2. è®¾ç½®è½¬ç½®æ“ä½œ
    cublasOperation_t opA = CUBLAS_OP_T;   // W è½¬ç½®
    cublasOperation_t opB = CUBLAS_OP_N;   // A ä¸è½¬ç½®
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_TRANSA, &opA, sizeof(opA));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_TRANSB, &opB, sizeof(opB));
    
    // 3. è®¾ç½® outer vector scaling (SM90+)
    int8_t fastAccuMode = 1;
    cublasLtMatmulDescSetAttribute(matmulDesc,
        CUBLASLT_MATMUL_DESC_FAST_ACCUM, &fastAccuMode, sizeof(fastAccuMode));
    
    // Scale æ¨¡å¼å’ŒæŒ‡é’ˆ
    int32_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F;
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &scale_w, sizeof(scale_w));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &scale_a, sizeof(scale_a));
    
    // 4. è®¾ç½® Bias (å¦‚æœæœ‰)
    if (bias != nullptr) {
        cublasLtEpilogue_t epilogue = CUBLASLT_EPILOGUE_BIAS;
        cublasLtMatmulDescSetAttribute(matmulDesc, 
            CUBLASLT_MATMUL_DESC_EPILOGUE, &epilogue, sizeof(epilogue));
        cublasLtMatmulDescSetAttribute(matmulDesc, 
            CUBLASLT_MATMUL_DESC_BIAS_POINTER, &bias, sizeof(bias));
        cublasLtMatmulDescSetAttribute(matmulDesc, 
            CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE, &biasType, sizeof(biasType));
    }
    
    // 5. åˆ›å»ºçŸ©é˜µå¸ƒå±€
    // W: è¡Œä¸»åº [N, K] â†’ å£°æ˜ä¸ºåˆ—ä¸»åº [K, N]ï¼Œlda = K
    cublasLtMatrixLayout_t Adesc;
    cublasLtMatrixLayoutCreate(&Adesc, CUDA_R_8F_E4M3, K, N, K);
    
    // A: è¡Œä¸»åº [M, K] â†’ å£°æ˜ä¸ºåˆ—ä¸»åº [K, M]ï¼Œldb = K
    cublasLtMatrixLayout_t Bdesc;
    cublasLtMatrixLayoutCreate(&Bdesc, CUDA_R_8F_E4M3, K, M, K);
    
    // D: åˆ—ä¸»åº [N, M]ï¼Œldc = N â†’ è¯»ä¸ºè¡Œä¸»åº [M, N]
    cublasLtMatrixLayout_t Ddesc;
    cublasLtMatrixLayoutCreate(&Ddesc, CUDA_R_16BF, N, M, N);
    
    // 6. è·å–æœ€ä¼˜ç®—æ³•
    cublasLtMatmulPreference_t preference;
    cublasLtMatmulPreferenceCreate(&preference);
    
    size_t workspaceSize = 64 * 1024 * 1024;  // 64 MB
    cublasLtMatmulPreferenceSetAttribute(preference, 
        CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES, &workspaceSize, sizeof(workspaceSize));
    
    cublasLtMatmulHeuristicResult_t heuristicResult;
    int returnedResults = 0;
    cublasLtMatmulAlgoGetHeuristic(handle, matmulDesc, 
        Adesc, Bdesc, Ddesc, Ddesc,
        preference, 1, &heuristicResult, &returnedResults);
    
    // 7. åˆ†é… workspace
    void* workspace = nullptr;
    cudaMalloc(&workspace, heuristicResult.workspaceSize);
    
    // 8. æ‰§è¡Œ GEMM
    float alpha = 1.0f, beta = 0.0f;
    cublasLtMatmul(
        handle, matmulDesc,
        &alpha,
        W_ptr, Adesc,   // ç¬¬ä¸€ä¸ªçŸ©é˜µ: W
        A_ptr, Bdesc,   // ç¬¬äºŒä¸ªçŸ©é˜µ: A
        &beta,
        D_ptr, Ddesc,   // C (unused, beta=0)
        D_ptr, Ddesc,   // D (output)
        &heuristicResult.algo,
        workspace, heuristicResult.workspaceSize,
        stream
    );
    
    // 9. æ¸…ç†
    cudaFree(workspace);
    cublasLtMatmulPreferenceDestroy(preference);
    cublasLtMatrixLayoutDestroy(Adesc);
    cublasLtMatrixLayoutDestroy(Bdesc);
    cublasLtMatrixLayoutDestroy(Ddesc);
    cublasLtMatmulDescDestroy(matmulDesc);
    
    return CUBLAS_STATUS_SUCCESS;
}
```

#### 11.6.2 å…³é”®æ³¨æ„äº‹é¡¹

| è¦ç‚¹ | è¯´æ˜ |
|------|------|
| **çŸ©é˜µå‚æ•°é¡ºåº** | ç¬¬ä¸€ä¸ªæ˜¯ Wï¼ˆweightï¼‰ï¼Œç¬¬äºŒä¸ªæ˜¯ Aï¼ˆactivationï¼‰ |
| **op é…ç½®** | opA=Tï¼ˆè½¬ç½® Wï¼‰ï¼ŒopB=Nï¼ˆA ä¸è½¬ç½®ï¼‰ |
| **Layout å£°æ˜** | è¡Œä¸»åºæ•°æ®å£°æ˜ä¸ºåˆ—ä¸»åºï¼Œç»´åº¦äº¤æ¢ |
| **Scale äº¤æ¢** | cuBLASLt çš„ scale_A â†’ weight_scaleï¼Œscale_B â†’ input_scale |
| **Bias ç±»å‹** | éœ€è¦ä¸è¾“å‡ºç±»å‹åŒ¹é…æˆ–å…¼å®¹ |
| **Workspace** | é¢„åˆ†é…è¶³å¤Ÿç©ºé—´ï¼Œæ¨è 64MB |
| **Handle å¤ç”¨** | å…¨å±€ç¼“å­˜ handleï¼Œé¿å…é‡å¤åˆ›å»º |
| **Algorithm ç¼“å­˜** | ç›¸åŒé—®é¢˜è§„æ¨¡å¯å¤ç”¨å¯å‘å¼ç»“æœ |

#### 11.6.3 Python ä¾§é€‚é…

åœ¨ `cublaslt_w8a8_scaled_mm` ä¸­ï¼š

```python
def cublaslt_w8a8_scaled_mm(
    *,
    qinput: torch.Tensor,     # [M, K] FP8 è¡Œä¸»åº
    weight: torch.Tensor,     # [K, N] FP8 "åˆ—ä¸»åº"ï¼ˆå®é™…æ˜¯ .t() åçš„ viewï¼‰
    out_dtype: torch.dtype,
    scale_a: torch.Tensor,    # [M, 1] input scale
    scale_b: torch.Tensor,    # [N, 1] weight scale
    bias: torch.Tensor,       # [N] æˆ– None
    output_shape: list,
    **kwargs,
) -> torch.Tensor:
    """
    cuBLASLt FP8 Scaled MM
    
    å…³é”®ç†è§£ï¼š
    - vLLM ä¼ æ¥çš„ weight æ˜¯ [K,N] ä½† stride=(1,K)ï¼Œæ˜¯ .t() åçš„ view
    - ç‰©ç†å†…å­˜å®é™…æ˜¯ [N,K] è¡Œä¸»åºå­˜å‚¨
    - æˆ‘ä»¬éœ€è¦å† .t() æ¶ˆé™¤è¿™ä¸ªå‡è½¬ç½®ï¼Œè®© stride å’Œç‰©ç†å†…å­˜ä¸€è‡´
    """
    M, K = qinput.shape
    N = weight.shape[1]  # weight å½“å‰ shape æ˜¯ [K, N]
    
    # å…³é”®ï¼šæ¶ˆé™¤ .t() é€ æˆçš„ stride ä¸ä¸€è‡´
    # weight.t() å°† [K,N] stride=(1,K) å˜å› [N,K] stride=(K,1)
    # è¿™æ · stride å°±å’Œç‰©ç†å†…å­˜å¸ƒå±€ï¼ˆè¡Œä¸»åº [N,K]ï¼‰ä¸€è‡´äº†
    weight_row_major = weight.t()  # [N, K] è¡Œä¸»åºï¼Œæ— éœ€ contiguousï¼ˆæœ¬èº«å°±æ˜¯è¿ç»­çš„ï¼‰
    
    # è°ƒç”¨ cuBLASLt (æ³¨æ„ scale é¡ºåºäº¤æ¢)
    output = ops.cublaslt_scaled_mm(
        W=weight_row_major,        # [N, K] è¡Œä¸»åº
        A=qinput,                  # [M, K] è¡Œä¸»åº
        scale_W=scale_b.squeeze(), # [N] weight scale
        scale_A=scale_a.squeeze(), # [M] input scale
        bias=bias,                 # [N] bias
        out_dtype=out_dtype,
    )
    
    return output.view(*output_shape)
```

#### 11.6.4 å…³äº Bias å¹¿æ’­æ–¹å‘çš„æ¾„æ¸…

**cuBLASLt è®¡ç®—æµç¨‹**ï¼š

```
1. è¾“å…¥:
   W: [N,K] è¡Œä¸»åº â†’ cuBLASLt åˆ—ä¸»åºè¯»ä¸º [K,N]
   A: [M,K] è¡Œä¸»åº â†’ cuBLASLt åˆ—ä¸»åºè¯»ä¸º [K,M]

2. è®¡ç®—:
   opA=T: [K,N]^T = [N,K]
   opB=N: [K,M]
   D' = [N,K] Ã— [K,M] = [N,M]  (åˆ—ä¸»åºç»“æœ)

3. Bias å¹¿æ’­:
   bias: [N] å‘é‡
   åœ¨åˆ—ä¸»åº [N,M] ä¸­ï¼Œbias åŠ åˆ°"æ¯ä¸€åˆ—"ï¼ˆåˆ—ä¸»åºè§†è§’ï¼‰
   å³: D'[i,j] += bias[i], å¯¹æ‰€æœ‰ iâˆˆ[0,N), jâˆˆ[0,M)
   
4. è¾“å‡º:
   åˆ—ä¸»åº [N,M] å†™å…¥å†…å­˜
   æŒ‰è¡Œä¸»åºè§£è¯» = [M,N] âœ…
   
   ä»è¡Œä¸»åº [M,N] è§†è§’çœ‹:
   D[j,i] = D'[i,j] åŒ…å«äº† bias[i]
   å³æ¯ä¸€è¡Œ j çš„ç¬¬ i åˆ—éƒ½åŠ äº† bias[i]
   è¿™å°±æ˜¯"bias æ²¿ N ç»´åº¦å¹¿æ’­"çš„æ­£ç¡®è¡Œä¸º âœ…
```

**æ€»ç»“**ï¼š
- bias `[N]` åœ¨ cuBLASLt ä¸­ä¼šè‡ªåŠ¨æ­£ç¡®å¹¿æ’­
- æ— éœ€é¢å¤–å¤„ç†ï¼Œç›´æ¥ä¼ å…¥å³å¯

---

## 12. cuSPARSELt FP8 Linear æ–¹æ³•å¼€å‘è§„åˆ’

æœ¬ç« è¯¦ç»†åˆ†æ Phase 4-6 çš„ cuSPARSELt FP8 Linear æ–¹æ³•å¼€å‘ï¼ŒåŒ…æ‹¬ç¦»çº¿å·¥å…·é“¾ã€æ¨¡å‹åŠ è½½å’Œåœ¨çº¿ Kernel æ›¿æ¢ã€‚

### 12.1 æ•´ä½“æ¶æ„æ¦‚è§ˆ

cuSPARSELt FP8 Linear çš„å®ç°æ¶‰åŠä¸‰ä¸ªå…³é”®é˜¶æ®µï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    cuSPARSELt FP8 Linear ç«¯åˆ°ç«¯æµç¨‹                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 4: ç¦»çº¿å·¥å…·é“¾ (Offline)
â”œâ”€â”€ Prune:    Dense [N,K] â†’ Sparse [N,K] (æ»¡è¶³ Z:L ç¨€ç–çº¦æŸ)
â”œâ”€â”€ Slide:    Sparse [N,K] â†’ Slided [N,K'] (K' = K Ã— expand_ratio)
â”œâ”€â”€ Compress: Slided [N,K'] â†’ Compressed [N,K'/2] + Meta [N,K'/8]
â””â”€â”€ Search:   ç¦»çº¿æœç´¢æœ€ä¼˜ cuSPARSELt algo_id

Phase 5: æ¨¡å‹åŠ è½½ (Online Loader)
â”œâ”€â”€ ç¯å¢ƒå˜é‡æ£€æµ‹ (USE_CUSPARSELT=1)
â”œâ”€â”€ æ ¹æ®é…ç½®é€‰æ‹© checkpoints_slidesparse ç›®å½•
â””â”€â”€ DefaultModelLoader å®½å®¹åŠ è½½ï¼ˆshape åŒ¹é…å³å¯ï¼‰

Phase 6: åœ¨çº¿æ¨ç† (Online Inference)
â”œâ”€â”€ Quant+Slide:  BF16 [M,K] â†’ FP8 [M,K'] (Triton fused kernel)
â”œâ”€â”€ Sparse GEMM:  FP8 [M,K'] Ã— Compressed [N,K'/2] â†’ BF16 [M,N]
â””â”€â”€ Dequant+Bias: èåˆåœ¨ cuSPARSELt epilogue æˆ– Triton kernel
```

### 12.2 Phase 4ï¼šç¦»çº¿å·¥å…·é“¾å®Œæ•´æµç¨‹

#### 12.2.1 å½“å‰å·²æœ‰è„šæœ¬åˆ†æ

å½“å‰ `slidesparse/weight_convert/` ç›®å½•ä¸‹å·²æœ‰å¤šä¸ªç‹¬ç«‹è„šæœ¬ï¼š

| è„šæœ¬ | åŠŸèƒ½ | çŠ¶æ€ |
|------|------|------|
| `convert_checkpoint_quant_prune_to_Z_L.py` | é‡åŒ– + Z:L å‰ªæï¼ˆmagnitude/randomï¼‰ | âœ… å·²å®Œæˆ |
| `convert_weights_slidesparse.py` | Slide æ»‘åŠ¨æ‹“å±• + è´ªå©ªæ®‹å·®åˆ†é… | âœ… å·²å®Œæˆ |
| `convert_checkpoint_compress.py` | cuSPARSELt 2:4 å‹ç¼© | âœ… å·²å®Œæˆ |
| `convert_safetensors.py` | æ ¼å¼è½¬æ¢å·¥å…· | âœ… å·²å®Œæˆ |

**å…³é”®è„šæœ¬åŠŸèƒ½è¯´æ˜**ï¼š

1. **`convert_checkpoint_quant_prune_to_Z_L.py`**ï¼ˆç¬¬ 118-178 è¡Œï¼‰ï¼š
   - è¾“å…¥ï¼šBF16 Dense æƒé‡
   - è¾“å‡ºï¼šæŒ‡å®š dtypeï¼ˆFP8 E4M3/E5M2ã€INT8 ç­‰ï¼‰çš„ Z:L ç¨€ç–æƒé‡
   - æ ¸å¿ƒå‡½æ•°ï¼š`quant_and_prune_tensor()` å®ç°é‡åŒ– + å‰ªæèåˆ
   - å‰ªæç­–ç•¥ï¼š`magnitude`ï¼ˆæŒ‰ç»å¯¹å€¼å¤§å°ï¼‰æˆ– `random`ï¼ˆéšæœºï¼‰

2. **`convert_weights_slidesparse.py`**ï¼ˆç¬¬ 310-424 è¡Œï¼‰ï¼š
   - è¾“å…¥ï¼šZ:L ç¨€ç–æƒé‡ `[N, K]`
   - è¾“å‡ºï¼šSlided æƒé‡ `[N, K']`ï¼Œæ»¡è¶³ 2:4 ç¨€ç–çº¦æŸ
   - æ ¸å¿ƒå‡½æ•°ï¼š`slide_weight_tensor()` å®ç°è´ªå©ªæ®‹å·®åˆ†é…
   - éªŒè¯å‡½æ•°ï¼š`verify_2to4_sparsity()` éªŒè¯è¾“å‡ºæ»¡è¶³ 2:4 çº¦æŸ

3. **`convert_checkpoint_compress.py`**ï¼š
   - è¾“å…¥ï¼šSlided æƒé‡ `[N, K']`
   - è¾“å‡ºï¼šCompressed æƒé‡ `[N, K'/2]` + Meta `[N, K'/8]`
   - è°ƒç”¨ cuSPARSELt çš„ `cusparseLtSpMMACompress()` API

#### 12.2.2 TODOï¼šå®Œæ•´æµç¨‹è„šæœ¬

**å½“å‰ç¼ºå¤±**ï¼šç¼ºå°‘ä¸€ä¸ªç«¯åˆ°ç«¯çš„å®Œæ•´æµç¨‹è„šæœ¬ï¼Œèƒ½å¤Ÿä» HuggingFace çš„ safetensor æ ¼å¼ï¼ˆcompressed-tensor é‡åŒ–æ ¼å¼ï¼‰å‡ºå‘ï¼Œå®Œæˆ prune â†’ slide â†’ compress â†’ ä¿å­˜ä¸ºæ–°çš„ safetensor æ ¼å¼ã€‚

**æ¨èå®ç°æ–¹æ¡ˆ**ï¼šåˆ›å»º `preprocess_safetensor_slidesparse.py`

```
è¾“å…¥:
  - HuggingFace æ¨¡å‹è·¯å¾„ (å¦‚ Qwen2.5-7B-Instruct-FP8-dynamic)
  - ç¨€ç–é…ç½® (Z=2, L=8)
  - å‰ªææ¨¡å¼ (magnitude/random)

å¤„ç†æµç¨‹:
  1. åŠ è½½åŸå§‹ .safetensors æ–‡ä»¶
  2. å¯¹ç›®æ ‡å±‚ (wqkv, w13, w2, wo) æ‰§è¡Œ:
     a. Prune (å¦‚éœ€è¦)
     b. Slide (æ»‘åŠ¨æ‹“å±•)
     c. Compress (cuSPARSELt å‹ç¼©)
  3. ä¿å­˜å¤„ç†åçš„æƒé‡åˆ°æ–°ç›®å½•

è¾“å‡ºç›®å½•ç»“æ„:
  ./checkpoints_slidesparse/model-name/
  â”œâ”€â”€ model.safetensors          # å‹ç¼©åçš„æƒé‡
  â”œâ”€â”€ config.json                # åŸå§‹é…ç½®ï¼ˆå¤åˆ¶ï¼‰
  â”œâ”€â”€ tokenizer.json             # åŸå§‹åˆ†è¯å™¨ï¼ˆå¤åˆ¶ï¼‰
  â””â”€â”€ slidesparse_config.json    # SlideSparse å…ƒæ•°æ®
      â”œâ”€â”€ sparsity: "2:8"
      â”œâ”€â”€ expand_ratio: 1.5
      â””â”€â”€ layer_info: {...}
```

**å®ç°è¦ç‚¹**ï¼š

1. **å¤ç”¨ç°æœ‰è„šæœ¬çš„æ ¸å¿ƒå‡½æ•°**ï¼š
   - ä» `convert_checkpoint_quant_prune_to_Z_L.py` å¯¼å…¥ `quant_and_prune_tensor()`
   - ä» `convert_weights_slidesparse.py` å¯¼å…¥ `slide_weight_tensor()`ã€`verify_2to4_sparsity()`

2. **é€‚é… compressed-tensor æ ¼å¼**ï¼š
   - compressed-tensor æ ¼å¼çš„ FP8 æƒé‡å·²ç»æ˜¯é‡åŒ–å¥½çš„
   - åªéœ€è¦å¯¹æƒé‡æ‰§è¡Œ slide + compressï¼Œä¸éœ€è¦å†é‡åŒ–

3. **ä¿æŒ key åç§°ä¸å˜**ï¼š
   - `model.layers.0.self_attn.q_proj.weight` ä¿æŒä¸å˜
   - åªæ”¹å˜ shapeï¼š`[N, K]` â†’ `[N, K'/2]`

#### 12.2.3 å…³é”®ç»´åº¦å˜åŒ–åˆ†æ

ä»¥ 2:8 ç¨€ç–ä¸ºä¾‹ï¼ˆ`Z=2, L=8, expand_ratio=1.5`ï¼‰ï¼š

| é˜¶æ®µ | æƒé‡ Shape | è¯´æ˜ |
|------|-----------|------|
| åŸå§‹ Dense | `[N, K]` | FP8 E4M3 |
| Prune å | `[N, K]` | æ»¡è¶³ 2:8 ç¨€ç–ï¼ˆæ¯ 8 ä¸ªè‡³å°‘ 2 ä¸ªé›¶ï¼‰ |
| Slide å | `[N, KÃ—1.5]` | æ»¡è¶³ 2:4 ç¨€ç–ï¼ˆæ¯ 4 ä¸ªè‡³å°‘ 2 ä¸ªé›¶ï¼‰ |
| Compress å | `[N, KÃ—0.75]` | 2:4 å‹ç¼©å‡åŠï¼ˆKÃ—1.5/2 = KÃ—0.75ï¼‰ |
| Meta | `[N, KÃ—1.5/8]` | ç¨€ç–å…ƒæ•°æ® |

**ç¤ºä¾‹**ï¼ˆQwen2.5-7B, hidden_dim=4096ï¼‰ï¼š
- åŸå§‹ï¼š`[4096, 4096]`
- Slide åï¼š`[4096, 6144]`
- Compress åï¼š`[4096, 3072]`
- Metaï¼š`[4096, 768]`

---

### 12.3 Phase 5ï¼šæ¨¡å‹åŠ è½½æ–¹æ¡ˆåˆ†æ

#### 12.3.1 éœ€æ±‚åˆ†æ

å½“å¯ç”¨ SlideSparse cuSPARSELt è·¯å¾„æ—¶ï¼ˆ`USE_CUSPARSELT=1`ï¼‰ï¼Œéœ€è¦åŠ è½½ç»è¿‡ç¦»çº¿å¤„ç†çš„æƒé‡ã€‚æ ¸å¿ƒéœ€æ±‚ï¼š

1. **è·¯å¾„é€‰æ‹©**ï¼šæ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©ä¸åŒçš„ checkpoints ç›®å½•
2. **Shape åŒ¹é…**ï¼š`create_weights()` å®šä¹‰çš„å‚æ•° shape éœ€è¦ä¸ safetensor ä¸­çš„ shape ä¸€è‡´
3. **æœ€å°ä¾µå…¥**ï¼šä¸ä¿®æ”¹ vLLM çš„ Loader æ ¸å¿ƒé€»è¾‘

#### 12.3.2 æ¨èæ–¹æ¡ˆ

**æ–¹æ¡ˆä¸€ï¼šç¯å¢ƒå˜é‡æ§åˆ¶ model_path å‰ç¼€ï¼ˆæ¨èï¼‰**

æœ€ç®€å•çš„æ–¹æ¡ˆæ˜¯åœ¨å¯åŠ¨æ—¶é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶æŒ‡å®š checkpoint è·¯å¾„ï¼š

```bash
# ä½¿ç”¨ cuBLASLt/CUTLASSï¼ˆdense æƒé‡ï¼‰
DISABLE_SLIDESPARSE=0 USE_CUBLASLT=1 vllm serve ./checkpoints/Qwen2.5-7B-FP8

# ä½¿ç”¨ cuSPARSELtï¼ˆslidesparse æƒé‡ï¼‰
DISABLE_SLIDESPARSE=0 USE_CUSPARSELT=1 vllm serve ./checkpoints_slidesparse/Qwen2.5-7B-FP8-slidesparse
```

**ä¼˜ç‚¹**ï¼šé›¶ä»£ç ä¿®æ”¹ï¼Œå®Œå…¨ç”±ç”¨æˆ·æ§åˆ¶
**ç¼ºç‚¹**ï¼šç”¨æˆ·éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ­£ç¡®çš„è·¯å¾„

**æ–¹æ¡ˆäºŒï¼šè‡ªåŠ¨è·¯å¾„æ˜ å°„ï¼ˆä¸­ç­‰å¤æ‚åº¦ï¼‰**

åœ¨ `SlideSparseConfig` æˆ–æ¨¡å‹åŠ è½½å…¥å£å¤„æ·»åŠ è·¯å¾„æ˜ å°„é€»è¾‘ï¼š

```python
# slidesparse/core/config.py æ·»åŠ 

def get_model_path(original_path: str) -> str:
    """
    æ ¹æ®ç¯å¢ƒå˜é‡è‡ªåŠ¨æ˜ å°„æ¨¡å‹è·¯å¾„
    
    Args:
        original_path: åŸå§‹æ¨¡å‹è·¯å¾„
        
    Returns:
        å¦‚æœ USE_CUSPARSELT=1ï¼Œè¿”å› slidesparse å¤„ç†åçš„è·¯å¾„
        å¦åˆ™è¿”å›åŸå§‹è·¯å¾„
    """
    if not is_cusparselt_enabled():
        return original_path
    
    # æ–¹æ¡ˆ Aï¼šç›®å½•æ˜ å°„
    # ./checkpoints/model -> ./checkpoints_slidesparse/model-slidesparse
    base_dir = os.path.dirname(original_path)
    model_name = os.path.basename(original_path)
    slidesparse_dir = base_dir.replace("checkpoints", "checkpoints_slidesparse")
    return os.path.join(slidesparse_dir, f"{model_name}-slidesparse")
```

**ä¼˜ç‚¹**ï¼šå¯¹ç”¨æˆ·é€æ˜ï¼Œè‡ªåŠ¨åˆ‡æ¢
**ç¼ºç‚¹**ï¼šéœ€è¦çº¦å®šç›®å½•å‘½åè§„èŒƒ

**æ–¹æ¡ˆä¸‰ï¼šé…ç½®æ–‡ä»¶é©±åŠ¨ï¼ˆé«˜çµæ´»æ€§ï¼‰**

ä½¿ç”¨é…ç½®æ–‡ä»¶æ˜¾å¼æŒ‡å®šè·¯å¾„æ˜ å°„ï¼š

```json
// slidesparse_paths.json
{
    "path_mappings": {
        "Qwen2.5-7B-Instruct-FP8-dynamic": {
            "dense": "./checkpoints/Qwen2.5-7B-Instruct-FP8-dynamic",
            "slidesparse": "./checkpoints_slidesparse/Qwen2.5-7B-Instruct-FP8-slidesparse"
        }
    }
}
```

**ä¼˜ç‚¹**ï¼šé«˜åº¦çµæ´»ï¼Œæ”¯æŒä»»æ„æ˜ å°„
**ç¼ºç‚¹**ï¼šéœ€è¦ç»´æŠ¤é…ç½®æ–‡ä»¶

#### 12.3.3 create_weights() é€‚é…

æ— è®ºé‡‡ç”¨å“ªç§è·¯å¾„æ–¹æ¡ˆï¼Œ`SlideSparseLinearMethod.create_weights()` éƒ½éœ€è¦æ­£ç¡®å®šä¹‰å‚æ•° shapeï¼š

**å½“å‰å®ç°åˆ†æ**ï¼ˆ`SlideSparseLinearMethod_FP8.py` ç¬¬ 485-506 è¡Œï¼‰ï¼š

```python
def create_weights(self, layer, ...):
    # å½“å‰ï¼šå®Œå…¨å§”æ‰˜ç»™åŸå§‹ scheme
    return self.original_scheme.create_weights(...)
```

**cuSPARSELt é€‚é…éœ€æ±‚**ï¼š

```python
def create_weights(self, layer, input_size_per_partition, output_partition_sizes, ...):
    """
    ä¸º cuSPARSELt åˆ›å»ºå‹ç¼©åçš„æƒé‡å‚æ•°
    
    å…³é”®ï¼šshape å¿…é¡»ä¸ç¦»çº¿å¤„ç†åçš„ safetensor ä¸€è‡´
    """
    if not is_cusparselt_enabled():
        # Dense è·¯å¾„ï¼šå§”æ‰˜ç»™åŸå§‹ scheme
        return self.original_scheme.create_weights(...)
    
    # Sparse è·¯å¾„ï¼šå®šä¹‰å‹ç¼©åçš„ shape
    K_original = input_size_per_partition
    N = sum(output_partition_sizes)
    
    # è®¡ç®— slide åçš„ K'
    expand_ratio = get_expand_ratio()  # å¦‚ 1.5 for 2:8
    K_expanded = int(K_original * expand_ratio)
    
    # Compress åå‡åŠ
    K_compressed = K_expanded // 2
    
    # åˆ›å»ºå‹ç¼©åçš„æƒé‡å‚æ•°
    weight = Parameter(
        torch.empty(N, K_compressed, dtype=torch.float8_e4m3fn),
        requires_grad=False,
    )
    layer.register_parameter("weight", weight)
    
    # åˆ›å»ºç¨€ç–å…ƒæ•°æ®å‚æ•°
    weight_meta = Parameter(
        torch.empty(N, K_expanded // 8, dtype=torch.uint8),
        requires_grad=False,
    )
    layer.register_parameter("weight_meta", weight_meta)
    
    # Scale ç­‰å…¶ä»–å‚æ•°
    ...
```

---

### 12.4 Phase 6ï¼šcuSPARSELt Linear æ–¹æ³•æ›¿æ¢

#### 12.4.1 åœ¨çº¿ Kernel æ›¿æ¢ç­–ç•¥

cuSPARSELt çš„æ ¸å¿ƒæ›¿æ¢åœ¨äº **Quant+Slide** èåˆ kernelï¼š

| æ­¥éª¤ | cuBLASLt è·¯å¾„ | cuSPARSELt è·¯å¾„ |
|------|--------------|-----------------|
| Quant | `quant_only_fp8_kernel()` | `fused_quant_slide_fp8()` |
| GEMM | cuBLASLt dense GEMM | cuSPARSELt sparse GEMM |
| Dequant | `dequant_bias_kernel()` | `dequant_bias_kernel()` |

**å…³é”®åŒºåˆ«**ï¼šcuSPARSELt éœ€è¦å°† Quant å’Œ Slide èåˆæˆä¸€ä¸ª kernelï¼Œè¾“å‡º slided åçš„ FP8 æ¿€æ´»ã€‚

#### 12.4.2 fused_quant_slide_triton ç°æœ‰å®ç°åˆ†æ

å½“å‰ `slidesparse/csrc/fused_quant_slide_trition/` ç›®å½•å·²æœ‰å®Œæ•´çš„ Triton å®ç°ï¼š

> **æ³¨æ„**ï¼šç›®å½•å `fused_quant_slide_trition` æ˜¯åŸå§‹å‘½åï¼ˆæ‹¼å†™ä¸ triton ç•¥æœ‰ä¸åŒï¼‰ï¼Œå¼•ç”¨æ—¶éœ€ä¿æŒä¸€è‡´ã€‚

| æ–‡ä»¶ | åŠŸèƒ½ | çŠ¶æ€ |
|------|------|------|
| `slide_L6_fp8.py` | L=6 FP8 Quant+Slide | âœ… å·²å®Œæˆ |
| `slide_L8_fp8.py` | L=8 FP8 Quant+Slide | âœ… å·²å®Œæˆ |
| `slide_L6_int8.py` | L=6 INT8 Quant+Slide | âœ… å·²å®Œæˆ |
| `slide_L8_int8.py` | L=8 INT8 Quant+Slide | âœ… å·²å®Œæˆ |
| `*_autotuned.py` | å¯¹åº”çš„å›ºå®šé…ç½®ç‰ˆæœ¬ | âœ… å·²å®Œæˆ |

**æ ¸å¿ƒå‡½æ•°ç­¾å**ï¼ˆ`slide_L8_fp8.py` ç¬¬ 143-174 è¡Œï¼‰ï¼š

```python
def fused_quant_slide(x: torch.Tensor):
    """
    Fused FP8 E4M3 Quantization + SlideSparse Slide
    
    Args:
        x: Input tensor [M, K_in], bf16/fp16/fp32
    
    Returns:
        y: Output tensor [M, K_out_padded] (fp8_e4m3fn)
        scale: Scale tensor [M], fp32
    """
```

**è¾“å‡ºç»´åº¦è®¡ç®—**ï¼ˆä»¥ L=8 ä¸ºä¾‹ï¼‰ï¼š
- `num_groups = ceil(K_in / L)`
- `K_out = num_groups Ã— NUM_WINDOWS Ã— 4 = num_groups Ã— 3 Ã— 4 = num_groups Ã— 12`
- è¾“å‡ºæ˜¯ padded åˆ° 16 å­—èŠ‚å¯¹é½çš„

#### 12.4.3 cuSPARSELt_FP8_linear å®ç°æ–¹æ¡ˆ

**ä¿®æ”¹ `cuSPARSELt_FP8_linear()` å‡½æ•°**ï¼ˆ`SlideSparseLinearMethod_FP8.py` ç¬¬ 265-299 è¡Œï¼‰ï¼š

å½“å‰å®ç°ä½¿ç”¨ vLLM åŸç”Ÿçš„ `QuantFP8` è¿›è¡Œé‡åŒ–ï¼Œéœ€è¦æ›¿æ¢ä¸º `fused_quant_slide_fp8()`:

```python
def cuSPARSELt_FP8_linear(
    *,
    input: torch.Tensor,          # [M, K] BF16
    weight: torch.Tensor,         # [K_compressed, N] FP8 (å‹ç¼©å)
    weight_meta: torch.Tensor,    # [K'/8, N] uint8 (ç¨€ç–å…ƒæ•°æ®)
    out_dtype: torch.dtype,
    scale_b: torch.Tensor,
    bias: Optional[torch.Tensor],
    output_shape: list,
    L: int = 8,                   # ç¨€ç–é…ç½®
    **kwargs,
) -> torch.Tensor:
    """cuSPARSELt 2:4 sparse FP8 GEMM + fused_quant_slide"""
    
    # 1. Fused Quant + Slide (ä½¿ç”¨ Triton kernel)
    #    è¾“å…¥: [M, K] BF16
    #    è¾“å‡º: [M, K'] FP8, [M] scale
    from slidesparse.csrc.fused_quant_slide_trition.slide_L8_fp8_autotuned import fused_quant_slide
    qinput_slided, scale_a = fused_quant_slide(input)
    
    # 2. cuSPARSELt Sparse GEMM
    #    è¾“å…¥: [M, K'] FP8 (slided activation)
    #          [N, K'/2] FP8 (compressed weight)
    #          [N, K'/8] uint8 (metadata)
    #    è¾“å‡º: [M, N] BF16 æˆ– FP32 (inner_dtype)
    ext = _get_gemm_extension("cusparselt")
    gemm_output = ext.cusparselt_fp8_sparse_mm(
        qinput_slided,    # [M, K']
        weight,           # [N, K'/2] compressed
        weight_meta,      # [N, K'/8] metadata
        get_inner_dtype_str(),
    )
    
    # 3. Dequant + Bias (ä½¿ç”¨ Triton kernel)
    output = dequant_bias_kernel(gemm_output, scale_a, scale_b, bias, out_dtype)
    
    return output.view(*output_shape)
```

#### 12.4.4 cuSPARSELt GEMM æ¥å£è®¾è®¡

éœ€è¦åœ¨ `slidesparse/csrc/cusparselt_gemm/` ä¸‹å®ç° cuSPARSELt çš„ Python/C++ ç»‘å®šï¼š

**Python æ¥å£**ï¼š

```python
def cusparselt_fp8_sparse_mm(
    A: torch.Tensor,       # [M, K'] FP8 (slided dense activation)
    B_nzs: torch.Tensor,   # [N, K'/2] FP8 (compressed weight nonzeros)
    B_meta: torch.Tensor,  # [N, K'/8] uint8 (sparsity metadata)
    out_dtype: str,        # "bf16" or "fp32"
    algo_id: int = -1,     # -1 = auto select
) -> torch.Tensor:
    """
    cuSPARSELt 2:4 Sparse FP8 GEMM
    
    Computes: C[M, N] = A[M, K'] Ã— B_sparse[K', N]
    
    Where B_sparse is the sparse matrix reconstructed from B_nzs and B_meta.
    
    Returns:
        Output tensor [M, N] with dtype specified by out_dtype
    """
```

**C++ å®ç°è¦ç‚¹**ï¼š

```cpp
// cusparselt_fp8_gemm.cu

torch::Tensor cusparselt_fp8_sparse_mm(
    torch::Tensor A,        // [M, K'] FP8
    torch::Tensor B_nzs,    // [N, K'/2] FP8
    torch::Tensor B_meta,   // [N, K'/8] uint8
    const std::string& out_dtype,
    int algo_id
) {
    // 1. åˆ›å»º cuSPARSELt å¥æŸ„å’Œæè¿°ç¬¦
    cusparseLtHandle_t handle;
    cusparseLtInit(&handle);
    
    // 2. åˆ›å»ºç¨€ç–çŸ©é˜µæè¿°ç¬¦
    cusparseLtStructuredDescriptor_t matB;
    cusparseLtStructuredDescriptorInit(
        &handle, &matB,
        N, K_expanded,
        N,  // leading dimension
        16, // alignment
        CUDA_R_8F_E4M3,  // data type
        CUSPARSE_ORDER_ROW,
        CUSPARSELT_SPARSITY_50_PERCENT  // 2:4 sparsity
    );
    
    // 3. è®¾ç½®å‹ç¼©åçš„æ•°æ®
    // B_nzs å’Œ B_meta å·²ç»æ˜¯ç¦»çº¿å‹ç¼©å¥½çš„
    
    // 4. æ‰§è¡Œ Sparse GEMM
    // ...
    
    return output;
}
```

---

### 12.5 å½“å‰å·¥ä½œè¿›å±•æ€»ç»“

| é˜¶æ®µ | ç»„ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|------|
| **Phase 3** | cuBLASLt FP8 GEMM | âœ… å®Œæˆ | `cuBLASLt_FP8_linear()` å·²å®ç° |
| **Phase 3** | Triton quant kernel | âœ… å®Œæˆ | `quant_only_fp8_kernel()` å·²å®ç° |
| **Phase 3** | Triton dequant kernel | âœ… å®Œæˆ | `dequant_bias_kernel()` å·²å®ç° |
| **Phase 4** | Prune è„šæœ¬ | âœ… å®Œæˆ | `convert_checkpoint_quant_prune_to_Z_L.py` |
| **Phase 4** | Slide è„šæœ¬ | âœ… å®Œæˆ | `convert_weights_slidesparse.py` |
| **Phase 4** | Compress è„šæœ¬ | âœ… å®Œæˆ | `convert_checkpoint_compress.py` |
| **Phase 4** | å®Œæ•´æµç¨‹è„šæœ¬ | âŒ TODO | éœ€è¦åˆ›å»ºç«¯åˆ°ç«¯ safetensor è½¬æ¢è„šæœ¬ |
| **Phase 5** | è·¯å¾„æ˜ å°„æœºåˆ¶ | âŒ TODO | éœ€è¦å®ç°ç¯å¢ƒå˜é‡é©±åŠ¨çš„è·¯å¾„é€‰æ‹© |
| **Phase 5** | create_weights é€‚é… | âŒ TODO | éœ€è¦æ”¯æŒå‹ç¼©åçš„ shape |
| **Phase 6** | fused_quant_slide kernel | âœ… å®Œæˆ | `slide_L{6,8}_{fp8,int8}.py` |
| **Phase 6** | cuSPARSELt GEMM | âŒ TODO | éœ€è¦å®ç° C++/Python ç»‘å®š |
| **Phase 6** | cuSPARSELt_FP8_linear | âŒ TODO | éœ€è¦æ›¿æ¢ä¸º fused_quant_slide |

### 12.6 ä¸‹ä¸€æ­¥å·¥ä½œä¼˜å…ˆçº§

1. **é«˜ä¼˜å…ˆçº§**ï¼š
   - å®ç°å®Œæ•´çš„ç¦»çº¿æµç¨‹è„šæœ¬ï¼ˆsafetensor ç«¯åˆ°ç«¯ï¼‰
   - å®ç° cuSPARSELt GEMM C++ ç»‘å®š

2. **ä¸­ä¼˜å…ˆçº§**ï¼š
   - å®ç°æ¨¡å‹è·¯å¾„è‡ªåŠ¨æ˜ å°„æœºåˆ¶
   - ä¿®æ”¹ `cuSPARSELt_FP8_linear()` ä½¿ç”¨ `fused_quant_slide`

3. **ä½ä¼˜å…ˆçº§**ï¼š
   - æ€§èƒ½ä¼˜åŒ–å’Œ AutoTune
   - INT8 ç‰ˆæœ¬æ”¯æŒ

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.6*  
*æ›´æ–°æ—¥æœŸï¼š2026-01*  
*ä½œè€…ï¼šSlideSparse Team*
