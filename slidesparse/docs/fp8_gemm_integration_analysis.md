# SlideSparse Phase 3: FP8 GEMM é›†æˆæŠ€æœ¯åˆ†æ

> æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æ vLLM ä¸­ FP8 GEMM çš„å®ç°æ¶æ„ï¼ŒåŒ…æ‹¬ compressed-tensors æ ¼å¼ã€CUTLASS å†…æ ¸å®ç°ä»¥åŠ cuBLASLt æ›¿æ¢æ–¹æ¡ˆã€‚

---

## ç›®å½•

1. [Compressed-Tensors è½¬å‘æ¶æ„åˆ†æ](#1-compressed-tensors-è½¬å‘æ¶æ„åˆ†æ)
2. [å½“å‰ CUTLASS FP8 GEMM å®ç°è¯¦è§£](#2-å½“å‰-cutlass-fp8-gemm-å®ç°è¯¦è§£)
3. [cuBLASLt æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹](#3-cublaslt-æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹)
4. [å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹](#4-å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹)

---

## 1. Compressed-Tensors è½¬å‘æ¶æ„åˆ†æ

### 1.1 ä¸ºä»€ä¹ˆä½¿ç”¨ Compressed-Tensors è€Œä¸æ˜¯åŸç”Ÿ FP8/INT8

**æ ¸å¿ƒåŸå› ï¼šCompressed-Tensors æ˜¯ä¸€ä¸ªå…ƒæ ¼å¼ï¼ˆMeta-Formatï¼‰**

HuggingFace ä¸Šçš„é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ RedHat çš„ W8A8ã€FP8-dynamic æ¨¡å‹ï¼‰ä½¿ç”¨ `compressed-tensors` ä½œä¸ºé‡åŒ–é…ç½®æ ¼å¼ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªå…·ä½“çš„é‡åŒ–å®ç°ï¼Œè€Œæ˜¯ä¸€ä¸ª**é…ç½®è§£æå±‚**ï¼Œå®ƒä¼šï¼š

1. **è¯»å–æ¨¡å‹çš„ `config.json`** ä¸­çš„é‡åŒ–é…ç½®
2. **è‡ªåŠ¨æ£€æµ‹é‡åŒ–ç±»å‹**ï¼ˆFP8ã€INT8ã€W4A16 ç­‰ï¼‰
3. **é€‰æ‹©å¯¹åº”çš„ Scheme**ï¼ˆ`CompressedTensorsW8A8Fp8`ã€`CompressedTensorsW8A8Int8` ç­‰ï¼‰

```python
# vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
class CompressedTensorsConfig(QuantizationConfig):
    def get_scheme(self, layer, layer_name):
        # æ ¹æ® layer ç±»å‹å’Œé…ç½®é€‰æ‹© scheme
        scheme = self._get_scheme_from_parts(...)
        
        # âœ… æˆ‘ä»¬çš„ cuBLASLt åŒ…è£…ç‚¹
        scheme = wrap_scheme_with_cublaslt(scheme)
        return scheme
```

**å…¸å‹çš„æ¨¡å‹é…ç½®ç¤ºä¾‹**ï¼ˆ`config.json`ï¼‰ï¼š
```json
{
  "quantization_config": {
    "quant_method": "compressed-tensors",
    "config_groups": {
      "group_0": {
        "weights": {
          "num_bits": 8,
          "type": "float",
          "strategy": "channel"
        },
        "input_activations": {
          "num_bits": 8,
          "type": "float",
          "strategy": "token"
        }
      }
    }
  }
}
```

### 1.2 å½“å‰ SlideSparse cuBLASLt è½¬å‘æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç”¨æˆ·åŠ è½½ FP8 é‡åŒ–æ¨¡å‹                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CompressedTensorsConfig.get_scheme()                   â”‚
â”‚                                                                     â”‚
â”‚   1. è§£æ config_groups é…ç½®                                         â”‚
â”‚   2. è°ƒç”¨ _get_scheme_from_parts() â†’ CompressedTensorsW8A8Fp8       â”‚
â”‚   3. âœ… wrap_scheme_with_cublaslt(scheme) åŒ…è£…                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CuBLASLtSchemeWrapper                            â”‚
â”‚                                                                     â”‚
â”‚   - _original_scheme: CompressedTensorsW8A8Fp8                      â”‚
â”‚   - create_weights()      â†’ å§”æ‰˜ç»™åŸå§‹ scheme                        â”‚
â”‚   - process_weights_after_loading() â†’ å§”æ‰˜ç»™åŸå§‹ scheme              â”‚
â”‚   - apply_weights()       â†’ è°ƒç”¨ CuBLASLtFp8LinearOp                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CuBLASLtFp8LinearOp.apply()                      â”‚
â”‚                                                                     â”‚
â”‚   å½“å‰å®ç°ï¼ˆUSE_REAL_CUBLASLT=Falseï¼‰:                               â”‚
â”‚   - è°ƒç”¨ vLLM åŸç”Ÿ Fp8LinearOpï¼ˆä½¿ç”¨ CUTLASSï¼‰                       â”‚
â”‚                                                                     â”‚
â”‚   ç›®æ ‡å®ç°ï¼ˆUSE_REAL_CUBLASLT=Trueï¼‰:                                â”‚
â”‚   - è°ƒç”¨ cuBLASLt FP8 matmul API                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 è½¬å‘æ¶æ„éªŒè¯

**å½“å‰æ¶æ„å·²ç¡®è®¤æ­£ç¡®ï¼š**

| æ£€æŸ¥é¡¹ | çŠ¶æ€ | è¯´æ˜ |
|--------|------|------|
| ç¯å¢ƒå˜é‡æ£€æµ‹ | âœ… | `VLLM_USE_CUBLASLT=1` æˆ– `SLIDESPARSE_USE_CUBLASLT=1` |
| Scheme åŒ…è£… | âœ… | `wrap_scheme_with_cublaslt()` åœ¨ `get_scheme()` ä¸­è°ƒç”¨ |
| æƒé‡åˆ›å»º | âœ… | å§”æ‰˜ç»™åŸå§‹ schemeï¼Œä¿æŒå…¼å®¹æ€§ |
| æƒé‡åŠ è½½ | âœ… | å§”æ‰˜ç»™åŸå§‹ schemeï¼Œsafetensor æ ¼å¼æ­£å¸¸åŠ è½½ |
| æ¨ç†è·¯å¾„ | âœ… | `apply_weights()` æ­£ç¡®è°ƒç”¨ `CuBLASLtFp8LinearOp` |

---

## 2. å½“å‰ CUTLASS FP8 GEMM å®ç°è¯¦è§£

### 2.1 å®Œæ•´è°ƒç”¨é“¾

```
CompressedTensorsW8A8Fp8.apply_weights()
    â”‚
    â”œâ”€â†’ QuantFP8.apply()  [è¾“å…¥é‡åŒ–ï¼Œå¯é€‰]
    â”‚       â””â”€â†’ per-token / per-tensor é‡åŒ–
    â”‚
    â””â”€â†’ Fp8LinearOp.apply()
            â”‚
            â”œâ”€â†’ cutlass_w8a8_scaled_mm()  [CUDA 12.0+, SM90+]
            â”‚       â””â”€â†’ ops.cutlass_scaled_mm()
            â”‚               â””â”€â†’ cutlass_scaled_mm_sm90_fp8()
            â”‚
            â”œâ”€â†’ ops.scaled_fp8_quant()  [Flash-attention è·¯å¾„]
            â”‚
            â””â”€â†’ torch._scaled_mm()  [Fallback]
```

### 2.2 è¾“å…¥é‡åŒ–è¿‡ç¨‹ï¼ˆQuantFP8ï¼‰

```python
# vllm/model_executor/layers/quantization/input_quant_fp8.py
class QuantFP8(CustomOp):
    """FP8 è¾“å…¥é‡åŒ–ï¼Œæ”¯æŒä¸‰ç§ç­–ç•¥"""
    
    # é‡åŒ–å…¬å¼: x_fp8 = x / scale
    # scale è®¡ç®—: scale = max(|x|) / fp8_max
    
    def __init__(self, quant_config):
        self.strategy = quant_config.input_strategy
        # "tensor" - æ•´ä¸ª tensor å…±äº«ä¸€ä¸ª scale
        # "token"  - æ¯è¡Œï¼ˆtokenï¼‰ä¸€ä¸ª scale  
        # "group"  - æ¯ group_size ä¸ªå…ƒç´ ä¸€ä¸ª scale
```

**é‡åŒ–ç­–ç•¥è¯´æ˜ï¼š**

| ç­–ç•¥ | scale å½¢çŠ¶ | è¯´æ˜ |
|------|-----------|------|
| `per-tensor` | `[1]` | æ•´ä¸ªè¾“å…¥å…±äº«ä¸€ä¸ª scaleï¼Œç²¾åº¦æœ€ä½ä½†æœ€å¿« |
| `per-token` | `[M, 1]` | æ¯è¡Œä¸€ä¸ª scaleï¼ŒLLM æ¨ç†çš„å…¸å‹é…ç½® |
| `per-channel` | `[1, K]` | æ¯åˆ—ä¸€ä¸ª scaleï¼Œæƒé‡é‡åŒ–å¸¸ç”¨ |

### 2.3 GEMM Layout è®¾è®¡

**vLLM CUTLASS FP8 GEMM Layout:**

```
é—®é¢˜å®šä¹‰: C[M,N] = A[M,K] Ã— B[K,N]

å®é™…å­˜å‚¨ï¼ˆCUTLASS å†…éƒ¨ï¼‰:
    A: RowMajor    [M, K]  - æ¯è¡Œè¿ç»­å­˜å‚¨
    B: ColumnMajor [K, N]  - ç­‰ä»·äº [N, K]^Tï¼Œæ¯åˆ—è¿ç»­å­˜å‚¨  
    C: RowMajor    [M, N]
    D: RowMajor    [M, N]

åœ¨ vLLM ä¸­:
    input (x):   [batch, hidden_dim] = [M, K]  RowMajor
    weight (w):  [out_features, in_features] = [N, K]  å®é™…å­˜å‚¨
                 ä¼ ç»™ CUTLASS æ—¶ä½œä¸º ColumnMajor [K, N]
    output:      [batch, out_features] = [M, N]  RowMajor
```

**swap_ab æœºåˆ¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ï¼š**

```cpp
// csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh

// å½“ M å¾ˆå°æ—¶ï¼ˆdecode é˜¶æ®µï¼‰ï¼Œäº¤æ¢ A å’Œ B ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
template <bool swap_ab = false>
void cutlass_scaled_mm_sm90_fp8_dispatch(...) {
    // swap_ab=true æ—¶:
    //   å®é™…è®¡ç®—: D^T = B^T Ã— A^T
    //   ç­‰ä»·äº:   D   = A Ã— B
    //   ä½†åˆ©ç”¨äº† B çš„æ›´å¥½çš„å†…å­˜è®¿é—®æ¨¡å¼
}

// é€‰æ‹©é€»è¾‘
if (M <= 64) {
    // å° M åœºæ™¯ï¼ˆdecodeï¼‰ï¼Œä½¿ç”¨ swap_ab
    cutlass_scaled_mm_sm90_fp8_dispatch<true>(...);
} else {
    // å¤§ M åœºæ™¯ï¼ˆprefillï¼‰ï¼Œä¸ä½¿ç”¨ swap_ab
    cutlass_scaled_mm_sm90_fp8_dispatch<false>(...);
}
```

### 2.4 Epilogueï¼ˆèåˆåå¤„ç†ï¼‰è¯¦è§£

**CUTLASS 3.x Epilogue è®¡ç®—å…¬å¼ï¼š**

```
D = scale_a * (scale_b * Accumulator) + bias

å…·ä½“å±•å¼€ï¼ˆScaledEpilogueBiasï¼‰:
    Compute0: tmp = scale_b * Accum      (é€å…ƒç´ æˆ–é€è¡Œ)
    Compute1: D = scale_a * tmp + bias   (é€å…ƒç´ æˆ–é€åˆ—)
```

**Epilogue ç±»å‹å®šä¹‰ï¼š**

```cpp
// csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp

// åŸºç¡€ Epilogueï¼ˆæ—  biasï¼‰
struct ScaledEpilogue {
    // scale_a: ColOrScalarLoad - æ¯åˆ—ä¸€ä¸ª scale æˆ–å…¨å±€ scalar
    // scale_b: RowOrScalarLoad - æ¯è¡Œä¸€ä¸ª scale æˆ–å…¨å±€ scalar
    
    using EVTCompute = Sm90EVT<
        Compute1,           // D = scale_a * tmp
        ScaleA,             // ColOrScalarLoad
        Sm90EVT<
            Compute0,       // tmp = scale_b * Accum
            ScaleB,         // RowOrScalarLoad
            Accum           // ç´¯åŠ å™¨è¾“å‡º
        >
    >;
};

// å¸¦ Bias çš„ Epilogue
struct ScaledEpilogueBias {
    // bias: RowLoad - æ¯è¡Œä¸€ä¸ª biasï¼ˆå¹¿æ’­åˆ°æ‰€æœ‰åˆ—ï¼‰
    
    using EVTCompute = Sm90EVT<
        Compute1,           // D = scale_a * tmp + bias
        ScaleA,             // ColOrScalarLoad
        Sm90EVT<
            Compute0,       // tmp = scale_b * Accum
            ScaleB,         // RowOrScalarLoad
            Accum
        >,
        Bias                // RowLoad
    >;
};

// swap_ab åœºæ™¯çš„ Biasï¼ˆåˆ—å¹¿æ’­ï¼‰
struct ScaledEpilogueColumnBias {
    // å½“ swap_ab=true æ—¶ï¼Œbias éœ€è¦åˆ—æ–¹å‘åŠ è½½
    using Bias = ColLoad<float>;
};
```

**Scale åŠ è½½æ¨¡å¼ï¼š**

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| `ScalarLoad` | å…¨å±€å•ä¸€ scale | per-tensor é‡åŒ– |
| `RowLoad` | æ¯è¡Œä¸€ä¸ª scale | per-token (activation) |
| `ColLoad` | æ¯åˆ—ä¸€ä¸ª scale | per-channel (weight) |
| `RowOrScalarLoad` | è¿è¡Œæ—¶é€‰æ‹© | å…¼å®¹ä¸¤ç§æ¨¡å¼ |
| `ColOrScalarLoad` | è¿è¡Œæ—¶é€‰æ‹© | å…¼å®¹ä¸¤ç§æ¨¡å¼ |

### 2.5 Kernel é€‰æ‹©ç­–ç•¥

```cpp
// csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh

template <typename OutType, bool swap_ab, bool with_bias>
void cutlass_gemm_sm90_fp8_dispatch(int M, int N, int K, ...) {
    
    // æ ¹æ®é—®é¢˜è§„æ¨¡é€‰æ‹©æœ€ä¼˜ kernel é…ç½®
    if (M <= 16) {
        // æå° Mï¼šä½¿ç”¨ M16N128K128 tile
        using TileShape = Shape<_16, _128, _128>;
        using ClusterShape = Shape<_1, _2, _1>;
        
    } else if (M <= 64) {
        // å° Mï¼šä½¿ç”¨ M64N128K128 tile
        using TileShape = Shape<_64, _128, _128>;
        using ClusterShape = Shape<_1, _2, _1>;
        
    } else if (M <= 128) {
        // ä¸­ç­‰ Mï¼šä½¿ç”¨ M128N128K128 tile
        using TileShape = Shape<_128, _128, _128>;
        using ClusterShape = Shape<_1, _1, _1>;
        
    } else if (N >= 8192) {
        // å¤§ Nï¼ˆå®½çŸ©é˜µï¼‰ï¼šä½¿ç”¨ä¸“é—¨é…ç½®
        using TileShape = Shape<_128, _256, _64>;
        using ClusterShape = Shape<_2, _1, _1>;
        
    } else {
        // é»˜è®¤é…ç½®
        using TileShape = Shape<_128, _128, _128>;
        using ClusterShape = Shape<_2, _1, _1>;
    }
}
```

---

## 3. cuBLASLt æ›¿æ¢æ–¹æ¡ˆä¸æ³¨æ„äº‹é¡¹

### 3.1 cuBLASLt FP8 GEMM æ ¸å¿ƒæ¦‚å¿µ

**åŸºæœ¬è®¡ç®—å…¬å¼ï¼ˆTensorwide Scalingï¼‰ï¼š**

```
D = scaleD * (Î± * scaleA * scaleB * op(A) Ã— op(B) + Î² * scaleC * C)
```

**vLLM åœºæ™¯ç®€åŒ–ï¼ˆæ—  scaleC/scaleDï¼ŒÎ²=0ï¼‰ï¼š**

```
D = Î± * scaleA * scaleB * op(A) Ã— op(B) + bias
```

### 3.2 Layout å¯¹åº”å…³ç³»

| vLLM/CUTLASS | cuBLASLt | è¯´æ˜ |
|--------------|----------|------|
| A: RowMajor | A: ColumnMajor + CUBLAS_OP_T | è½¬ç½®åç­‰ä»· |
| B: ColumnMajor | B: ColumnMajor + CUBLAS_OP_N | ç›´æ¥å¯¹åº” |
| C/D: RowMajor | C/D: ColumnMajor + è½¬ç½® | éœ€è¦é¢å¤–å¤„ç† |

**æ¨èåšæ³•ï¼šä½¿ç”¨ TN æ ¼å¼**

```cpp
// cuBLASLt æœ€ä¼˜é…ç½®ï¼ˆAda/Hopperï¼‰
CUBLAS_OP_T  // A è½¬ç½®
CUBLAS_OP_N  // B ä¸è½¬ç½®
```

### 3.3 Scale å¤„ç†æ–¹å¼å¯¹æ¯”

**CUTLASS æ–¹å¼ï¼š**
- `scale_a`ï¼šper-tokenï¼ˆåˆ—å‘é‡ï¼‰æˆ– scalar
- `scale_b`ï¼šper-channelï¼ˆè¡Œå‘é‡ï¼‰æˆ– scalar
- èåˆåœ¨ Epilogue ä¸­è®¡ç®—

**cuBLASLt æ–¹å¼ï¼š**
- `CUBLASLT_MATMUL_DESC_A_SCALE_POINTER`ï¼šæŒ‡å‘ scaleA
- `CUBLASLT_MATMUL_DESC_B_SCALE_POINTER`ï¼šæŒ‡å‘ scaleB
- æ”¯æŒä¸¤ç§ Scale Modeï¼š
  - `CUBLASLT_MATMUL_MATRIX_SCALE_SCALAR_32F`ï¼šper-tensorï¼ˆé»˜è®¤ï¼‰
  - `CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F`ï¼šper-row/colï¼ˆSM90+ï¼‰

### 3.4 Bias å¤„ç†

**cuBLASLt Epilogue é€‰é¡¹ï¼š**

```cpp
// è®¾ç½® epilogue ç±»å‹
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_EPILOGUE,
    &epilogue,  // CUBLASLT_EPILOGUE_BIAS
    sizeof(epilogue)
);

// è®¾ç½® bias æŒ‡é’ˆ
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_BIAS_POINTER,
    &bias_ptr,
    sizeof(bias_ptr)
);

// è®¾ç½® bias æ•°æ®ç±»å‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸è¾“å‡ºç›¸åŒï¼‰
cublasLtMatmulDescSetAttribute(
    matmulDesc,
    CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE,
    &bias_type,  // CUDA_R_32F for FP8 kernels
    sizeof(bias_type)
);
```

**é‡è¦é™åˆ¶ï¼š**
- Bias å‘é‡é•¿åº¦å¿…é¡»ç­‰äºè¾“å‡ºçŸ©é˜µè¡Œæ•°ï¼ˆMï¼‰
- Bias è¢«å¹¿æ’­åˆ°æ‰€æœ‰åˆ—
- FP8 kernel çš„ bias ç±»å‹é€šå¸¸æ˜¯ `CUDA_R_16BF` æˆ– `CUDA_R_32F`

### 3.5 å®Œæ•´ cuBLASLt FP8 GEMM å®ç°æ¡†æ¶

```cpp
// ä¼ªä»£ç ç¤ºä¾‹
cublasStatus_t cublaslt_fp8_gemm(
    int M, int N, int K,
    const void* A,        // FP8 input [M, K]
    const void* B,        // FP8 weight [K, N] (stored as [N, K]^T)
    void* D,              // Output [M, N]
    const float* scale_a, // per-token scale [M] or scalar
    const float* scale_b, // per-channel scale [N] or scalar
    const float* bias,    // optional bias [M]
    bool is_scale_a_scalar,
    bool is_scale_b_scalar,
    cudaStream_t stream
) {
    cublasLtHandle_t handle;
    cublasLtCreate(&handle);
    
    // 1. åˆ›å»ºçŸ©é˜µä¹˜æ³•æè¿°ç¬¦
    cublasLtMatmulDesc_t matmulDesc;
    cublasComputeType_t computeType = CUBLAS_COMPUTE_32F;
    cudaDataType_t scaleType = CUDA_R_32F;
    cublasLtMatmulDescCreate(&matmulDesc, computeType, scaleType);
    
    // 2. è®¾ç½®è½¬ç½®æ“ä½œï¼ˆTN æ ¼å¼ï¼‰
    cublasOperation_t opA = CUBLAS_OP_T;
    cublasOperation_t opB = CUBLAS_OP_N;
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSA, &opA, sizeof(opA));
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_TRANSB, &opB, sizeof(opB));
    
    // 3. è®¾ç½® Scale æŒ‡é’ˆ
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &scale_a, sizeof(scale_a));
    cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &scale_b, sizeof(scale_b));
    
    // 4. è®¾ç½® Scale Modeï¼ˆper-tensor vs per-row/colï¼‰
    if (!is_scale_a_scalar || !is_scale_b_scalar) {
        // Outer vector scalingï¼ˆSM90+ onlyï¼‰
        int32_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F;
        if (!is_scale_a_scalar) {
            cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode));
        }
        if (!is_scale_b_scalar) {
            cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode));
        }
    }
    
    // 5. è®¾ç½® Biasï¼ˆå¦‚æœæœ‰ï¼‰
    if (bias != nullptr) {
        cublasLtEpilogue_t epilogue = CUBLASLT_EPILOGUE_BIAS;
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_EPILOGUE, &epilogue, sizeof(epilogue));
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_BIAS_POINTER, &bias, sizeof(bias));
        
        cudaDataType_t biasType = CUDA_R_32F;  // æˆ– CUDA_R_16BF
        cublasLtMatmulDescSetAttribute(matmulDesc, CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE, &biasType, sizeof(biasType));
    }
    
    // 6. åˆ›å»ºçŸ©é˜µå¸ƒå±€
    cublasLtMatrixLayout_t Adesc, Bdesc, Ddesc;
    
    // A: [K, M] ColumnMajor (å› ä¸º opA=T, å®é™…è¯»å– [M, K] RowMajor)
    cublasLtMatrixLayoutCreate(&Adesc, CUDA_R_8F_E4M3, K, M, K);
    
    // B: [K, N] ColumnMajor
    cublasLtMatrixLayoutCreate(&Bdesc, CUDA_R_8F_E4M3, K, N, K);
    
    // D: [M, N] (éœ€è¦æ ¹æ®è¾“å‡ºç±»å‹è®¾ç½®)
    cublasLtMatrixLayoutCreate(&Ddesc, CUDA_R_16BF, M, N, M);
    
    // 7. è·å–ç®—æ³•å¯å‘å¼
    cublasLtMatmulPreference_t preference;
    cublasLtMatmulPreferenceCreate(&preference);
    
    size_t workspaceSize = 64 * 1024 * 1024;  // 64 MB
    cublasLtMatmulPreferenceSetAttribute(preference, CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES, &workspaceSize, sizeof(workspaceSize));
    
    cublasLtMatmulHeuristicResult_t heuristicResult;
    int returnedResults = 0;
    cublasLtMatmulAlgoGetHeuristic(handle, matmulDesc, Adesc, Bdesc, Ddesc, Ddesc, preference, 1, &heuristicResult, &returnedResults);
    
    // 8. æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
    float alpha = 1.0f;
    float beta = 0.0f;
    void* workspace = nullptr;
    cudaMalloc(&workspace, heuristicResult.workspaceSize);
    
    cublasLtMatmul(
        handle, matmulDesc,
        &alpha,
        A, Adesc,
        B, Bdesc,
        &beta,
        D, Ddesc,  // C = D for in-place
        D, Ddesc,
        &heuristicResult.algo,
        workspace, heuristicResult.workspaceSize,
        stream
    );
    
    // æ¸…ç†
    cudaFree(workspace);
    cublasLtMatmulPreferenceDestroy(preference);
    cublasLtMatrixLayoutDestroy(Adesc);
    cublasLtMatrixLayoutDestroy(Bdesc);
    cublasLtMatrixLayoutDestroy(Ddesc);
    cublasLtMatmulDescDestroy(matmulDesc);
    cublasLtDestroy(handle);
    
    return CUBLAS_STATUS_SUCCESS;
}
```

### 3.6 å…³é”®æ³¨æ„äº‹é¡¹

#### 3.6.1 æ•°æ®ç±»å‹æ”¯æŒ

| Atype | Btype | Ctype | Dtype | æ”¯æŒ |
|-------|-------|-------|-------|------|
| E4M3 | E4M3 | BF16 | BF16 | âœ… |
| E4M3 | E4M3 | FP16 | FP16 | âœ… |
| E4M3 | E4M3 | FP32 | FP32 | âœ… |
| E5M2 | E4M3 | BF16 | BF16 | âœ… |
| E4M3 | E5M2 | BF16 | BF16 | âœ… |

#### 3.6.2 Scale Mode é™åˆ¶ï¼ˆOuter Vector Scalingï¼‰

```cpp
// SM90 (Hopper) ç‹¬æœ‰åŠŸèƒ½
CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F

// é™åˆ¶:
// 1. ä»…æ”¯æŒ SM90+
// 2. scaleD ä¸æ”¯æŒï¼ˆè¾“å‡ºå¿…é¡»æ˜¯ FP16/BF16/FP32ï¼‰
// 3. scaleA é•¿åº¦ = M, scaleB é•¿åº¦ = N
```

#### 3.6.3 å¯¹é½è¦æ±‚

- æ‰€æœ‰çŸ©é˜µæŒ‡é’ˆå¿…é¡» 16 å­—èŠ‚å¯¹é½
- ç»´åº¦ M, K æœ€å¥½æ˜¯ 16 çš„å€æ•°
- workspace å¿…é¡» 256 å­—èŠ‚å¯¹é½

#### 3.6.4 æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å¤ç”¨ Handle å’Œ Descriptor**ï¼šåˆ›å»ºå¼€é”€å¤§ï¼Œåº”å…¨å±€å¤ç”¨
2. **ç¼“å­˜ Heuristic ç»“æœ**ï¼šç›¸åŒé—®é¢˜è§„æ¨¡å¯å¤ç”¨ç®—æ³•é€‰æ‹©
3. **Workspace é¢„åˆ†é…**ï¼šé¿å…è¿è¡Œæ—¶åˆ†é…
4. **ä½¿ç”¨ Fast Accumulation**ï¼š`CUBLASLT_MATMUL_DESC_FAST_ACCUM = 1`

---

## 4. å®ç°è®¡åˆ’ä¸ä»£ç ç¤ºä¾‹

### 4.1 ä¿®æ”¹ CuBLASLtFp8LinearOp

```python
# slidesparse/core/cublaslt_linear_method.py

class CuBLASLtFp8LinearOp:
    USE_REAL_CUBLASLT = True  # å¯ç”¨çœŸå® cuBLASLt
    
    def __init__(self):
        # åˆå§‹åŒ– cuBLASLt handleï¼ˆå•ä¾‹ï¼‰
        self._handle = self._get_or_create_handle()
        
    @classmethod
    def _get_or_create_handle(cls):
        # å…¨å±€ handle ç¼“å­˜
        if not hasattr(cls, '_global_handle'):
            cls._global_handle = cublaslt_create_handle()
        return cls._global_handle
    
    def apply(
        self,
        x: torch.Tensor,           # [M, K] FP8
        weight: torch.Tensor,      # [N, K] FP8
        x_scale: torch.Tensor,     # [M] or [1]
        weight_scale: torch.Tensor,# [N] or [1]
        bias: Optional[torch.Tensor] = None,
        output_dtype: torch.dtype = torch.bfloat16,
    ) -> torch.Tensor:
        
        M, K = x.shape
        N = weight.shape[0]
        
        # ç¡®å®š scale mode
        is_x_scale_scalar = (x_scale.numel() == 1)
        is_w_scale_scalar = (weight_scale.numel() == 1)
        
        # è°ƒç”¨ cuBLASLt kernel
        output = cublaslt_fp8_gemm(
            self._handle,
            x.data_ptr(),
            weight.data_ptr(),
            x_scale.data_ptr(),
            weight_scale.data_ptr(),
            bias.data_ptr() if bias is not None else None,
            M, N, K,
            is_x_scale_scalar,
            is_w_scale_scalar,
            output_dtype,
            torch.cuda.current_stream().cuda_stream,
        )
        
        return output
```

### 4.2 CUDA ç»‘å®šå®ç°

éœ€è¦åœ¨ `csrc/` ç›®å½•ä¸‹æ·»åŠ  cuBLASLt wrapperï¼š

```cpp
// csrc/quantization/cublaslt_fp8_gemm.cu

#include <cublasLt.h>
#include <torch/extension.h>

// å…¨å±€ handle ç®¡ç†
class CublasLtHandlePool {
public:
    static cublasLtHandle_t get() {
        static thread_local cublasLtHandle_t handle = nullptr;
        if (handle == nullptr) {
            cublasLtCreate(&handle);
        }
        return handle;
    }
};

torch::Tensor cublaslt_fp8_gemm(
    torch::Tensor A,        // [M, K] FP8
    torch::Tensor B,        // [N, K] FP8 (stored transposed)
    torch::Tensor scale_a,  // [M] or [1]
    torch::Tensor scale_b,  // [N] or [1]
    c10::optional<torch::Tensor> bias,
    bool is_scale_a_scalar,
    bool is_scale_b_scalar,
    torch::Dtype output_dtype
) {
    // å®ç°å‚è§ 3.5 èŠ‚æ¡†æ¶
}

// Python ç»‘å®š
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("cublaslt_fp8_gemm", &cublaslt_fp8_gemm, "cuBLASLt FP8 GEMM");
}
```

### 4.3 CMake é›†æˆ

```cmake
# cmake/cublaslt_extension.cmake

find_package(CUDAToolkit REQUIRED)

add_library(cublaslt_fp8_gemm SHARED
    csrc/quantization/cublaslt_fp8_gemm.cu
)

target_link_libraries(cublaslt_fp8_gemm
    CUDA::cublasLt
    ${TORCH_LIBRARIES}
)
```

---

## 5. æµ‹è¯•ä¸éªŒè¯è®¡åˆ’

### 5.1 æ­£ç¡®æ€§æµ‹è¯•

```python
def test_cublaslt_fp8_gemm_correctness():
    """å¯¹æ¯” cuBLASLt ä¸ CUTLASS ç»“æœ"""
    M, N, K = 1024, 4096, 4096
    
    # éšæœºç”Ÿæˆ FP8 è¾“å…¥
    x = torch.randn(M, K, device='cuda').to(torch.float8_e4m3fn)
    w = torch.randn(N, K, device='cuda').to(torch.float8_e4m3fn)
    scale_x = torch.rand(M, device='cuda')
    scale_w = torch.rand(N, device='cuda')
    bias = torch.randn(N, device='cuda', dtype=torch.bfloat16)
    
    # CUTLASS å‚è€ƒå®ç°
    ref = cutlass_scaled_mm(x, w, scale_x, scale_w, bias)
    
    # cuBLASLt å®ç°
    out = cublaslt_fp8_gemm(x, w, scale_x, scale_w, bias)
    
    # éªŒè¯ï¼ˆFP8 è®¡ç®—å…è®¸ä¸€å®šè¯¯å·®ï¼‰
    assert torch.allclose(out, ref, rtol=1e-2, atol=1e-2)
```

### 5.2 æ€§èƒ½æµ‹è¯•

```python
def benchmark_cublaslt_vs_cutlass():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    configs = [
        (1, 4096, 4096),     # decode
        (32, 4096, 4096),    # small batch
        (128, 4096, 4096),   # medium batch
        (1024, 4096, 4096),  # large batch
        (4096, 4096, 4096),  # prefill
    ]
    
    for M, N, K in configs:
        # é¢„çƒ­
        for _ in range(10):
            cutlass_run(M, N, K)
            cublaslt_run(M, N, K)
        
        # æµ‹é‡
        cutlass_time = benchmark(cutlass_run, M, N, K, iters=100)
        cublaslt_time = benchmark(cublaslt_run, M, N, K, iters=100)
        
        print(f"[{M}, {N}, {K}] CUTLASS: {cutlass_time:.2f}ms, cuBLASLt: {cublaslt_time:.2f}ms")
```

---

## 6. æ€»ç»“

### 6.1 å½“å‰çŠ¶æ€

| ç»„ä»¶ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| ç¯å¢ƒå˜é‡åˆ‡æ¢ | âœ… å®Œæˆ | `VLLM_USE_CUBLASLT=1` |
| Scheme åŒ…è£…æ¶æ„ | âœ… å®Œæˆ | `CuBLASLtSchemeWrapper` |
| æƒé‡åŠ è½½å…¼å®¹ | âœ… å®Œæˆ | å§”æ‰˜ç»™åŸå§‹ scheme |
| cuBLASLt çœŸå®å®ç° | ğŸ”„ å¾…å¼€å‘ | æœ¬æ–‡æ¡£æä¾›æ¡†æ¶ |

### 6.2 å¼€å‘ä¼˜å…ˆçº§

1. **é«˜ä¼˜å…ˆçº§**ï¼šå®ç°åŸºç¡€ cuBLASLt FP8 GEMMï¼ˆper-tensor scaleï¼‰
2. **ä¸­ä¼˜å…ˆçº§**ï¼šæ”¯æŒ per-token/per-channel scaleï¼ˆOuter Vector Scalingï¼‰
3. **ä½ä¼˜å…ˆçº§**ï¼šBias èåˆã€GELU/ReLU èåˆ

### 6.3 é£é™©ä¸æŒ‘æˆ˜

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|---------|
| Layout ä¸åŒ¹é… | ç»“æœé”™è¯¯ | ä»”ç»†éªŒè¯è½¬ç½®é€»è¾‘ |
| Scale mode é™åˆ¶ | SM89 ä¸æ”¯æŒ outer vector | å›é€€åˆ° per-tensor |
| æ€§èƒ½å›é€€ | å° M åœºæ™¯å¯èƒ½æ›´æ…¢ | æ ¹æ®è§„æ¨¡åŠ¨æ€é€‰æ‹©åç«¯ |
| cuBLAS ç‰ˆæœ¬å…¼å®¹ | API å·®å¼‚ | ç‰ˆæœ¬æ£€æµ‹ + æ¡ä»¶ç¼–è¯‘ |

---

## 7. å½“å‰å¤–æŒ‚æ–¹æ³•çš„å§”æ‰˜è¯¦ç»†åˆ†æ

æœ¬ç« è¯¦ç»†è¯´æ˜å½“å‰ `CuBLASLtFp8LinearMethod` / `CuBLASLtSchemeWrapper` ä¸­å„æ­¥éª¤çš„å§”æ‰˜æƒ…å†µã€‚

### 7.1 FP8 å§”æ‰˜é“¾è·¯æ€»è§ˆ

| æ­¥éª¤ | å½“å‰çŠ¶æ€ | å¤–æŒ‚å‡½æ•° | è½¬å‘ç›®æ ‡ | vLLM å…·ä½“ä½ç½® |
|------|----------|----------|----------|---------------|
| **æƒé‡åŠ è½½** | âœ… å§”æ‰˜ | `create_weights()` | `original_scheme.create_weights()` | `compressed_tensors_w8a8_fp8.py:84-130` |
| **æƒé‡å¤„ç†** | âœ… å§”æ‰˜ | `process_weights_after_loading()` | `original_scheme.process_weights_after_loading()` | `compressed_tensors_w8a8_fp8.py:132-172` |
| **è¾“å…¥åŠ è½½** | N/A | - | ç”± PyTorch è‡ªåŠ¨å¤„ç† | - |
| **è¾“å…¥é‡åŒ–** | âœ… å§”æ‰˜ | `apply()` â†’ `_fp8_linear_op.apply()` | `Fp8LinearOp.quant_fp8()` | `w8a8_utils.py:462-467` â†’ `QuantFP8` |
| **GEMM+åé‡åŒ–** | âœ… å§”æ‰˜ | `apply()` â†’ `_fp8_linear_op.apply()` | `cutlass_w8a8_scaled_mm()` | `w8a8_utils.py:150-165` |
| **è¾“å‡ºè¿”å›** | âœ… å§”æ‰˜ | `apply()` è¿”å› | `_fp8_linear_op.apply()` è¿”å› | BF16 tensor |

### 7.2 FP8 å„æ­¥éª¤è¯¦ç»†è¯´æ˜

#### 7.2.1 æƒé‡åŠ è½½ (`create_weights`)

```
CuBLASLtFp8LinearMethod.create_weights()
    â”‚
    â””â”€â†’ self.original_scheme.create_weights()  # å®Œå…¨å§”æ‰˜
            â”‚
            â””â”€â†’ CompressedTensorsW8A8Fp8.create_weights()
                    â”‚
                    â”œâ”€â†’ create_fp8_weight_parameter()      # åˆ›å»º FP8 æƒé‡ [N, K]
                    â”œâ”€â†’ create_fp8_scale_parameter()       # åˆ›å»º weight_scale
                    â””â”€â†’ create_fp8_input_scale() (å¯é€‰)    # é™æ€é‡åŒ–æ—¶åˆ›å»º input_scale
```

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ç»™åŸå§‹ schemeï¼Œä¸åšä»»ä½•ä¿®æ”¹ã€‚
**åç»­è®¡åˆ’**ï¼šå¦‚éœ€è‡ªå®šä¹‰æƒé‡æ ¼å¼ï¼Œéœ€è¦åœ¨æ­¤å¤„ä»‹å…¥ã€‚

#### 7.2.2 æƒé‡å¤„ç† (`process_weights_after_loading`)

```
CuBLASLtFp8LinearMethod.process_weights_after_loading()
    â”‚
    â””â”€â†’ self.original_scheme.process_weights_after_loading()  # å®Œå…¨å§”æ‰˜
            â”‚
            â””â”€â†’ CompressedTensorsW8A8Fp8.process_weights_after_loading()
                    â”‚
                    â”œâ”€â†’ process_fp8_weight_tensor_strategy()   # per-tensor
                    â”œâ”€â†’ process_fp8_weight_channel_strategy()  # per-channel â† Qwen ä½¿ç”¨
                    â””â”€â†’ process_fp8_weight_block_strategy()    # block
                    â”‚
                    â””â”€â†’ weight = weight.t()  # å…³é”®ï¼šæƒé‡è½¬ç½®ä¸º [K, N]
```

**å…³é”®å¤„ç†**ï¼š
- æ ¹æ®ç­–ç•¥å¤„ç† weight_scaleï¼ˆper-tensor/per-channel/blockï¼‰
- **æƒé‡è½¬ç½®**ï¼šä» `[N, K]` è½¬ä¸º `[K, N]`
- å°† weight å’Œ weight_scale è½¬ä¸º `torch.nn.Parameter`

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ã€‚
**æ³¨æ„**ï¼šcuBLASLt éœ€è¦ç‰¹å®š layoutï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹æ­¤å¤„ã€‚

#### 7.2.3 è¾“å…¥é‡åŒ–

```
CuBLASLtFp8LinearOp.apply()
    â”‚
    â””â”€â†’ self._fp8_linear_op.apply()  # å§”æ‰˜ç»™ vLLM çš„ Fp8LinearOp
            â”‚
            â””â”€â†’ Fp8LinearOp.apply() [w8a8_utils.py:440-490]
                    â”‚
                    â””â”€â†’ self.quant_fp8(input_2d, input_scale, input_scale_ub)
                            â”‚
                            â””â”€â†’ QuantFP8.__call__() [input_quant_fp8.py]
                                    â”‚
                                    â””â”€â†’ ops.scaled_fp8_quant(input, scale)
                                            â”‚
                                            â””â”€â†’ è¿”å› (qinput, x_scale)
                                                     FP8      FP32
```

**é‡åŒ–å…¬å¼**ï¼š`qinput = input / scale`ï¼Œå…¶ä¸­ `scale = max(|input|) / fp8_max`

**Scale å½¢çŠ¶ï¼ˆQwen FP8 é…ç½®ï¼‰**ï¼š
- `x_scale`: `[M, 1]` (per-token)
- `weight_scale`: `[N, 1]` (per-channel)

**å½“å‰çŠ¶æ€**ï¼šå®Œå…¨å§”æ‰˜ç»™ `QuantFP8`ã€‚
**åç»­è®¡åˆ’**ï¼šå¦‚éœ€è‡ªå®šä¹‰é‡åŒ–ï¼Œåœ¨ `CuBLASLtFp8LinearOp.apply()` ä¸­ç›´æ¥è°ƒç”¨è‡ªå·±çš„é‡åŒ–å‡½æ•°ã€‚

#### 7.2.4 GEMM + åé‡åŒ–

```
Fp8LinearOp.apply() [w8a8_utils.py:480-490]
    â”‚
    â”œâ”€â†’ dispatch_w8a8_scaled_mm(preferred_backend, ...)  # é€‰æ‹©åç«¯
    â”‚       â”‚
    â”‚       â””â”€â†’ è¿”å› cutlass_w8a8_scaled_mm (CUDA + SM90)
    â”‚
    â””â”€â†’ cutlass_w8a8_scaled_mm() [w8a8_utils.py:150-165]
            â”‚
            â””â”€â†’ ops.cutlass_scaled_mm(qinput, weight, scale_a, scale_b, bias)
                    â”‚
                    â””â”€â†’ C++ è°ƒç”¨: cutlass_scaled_mm_sm90_fp8()
                            â”‚
                            â””â”€â†’ output = scale_a * (scale_b * (qinput @ weight.T)) + bias
```

**GEMM å‚æ•°**ï¼š
| å‚æ•° | å½¢çŠ¶ | è¯´æ˜ |
|------|------|------|
| `qinput` | `[M, K]` FP8 | é‡åŒ–åçš„è¾“å…¥ |
| `weight` | `[K, N]` FP8 | è½¬ç½®åçš„æƒé‡ |
| `scale_a` | `[M, 1]` FP32 | è¾“å…¥ scale (per-token) |
| `scale_b` | `[N, 1]` FP32 | æƒé‡ scale (per-channel) |
| `bias` | `[N]` BF16 | å¯é€‰åç½® |
| `output` | `[M, N]` BF16 | è¾“å‡º |

**å½“å‰çŠ¶æ€**ï¼šé€šè¿‡ `_fp8_linear_op.apply()` é—´æ¥å§”æ‰˜ç»™ cutlassã€‚
**æ›¿æ¢ç‚¹**ï¼šè¿™é‡Œæ˜¯ cuBLASLt æ›¿æ¢çš„å…³é”®ä½ç½®ã€‚

### 7.3 INT8 å§”æ‰˜é“¾è·¯æ€»è§ˆ

| æ­¥éª¤ | å½“å‰çŠ¶æ€ | å¤–æŒ‚å‡½æ•° | è½¬å‘ç›®æ ‡ | vLLM å…·ä½“ä½ç½® |
|------|----------|----------|----------|---------------|
| **æƒé‡åŠ è½½** | âŒ æœªæ”¯æŒ | - | - | `compressed_tensors_w8a8_int8.py:43-96` |
| **æƒé‡å¤„ç†** | âŒ æœªæ”¯æŒ | - | - | `cutlass.py:34-109` (kernel.process_weights) |
| **è¾“å…¥é‡åŒ–** | âŒ æœªæ”¯æŒ | - | `ops.scaled_int8_quant()` | `cutlass.py:127-129` |
| **GEMM+åé‡åŒ–** | âŒ æœªæ”¯æŒ | - | `ops.cutlass_scaled_mm()` | `cutlass.py:144-147` |

### 7.4 INT8 è¯¦ç»†è¯´æ˜

å½“å‰ `wrap_scheme_with_cublaslt()` **ä¸æ”¯æŒ INT8**ï¼Œä»…æ£€æµ‹ `W8A8Fp8`ï¼š

```python
# cublaslt_linear_method.py:287
if "W8A8Fp8" in scheme_name:
    return CuBLASLtFp8LinearMethod(original_scheme)
else:
    # INT8 ä¼šèµ°è¿™é‡Œï¼Œè¿”å›åŸå§‹ scheme
    return original_scheme
```

**INT8 çš„æ¶æ„å·®å¼‚**ï¼š

1. **Scheme ç±»**ï¼š`CompressedTensorsW8A8Int8`
2. **Kernel é€‰æ‹©**ï¼šä½¿ç”¨ `ScaledMMLinearKernel` æ¶æ„
   - `CutlassScaledMMLinearKernel` (CUDA)
   - `TorchScaledMMLinearKernel` (fallback)
3. **é‡åŒ–å‡½æ•°**ï¼š`ops.scaled_int8_quant()` vs FP8 çš„ `ops.scaled_fp8_quant()`
4. **éå¯¹ç§°æ”¯æŒ**ï¼šINT8 æ”¯æŒ asymmetric quantization (AZP)

**INT8 å…³é”®å‡½æ•°ä½ç½®**ï¼š

| å‡½æ•° | æ–‡ä»¶ | è¯´æ˜ |
|------|------|------|
| `create_weights` | `compressed_tensors_w8a8_int8.py:43-96` | åˆ›å»º INT8 æƒé‡å’Œ scale |
| `process_weights_after_loading` | `cutlass.py:34-109` | æƒé‡è½¬ç½®ã€scale å¤„ç†ã€AZP è®¡ç®— |
| `apply_weights` | `cutlass.py:115-147` | INT8 é‡åŒ– + GEMM |
| `scaled_int8_quant` | `_custom_ops` | INT8 åŠ¨æ€/é™æ€é‡åŒ– |
| `cutlass_scaled_mm_azp` | `_custom_ops` | å¸¦ AZP çš„ INT8 GEMM |

### 7.5 åç»­æ¥ç®¡è®¡åˆ’

#### FP8 æ¥ç®¡æ­¥éª¤

1. **ä¿æŒå§”æ‰˜**ï¼š`create_weights()`, `process_weights_after_loading()`
2. **è‡ªä¸»å®ç°**ï¼š
   - åœ¨ `CuBLASLtFp8LinearOp.apply()` ä¸­ï¼š
     - ç›´æ¥è°ƒç”¨è‡ªå®šä¹‰çš„ FP8 é‡åŒ–å‡½æ•°ï¼ˆæˆ–å¤ç”¨ `QuantFP8`ï¼‰
     - è°ƒç”¨ cuBLASLt FP8 GEMM kernel
     - è¿”å› BF16 è¾“å‡º

```python
# CuBLASLtFp8LinearOp._apply_cublaslt() çš„ç›®æ ‡å®ç°
def _apply_cublaslt(self, input, weight, weight_scale, ...):
    # 1. è¾“å…¥é‡åŒ–ï¼ˆå¯å¤ç”¨ QuantFP8 æˆ–è‡ªå®ç°ï¼‰
    qinput, x_scale = self.quant_fp8(input, input_scale)
    
    # 2. cuBLASLt GEMMï¼ˆæ›¿æ¢ cutlass_scaled_mmï¼‰
    output = cublaslt_fp8_gemm(
        qinput, weight, x_scale, weight_scale, bias
    )
    
    # 3. è¿”å› BF16 è¾“å‡º
    return output
```

#### INT8 æ”¯æŒè®¡åˆ’

1. åˆ›å»º `CuBLASLtInt8LinearMethod` ç±»
2. åœ¨ `wrap_scheme_with_cublaslt()` ä¸­æ·»åŠ  INT8 æ£€æµ‹
3. å®ç° INT8 ç‰ˆæœ¬çš„ `apply_weights()`

---

## 8. æµ‹è¯•æ¡†æ¶è®¾è®¡

### 8.1 æµ‹è¯•è„šæœ¬æ¶æ„

æµ‹è¯•è„šæœ¬ `test_cublaslt_00_kernel.py` è®¾è®¡ä¸ºå¯ç‹¬ç«‹è¿è¡Œï¼Œç›´æ¥æµ‹è¯• GEMM kernel çš„æ­£ç¡®æ€§å’Œæ€§èƒ½ã€‚

```
æµ‹è¯•æµç¨‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ç”Ÿæˆæµ‹è¯•æ•°æ®                                             â”‚
â”‚     - input_bf16 [M, K] - BF16 æ ¼å¼                         â”‚
â”‚     - weight_fp8 [N, K] - FP8 æ ¼å¼ï¼ˆè½¬ç½®å [K, N]ï¼‰           â”‚
â”‚     - weight_scale [N, 1] - per-channel                     â”‚
â”‚     - bias [N] - å¯é€‰                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. è°ƒç”¨è¢«æµ‹ kernelï¼ˆé€šè¿‡ CuBLASLtFp8LinearOpï¼‰               â”‚
â”‚     - å†…éƒ¨ä¼šè¿›è¡Œ BF16 â†’ FP8 é‡åŒ–ï¼ˆper-token dynamicï¼‰        â”‚
â”‚     - æ‰§è¡Œ FP8 GEMMï¼ˆå½“å‰æ˜¯ cutlassï¼Œå°†æ›¿æ¢ä¸º cuBLASLtï¼‰      â”‚
â”‚     - è¿”å› BF16 è¾“å‡º                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. è®¡ç®—å‚è€ƒç»“æœï¼ˆæ¨¡æ‹Ÿå®Œæ•´ FP8 GEMM æµç¨‹ï¼‰                    â”‚
â”‚     - è¾“å…¥é‡åŒ–ï¼šinput_bf16 â†’ input_fp8, x_scale             â”‚
â”‚     - FP8 çŸ©é˜µä¹˜ï¼šinput_fp8 @ weight_fp8_t                   â”‚
â”‚     - åé‡åŒ–ï¼šresult * x_scale * weight_scale               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. æ¯”è¾ƒç»“æœå’Œæµ‹é‡æ€§èƒ½                                       â”‚
â”‚     - æ­£ç¡®æ€§ï¼štorch.allclose(output, reference)             â”‚
â”‚     - æ€§èƒ½ï¼šæµ‹é‡ååé‡ (TFLOPS)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 æµ‹è¯•æ¥å£è®¾è®¡

ä¸ºæ”¯æŒç‹¬ç«‹æµ‹è¯•ï¼Œåœ¨ `CuBLASLtFp8LinearOp` ä¸­å·²æ·»åŠ ä¸“ç”¨æ¥å£ï¼š

```python
class CuBLASLtFp8LinearOp:
    def apply_for_test(
        self,
        input: torch.Tensor,           # [M, K] BF16ï¼Œä¼šè¢«é‡åŒ–
        weight: torch.Tensor,          # [K, N] FP8ï¼Œå·²é‡åŒ–å·²è½¬ç½®ï¼Œcolumn-major
        weight_scale: torch.Tensor,    # [N, 1] FP32
        out_dtype: torch.dtype = torch.bfloat16,
        input_scale: torch.Tensor | None = None,  # None = dynamic quant
        bias: torch.Tensor | None = None,
    ) -> torch.Tensor:
        """
        ä¸“ç”¨äºæµ‹è¯•çš„æ¥å£ï¼Œè·³è¿‡ layer å¯¹è±¡ä¾èµ–
        """
        ...
```

### 8.3 æµ‹è¯•è„šæœ¬ä½¿ç”¨

```bash
# è¿è¡Œå•ä¸ªè§„æ¨¡æµ‹è¯•
python slidesparse/test/test_cublaslt_00_kernel.py --m 256 --n 896 --k 896

# è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
python slidesparse/test/test_cublaslt_00_kernel.py --all
```

### 8.4 æµ‹è¯•ç»“æœï¼ˆRTX 5080, SM 12.0ï¼‰

| M | N | K | Bias | Time(ms) | TFLOPS | Status |
|---:|---:|---:|:---:|---:|---:|:---:|
| 1 | 896 | 896 | No | 0.050 | 0.03 | âœ… |
| 1 | 4864 | 896 | No | 0.049 | 0.18 | âœ… |
| 32 | 896 | 896 | No | 0.049 | 1.05 | âœ… |
| 32 | 4864 | 896 | No | 0.049 | 5.68 | âœ… |
| 128 | 4864 | 896 | No | 0.049 | 22.76 | âœ… |
| 128 | 896 | 4864 | Yes | 0.105 | 10.66 | âœ… |
| 512 | 4864 | 896 | No | 0.061 | 73.64 | âœ… |
| 1024 | 4864 | 896 | No | 0.090 | 98.87 | âœ… |
| 2048 | 2048 | 2048 | No | 0.181 | 95.14 | âœ… |
| 4096 | 4096 | 4096 | No | 1.232 | 111.52 | âœ… |

**æµ‹è¯•ç¯å¢ƒè¯´æ˜**ï¼š
- å½“å‰åº•å±‚ kernel ä¸º CUTLASS `cutlass_scaled_mm`
- æ›¿æ¢ä¸º cuBLASLt åï¼Œéœ€é‡æ–°è¿è¡Œæµ‹è¯•éªŒè¯æ­£ç¡®æ€§å’Œæ€§èƒ½å·®å¼‚
- è¯¯å·®å®¹é™ï¼šrtol=0.05, atol=0.05ï¼ˆFP8 é‡åŒ–è¯¯å·®æ­£å¸¸èŒƒå›´ï¼‰

---

## 9. vLLM GEMM åç«¯åˆ†å‘æœºåˆ¶æ·±åº¦åˆ†æ

æœ¬ç« è¯¦ç»†è§£ç­”å…³äº FlashInferã€Paddingã€Triton å›é€€ç­‰å…³é”®é—®é¢˜ã€‚

### 9.1 FlashInfer æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™ä¼šè¢«è°ƒç”¨ï¼Ÿ

#### 9.1.1 FlashInfer ç®€ä»‹

**FlashInfer** æ˜¯ç”± NVIDIA å’Œç¤¾åŒºå¼€å‘çš„**é«˜æ€§èƒ½ Attention å’Œ GEMM å†…æ ¸åº“**ï¼Œä¸“é—¨é’ˆå¯¹ LLM æ¨ç†ä¼˜åŒ–ã€‚å®ƒæä¾›äº†ï¼š

1. **FlashAttention å˜ä½“**ï¼šé«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—
2. **FP8 GEMM**ï¼šåŸºäº `bmm_fp8` çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•
3. **MoE ç›¸å…³ç®—å­**ï¼šFused MoEã€AlltoAll ç­‰

**å…³é”®ä»£ç ä½ç½®**ï¼š`vllm/utils/flashinfer.py`

```python
# flashinfer_scaled_fp8_mm çš„å®ç°
def flashinfer_scaled_fp8_mm(
    a: torch.Tensor,  # [M, K] FP8
    b: torch.Tensor,  # [K, N] FP8
    scale_a: torch.Tensor,  # scalar only!
    scale_b: torch.Tensor,  # scalar only!
    out_dtype: torch.dtype,
    bias: torch.Tensor | None = None,
) -> torch.Tensor:
    # âš ï¸ é‡è¦é™åˆ¶ï¼šåªæ”¯æŒ per-tensor scale
    assert scale_a.numel() == 1 and scale_b.numel() == 1
    
    output = bmm_fp8(
        a.unsqueeze(0),
        b.unsqueeze(0),
        scale_a,
        scale_b,
        out_dtype,
        "auto",
    ).view(a.shape[0], b.shape[1])
    
    if bias is not None:
        output = output + bias
    return output
```

#### 9.1.2 FlashInfer çš„å¯ç”¨æ¡ä»¶

åœ¨ `Fp8LinearOp.__init__()` ä¸­ï¼ˆ[w8a8_utils.py:405-416](vllm/model_executor/layers/quantization/utils/w8a8_utils.py#L405-L416)ï¼‰ï¼š

```python
class Fp8LinearOp:
    def __init__(self, ...):
        if current_platform.is_rocm():
            self.preferred_backend = "rocm"
        elif current_platform.is_cuda() and cutlass_fp8_supported():
            # å…³é”®æ¡ä»¶ï¼šCC >= 100 (Blackwell B100/B200) ä¸” flashinfer å¯ç”¨
            if has_flashinfer() and current_platform.has_device_capability(100):
                self.preferred_backend = "flashinfer"
            else:
                self.preferred_backend = "cutlass"
        else:
            self.preferred_backend = "torch"
```

**FlashInfer FP8 GEMM å¯ç”¨æ¡ä»¶**ï¼š
| æ¡ä»¶ | è¯´æ˜ |
|------|------|
| `has_flashinfer()` | FlashInfer Python åŒ…å·²å®‰è£… |
| `has_device_capability(100)` | **SM >= 100 (Blackwell)** |
| `per_tensor_weights and per_tensor_activations` | **å¿…é¡»æ˜¯ per-tensor é‡åŒ–** |

#### 9.1.3 ä¸ºä»€ä¹ˆ RTX 5080 ä¸èµ° FlashInferï¼Ÿ

**RTX 5080 æ˜¯ SM 12.0 (Blackwell Consumer)**ï¼Œæ»¡è¶³ CC >= 100 çš„æ¡ä»¶ã€‚ä½†æ˜¯ï¼Œè®©æˆ‘ä»¬çœ‹ `dispatch_w8a8_scaled_mm` çš„é€»è¾‘ï¼ˆ[w8a8_utils.py:363-378](vllm/model_executor/layers/quantization/utils/w8a8_utils.py#L363-L378)ï¼‰ï¼š

```python
def dispatch_w8a8_scaled_mm(
    preferred_backend: str, per_tensor_weights: bool, per_tensor_activations: bool
) -> Callable[..., torch.Tensor]:
    
    # æƒ…å†µ 1: per-tensor W å’Œ per-tensor A
    if per_tensor_weights and per_tensor_activations:
        if preferred_backend == "flashinfer":
            return flashinfer_w8a8_scaled_mm  # âœ… èµ° FlashInfer
        if preferred_backend == "cutlass":
            return cutlass_w8a8_scaled_mm
        ...
    
    # æƒ…å†µ 2: per-channel W æˆ– per-token Aï¼ˆæˆ‘ä»¬çš„ Qwen FP8 é…ç½®ï¼‰
    # cutlass_scaled_mm supports per tensor/channel W and per tensor/token A
    if preferred_backend == "cutlass" or preferred_backend == "flashinfer":
        return cutlass_w8a8_scaled_mm  # âš ï¸ å›é€€åˆ° CUTLASSï¼
```

**ç»“è®º**ï¼š
- **Qwen FP8 æ¨¡å‹ä½¿ç”¨ per-channel weight + per-token activation**
- FlashInfer çš„ `bmm_fp8` **åªæ”¯æŒ per-tensor scale**
- å› æ­¤å³ä½¿ preferred_backend="flashinfer"ï¼Œä¹Ÿä¼š**å›é€€åˆ° CUTLASS**

**ç®€å•æ€»ç»“**ï¼š

| æ˜¾å¡ | SM | preferred_backend | å®é™…ä½¿ç”¨ | åŸå›  |
|------|-----|-------------------|---------|------|
| H100 | 90 | cutlass | CUTLASS | SM < 100 |
| B100/B200 | 100 | flashinfer | CUTLASS | per-channel/per-token ä¸æ”¯æŒ |
| RTX 5080 | 120 | flashinfer | CUTLASS | per-channel/per-token ä¸æ”¯æŒ |
| ä»»æ„ | - | - | FlashInfer | ä»…å½“ per-tensor W + per-tensor A |

### 9.2 Padding æœºåˆ¶è¯¦è§£

#### 9.2.1 ä¸ºä»€ä¹ˆéœ€è¦ Paddingï¼Ÿ

CUTLASS å’Œ cuBLASLt çš„ FP8 GEMM å†…æ ¸å¯¹çŸ©é˜µç»´åº¦æœ‰å¯¹é½è¦æ±‚ï¼š
- **M, K, N æœ€å¥½æ˜¯ 16 çš„å€æ•°**
- ä¸å¯¹é½ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™æˆ–è°ƒç”¨ fallback è·¯å¾„

#### 9.2.2 vLLM çš„ Padding ç­–ç•¥

åœ¨ `Fp8LinearOp.__init__()` ä¸­ï¼ˆ[w8a8_utils.py:420-430](vllm/model_executor/layers/quantization/utils/w8a8_utils.py#L420-L430)ï¼‰ï¼š

```python
class Fp8LinearOp:
    def __init__(self, ..., pad_output: bool | None = None):
        # pad_output çš„é»˜è®¤å€¼é€»è¾‘
        if pad_output is None:
            config = get_current_vllm_config().compilation_config
            pad_output = (
                # æ¡ä»¶1: æ²¡æœ‰ä½¿ç”¨ torch.compile
                config.mode < CompilationMode.VLLM_COMPILE
                # æ¡ä»¶2: ä½¿ç”¨ torch åç«¯ï¼ˆä¸æ˜¯ cutlass/flashinferï¼‰
                and self.preferred_backend == "torch"
            )
        
        # å¦‚æœéœ€è¦ paddingï¼Œpad åˆ° 17ï¼ˆè€Œä¸æ˜¯ 16ï¼‰
        # è¿™æ˜¯å› ä¸º torch._scaled_mm åœ¨ batch > 16 æ—¶æ€§èƒ½æ›´å¥½
        self.output_padding = 17 if pad_output else None
```

**å…³é”®ç»“è®º**ï¼š

| preferred_backend | torch.compile | æ˜¯å¦ Padding |
|-------------------|---------------|--------------|
| cutlass | å¦ | âŒ ä¸ Padding |
| cutlass | æ˜¯ | âŒ ä¸ Padding |
| flashinfer | å¦ | âŒ ä¸ Padding |
| flashinfer | æ˜¯ | âŒ ä¸ Padding |
| torch | å¦ | âœ… Padding åˆ° 17 |
| torch | æ˜¯ | âŒ ä¸ Paddingï¼ˆä¼šç ´ååŠ¨æ€ shapeï¼‰ |

#### 9.2.3 Padding åœ¨å“ªé‡Œå‘ç”Ÿï¼Ÿ

Padding å‘ç”Ÿåœ¨ `QuantFP8` çš„é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œ**ä¸æ˜¯åœ¨ GEMM é˜¶æ®µ**ï¼š

```python
# input_quant_fp8.py
class QuantFP8(CustomOp):
    def __init__(self, ..., num_token_padding: int | None = None):
        self.num_token_padding = num_token_padding
    
    def __call__(self, input, scale, scale_ub):
        if self.num_token_padding:
            # å¯¹è¾“å…¥è¿›è¡Œ padding
            input = pad_to(input, self.num_token_padding)
        return ops.scaled_fp8_quant(input, scale)
```

ç„¶ååœ¨ GEMM è¾“å‡ºæ—¶é€šè¿‡ `torch.narrow` æˆªå–æœ‰æ•ˆéƒ¨åˆ†ï¼š

```python
def torch_per_tensor_w8a8_scaled_mm(...):
    output = torch._scaled_mm(qinput, weight, ...)
    # æˆªå–æœ‰æ•ˆè¾“å‡ºï¼ˆå»é™¤ padding éƒ¨åˆ†ï¼‰
    return torch.narrow(output, 0, 0, qinput.shape[0]).view(*output_shape)
```

#### 9.2.4 å¦‚æœ M=31 ä¼šæ€æ ·ï¼Ÿ

**å¯¹äº CUTLASS/cuBLASLt åç«¯**ï¼ˆæˆ‘ä»¬çš„æƒ…å†µï¼‰ï¼š

1. **ä¸ä¼šè¿›è¡Œ Padding**ï¼š`output_padding = None`
2. **CUTLASS å¯ä»¥å¤„ç†ä»»æ„ M**ï¼šé€šè¿‡ tile masking å¤„ç†è¾¹ç•Œ
3. **æ€§èƒ½å¯èƒ½ç•¥æœ‰ä¸‹é™**ï¼šéå¯¹é½è®¿é—®ä¼šå¯¼è‡´éƒ¨åˆ† tile æµªè´¹

**å¯¹äº torch åç«¯**ï¼ˆfallbackï¼‰ï¼š
1. **ä¼š Padding åˆ° 17**ï¼šå¦‚æœ M < 17
2. **ç„¶å narrow å› M**ï¼šè¾“å‡ºæ—¶æˆªå–

### 9.3 Triton å›é€€æœºåˆ¶

#### 9.3.1 ä½•æ—¶ä¼šå›é€€åˆ° Tritonï¼Ÿ

åœ¨ `ops.cutlass_scaled_mm()` å‡½æ•°ä¸­ï¼ˆ[_custom_ops.py:863-875](vllm/_custom_ops.py#L863-L875)ï¼‰ï¼š

```python
def cutlass_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias=None):
    # ...
    
    # å…³é”®æ£€æŸ¥ï¼šb çš„ç»´åº¦æ˜¯å¦å¯¹ 16 å¯¹é½
    cutlass_compatible_b = b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0
    
    if current_platform.is_rocm() or not cutlass_compatible_b:
        # å›é€€åˆ° Triton å®ç°
        from vllm.model_executor.layers.quantization.compressed_tensors.triton_scaled_mm import (
            triton_scaled_mm,
        )
        out = triton_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias)
    else:
        # ä½¿ç”¨ CUTLASS
        out = torch.empty((a.shape[0], b.shape[1]), dtype=out_dtype, device=a.device)
        torch.ops._C.cutlass_scaled_mm(out, a, b, scale_a, scale_b, bias)
    
    return out.view(*target_shape)
```

#### 9.3.2 è¿™é‡Œçš„ B æ˜¯ä»€ä¹ˆï¼Ÿ

**B æ˜¯æƒé‡çŸ©é˜µï¼ˆWeightï¼‰**ï¼Œä¸æ˜¯æ¿€æ´»ï¼ˆActivationï¼‰ï¼

```python
# åœ¨ cutlass_w8a8_scaled_mm ä¸­çš„è°ƒç”¨
ops.cutlass_scaled_mm(
    qinput,   # A: æ¿€æ´» [M, K]
    weight,   # B: æƒé‡ [K, N]ï¼ˆå·²è½¬ç½®ï¼‰
    ...
)
```

**æƒé‡åœ¨åŠ è½½æ—¶å·²ç»è½¬ç½®**ï¼Œæ‰€ä»¥ï¼š
- åŸå§‹æƒé‡ï¼š`[N, K]`ï¼ˆout_features, in_featuresï¼‰
- è½¬ç½®åæƒé‡ï¼š`[K, N]`
- `b.shape[0] = K`ï¼Œ`b.shape[1] = N`

#### 9.3.3 æƒé‡æ˜¯å¦å¯¹é½ï¼Ÿ

**é€šå¸¸æƒé‡æ˜¯å¯¹é½çš„**ï¼Œå› ä¸ºï¼š
- `K = hidden_dim`ï¼ˆå¦‚ 896, 4096 ç­‰ï¼‰é€šå¸¸æ˜¯ 16 çš„å€æ•°
- `N = out_features`ï¼ˆå¦‚ 896, 4864 ç­‰ï¼‰é€šå¸¸ä¹Ÿæ˜¯ 16 çš„å€æ•°

**ä½†å¦‚æœä¸å¯¹é½**ï¼ˆä¾‹å¦‚æŸäº›ç‰¹æ®Šæ¨¡å‹ï¼‰ï¼Œå°±ä¼šå›é€€åˆ° Tritonã€‚

#### 9.3.4 M ä¸å¯¹é½ä¼šå½±å“å—ï¼Ÿ

**M ä¸å¯¹é½ä¸ä¼šå¯¼è‡´å›é€€åˆ° Triton**ï¼

æ£€æŸ¥çš„æ˜¯ `b.shape`ï¼ˆæƒé‡çš„ K å’Œ Nï¼‰ï¼Œä¸æ˜¯ `a.shape`ï¼ˆæ¿€æ´»çš„ Mï¼‰ã€‚

CUTLASS å†…æ ¸é€šè¿‡ tile masking å¤„ç† M è¾¹ç•Œï¼Œæ‰€ä»¥ï¼š
- M=31 â†’ ä½¿ç”¨ CUTLASS
- M=1 â†’ ä½¿ç”¨ CUTLASS
- åªæœ‰ K æˆ– N ä¸å¯¹é½æ‰å›é€€åˆ° Triton

### 9.4 torch.ops._C.cutlass_scaled_mm çš„æ¥æº

#### 9.4.1 ç»‘å®šä½ç½®

`torch.ops._C.cutlass_scaled_mm` æ˜¯é€šè¿‡ PyTorch çš„ C++ æ‰©å±•æœºåˆ¶æ³¨å†Œçš„ã€‚

**å£°æ˜**ï¼ˆ[csrc/torch_bindings.cpp:436-439](csrc/torch_bindings.cpp#L436-L439)ï¼‰ï¼š
```cpp
ops.def(
    "cutlass_scaled_mm(Tensor! out, Tensor a,"
    " Tensor b, Tensor a_scales, Tensor b_scales, Tensor? bias) -> ()");
ops.impl("cutlass_scaled_mm", torch::kCUDA, &cutlass_scaled_mm);
```

**å®ç°å…¥å£**ï¼ˆ[csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu:176-231](csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu#L176-L231)ï¼‰ï¼š
```cpp
void cutlass_scaled_mm(torch::Tensor& c, torch::Tensor const& a,
                       torch::Tensor const& b, torch::Tensor const& a_scales,
                       torch::Tensor const& b_scales,
                       std::optional<torch::Tensor> const& bias) {
    // æ ¹æ® SM ç‰ˆæœ¬åˆ†å‘åˆ°ä¸åŒå®ç°
    int32_t version_num = get_sm_version_num();
    
    if (version_num >= 120) {
        cutlass_scaled_mm_sm120(c, a, b, a_scales, b_scales, bias);  // Blackwell Consumer
    } else if (version_num >= 100) {
        cutlass_scaled_mm_sm100(c, a, b, a_scales, b_scales, bias);  // Blackwell Datacenter
    } else if (version_num >= 90) {
        cutlass_scaled_mm_sm90(c, a, b, a_scales, b_scales, bias);   // Hopper
    } else if (version_num == 89) {
        cutlass_scaled_mm_sm89(c, a, b, a_scales, b_scales, bias);   // Ada Lovelace
    } else if (version_num >= 80) {
        cutlass_scaled_mm_sm80(c, a, b, a_scales, b_scales, bias);   // Ampere
    }
    // ...
}
```

#### 9.4.2 CUTLASS æ˜¯ä»€ä¹ˆï¼Ÿ

**CUTLASS** (CUDA Templates for Linear Algebra Subroutines) æ˜¯ NVIDIA å¼€æºçš„**é«˜æ€§èƒ½ GEMM æ¨¡æ¿åº“**ï¼š

- GitHub: https://github.com/NVIDIA/cutlass
- æä¾›å„ç§æ•°æ®ç±»å‹çš„ GEMM å®ç°ï¼ˆFP8, INT8, FP16, BF16 ç­‰ï¼‰
- é’ˆå¯¹æ¯ä»£ GPU æ¶æ„ä¼˜åŒ–ï¼ˆSM75/80/89/90/100/120ï¼‰
- vLLM ä½¿ç”¨ CUTLASS ä½œä¸ºé»˜è®¤çš„ FP8 GEMM åç«¯

**ä½ å¯ä»¥æŠŠå®ƒç†è§£ä¸º**ï¼šä¸€ä¸ªå¼€æºçš„ã€é«˜æ€§èƒ½çš„ GEMM é»‘ç›’ï¼ŒvLLM å·²ç»å°è£…å¥½äº†ã€‚

### 9.5 å½“å‰ CuBLASLtFp8LinearOp.apply çš„å®Œæ•´æ€§åˆ†æ

#### 9.5.1 å½“å‰å®ç°å›é¡¾

å½“å‰çš„ `CuBLASLtFp8LinearOp.apply()` å·²ç»æ˜¯å®Œæ•´çš„ **quant + GEMM + dequant** æµç¨‹ï¼š

```python
def apply(self, input, weight, weight_scale, out_dtype, input_scale, input_scale_ub, bias):
    # 1. å±•å¹³è¾“å…¥
    input_2d = input.view(-1, input.shape[-1])
    output_shape = [*input.shape[:-1], weight.shape[1]]
    
    # 2. é‡åŒ–ï¼ˆä½¿ç”¨è‡ªå·±çš„ QuantFP8 å®ä¾‹ï¼‰
    if input.dtype != current_platform.fp8_dtype():
        qinput, x_scale = self.quant_fp8(input_2d, input_scale, input_scale_ub)
    else:
        qinput, x_scale = input_2d, input_scale
    
    # 3. GEMM + åé‡åŒ–ï¼ˆå½“å‰è°ƒç”¨ cutlassï¼Œåç»­æ›¿æ¢ä¸º cuBLASLtï¼‰
    return cublaslt_w8a8_scaled_mm(
        qinput=qinput,
        weight=weight,
        out_dtype=out_dtype,
        scale_a=x_scale,
        scale_b=weight_scale,
        bias=bias,
        output_shape=output_shape,
    )
```

#### 9.5.2 æ˜¯å¦æœ‰é—®é¢˜ï¼Ÿ

**å½“å‰å®ç°æ˜¯æ­£ç¡®çš„**ï¼Œä¸ vLLM åŸç”Ÿ `Fp8LinearOp.apply()` é€»è¾‘ä¸€è‡´ã€‚

**æ½œåœ¨é—®é¢˜å’Œæ³¨æ„äº‹é¡¹**ï¼š

| é—®é¢˜ | å½“å‰çŠ¶æ€ | è¯´æ˜ |
|------|----------|------|
| Padding | âœ… æ— é—®é¢˜ | æˆ‘ä»¬ä½¿ç”¨ `num_token_padding=None`ï¼Œä¸åš padding |
| Scale mode æ£€æµ‹ | âš ï¸ å¯æ”¹è¿› | å½“å‰ç›´æ¥è°ƒç”¨ CUTLASSï¼Œæ²¡æœ‰æ£€æµ‹ per-tensor/per-token |
| åç«¯åˆ†å‘ | âš ï¸ ç®€åŒ– | è·³è¿‡äº† `dispatch_w8a8_scaled_mm()`ï¼Œç›´æ¥ç”¨ cutlass |

#### 9.5.3 åç»­æ›¿æ¢ä½ç½®

**åªéœ€è¦ä¿®æ”¹ `cublaslt_w8a8_scaled_mm` å‡½æ•°**å³å¯å®Œæˆ cuBLASLt æ›¿æ¢ï¼š

```python
def cublaslt_w8a8_scaled_mm(*, qinput, weight, out_dtype, scale_a, scale_b, bias, output_shape):
    """
    å½“å‰å®ç°ï¼šè°ƒç”¨ cutlass_scaled_mmï¼ˆéªŒè¯æ¶æ„æ­£ç¡®æ€§ï¼‰
    åç»­å®ç°ï¼šæ›¿æ¢ä¸ºçœŸæ­£çš„ cuBLASLt kernel
    """
    # TODO: Phase 3 å®Œæˆåæ›¿æ¢ä¸ºçœŸæ­£çš„ cuBLASLt kernel
    # output = ops.cublaslt_scaled_mm(qinput, weight, scale_a, scale_b, bias)
    
    # å½“å‰ï¼šè°ƒç”¨ cutlass
    output = ops.cutlass_scaled_mm(
        qinput, weight, out_dtype=out_dtype, scale_a=scale_a, scale_b=scale_b, bias=bias
    )
    return output.view(*output_shape)
```

#### 9.5.4 æ›¿æ¢æ—¶éœ€è¦æ³¨æ„çš„ç‚¹

1. **Layout ä¸€è‡´æ€§**ï¼š
   - CUTLASS çš„ B æ˜¯ column-major `[K, N]`ï¼ˆstride: K=1, N=Kï¼‰
   - cuBLASLt ä¹Ÿä½¿ç”¨ column-majorï¼Œä½†å¯èƒ½éœ€è¦è°ƒæ•´ leading dimension

2. **Scale å¤„ç†**ï¼š
   - `scale_a`: per-token `[M, 1]` æˆ– per-tensor `[1]`
   - `scale_b`: per-channel `[N, 1]` æˆ– per-tensor `[1]`
   - cuBLASLt çš„ per-row/col scale éœ€è¦ `OUTER_VEC_32F` modeï¼ˆSM90+ onlyï¼‰

3. **è¾“å‡ºæ ¼å¼**ï¼š
   - ç¡®ä¿ cuBLASLt è¾“å‡ºä¸ CUTLASS ä¸€è‡´ï¼ˆBF16ï¼Œç›¸åŒ shapeï¼‰

4. **å¯¹é½è¦æ±‚**ï¼š
   - cuBLASLt å¯¹æŒ‡é’ˆå¯¹é½æœ‰æ›´ä¸¥æ ¼è¦æ±‚ï¼ˆ16 å­—èŠ‚ï¼‰
   - éœ€è¦æ£€æŸ¥ qinputã€weight æ˜¯å¦æ»¡è¶³

---

## 10. æ€»ç»“ä¸ä¸‹ä¸€æ­¥

### 10.1 å…³é”®å‘ç°

1. **FlashInfer FP8 GEMM åªæ”¯æŒ per-tensor scale**ï¼Œæˆ‘ä»¬çš„ Qwen FP8ï¼ˆper-channel W + per-token Aï¼‰ä¸ä¼šä½¿ç”¨å®ƒ
2. **Padding åªå¯¹ torch åç«¯ç”Ÿæ•ˆ**ï¼ŒCUTLASS ä¸éœ€è¦ padding
3. **Triton å›é€€åªçœ‹æƒé‡ç»´åº¦**ï¼ˆK å’Œ Nï¼‰ï¼ŒM ä¸å¯¹é½ä¸ä¼šè§¦å‘å›é€€
4. **å½“å‰ `CuBLASLtFp8LinearOp.apply()` å®ç°å®Œæ•´æ­£ç¡®**ï¼Œåªéœ€æ›¿æ¢ `cublaslt_w8a8_scaled_mm` å³å¯

### 10.2 ä¸‹ä¸€æ­¥æ“ä½œ

1. åœ¨ `csrc/` ç›®å½•å®ç°çœŸæ­£çš„ cuBLASLt FP8 GEMM wrapper
2. æ›¿æ¢ `cublaslt_w8a8_scaled_mm` ä¸­çš„ `ops.cutlass_scaled_mm` è°ƒç”¨
3. è¿è¡Œæµ‹è¯•éªŒè¯æ­£ç¡®æ€§å’Œæ€§èƒ½

---

## 11. cuBLASLt é›†æˆå…³é”®é—®é¢˜åˆ†æ

æœ¬ç« é’ˆå¯¹ cuBLASLt æ›¿æ¢ CUTLASS çš„å…³é”®æŠ€æœ¯é—®é¢˜è¿›è¡Œè¯¦ç»†åˆ†æã€‚

### 11.1 é—®é¢˜æ¦‚è§ˆä¸å®ç°è®¡åˆ’

| é—®é¢˜ç¼–å· | é—®é¢˜æè¿° | çŠ¶æ€ |
|---------|---------|------|
| Q1 | Layout åˆ†æï¼šCUTLASS çš„ A/W/Output å¸ƒå±€ | âœ… å·²åˆ†æ |
| Q2 | cuBLASLt T/N+C/C æ ¼å¼ä¸ vLLM çš„å¯¹æ¥ | âœ… å·²åˆ†æ |
| Q3 | Scale ç»´åº¦ä¸åé‡åŒ–æœºåˆ¶ | âœ… å·²åˆ†æ |
| Q4 | Bias å¹¿æ’­æ–¹å‘ | âœ… å·²åˆ†æ |
| Q5 | cuBLASLtMatmul API è°ƒç”¨è¦ç‚¹ | âœ… å·²åˆ†æ |

---

### 11.2 Q1: CUTLASS çš„ A/W/Output Layout åˆ†æ

#### 11.2.1 Safetensor åŸå§‹å­˜å‚¨æ ¼å¼

ä» checkpoint åˆ†æï¼ˆQwen2.5-0.5B-FP8ï¼‰ï¼š

```
Weight åŸå§‹æ ¼å¼:
    down_proj.weight:       [896, 4864]   FP8    â†’ [N, K] è¡Œä¸»åº
    down_proj.weight_scale: [896, 1]      BF16   â†’ [N, 1] per-channel
    gate_proj.weight:       [4864, 896]   FP8    â†’ [N, K] è¡Œä¸»åº
    gate_proj.weight_scale: [4864, 1]     BF16   â†’ [N, 1] per-channel

Bias æ ¼å¼ï¼ˆä»… QKV proj æœ‰ biasï¼‰:
    q_proj.bias: [896]  BF16 â†’ [N] 1D å‘é‡
    k_proj.bias: [128]  BF16 â†’ [N] 1D å‘é‡
```

**å…³é”®å‘ç°**ï¼š
- Weight: `[N, K]` è¡Œä¸»åºï¼ˆN=out_features, K=in_featuresï¼‰
- weight_scale: `[N, 1]` per-channelï¼ˆ**ä¸æ˜¯ `[1, K]`**ï¼Œä½ ä¹‹å‰çš„çŒœæµ‹éœ€è¦ä¿®æ­£ï¼‰
- Bias: `[N]` 1D å‘é‡

#### 11.2.2 vLLM æƒé‡å¤„ç†æµç¨‹

åœ¨ `compressed_tensors_w8a8_fp8.py` çš„ `process_weights_after_loading()` ä¸­ï¼š

```python
# ç¬¬ 145/151 è¡Œï¼šå…³é”®çš„è½¬ç½®æ“ä½œ
if self.strategy == QuantizationStrategy.TENSOR:
    weight, weight_scale, input_scale = process_fp8_weight_tensor_strategy(...)
    weight = weight.t()   # [N, K] â†’ [K, N]

elif self.strategy == QuantizationStrategy.CHANNEL:
    weight, weight_scale, input_scale = process_fp8_weight_channel_strategy(...)
    weight = weight.t()   # [N, K] â†’ [K, N]
```

**è½¬ç½®åçš„æƒé‡æ ¼å¼**ï¼š
- `weight`: `[K, N]`ï¼Œä½†åœ¨ PyTorch ä¸­ `.t()` å **stride å˜åŒ–**
- åŸå§‹ `[N, K]` è¡Œä¸»åºçš„ stride æ˜¯ `(K, 1)`
- `.t()` åå˜æˆ `[K, N]`ï¼Œstride æ˜¯ `(1, K)` â†’ **è¿™å°±æ˜¯åˆ—ä¸»åºï¼**

#### 11.2.3 CUTLASS æœŸæœ›çš„ Layout

ä» `scaled_mm_entry.cu` ç¬¬ 186-188 è¡Œçš„æ£€æŸ¥ï¼š

```cpp
// Check for strides and alignment
TORCH_CHECK(a.stride(1) == 1 && c.stride(1) == 1);  // Row-major
TORCH_CHECK(b.stride(0) == 1);                      // Column-major
```

ä» `scaled_mm.cuh` ç¬¬ 73-75 è¡Œçš„å®šä¹‰ï¼š

```cpp
ElementAB, cutlass::layout::RowMajor, AlignmentAB,     // A: RowMajor
ElementAB, cutlass::layout::ColumnMajor, AlignmentAB,  // B: ColumnMajor
```

**CUTLASS æœŸæœ›çš„è¾“å…¥**ï¼š

| çŸ©é˜µ | å½¢çŠ¶ | Layout | stride | è¯´æ˜ |
|------|------|--------|--------|------|
| A (input) | `[M, K]` | RowMajor | `(K, 1)` | æ¿€æ´»ï¼Œæ¯è¡Œè¿ç»­ |
| B (weight) | `[K, N]` | ColumnMajor | `(1, K)` | æƒé‡ï¼Œæ¯åˆ—è¿ç»­ |
| C (output) | `[M, N]` | RowMajor | `(N, 1)` | è¾“å‡ºï¼Œæ¯è¡Œè¿ç»­ |

**PyTorch è§†è§’**ï¼š
- `A [M, K]` è¡Œä¸»åº â†’ stride `(K, 1)` âœ…
- `B [K, N]` åˆ—ä¸»åº = `[N, K].t()` â†’ stride `(1, K)` âœ…
- `C [M, N]` è¡Œä¸»åº â†’ stride `(N, 1)` âœ…

#### 11.2.4 CUTLASS è®¡ç®—å…¬å¼ç¡®è®¤

```
CUTLASS è®¡ç®—: C[M,N] = A[M,K] Ã— B[K,N]
```

å…¶ä¸­ B æ˜¯åˆ—ä¸»åºå­˜å‚¨çš„ `[K, N]`ï¼Œç­‰ä»·äº PyTorch ä¸­ `weight.t()`ã€‚

---

### 11.3 Q2: cuBLASLt T/N+C/C æ ¼å¼å¯¹æ¥

#### 11.3.1 ä½ çš„éœ€æ±‚ç¡®è®¤

ä½ éœ€è¦ä½¿ç”¨ **W åœ¨å·¦ï¼ŒA åœ¨å³** çš„è®¡ç®—é¡ºåºï¼š
```
cuBLASLt è®¡ç®—: D = W Ã— A^T
```

å¹¶ä¸”è¦æ±‚ **T/N + C/C/C** æ ¼å¼ï¼ˆA è½¬ç½®ï¼ŒB ä¸è½¬ç½®ï¼Œå…¨éƒ¨åˆ—ä¸»åºï¼‰ã€‚

#### 11.3.2 Layout æ¨å¯¼

**è®¾å®š**ï¼š
- PyTorch ä¼ å…¥çš„ `A [M, K]` è¡Œä¸»åºï¼Œstride `(K, 1)`
- PyTorch ä¼ å…¥çš„ `W [K, N]` åˆ—ä¸»åºï¼ˆå³ `[N, K].t()`ï¼‰ï¼Œstride `(1, K)`

**cuBLASLt ç”¨åˆ—ä¸»åºè¯»å–è¡Œä¸»åº = éšå¼è½¬ç½®**ï¼š

| çŸ©é˜µ | PyTorch å­˜å‚¨ | cuBLASLt è¯»å–æ–¹å¼ | å®é™…è¯»åˆ°çš„ |
|------|-------------|------------------|-----------|
| A `[M, K]` row | å†…å­˜: `MÃ—K` è¿ç»­ | åˆ—ä¸»åºè¯» | `A^T [K, M]` |
| W `[K, N]` col | å†…å­˜: `KÃ—N` è¿ç»­ | åˆ—ä¸»åºè¯» | `W [K, N]` |

**è®¡ç®—è¿‡ç¨‹**ï¼š

```
cuBLASLt T/N é…ç½®:
    opA = CUBLAS_OP_T  â†’ å¯¹ "åˆ—ä¸»åºè¯»åˆ°çš„ A^T" å†è½¬ç½® â†’ å¾—åˆ° A [M, K]
    opB = CUBLAS_OP_N  â†’ å¯¹ "åˆ—ä¸»åºè¯»åˆ°çš„ W" ä¸è½¬ç½® â†’ å¾—åˆ° W [K, N]
    
ç­‰ç­‰ï¼Œè¿™å’Œä½ æƒ³è¦çš„ WÃ—A^T ä¸ä¸€æ ·ï¼
```

**é‡æ–°ç†è§£ä½ çš„éœ€æ±‚**ï¼š

ä½ è¯´çš„ "W åœ¨å·¦ï¼ŒA åœ¨å³" æ˜¯æŒ‡ cuBLASLt API çš„å‚æ•°é¡ºåºï¼Œè€Œä¸æ˜¯æ•°å­¦ä¸Šçš„çŸ©é˜µä¹˜æ³•é¡ºåºã€‚

è®©æˆ‘é‡æ–°æ¨å¯¼ï¼š

```
ä½ æƒ³è¦çš„æœ€ç»ˆè®¡ç®—ï¼ˆæ•°å­¦ä¸Šï¼‰: Output[M, N] = A[M, K] Ã— W^T[K, N]

ä½† vLLM ä¼ ç»™ä½ çš„ W å·²ç»æ˜¯ [K, N] åˆ—ä¸»åºäº†ï¼ˆå·²è½¬ç½®è¿‡ï¼‰ï¼

æ‰€ä»¥å®é™…è®¡ç®—: Output[M, N] = A[M, K] Ã— W_transposed[K, N]
```

#### 11.3.3 æ­£ç¡®çš„ cuBLASLt é…ç½®

**æ–¹æ¡ˆï¼šè®© cuBLASLt è®¡ç®— D = A Ã— B**

cuBLASLt é»˜è®¤æ˜¯åˆ—ä¸»åºï¼Œç”¨è¡Œä¸»åºæ•°æ®æ—¶éœ€è¦æŠ€å·§ï¼š

```
A: [M, K] è¡Œä¸»åºï¼Œstride (K, 1)
   â†’ cuBLASLt ç”¨åˆ—ä¸»åºè¯» â†’ è¯»æˆ [K, M]
   â†’ opA = T è½¬ç½®å›æ¥ â†’ å¾—åˆ° [M, K]

B: [K, N] åˆ—ä¸»åºï¼Œstride (1, K)  
   â†’ cuBLASLt ç”¨åˆ—ä¸»åºè¯» â†’ æ­£å¥½è¯»æˆ [K, N]
   â†’ opB = N ä¸è½¬ç½® â†’ å¾—åˆ° [K, N]

D: [M, N] è¡Œä¸»åº
   â†’ cuBLASLt ç”¨åˆ—ä¸»åºå†™ â†’ å†™æˆ [N, M]^T
   â†’ ä½†è¡Œä¸»åºçš„ [M, N] å­˜å‚¨ç­‰äºåˆ—ä¸»åºçš„ [N, M] âœ…
```

**ä½†æ˜¯ä½ è¦æ±‚ "W åœ¨å·¦ï¼ŒA åœ¨å³"ï¼**

è¿™éœ€è¦äº¤æ¢ A å’Œ B çš„ä½ç½®ï¼Œåˆ©ç”¨ `(AÃ—B)^T = B^T Ã— A^T` çš„æ€§è´¨ï¼š

```
cuBLASLt å‚æ•°é¡ºåº: D' = B' Ã— A'  ï¼ˆB'åœ¨å·¦ï¼ŒA'åœ¨å³ï¼‰

è®¾ï¼š
    B' = A^T = [K, M] ï¼ˆç”¨åˆ—ä¸»åºè¯»è¡Œä¸»åºçš„ A[M,K]ï¼‰
    A' = W   = [K, N] ï¼ˆåˆ—ä¸»åºçš„ Wï¼‰
    
é‚£ä¹ˆï¼š
    D' = A^T Ã— W = [K, M]^T Ã— [K, N]  ???  ç»´åº¦ä¸å¯¹ï¼
```

**æ­£ç¡®ç†è§£**ï¼šä½ æƒ³è¦çš„æ˜¯ cuBLASLt è®¡ç®— `D^T = W^T Ã— A^T`ï¼Œç„¶åç»“æœè‡ªåŠ¨å˜æˆ `D`ï¼š

```
å…³ç³»: D = A Ã— W  ç­‰ä»·äº  D^T = W^T Ã— A^T

æ‰€ä»¥:
    opA = T (å¯¹ cuBLAS çš„ç¬¬ä¸€ä¸ªçŸ©é˜µ W)
    opB = T (å¯¹ cuBLAS çš„ç¬¬äºŒä¸ªçŸ©é˜µ A)
    
ä½†è¿™æ˜¯ T/T é…ç½®ï¼Œä¸æ˜¯ä½ è¦çš„ T/Nï¼
```

#### 11.3.4 T/N + C/C/C é…ç½®çš„æ­£ç¡®ç”¨æ³•

è®©æˆ‘é‡æ–°ç†è§£ä½ çš„æ„å›¾ã€‚T/N æ„å‘³ç€ï¼š
- ç¬¬ä¸€ä¸ªçŸ©é˜µå‚æ•°ï¼šè½¬ç½®
- ç¬¬äºŒä¸ªçŸ©é˜µå‚æ•°ï¼šä¸è½¬ç½®

```
cublasLtMatmul çš„æ ‡å‡†è®¡ç®—: D = Î± Ã— op(A) Ã— op(B) + Î² Ã— C

è®¾:
    ç¬¬ä¸€ä¸ªçŸ©é˜µå‚æ•° = W_stored [N, K] è¡Œä¸»åº
    ç¬¬äºŒä¸ªçŸ©é˜µå‚æ•° = A_stored [M, K] è¡Œä¸»åº
    opA = T â†’ W_stored^T = [K, N]
    opB = N â†’ A_stored   = [M, K]  ä½†ç»´åº¦ä¸åŒ¹é…ï¼
```

**é—®é¢˜**ï¼š`op(A) Ã— op(B) = [K, N] Ã— [M, K]` ç»´åº¦ä¸å¯¹ï¼

**æ­£ç¡®æ–¹æ¡ˆ**ï¼šäº¤æ¢è¾“å…¥é¡ºåº

```
è®¾:
    ç¬¬ä¸€ä¸ªçŸ©é˜µå‚æ•° = W_stored [N, K] è¡Œä¸»åºï¼Œä¼ ç»™ cuBLAS æ—¶å‘Šè¯‰å®ƒæ˜¯ [K, N] åˆ—ä¸»åº
    ç¬¬äºŒä¸ªçŸ©é˜µå‚æ•° = A_stored [M, K] è¡Œä¸»åºï¼Œä¼ ç»™ cuBLAS æ—¶å‘Šè¯‰å®ƒæ˜¯ [K, M] åˆ—ä¸»åº
    opA = N â†’ [K, N]
    opB = T â†’ [K, M]^T = [M, K]  ç»´åº¦è¿˜æ˜¯ä¸å¯¹ï¼
```

**æœ€ç»ˆæ­£ç¡®é…ç½®ï¼ˆN/T + C/C/Cï¼‰**ï¼š

```
ç›®æ ‡: D[M, N] = A[M, K] Ã— W[K, N]

cuBLASLt é…ç½®ï¼ˆåˆ©ç”¨è¡Œä¸»åº = åˆ—ä¸»åºè½¬ç½®çš„ç‰¹æ€§ï¼‰:
    å®é™…ä¼ å…¥:
        A_ptr: æŒ‡å‘ W_stored çš„å†…å­˜ï¼ˆè¡Œä¸»åº [N, K]ï¼‰
        B_ptr: æŒ‡å‘ A_stored çš„å†…å­˜ï¼ˆè¡Œä¸»åº [M, K]ï¼‰
        C_ptr/D_ptr: è¾“å‡ºå†…å­˜
    
    å‘Šè¯‰ cuBLASLtï¼ˆåˆ—ä¸»åºè§†è§’ï¼‰:
        A: [K, N] åˆ—ä¸»åºï¼ˆå®é™…æ˜¯è¡Œä¸»åº [N, K] çš„å¦ä¸€ç§è§£è¯»ï¼‰
        B: [K, M] åˆ—ä¸»åºï¼ˆå®é™…æ˜¯è¡Œä¸»åº [M, K] çš„å¦ä¸€ç§è§£è¯»ï¼‰
        opA = N â†’ A ä¸å˜ï¼Œ[K, N]
        opB = T â†’ B è½¬ç½®ï¼Œ[K, M]^T = [M, K]
        
    è®¡ç®—: D = op(A) Ã— op(B) = [K, N] Ã— [M, K]  ç»´åº¦è¿˜æ˜¯ä¸å¯¹ï¼
```

**æˆ‘æ˜ç™½äº†ï¼ä½ éœ€è¦çš„æ˜¯ D^T çš„è®¡ç®—**ï¼š

```
ç›®æ ‡: D[M, N] = A[M, K] Ã— W[K, N]
ç­‰ä»·: D^T[N, M] = W^T[N, K] Ã— A^T[K, M]

cuBLASLt é…ç½®:
    ä¼ å…¥:
        A_ptr â†’ W_stored [N, K] è¡Œä¸»åº â†’ cuBLAS åˆ—ä¸»åºè¯»ä¸º [K, N]
        B_ptr â†’ A_stored [M, K] è¡Œä¸»åº â†’ cuBLAS åˆ—ä¸»åºè¯»ä¸º [K, M]
        
    opA = T â†’ [K, N]^T = [N, K]
    opB = N â†’ [K, M]
    
    è®¡ç®—: D' = op(A) Ã— op(B) = [N, K] Ã— [K, M] = [N, M] âœ…
    
    è¾“å‡º:
        D_ptr â†’ åˆ—ä¸»åºå†™ [N, M] â†’ è¡Œä¸»åºè¯»ä¸º [M, N] âœ…
```

**æœ€ç»ˆç­”æ¡ˆ**ï¼š

```cpp
// cuBLASLt T/N + Col/Col/Col é…ç½®
cublasOperation_t opA = CUBLAS_OP_T;   // å¯¹ Wï¼ˆç¬¬ä¸€ä¸ªå‚æ•°ï¼‰è½¬ç½®
cublasOperation_t opB = CUBLAS_OP_N;   // å¯¹ Aï¼ˆç¬¬äºŒä¸ªå‚æ•°ï¼‰ä¸è½¬ç½®

// çŸ©é˜µå¸ƒå±€
// W: è¡Œä¸»åº [N, K]ï¼Œä¼ ç»™ cuBLASLt å£°æ˜ä¸ºåˆ—ä¸»åº [K, N]ï¼Œlda = K
// A: è¡Œä¸»åº [M, K]ï¼Œä¼ ç»™ cuBLASLt å£°æ˜ä¸ºåˆ—ä¸»åº [K, M]ï¼Œldb = K
// D: åˆ—ä¸»åºå†™å‡º [N, M]ï¼Œç­‰äºè¡Œä¸»åº [M, N]ï¼Œldc = N

// è®¡ç®—: D' = W^T Ã— A = [K, N]^T Ã— [K, M] = [N, K] Ã— [K, M] = [N, M]
// è¾“å‡º: åˆ—ä¸»åº [N, M] = è¡Œä¸»åº [M, N] âœ…
```

---

### 11.4 Q3: Scale ç»´åº¦ä¸åé‡åŒ–æœºåˆ¶

#### 11.4.1 å®é™…çš„ Scale ç»´åº¦ï¼ˆä» checkpoint ç¡®è®¤ï¼‰

```
scale_a (input_scale):  per-token dynamic â†’ [M, 1] FP32
scale_b (weight_scale): per-channel       â†’ [N, 1] FP32 (ä¸æ˜¯ [1, K]!)
```

**ä½ ä¹‹å‰çš„çŒœæµ‹éœ€è¦ä¿®æ­£**ï¼šweight_scale æ˜¯ `[N, 1]` ä¸æ˜¯ `[1, K]`ã€‚

#### 11.4.2 CUTLASS çš„åé‡åŒ–å…¬å¼

ä» `scaled_mm_epilogues_c3x.hpp` åˆ†æï¼š

```cpp
// ScaledEpilogue çš„è®¡ç®—
using ScaleA = ColOrScalarLoad<float>;  // åˆ—æ–¹å‘åŠ è½½ â†’ [M, 1] å¹¿æ’­åˆ° [M, N]
using ScaleB = RowOrScalarLoad<float>;  // è¡Œæ–¹å‘åŠ è½½ â†’ [N, 1].T = [1, N] å¹¿æ’­åˆ° [M, N]

// EVTCompute0: tmp = ScaleB Ã— Accum
// EVTCompute1: D = ScaleA Ã— tmp

// å±•å¼€: D = ScaleA Ã— (ScaleB Ã— Accum)
//         = ScaleA[M,1] âŠ— (ScaleB[1,N] âŠ— Accum[M,N])
//         = (ScaleA[M,1] âŠ— ScaleB[1,N]) âŠ— Accum[M,N]  (å¹¿æ’­é€å…ƒç´ ä¹˜)
```

**CUTLASS åé‡åŒ–å…¬å¼**ï¼š

```
D[M,N] = scale_a[M,1] âŠ™ scale_b[1,N] âŠ™ (qA[M,K] Ã— qW[K,N])

å…¶ä¸­:
    qA: é‡åŒ–åçš„æ¿€æ´» [M, K] FP8
    qW: é‡åŒ–åçš„æƒé‡ [K, N] FP8  
    scale_a: [M, 1] å¹¿æ’­åˆ° [M, N]
    scale_b: [N, 1]^T = [1, N] å¹¿æ’­åˆ° [M, N]
    âŠ™: å¹¿æ’­é€å…ƒç´ ä¹˜æ³•
```

#### 11.4.3 cuBLASLt çš„ Outer Vector Scalingï¼ˆSM90+ï¼‰

ä»å®˜æ–¹æ–‡æ¡£ 3.1.4.3 èŠ‚ï¼š

```
Outer Vector Scaling for FP8 Data Types:

D_ij = Î± Ã— scale_A^i Ã— scale_B^j Ã— Î£(a_il Ã— b_lj) + Î² Ã— scale_C Ã— C_ij

å…¶ä¸­:
    scale_A: é•¿åº¦ä¸º M çš„å‘é‡ï¼Œæ¯è¡Œä¸€ä¸ª scale
    scale_B: é•¿åº¦ä¸º N çš„å‘é‡ï¼Œæ¯åˆ—ä¸€ä¸ª scale
```

**å¯ç”¨æ–¹æ³•**ï¼š

```cpp
// è®¾ç½® outer vector scaling mode
int32_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F;
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode));
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode));

// è®¾ç½® scale æŒ‡é’ˆ
float* scaleA = ...;  // é•¿åº¦ M
float* scaleB = ...;  // é•¿åº¦ N
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &scaleA, sizeof(scaleA));
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &scaleB, sizeof(scaleB));
```

#### 11.4.4 Scale é€‚é…é—®é¢˜

**CUTLASS çš„ scale è¯­ä¹‰**ï¼ˆAåœ¨å·¦ï¼‰ï¼š
- scale_a: å¯¹åº” A (input)ï¼Œç»´åº¦ [M, 1]
- scale_b: å¯¹åº” B (weight)ï¼Œç»´åº¦ [N, 1]

**cuBLASLt çš„ scale è¯­ä¹‰ï¼ˆWåœ¨å·¦ï¼ŒAåœ¨å³ï¼‰**ï¼š

ç”±äºæˆ‘ä»¬äº¤æ¢äº† A å’Œ B çš„ä½ç½®ï¼ˆW ä½œä¸º cuBLASLt çš„ç¬¬ä¸€ä¸ªå‚æ•°ï¼‰ï¼Œéœ€è¦ç›¸åº”è°ƒæ•´ï¼š

```
cuBLASLt è®¡ç®—: D'[N, M] = W'[N, K] Ã— A'[K, M]

å…¶ä¸­:
    W' = W^T (é€šè¿‡ opA=T å®ç°)
    A' = A^T (é€šè¿‡åˆ—ä¸»åºè¯»è¡Œä¸»åºå®ç°)

scale å¯¹åº”:
    cuBLASLt çš„ scale_A â†’ å¯¹åº” W â†’ ç»´åº¦ [N] (å› ä¸º op(W) çš„è¡Œæ•°æ˜¯ N)
    cuBLASLt çš„ scale_B â†’ å¯¹åº” A â†’ ç»´åº¦ [M] (å› ä¸º op(A) çš„åˆ—æ•°æ˜¯ M)
```

**å…³é”®é€‚é…**ï¼šéœ€è¦äº¤æ¢ä¼ å…¥ cuBLASLt çš„ scaleï¼š

```python
# vLLM ä¼ æ¥çš„:
#   scale_a: [M, 1] â†’ å¯¹åº” input
#   scale_b: [N, 1] â†’ å¯¹åº” weight

# ä¼ ç»™ cuBLASLt (Wåœ¨å·¦):
#   cublaslt_scale_A â†’ scale_b.squeeze() â†’ [N]  (weight scale)
#   cublaslt_scale_B â†’ scale_a.squeeze() â†’ [M]  (input scale)
```

---

### 11.5 Q4: Bias å¹¿æ’­æ–¹å‘

#### 11.5.1 Bias çš„å­˜å‚¨æ ¼å¼

ä» checkpoint ç¡®è®¤ï¼š
```
bias: [N] 1D å‘é‡ï¼ˆN = out_featuresï¼‰
```

#### 11.5.2 CUTLASS çš„ Bias å¤„ç†

ä» `scaled_mm_epilogues_c3x.hpp`ï¼š

```cpp
// ScaledEpilogueBias ä¸­
using Bias = RowLoad<ElementD>;  // è¡Œæ–¹å‘åŠ è½½

// è®¡ç®—å…¬å¼:
// D = ScaleA Ã— (ScaleB Ã— Accum) + Bias
// å…¶ä¸­ Bias æ˜¯ RowLoadï¼Œå¹¿æ’­åˆ°æ¯ä¸€è¡Œ
```

**Bias å¹¿æ’­æ–¹å‘**ï¼š`[1, N]` å¹¿æ’­åˆ° `[M, N]`ï¼Œå³**æ²¿ N ç»´åº¦ï¼ˆåˆ—æ–¹å‘ï¼‰å¹¿æ’­**ã€‚

æ¯ä¸€è¡Œï¼ˆæ¯ä¸ª tokenï¼‰åŠ ä¸Šç›¸åŒçš„ bias å‘é‡ã€‚

#### 11.5.3 cuBLASLt çš„ Bias å¤„ç†

```cpp
// è®¾ç½® epilogue ä¸º BIAS
cublasLtEpilogue_t epilogue = CUBLASLT_EPILOGUE_BIAS;
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_EPILOGUE, &epilogue, sizeof(epilogue));

// è®¾ç½® bias æŒ‡é’ˆå’Œç±»å‹
void* biasPtr = ...;  // [N] å‘é‡
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_BIAS_POINTER, &biasPtr, sizeof(biasPtr));

cudaDataType_t biasType = CUDA_R_16BF;  // BF16
cublasLtMatmulDescSetAttribute(matmulDesc, 
    CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE, &biasType, sizeof(biasType));
```

**æ³¨æ„**ï¼šç”±äºæˆ‘ä»¬è®¡ç®—çš„æ˜¯ `D'[N, M]` ç„¶åå­˜å‚¨ä¸ºè¡Œä¸»åº `[M, N]`ï¼Œbias çš„å¹¿æ’­æ–¹å‘ä¹Ÿéœ€è¦è€ƒè™‘ã€‚

å®é™…ä¸Šï¼ŒcuBLASLt çš„ bias æ˜¯åŠ åœ¨è¾“å‡ºçŸ©é˜µçš„**åˆ—æ–¹å‘**ï¼ˆå› ä¸ºå®ƒç”¨åˆ—ä¸»åºï¼‰ã€‚
- è¾“å‡º `D'[N, M]` åˆ—ä¸»åº
- bias `[N]` åŠ åˆ°æ¯ä¸€åˆ—
- ç­‰ä»·äºè¡Œä¸»åº `D[M, N]` ä¸­ï¼Œbias `[N]` åŠ åˆ°æ¯ä¸€è¡Œ âœ…

---

### 11.6 Q5: cuBLASLtMatmul API è°ƒç”¨è¦ç‚¹

#### 11.6.1 å®Œæ•´çš„ API è°ƒç”¨æ¡†æ¶

```cpp
#include <cublasLt.h>

cublasStatus_t cublaslt_fp8_gemm_impl(
    cublasLtHandle_t handle,
    int M, int N, int K,
    const void* W_ptr,        // è¡Œä¸»åº [N, K] FP8
    const void* A_ptr,        // è¡Œä¸»åº [M, K] FP8
    void* D_ptr,              // è¡Œä¸»åº [M, N] BF16
    const float* scale_w,     // [N] weight scale
    const float* scale_a,     // [M] input scale
    const void* bias,         // [N] bias (å¯é€‰)
    cudaDataType_t biasType,
    cudaStream_t stream
) {
    // 1. åˆ›å»º matmul æè¿°ç¬¦
    cublasLtMatmulDesc_t matmulDesc;
    cublasComputeType_t computeType = CUBLAS_COMPUTE_32F;
    cublasLtMatmulDescCreate(&matmulDesc, computeType, CUDA_R_32F);
    
    // 2. è®¾ç½®è½¬ç½®æ“ä½œ
    cublasOperation_t opA = CUBLAS_OP_T;   // W è½¬ç½®
    cublasOperation_t opB = CUBLAS_OP_N;   // A ä¸è½¬ç½®
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_TRANSA, &opA, sizeof(opA));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_TRANSB, &opB, sizeof(opB));
    
    // 3. è®¾ç½® outer vector scaling (SM90+)
    int8_t fastAccuMode = 1;
    cublasLtMatmulDescSetAttribute(matmulDesc,
        CUBLASLT_MATMUL_DESC_FAST_ACCUM, &fastAccuMode, sizeof(fastAccuMode));
    
    // Scale æ¨¡å¼å’ŒæŒ‡é’ˆ
    int32_t scaleMode = CUBLASLT_MATMUL_MATRIX_SCALE_OUTER_VEC_32F;
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_A_SCALE_MODE, &scaleMode, sizeof(scaleMode));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_B_SCALE_MODE, &scaleMode, sizeof(scaleMode));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_A_SCALE_POINTER, &scale_w, sizeof(scale_w));
    cublasLtMatmulDescSetAttribute(matmulDesc, 
        CUBLASLT_MATMUL_DESC_B_SCALE_POINTER, &scale_a, sizeof(scale_a));
    
    // 4. è®¾ç½® Bias (å¦‚æœæœ‰)
    if (bias != nullptr) {
        cublasLtEpilogue_t epilogue = CUBLASLT_EPILOGUE_BIAS;
        cublasLtMatmulDescSetAttribute(matmulDesc, 
            CUBLASLT_MATMUL_DESC_EPILOGUE, &epilogue, sizeof(epilogue));
        cublasLtMatmulDescSetAttribute(matmulDesc, 
            CUBLASLT_MATMUL_DESC_BIAS_POINTER, &bias, sizeof(bias));
        cublasLtMatmulDescSetAttribute(matmulDesc, 
            CUBLASLT_MATMUL_DESC_BIAS_DATA_TYPE, &biasType, sizeof(biasType));
    }
    
    // 5. åˆ›å»ºçŸ©é˜µå¸ƒå±€
    // W: è¡Œä¸»åº [N, K] â†’ å£°æ˜ä¸ºåˆ—ä¸»åº [K, N]ï¼Œlda = K
    cublasLtMatrixLayout_t Adesc;
    cublasLtMatrixLayoutCreate(&Adesc, CUDA_R_8F_E4M3, K, N, K);
    
    // A: è¡Œä¸»åº [M, K] â†’ å£°æ˜ä¸ºåˆ—ä¸»åº [K, M]ï¼Œldb = K
    cublasLtMatrixLayout_t Bdesc;
    cublasLtMatrixLayoutCreate(&Bdesc, CUDA_R_8F_E4M3, K, M, K);
    
    // D: åˆ—ä¸»åº [N, M]ï¼Œldc = N â†’ è¯»ä¸ºè¡Œä¸»åº [M, N]
    cublasLtMatrixLayout_t Ddesc;
    cublasLtMatrixLayoutCreate(&Ddesc, CUDA_R_16BF, N, M, N);
    
    // 6. è·å–æœ€ä¼˜ç®—æ³•
    cublasLtMatmulPreference_t preference;
    cublasLtMatmulPreferenceCreate(&preference);
    
    size_t workspaceSize = 64 * 1024 * 1024;  // 64 MB
    cublasLtMatmulPreferenceSetAttribute(preference, 
        CUBLASLT_MATMUL_PREF_MAX_WORKSPACE_BYTES, &workspaceSize, sizeof(workspaceSize));
    
    cublasLtMatmulHeuristicResult_t heuristicResult;
    int returnedResults = 0;
    cublasLtMatmulAlgoGetHeuristic(handle, matmulDesc, 
        Adesc, Bdesc, Ddesc, Ddesc,
        preference, 1, &heuristicResult, &returnedResults);
    
    // 7. åˆ†é… workspace
    void* workspace = nullptr;
    cudaMalloc(&workspace, heuristicResult.workspaceSize);
    
    // 8. æ‰§è¡Œ GEMM
    float alpha = 1.0f, beta = 0.0f;
    cublasLtMatmul(
        handle, matmulDesc,
        &alpha,
        W_ptr, Adesc,   // ç¬¬ä¸€ä¸ªçŸ©é˜µ: W
        A_ptr, Bdesc,   // ç¬¬äºŒä¸ªçŸ©é˜µ: A
        &beta,
        D_ptr, Ddesc,   // C (unused, beta=0)
        D_ptr, Ddesc,   // D (output)
        &heuristicResult.algo,
        workspace, heuristicResult.workspaceSize,
        stream
    );
    
    // 9. æ¸…ç†
    cudaFree(workspace);
    cublasLtMatmulPreferenceDestroy(preference);
    cublasLtMatrixLayoutDestroy(Adesc);
    cublasLtMatrixLayoutDestroy(Bdesc);
    cublasLtMatrixLayoutDestroy(Ddesc);
    cublasLtMatmulDescDestroy(matmulDesc);
    
    return CUBLAS_STATUS_SUCCESS;
}
```

#### 11.6.2 å…³é”®æ³¨æ„äº‹é¡¹

| è¦ç‚¹ | è¯´æ˜ |
|------|------|
| **çŸ©é˜µå‚æ•°é¡ºåº** | ç¬¬ä¸€ä¸ªæ˜¯ Wï¼ˆweightï¼‰ï¼Œç¬¬äºŒä¸ªæ˜¯ Aï¼ˆactivationï¼‰ |
| **op é…ç½®** | opA=Tï¼ˆè½¬ç½® Wï¼‰ï¼ŒopB=Nï¼ˆA ä¸è½¬ç½®ï¼‰ |
| **Layout å£°æ˜** | è¡Œä¸»åºæ•°æ®å£°æ˜ä¸ºåˆ—ä¸»åºï¼Œç»´åº¦äº¤æ¢ |
| **Scale äº¤æ¢** | cuBLASLt çš„ scale_A â†’ weight_scaleï¼Œscale_B â†’ input_scale |
| **Bias ç±»å‹** | éœ€è¦ä¸è¾“å‡ºç±»å‹åŒ¹é…æˆ–å…¼å®¹ |
| **Workspace** | é¢„åˆ†é…è¶³å¤Ÿç©ºé—´ï¼Œæ¨è 64MB |
| **Handle å¤ç”¨** | å…¨å±€ç¼“å­˜ handleï¼Œé¿å…é‡å¤åˆ›å»º |
| **Algorithm ç¼“å­˜** | ç›¸åŒé—®é¢˜è§„æ¨¡å¯å¤ç”¨å¯å‘å¼ç»“æœ |

#### 11.6.3 Python ä¾§é€‚é…

åœ¨ `cublaslt_w8a8_scaled_mm` ä¸­ï¼š

```python
def cublaslt_w8a8_scaled_mm(
    *,
    qinput: torch.Tensor,     # [M, K] FP8 è¡Œä¸»åº
    weight: torch.Tensor,     # [K, N] FP8 "åˆ—ä¸»åº"ï¼ˆå®é™…æ˜¯ .t() åçš„ viewï¼‰
    out_dtype: torch.dtype,
    scale_a: torch.Tensor,    # [M, 1] input scale
    scale_b: torch.Tensor,    # [N, 1] weight scale
    bias: torch.Tensor,       # [N] æˆ– None
    output_shape: list,
    **kwargs,
) -> torch.Tensor:
    """
    cuBLASLt FP8 Scaled MM
    
    å…³é”®ç†è§£ï¼š
    - vLLM ä¼ æ¥çš„ weight æ˜¯ [K,N] ä½† stride=(1,K)ï¼Œæ˜¯ .t() åçš„ view
    - ç‰©ç†å†…å­˜å®é™…æ˜¯ [N,K] è¡Œä¸»åºå­˜å‚¨
    - æˆ‘ä»¬éœ€è¦å† .t() æ¶ˆé™¤è¿™ä¸ªå‡è½¬ç½®ï¼Œè®© stride å’Œç‰©ç†å†…å­˜ä¸€è‡´
    """
    M, K = qinput.shape
    N = weight.shape[1]  # weight å½“å‰ shape æ˜¯ [K, N]
    
    # å…³é”®ï¼šæ¶ˆé™¤ .t() é€ æˆçš„ stride ä¸ä¸€è‡´
    # weight.t() å°† [K,N] stride=(1,K) å˜å› [N,K] stride=(K,1)
    # è¿™æ · stride å°±å’Œç‰©ç†å†…å­˜å¸ƒå±€ï¼ˆè¡Œä¸»åº [N,K]ï¼‰ä¸€è‡´äº†
    weight_row_major = weight.t()  # [N, K] è¡Œä¸»åºï¼Œæ— éœ€ contiguousï¼ˆæœ¬èº«å°±æ˜¯è¿ç»­çš„ï¼‰
    
    # è°ƒç”¨ cuBLASLt (æ³¨æ„ scale é¡ºåºäº¤æ¢)
    output = ops.cublaslt_scaled_mm(
        W=weight_row_major,        # [N, K] è¡Œä¸»åº
        A=qinput,                  # [M, K] è¡Œä¸»åº
        scale_W=scale_b.squeeze(), # [N] weight scale
        scale_A=scale_a.squeeze(), # [M] input scale
        bias=bias,                 # [N] bias
        out_dtype=out_dtype,
    )
    
    return output.view(*output_shape)
```

#### 11.6.4 å…³äº Bias å¹¿æ’­æ–¹å‘çš„æ¾„æ¸…

**cuBLASLt è®¡ç®—æµç¨‹**ï¼š

```
1. è¾“å…¥:
   W: [N,K] è¡Œä¸»åº â†’ cuBLASLt åˆ—ä¸»åºè¯»ä¸º [K,N]
   A: [M,K] è¡Œä¸»åº â†’ cuBLASLt åˆ—ä¸»åºè¯»ä¸º [K,M]

2. è®¡ç®—:
   opA=T: [K,N]^T = [N,K]
   opB=N: [K,M]
   D' = [N,K] Ã— [K,M] = [N,M]  (åˆ—ä¸»åºç»“æœ)

3. Bias å¹¿æ’­:
   bias: [N] å‘é‡
   åœ¨åˆ—ä¸»åº [N,M] ä¸­ï¼Œbias åŠ åˆ°"æ¯ä¸€åˆ—"ï¼ˆåˆ—ä¸»åºè§†è§’ï¼‰
   å³: D'[i,j] += bias[i], å¯¹æ‰€æœ‰ iâˆˆ[0,N), jâˆˆ[0,M)
   
4. è¾“å‡º:
   åˆ—ä¸»åº [N,M] å†™å…¥å†…å­˜
   æŒ‰è¡Œä¸»åºè§£è¯» = [M,N] âœ…
   
   ä»è¡Œä¸»åº [M,N] è§†è§’çœ‹:
   D[j,i] = D'[i,j] åŒ…å«äº† bias[i]
   å³æ¯ä¸€è¡Œ j çš„ç¬¬ i åˆ—éƒ½åŠ äº† bias[i]
   è¿™å°±æ˜¯"bias æ²¿ N ç»´åº¦å¹¿æ’­"çš„æ­£ç¡®è¡Œä¸º âœ…
```

**æ€»ç»“**ï¼š
- bias `[N]` åœ¨ cuBLASLt ä¸­ä¼šè‡ªåŠ¨æ­£ç¡®å¹¿æ’­
- æ— éœ€é¢å¤–å¤„ç†ï¼Œç›´æ¥ä¼ å…¥å³å¯

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.5*  
*æ›´æ–°æ—¥æœŸï¼š2025-01*  
*ä½œè€…ï¼šSlideSparse Team*
