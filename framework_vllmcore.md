# vLLM æ ¸å¿ƒæ¡†æ¶è¯¦è§£ (Framework vLLM Core)

æœ¬æ–‡æ¡£æ·±å…¥ä»‹ç» vLLM æ ¸å¿ƒæ¨ç†æ¡†æ¶ `vllm/` ç›®å½•çš„ç»„ç»‡ç»“æ„ï¼Œå¹¶æ¢³ç†å…¸å‹æ¨¡å‹ï¼ˆå¦‚ Llama/Qwen2ï¼‰çš„è°ƒç”¨é“¾ã€‚

---

## 1. vllm/ ç›®å½•ç»“æ„æ€»è§ˆ

```
vllm/
â”œâ”€â”€ entrypoints/        # ğŸ”µ å…¥å£ç‚¹ï¼ˆAPIã€CLIã€LLMç±»ï¼‰
â”œâ”€â”€ engine/             # ğŸ”µ æ¨ç†å¼•æ“ï¼ˆV0/Legacyï¼‰
â”œâ”€â”€ v1/                 # ğŸ”µ V1 æ–°æ¶æ„ï¼ˆæ¨èï¼‰
â”œâ”€â”€ model_executor/     # ğŸ”´ æ¨¡å‹æ‰§è¡Œå™¨ï¼ˆæ ¸å¿ƒï¼‰
â”œâ”€â”€ attention/          # ğŸ”´ æ³¨æ„åŠ›æœºåˆ¶
â”œâ”€â”€ distributed/        # åˆ†å¸ƒå¼ç›¸å…³
â”œâ”€â”€ config/             # é…ç½®ç±»
â”œâ”€â”€ inputs/             # è¾“å…¥å¤„ç†
â”œâ”€â”€ outputs.py          # è¾“å‡ºå®šä¹‰
â”œâ”€â”€ sampling_params.py  # é‡‡æ ·å‚æ•°
â”œâ”€â”€ sequence.py         # åºåˆ—å®šä¹‰
â”œâ”€â”€ lora/               # LoRA æ”¯æŒ
â”œâ”€â”€ multimodal/         # å¤šæ¨¡æ€æ”¯æŒ
â”œâ”€â”€ tokenizers/         # åˆ†è¯å™¨
â”œâ”€â”€ transformers_utils/  # Transformers å·¥å…·
â”œâ”€â”€ platforms/          # å¹³å°é€‚é…ï¼ˆCUDA/ROCm/CPUç­‰ï¼‰
â”œâ”€â”€ compilation/        # ç¼–è¯‘ä¼˜åŒ–
â”œâ”€â”€ triton_utils/       # Triton å·¥å…·
â”œâ”€â”€ plugins/            # æ’ä»¶ç³»ç»Ÿ
â”œâ”€â”€ utils/              # é€šç”¨å·¥å…·
â””â”€â”€ _custom_ops.py      # è‡ªå®šä¹‰ç®—å­ç»‘å®š
```

---

## 2. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 2.1 `entrypoints/` - å…¥å£ç‚¹

æ‰€æœ‰ç”¨æˆ·æ¥å£çš„å…¥å£ï¼š

```
entrypoints/
â”œâ”€â”€ llm.py                  # â­ LLM ç±» - ç¦»çº¿æ¨ç†ä¸»å…¥å£
â”œâ”€â”€ api_server.py           # FastAPI æœåŠ¡å™¨
â”œâ”€â”€ openai/                 # OpenAI å…¼å®¹ API
â”‚   â”œâ”€â”€ api_server.py       # OpenAI API æœåŠ¡å™¨
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cli/                    # CLI å‘½ä»¤
â”‚   â”œâ”€â”€ main.py             # CLI ä¸»å…¥å£
â”‚   â”œâ”€â”€ benchmark/          # benchmark å‘½ä»¤
â”‚   â””â”€â”€ serve.py            # serve å‘½ä»¤
â”œâ”€â”€ chat_utils.py           # èŠå¤©å·¥å…·
â””â”€â”€ ...
```

**LLM ç±»çš„ä¸»è¦æ–¹æ³•**ï¼š
```python
class LLM:
    def __init__(self, model, ...):         # åˆå§‹åŒ–
    def generate(self, prompts, ...):       # æ–‡æœ¬ç”Ÿæˆ
    def chat(self, messages, ...):          # å¯¹è¯ç”Ÿæˆ
    def encode(self, prompts, ...):         # ç¼–ç ï¼ˆEmbeddingï¼‰
    def embed(self, prompts, ...):          # åµŒå…¥ç”Ÿæˆ
```

### 2.2 `engine/` - æ¨ç†å¼•æ“ï¼ˆLegacyï¼‰

V0 ç‰ˆæœ¬çš„å¼•æ“å®ç°ï¼ˆç°å·²æŒ‡å‘ V1ï¼‰ï¼š

```
engine/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ llm_engine.py           # ç°åœ¨æŒ‡å‘ v1 ç‰ˆæœ¬
â”œâ”€â”€ async_llm_engine.py     # å¼‚æ­¥å¼•æ“
â”œâ”€â”€ arg_utils.py            # å‚æ•°è§£æ
â””â”€â”€ protocol.py             # åè®®å®šä¹‰
```

**å½“å‰çŠ¶æ€**ï¼š`llm_engine.py` ç°åœ¨å®é™…ä¸Šå¯¼å…¥è‡ª `v1`ï¼š
```python
from vllm.v1.engine.llm_engine import LLMEngine as V1LLMEngine
LLMEngine = V1LLMEngine
```

### 2.3 `v1/` - V1 æ–°æ¶æ„ â­

vLLM çš„æ–°ä¸€ä»£æ¶æ„ï¼Œæ¨èä½¿ç”¨ï¼š

```
v1/
â”œâ”€â”€ engine/                 # V1 å¼•æ“
â”‚   â”œâ”€â”€ llm_engine.py       # â­ LLMEngine ä¸»ç±»
â”‚   â”œâ”€â”€ core_client.py      # å¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ input_processor.py  # è¾“å…¥å¤„ç†
â”‚   â””â”€â”€ output_processor.py # è¾“å‡ºå¤„ç†
â”œâ”€â”€ worker/                 # Worker å®ç°
â”‚   â”œâ”€â”€ gpu_model_runner.py # â­ GPU æ¨¡å‹è¿è¡Œå™¨
â”‚   â”œâ”€â”€ gpu_worker.py       # GPU Worker
â”‚   â”œâ”€â”€ cpu_model_runner.py # CPU æ¨¡å‹è¿è¡Œå™¨
â”‚   â””â”€â”€ worker_base.py      # Worker åŸºç±»
â”œâ”€â”€ attention/              # V1 æ³¨æ„åŠ›
â”œâ”€â”€ sample/                 # é‡‡æ ·å™¨
â”œâ”€â”€ spec_decode/            # æŠ•æœºè§£ç 
â”œâ”€â”€ outputs.py              # è¾“å‡ºå®šä¹‰
â””â”€â”€ ...
```

### 2.4 `model_executor/` - æ¨¡å‹æ‰§è¡Œå™¨ â­â­â­

è¿™æ˜¯æ•´ä¸ªæ¨ç†æ¡†æ¶çš„æ ¸å¿ƒï¼ŒåŒ…å«æ¨¡å‹å®šä¹‰å’Œæ‰§è¡Œé€»è¾‘ï¼š

```
model_executor/
â”œâ”€â”€ models/                 # ğŸ”´ æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ llama.py            # â­ Llama æ¨¡å‹
â”‚   â”œâ”€â”€ qwen2.py            # â­ Qwen2 æ¨¡å‹
â”‚   â”œâ”€â”€ mixtral.py          # Mixtral MoE æ¨¡å‹
â”‚   â”œâ”€â”€ deepseek_v2.py      # DeepSeek V2
â”‚   â”œâ”€â”€ registry.py         # æ¨¡å‹æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ interfaces.py       # æ¨¡å‹æ¥å£å®šä¹‰
â”‚   â””â”€â”€ ...ï¼ˆ200+ æ¨¡å‹æ–‡ä»¶ï¼‰
â”œâ”€â”€ layers/                 # ğŸ”´ æ¨¡å‹å±‚å®ç°
â”‚   â”œâ”€â”€ linear.py           # â­ çº¿æ€§å±‚ï¼ˆå«é‡åŒ–ï¼‰
â”‚   â”œâ”€â”€ activation.py       # æ¿€æ´»å‡½æ•°
â”‚   â”œâ”€â”€ layernorm.py        # LayerNorm
â”‚   â”œâ”€â”€ rotary_embedding/   # RoPE ä½ç½®ç¼–ç 
â”‚   â”œâ”€â”€ vocab_parallel_embedding.py  # è¯åµŒå…¥
â”‚   â”œâ”€â”€ logits_processor.py # Logits å¤„ç†
â”‚   â”œâ”€â”€ fused_moe/          # èåˆ MoE å±‚
â”‚   â””â”€â”€ quantization/       # ğŸ”´ é‡åŒ–å®ç°
â”‚       â”œâ”€â”€ fp8.py          # â­ FP8 é‡åŒ–
â”‚       â”œâ”€â”€ awq.py          # AWQ é‡åŒ–
â”‚       â”œâ”€â”€ gptq.py         # GPTQ é‡åŒ–
â”‚       â”œâ”€â”€ base_config.py  # é‡åŒ–åŸºç±»
â”‚       â””â”€â”€ utils/          # é‡åŒ–å·¥å…·
â”œâ”€â”€ model_loader/           # æ¨¡å‹åŠ è½½å™¨
â”œâ”€â”€ custom_op.py            # è‡ªå®šä¹‰ç®—å­
â””â”€â”€ parameter.py            # å‚æ•°å®šä¹‰
```

### 2.5 `attention/` - æ³¨æ„åŠ›æœºåˆ¶

```
attention/
â”œâ”€â”€ layer.py                # æ³¨æ„åŠ›å±‚å°è£…
â”œâ”€â”€ selector.py             # åç«¯é€‰æ‹©å™¨
â”œâ”€â”€ backends/               # æ³¨æ„åŠ›åç«¯
â”‚   â”œâ”€â”€ abstract.py         # æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ flash_attn.py       # FlashAttention
â”‚   â”œâ”€â”€ flashinfer.py       # FlashInfer
â”‚   â”œâ”€â”€ xformers.py         # xFormers
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ops/                    # æ³¨æ„åŠ›æ“ä½œ
â””â”€â”€ utils/                  # å·¥å…·å‡½æ•°
```

### 2.6 `config/` - é…ç½®ç±»

æ‰€æœ‰é…ç½®ç›¸å…³çš„å®šä¹‰ï¼š

```
config/
â”œâ”€â”€ __init__.py             # å¯¼å‡ºæ‰€æœ‰é…ç½®ç±»
â”œâ”€â”€ model.py                # æ¨¡å‹é…ç½®
â”œâ”€â”€ cache.py                # KV Cache é…ç½®
â”œâ”€â”€ parallel.py             # å¹¶è¡Œé…ç½®
â”œâ”€â”€ scheduler.py            # è°ƒåº¦å™¨é…ç½®
â”œâ”€â”€ vllm.py                 # VllmConfig ä¸»é…ç½®
â””â”€â”€ ...
```

---

## 3. å…¸å‹è°ƒç”¨é“¾åˆ†æï¼ˆLlama/Qwen2ï¼‰

### 3.1 å®Œæ•´è°ƒç”¨é“¾å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·ä»£ç å…¥å£                                      â”‚
â”‚  llm = LLM(model="Qwen/Qwen2.5-7B-Instruct")                           â”‚
â”‚  outputs = llm.generate(prompts, sampling_params)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM ç±» (vllm/entrypoints/llm.py)                                       â”‚
â”‚                                                                         â”‚
â”‚  def __init__(self, model, ...):                                        â”‚
â”‚      engine_args = EngineArgs(model=model, ...)                         â”‚
â”‚      self.llm_engine = LLMEngine.from_engine_args(engine_args)         â”‚
â”‚                                                                         â”‚
â”‚  def generate(self, prompts, sampling_params):                          â”‚
â”‚      self._validate_and_add_requests(prompts, params)                   â”‚
â”‚      outputs = self._run_engine()  # å¾ªç¯è°ƒç”¨ engine.step()             â”‚
â”‚      return outputs                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLMEngine (vllm/v1/engine/llm_engine.py)                               â”‚
â”‚                                                                         â”‚
â”‚  def __init__(...):                                                     â”‚
â”‚      self.input_processor = InputProcessor(...)                         â”‚
â”‚      self.output_processor = OutputProcessor(...)                       â”‚
â”‚      self.engine_core = EngineCoreClient.make_client(...)              â”‚
â”‚                                                                         â”‚
â”‚  def step(self):                                                        â”‚
â”‚      engine_core_outputs = self.engine_core.step()  # è°ƒç”¨æ ¸å¿ƒå¼•æ“      â”‚
â”‚      return self.output_processor.process(...)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EngineCoreClient â†’ EngineCore (vllm/v1/engine/core_client.py)          â”‚
â”‚                                                                         â”‚
â”‚  å†…éƒ¨ç»´æŠ¤ model_executorï¼Œè´Ÿè´£è°ƒåº¦å’Œç®¡ç†è¯·æ±‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPUModelRunner (vllm/v1/worker/gpu_model_runner.py)                    â”‚
â”‚                                                                         â”‚
â”‚  def execute_model(self, scheduler_output):                             â”‚
â”‚      # 1. å‡†å¤‡è¾“å…¥                                                       â”‚
â”‚      model_input = self._prepare_inputs(...)                            â”‚
â”‚      # 2. å‡†å¤‡æ³¨æ„åŠ›å…ƒæ•°æ®                                                â”‚
â”‚      attn_metadata = self._prepare_attention_metadata(...)              â”‚
â”‚      # 3. æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­                                                â”‚
â”‚      with set_forward_context(...):                                     â”‚
â”‚          hidden_states = self.model(                                    â”‚
â”‚              input_ids=model_input.input_ids,                           â”‚
â”‚              positions=model_input.positions,                           â”‚
â”‚              ...                                                        â”‚
â”‚          )                                                              â”‚
â”‚      # 4. è®¡ç®— logits å¹¶é‡‡æ ·                                             â”‚
â”‚      logits = self.model.compute_logits(hidden_states)                  â”‚
â”‚      sampler_output = self.sampler(logits, sampling_metadata)           â”‚
â”‚      return sampler_output                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Forward (ä»¥ Qwen2ForCausalLM ä¸ºä¾‹)                                â”‚
â”‚  vllm/model_executor/models/qwen2.py                                    â”‚
â”‚                                                                         â”‚
â”‚  class Qwen2ForCausalLM:                                                â”‚
â”‚      def forward(self, input_ids, positions, ...):                      â”‚
â”‚          hidden_states = self.model(input_ids, positions, ...)          â”‚
â”‚          return hidden_states                                           â”‚
â”‚                                                                         â”‚
â”‚  class Qwen2Model:                                                      â”‚
â”‚      def forward(self, input_ids, positions, ...):                      â”‚
â”‚          # 1. Embedding                                                 â”‚
â”‚          hidden_states = self.embed_tokens(input_ids)                   â”‚
â”‚          residual = None                                                â”‚
â”‚          # 2. å¾ªç¯æ‰€æœ‰ Decoder Layer                                     â”‚
â”‚          for layer in self.layers:                                      â”‚
â”‚              hidden_states, residual = layer(positions, hidden_states,  â”‚
â”‚                                              residual)                  â”‚
â”‚          # 3. æœ€ç»ˆ LayerNorm                                             â”‚
â”‚          hidden_states, _ = self.norm(hidden_states, residual)          â”‚
â”‚          return hidden_states                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qwen2DecoderLayer.forward()                                            â”‚
â”‚                                                                         â”‚
â”‚  def forward(self, positions, hidden_states, residual):                 â”‚
â”‚      # Self Attention                                                   â”‚
â”‚      if residual is None:                                               â”‚
â”‚          residual = hidden_states                                       â”‚
â”‚          hidden_states = self.input_layernorm(hidden_states)            â”‚
â”‚      else:                                                              â”‚
â”‚          hidden_states, residual = self.input_layernorm(hidden_states,  â”‚
â”‚                                                         residual)       â”‚
â”‚      hidden_states = self.self_attn(positions, hidden_states)           â”‚
â”‚                                                                         â”‚
â”‚      # MLP                                                              â”‚
â”‚      hidden_states, residual = self.post_attention_layernorm(           â”‚
â”‚          hidden_states, residual)                                       â”‚
â”‚      hidden_states = self.mlp(hidden_states)                            â”‚
â”‚      return hidden_states, residual                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                       â”‚
            â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qwen2Attention.forward()     â”‚   â”‚  Qwen2MLP.forward()                â”‚
â”‚                               â”‚   â”‚                                   â”‚
â”‚  # QKV æŠ•å½±                    â”‚   â”‚  # gate_up_proj (W13)             â”‚
â”‚  qkv, _ = self.qkv_proj(x)    â”‚   â”‚  gate_up, _ = self.gate_up_proj(x)â”‚
â”‚  q, k, v = qkv.split(...)     â”‚   â”‚  x = self.act_fn(gate_up)         â”‚
â”‚  # RoPE                        â”‚   â”‚  # down_proj (W2)                 â”‚
â”‚  q, k = self.rotary_emb(...)  â”‚   â”‚  x, _ = self.down_proj(x)         â”‚
â”‚  # Attention                   â”‚   â”‚  return x                         â”‚
â”‚  attn_output = self.attn(qkv) â”‚   â”‚                                   â”‚
â”‚  # O æŠ•å½±                      â”‚   â”‚                                   â”‚
â”‚  output, _ = self.o_proj(...)  â”‚   â”‚                                   â”‚
â”‚  return output                 â”‚   â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 å…³é”®æ–‡ä»¶åˆ—è¡¨

| å±‚çº§ | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|-----|---------|------|
| å…¥å£ | `vllm/entrypoints/llm.py` | LLM ç±»å®šä¹‰ |
| å¼•æ“ | `vllm/v1/engine/llm_engine.py` | V1 LLMEngine |
| è¿è¡Œå™¨ | `vllm/v1/worker/gpu_model_runner.py` | GPU æ¨¡å‹è¿è¡Œå™¨ |
| æ¨¡å‹ | `vllm/model_executor/models/qwen2.py` | Qwen2 æ¨¡å‹ |
| æ¨¡å‹ | `vllm/model_executor/models/llama.py` | Llama æ¨¡å‹ |
| çº¿æ€§å±‚ | `vllm/model_executor/layers/linear.py` | çº¿æ€§å±‚å®šä¹‰ |
| æ³¨æ„åŠ› | `vllm/attention/layer.py` | æ³¨æ„åŠ›å±‚ |
| é‡åŒ– | `vllm/model_executor/layers/quantization/fp8.py` | FP8 é‡åŒ– |

---

## 4. æ¨¡å‹å®šä¹‰è¯¦è§£ï¼ˆLlama/Qwen2ï¼‰

### 4.1 æ¨¡å‹ç±»å±‚æ¬¡ç»“æ„

```
nn.Module
    â”‚
    â”œâ”€â”€ LlamaForCausalLM / Qwen2ForCausalLM    # é¡¶å±‚æ¨¡å‹
    â”‚       â”‚
    â”‚       â”œâ”€â”€ LlamaModel / Qwen2Model        # ä¸»ä½“æ¨¡å‹
    â”‚       â”‚       â”‚
    â”‚       â”‚       â”œâ”€â”€ VocabParallelEmbedding  # è¯åµŒå…¥
    â”‚       â”‚       â”œâ”€â”€ LlamaDecoderLayer[]     # Decoder å±‚åˆ—è¡¨
    â”‚       â”‚       â”‚       â”‚
    â”‚       â”‚       â”‚       â”œâ”€â”€ LlamaAttention   # æ³¨æ„åŠ›
    â”‚       â”‚       â”‚       â”‚   â”œâ”€â”€ QKVParallelLinear  # Wqkv
    â”‚       â”‚       â”‚       â”‚   â”œâ”€â”€ RowParallelLinear  # Wo
    â”‚       â”‚       â”‚       â”‚   â””â”€â”€ Attention          # æ³¨æ„åŠ›è®¡ç®—
    â”‚       â”‚       â”‚       â”‚
    â”‚       â”‚       â”‚       â”œâ”€â”€ LlamaMLP         # MLP
    â”‚       â”‚       â”‚       â”‚   â”œâ”€â”€ MergedColumnParallelLinear  # W13
    â”‚       â”‚       â”‚       â”‚   â””â”€â”€ RowParallelLinear           # W2
    â”‚       â”‚       â”‚       â”‚
    â”‚       â”‚       â”‚       â”œâ”€â”€ RMSNorm (input)
    â”‚       â”‚       â”‚       â””â”€â”€ RMSNorm (post_attn)
    â”‚       â”‚       â”‚
    â”‚       â”‚       â””â”€â”€ RMSNorm (final)
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ParallelLMHead                  # LM Head
    â”‚       â””â”€â”€ LogitsProcessor                 # Logits å¤„ç†
```

### 4.2 å››ä¸ªå…³é”®çº¿æ€§å±‚

åœ¨ Llama/Qwen2 è¿™ç±» Dense æ¨¡å‹ä¸­ï¼Œæ¯å±‚æœ‰ 4 ä¸ªå…³é”®çš„çº¿æ€§æŠ•å½±ï¼š

| å±‚å | ç±»å‹ | è¾“å…¥ç»´åº¦ | è¾“å‡ºç»´åº¦ | è¯´æ˜ |
|-----|------|---------|---------|------|
| `qkv_proj` | QKVParallelLinear | hidden_size | (q+k+v)_size | Q/K/V æŠ•å½±åˆå¹¶ |
| `o_proj` | RowParallelLinear | head_dim * num_heads | hidden_size | è¾“å‡ºæŠ•å½± |
| `gate_up_proj` | MergedColumnParallelLinear | hidden_size | intermediate_size * 2 | Gate + Up åˆå¹¶ |
| `down_proj` | RowParallelLinear | intermediate_size | hidden_size | Down æŠ•å½± |

### 4.3 ä»£ç ç¤ºä¾‹ï¼šQwen2MLP

```python
# vllm/model_executor/models/qwen2.py

class Qwen2MLP(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
        quant_config: QuantizationConfig | None = None,
        prefix: str = "",
    ) -> None:
        super().__init__()
        # gate_up_proj åˆå¹¶äº† gate_proj å’Œ up_proj
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,  # [gate_size, up_size]
            bias=False,
            quant_config=quant_config,
            prefix=f"{prefix}.gate_up_proj",
        )
        # down_proj
        self.down_proj = RowParallelLinear(
            intermediate_size,
            hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=f"{prefix}.down_proj",
        )
        self.act_fn = SiluAndMul()

    def forward(self, x):
        gate_up, _ = self.gate_up_proj(x)   # GEMM: W13
        x = self.act_fn(gate_up)            # SiLU æ¿€æ´»
        x, _ = self.down_proj(x)            # GEMM: W2
        return x
```

### 4.4 ä»£ç ç¤ºä¾‹ï¼šQwen2Attention

```python
# vllm/model_executor/models/qwen2.py

class Qwen2Attention(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # QKV åˆå¹¶æŠ•å½±
        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=True,
            quant_config=quant_config,
            prefix=f"{prefix}.qkv_proj",
        )
        # è¾“å‡ºæŠ•å½±
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=f"{prefix}.o_proj",
        )
        self.rotary_emb = get_rope(...)
        self.attn = Attention(...)

    def forward(self, positions, hidden_states):
        qkv, _ = self.qkv_proj(hidden_states)  # GEMM: Wqkv
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        q, k = self.rotary_emb(positions, q, k)  # RoPE
        attn_output = self.attn(q, k, v)          # Attention
        output, _ = self.o_proj(attn_output)     # GEMM: Wo
        return output
```

---

## 5. çº¿æ€§å±‚å®ç°ï¼ˆLinear Layersï¼‰

### 5.1 çº¿æ€§å±‚ç±»å±‚æ¬¡ç»“æ„

```
LinearBase (CustomOp)
    â”‚
    â”œâ”€â”€ ReplicatedLinear          # å¤åˆ¶çº¿æ€§å±‚
    â”œâ”€â”€ ColumnParallelLinear      # åˆ—å¹¶è¡Œçº¿æ€§å±‚
    â”‚   â”œâ”€â”€ MergedColumnParallelLinear  # åˆå¹¶åˆ—å¹¶è¡Œï¼ˆç”¨äº MLPï¼‰
    â”‚   â””â”€â”€ QKVParallelLinear           # QKV å¹¶è¡Œï¼ˆç”¨äº Attentionï¼‰
    â””â”€â”€ RowParallelLinear         # è¡Œå¹¶è¡Œçº¿æ€§å±‚
```

### 5.2 LinearBase åŸºç±»

```python
# vllm/model_executor/layers/linear.py

class LinearBase(CustomOp):
    def __init__(
        self,
        input_size: int,
        output_size: int,
        skip_bias_add: bool = False,
        params_dtype: torch.dtype | None = None,
        quant_config: QuantizationConfig | None = None,  # é‡åŒ–é…ç½®
        prefix: str = "",
        ...
    ):
        # æ ¹æ® quant_config é€‰æ‹©é‡åŒ–æ–¹æ³•
        if quant_config is None:
            self.quant_method = UnquantizedLinearMethod()
        else:
            self.quant_method = quant_config.get_quant_method(self, prefix=prefix)
```

### 5.3 Forward æµç¨‹

```python
# ColumnParallelLinear.forward()
def forward(self, input_):
    bias = self.bias if not self.skip_bias_add else None
    
    # Matrix multiply - æ ¸å¿ƒ GEMM è°ƒç”¨
    assert self.quant_method is not None
    output_parallel = self.quant_method.apply(self, input_, bias)
    
    if self.gather_output and self.tp_size > 1:
        output = tensor_model_parallel_all_gather(output_parallel)
    else:
        output = output_parallel
    
    return output, output_bias
```

---

## 6. å¼•æ“é…ç½®ä¸å‚æ•°ä¼ é€’

### 6.1 é…ç½®ç±»å±‚æ¬¡

```
VllmConfig                          # é¡¶å±‚é…ç½®
    â”œâ”€â”€ ModelConfig                 # æ¨¡å‹é…ç½®
    â”œâ”€â”€ CacheConfig                 # KV Cache é…ç½®
    â”œâ”€â”€ ParallelConfig              # å¹¶è¡Œé…ç½®
    â”œâ”€â”€ SchedulerConfig             # è°ƒåº¦å™¨é…ç½®
    â”œâ”€â”€ DeviceConfig                # è®¾å¤‡é…ç½®
    â”œâ”€â”€ LoRAConfig                  # LoRA é…ç½®ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ MultiModalConfig            # å¤šæ¨¡æ€é…ç½®ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ SpeculativeConfig           # æŠ•æœºè§£ç é…ç½®ï¼ˆå¯é€‰ï¼‰
    â””â”€â”€ ObservabilityConfig         # å¯è§‚æµ‹æ€§é…ç½®
```

### 6.2 å‚æ•°æµå‘

```
ç”¨æˆ·å‚æ•° (model, dtype, quantization, ...)
         â”‚
         â–¼
    EngineArgs                      # vllm/engine/arg_utils.py
         â”‚
         â–¼
    VllmConfig.from_engine_args()   # åˆ›å»ºå®Œæ•´é…ç½®
         â”‚
         â”œâ”€â”€â†’ ModelConfig           # ä¼ ç»™æ¨¡å‹åŠ è½½å™¨
         â”œâ”€â”€â†’ CacheConfig           # ä¼ ç»™ KV Cache ç®¡ç†
         â”œâ”€â”€â†’ ParallelConfig        # ä¼ ç»™åˆ†å¸ƒå¼ç®¡ç†
         â””â”€â”€â†’ quant_config          # ä¼ ç»™é‡åŒ–å±‚
```

---

## 7. å°ç»“

vLLM çš„æ ¸å¿ƒæ¶æ„å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š

1. **å…¥å£å±‚** (`entrypoints/`): æä¾›ç”¨æˆ·å‹å¥½çš„ API
2. **å¼•æ“å±‚** (`engine/`, `v1/engine/`): ç®¡ç†è¯·æ±‚è°ƒåº¦å’Œç”Ÿå‘½å‘¨æœŸ
3. **æ‰§è¡Œå±‚** (`v1/worker/`): åœ¨ GPU ä¸Šæ‰§è¡Œæ¨¡å‹æ¨ç†
4. **æ¨¡å‹å±‚** (`model_executor/models/`): å…·ä½“æ¨¡å‹å®ç°
5. **ç®—å­å±‚** (`model_executor/layers/`): åº•å±‚è®¡ç®—ç®—å­

å¯¹äºæƒ³è¦ä¿®æ”¹çº¿æ€§å±‚ GEMM çš„åœºæ™¯ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ï¼š
- `vllm/model_executor/layers/linear.py` - çº¿æ€§å±‚å®šä¹‰
- `vllm/model_executor/layers/quantization/*.py` - é‡åŒ–æ–¹æ³•
- `vllm/_custom_ops.py` - åº•å±‚ç®—å­ç»‘å®š

è¯¦ç»†çš„ GEMM è°ƒç”¨é“¾è¯·å‚è€ƒ [framework_lineargemm.md](./framework_lineargemm.md)ã€‚
