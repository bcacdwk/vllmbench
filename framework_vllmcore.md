# vLLM æ ¸å¿ƒæ¡†æ¶è¯¦è§£ (Framework vLLM Core)

æœ¬æ–‡æ¡£æ·±å…¥ä»‹ç» vLLM æ ¸å¿ƒæ¨ç†æ¡†æ¶ `vllm/` ç›®å½•çš„ç»„ç»‡ç»“æ„ï¼Œå¹¶æ¢³ç†å…¸å‹æ¨¡å‹ï¼ˆå¦‚ Llama/Qwen2ï¼‰çš„å®Œæ•´è°ƒç”¨é“¾ã€‚æœ¬æ–‡æ¡£æ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£ vLLM çš„å†…éƒ¨æ¶æ„ï¼Œä»¥ä¾¿è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€æ€§èƒ½ä¼˜åŒ–æˆ–æ·»åŠ æ–°åŠŸèƒ½ã€‚

---

## 0. æ¦‚è¿°ï¼švLLM çš„åˆ†å±‚æ¶æ„

vLLM é‡‡ç”¨æ¸…æ™°çš„åˆ†å±‚æ¶æ„è®¾è®¡ï¼Œä»ç”¨æˆ·æ¥å£åˆ°åº•å±‚è®¡ç®—åˆ†ä¸ºå¤šä¸ªå±‚æ¬¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ç”¨æˆ·å±‚ (User Layer)                            â”‚
â”‚  LLM ç±»ã€OpenAI APIã€CLI å‘½ä»¤                                            â”‚
â”‚  vllm/entrypoints/                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          å¼•æ“å±‚ (Engine Layer)                           â”‚
â”‚  LLMEngineã€AsyncLLMEngineã€è¯·æ±‚è°ƒåº¦ã€KV Cache ç®¡ç†                      â”‚
â”‚  vllm/v1/engine/                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         æ‰§è¡Œå±‚ (Executor Layer)                          â”‚
â”‚  GPUModelRunnerã€Workerã€æ‰¹å¤„ç†ç®¡ç†                                      â”‚
â”‚  vllm/v1/worker/                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          æ¨¡å‹å±‚ (Model Layer)                            â”‚
â”‚  æ¨¡å‹å®šä¹‰ï¼ˆ200+ æ¨¡å‹ï¼‰ã€Transformer å±‚å®ç°                               â”‚
â”‚  vllm/model_executor/models/                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          ç®—å­å±‚ (Operator Layer)                         â”‚
â”‚  çº¿æ€§å±‚ã€æ³¨æ„åŠ›å±‚ã€LayerNormã€æ¿€æ´»å‡½æ•°ã€é‡åŒ–                              â”‚
â”‚  vllm/model_executor/layers/                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         å†…æ ¸å±‚ (Kernel Layer)                            â”‚
â”‚  CUDA/Triton Kernelã€FlashAttentionã€PagedAttention                     â”‚
â”‚  csrc/ã€vllm/attention/backends/                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **æ¨¡å—åŒ–**: æ¯ä¸ªæ¨¡å—èŒè´£å•ä¸€ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
2. **å¯é…ç½®æ€§**: é€šè¿‡ Config ç±»ç»Ÿä¸€ç®¡ç†æ‰€æœ‰é…ç½®
3. **å¯æ‰©å±•æ€§**: æ’ä»¶ç³»ç»Ÿæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å’Œç®—å­
4. **é«˜æ€§èƒ½**: CUDA Graphã€é‡åŒ–ã€æ‰¹å¤„ç†ç­‰ä¼˜åŒ–
5. **å…¼å®¹æ€§**: æ”¯æŒå¤šç§ç¡¬ä»¶å¹³å°ï¼ˆCUDAã€ROCmã€CPUï¼‰

---

## 1. vllm/ ç›®å½•ç»“æ„æ€»è§ˆ

```
vllm/
â”œâ”€â”€ __init__.py             # åŒ…åˆå§‹åŒ–ï¼Œå¯¼å‡ºå…¬å…± API
â”œâ”€â”€ entrypoints/            # ğŸ”µ å…¥å£ç‚¹ï¼ˆAPIã€CLIã€LLMç±»ï¼‰
â”œâ”€â”€ engine/                 # ğŸ”µ æ¨ç†å¼•æ“ï¼ˆLegacyï¼Œç°æŒ‡å‘ V1ï¼‰
â”œâ”€â”€ v1/                     # ğŸ”µ V1 æ–°æ¶æ„ï¼ˆå½“å‰ä¸»è¦å®ç°ï¼‰
â”œâ”€â”€ model_executor/         # ğŸ”´ æ¨¡å‹æ‰§è¡Œå™¨ï¼ˆæ ¸å¿ƒï¼‰
â”œâ”€â”€ attention/              # ğŸ”´ æ³¨æ„åŠ›æœºåˆ¶
â”œâ”€â”€ distributed/            # åˆ†å¸ƒå¼ç›¸å…³
â”œâ”€â”€ config/                 # é…ç½®ç±»
â”œâ”€â”€ inputs/                 # è¾“å…¥å¤„ç†
â”œâ”€â”€ outputs.py              # è¾“å‡ºå®šä¹‰
â”œâ”€â”€ sampling_params.py      # é‡‡æ ·å‚æ•°
â”œâ”€â”€ pooling_params.py       # æ± åŒ–å‚æ•°
â”œâ”€â”€ sequence.py             # åºåˆ—å®šä¹‰
â”œâ”€â”€ lora/                   # LoRA æ”¯æŒ
â”œâ”€â”€ multimodal/             # å¤šæ¨¡æ€æ”¯æŒ
â”œâ”€â”€ tokenizers/             # åˆ†è¯å™¨
â”œâ”€â”€ transformers_utils/     # Transformers å·¥å…·
â”œâ”€â”€ platforms/              # å¹³å°é€‚é…ï¼ˆCUDA/ROCm/CPUç­‰ï¼‰
â”œâ”€â”€ compilation/            # ç¼–è¯‘ä¼˜åŒ–ï¼ˆCUDA Graph ç­‰ï¼‰
â”œâ”€â”€ triton_utils/           # Triton å·¥å…·
â”œâ”€â”€ plugins/                # æ’ä»¶ç³»ç»Ÿ
â”œâ”€â”€ utils/                  # é€šç”¨å·¥å…·
â”œâ”€â”€ _custom_ops.py          # è‡ªå®šä¹‰ç®—å­ç»‘å®š
â”œâ”€â”€ forward_context.py      # å‰å‘ä¼ æ’­ä¸Šä¸‹æ–‡
â”œâ”€â”€ envs.py                 # ç¯å¢ƒå˜é‡
â””â”€â”€ logger.py               # æ—¥å¿—ç³»ç»Ÿ
```

---

## 2. æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 2.1 `entrypoints/` - å…¥å£ç‚¹ â­â­â­

æ‰€æœ‰ç”¨æˆ·æ¥å£çš„å…¥å£ï¼Œæ˜¯ä¸ vLLM äº¤äº’çš„ç¬¬ä¸€å±‚ï¼š

```
entrypoints/
â”œâ”€â”€ __init__.py             # å¯¼å‡º LLM ç±»ç­‰
â”œâ”€â”€ llm.py                  # â­ LLM ç±» - ç¦»çº¿æ¨ç†ä¸»å…¥å£
â”œâ”€â”€ api_server.py           # FastAPI æœåŠ¡å™¨ï¼ˆé€šç”¨ï¼‰
â”œâ”€â”€ openai/                 # OpenAI å…¼å®¹ API
â”‚   â”œâ”€â”€ api_server.py       # â­ OpenAI API æœåŠ¡å™¨
â”‚   â”œâ”€â”€ serving_chat.py     # Chat Completion å¤„ç†
â”‚   â”œâ”€â”€ serving_completion.py # Text Completion å¤„ç†
â”‚   â”œâ”€â”€ serving_embedding.py  # Embedding å¤„ç†
â”‚   â”œâ”€â”€ protocol.py         # API åè®®å®šä¹‰
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cli/                    # CLI å‘½ä»¤
â”‚   â”œâ”€â”€ main.py             # CLI ä¸»å…¥å£ (vllm å‘½ä»¤)
â”‚   â”œâ”€â”€ serve.py            # vllm serve å‘½ä»¤
â”‚   â”œâ”€â”€ benchmark/          # vllm bench å­å‘½ä»¤
â”‚   â”‚   â”œâ”€â”€ throughput.py   # ååé‡æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ latency.py      # å»¶è¿Ÿæµ‹è¯•
â”‚   â”‚   â””â”€â”€ serve.py        # æœåŠ¡æµ‹è¯•
â”‚   â””â”€â”€ ...
â”œâ”€â”€ chat_utils.py           # èŠå¤©å·¥å…·å‡½æ•°
â”œâ”€â”€ score_utils.py          # è¯„åˆ†å·¥å…·
â”œâ”€â”€ utils.py                # é€šç”¨å·¥å…·
â”œâ”€â”€ launcher.py             # å¯åŠ¨å™¨
â””â”€â”€ context.py              # ä¸Šä¸‹æ–‡ç®¡ç†
```

#### LLM ç±»è¯¦è§£ (`vllm/entrypoints/llm.py`)

è¿™æ˜¯ç”¨æˆ·ä½¿ç”¨ vLLM è¿›è¡Œç¦»çº¿æ¨ç†çš„ä¸»è¦å…¥å£ï¼š

```python
# vllm/entrypoints/llm.py (ç®€åŒ–ç‰ˆ)

class LLM:
    """An LLM for generating texts from given prompts and sampling parameters.
    
    This class includes a tokenizer, a language model (possibly distributed
    across multiple GPUs), and GPU memory space allocated for intermediate
    states (aka KV cache).
    """
    
    def __init__(
        self,
        model: str,                              # æ¨¡å‹è·¯å¾„æˆ– HuggingFace ID
        *,
        tokenizer: str | None = None,            # å¯é€‰çš„ tokenizer è·¯å¾„
        tokenizer_mode: str = "auto",            # tokenizer æ¨¡å¼
        skip_tokenizer_init: bool = False,       # æ˜¯å¦è·³è¿‡ tokenizer åˆå§‹åŒ–
        trust_remote_code: bool = False,         # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
        tensor_parallel_size: int = 1,           # å¼ é‡å¹¶è¡Œ GPU æ•°é‡
        dtype: str = "auto",                     # æ•°æ®ç±»å‹
        quantization: str | None = None,         # é‡åŒ–æ–¹æ³•
        gpu_memory_utilization: float = 0.9,     # GPU å†…å­˜åˆ©ç”¨ç‡
        swap_space: float = 4,                   # äº¤æ¢ç©ºé—´ (GiB)
        enforce_eager: bool = False,             # å¼ºåˆ¶ eager æ¨¡å¼
        **kwargs,
    ) -> None:
        """LLM constructor."""
        
        # 1. åˆ›å»ºå¼•æ“å‚æ•°
        engine_args = EngineArgs(
            model=model,
            tokenizer=tokenizer,
            tensor_parallel_size=tensor_parallel_size,
            dtype=dtype,
            quantization=quantization,
            gpu_memory_utilization=gpu_memory_utilization,
            ...
        )
        
        # 2. åˆ›å»º LLMEngine (å®é™…æ˜¯ V1 ç‰ˆæœ¬)
        self.llm_engine = LLMEngine.from_engine_args(
            engine_args=engine_args,
            usage_context=UsageContext.LLM_CLASS
        )
        
        # 3. åˆå§‹åŒ–è¯·æ±‚è®¡æ•°å™¨å’Œå…¶ä»–çŠ¶æ€
        self.request_counter = Counter()
        self.model_config = self.llm_engine.model_config
        self.input_processor = self.llm_engine.input_processor

    def generate(
        self,
        prompts: PromptType | Sequence[PromptType],
        sampling_params: SamplingParams | Sequence[SamplingParams] | None = None,
        *,
        use_tqdm: bool = True,
        lora_request: LoRARequest | None = None,
    ) -> list[RequestOutput]:
        """Generates the completions for the input prompts.
        
        Args:
            prompts: The prompts to the LLM.
            sampling_params: The sampling parameters for text generation.
            use_tqdm: Whether to show progress bar.
            lora_request: LoRA request to use for generation.
            
        Returns:
            A list of RequestOutput objects containing the generated texts.
        """
        # 1. éªŒè¯æ¨¡å‹ç±»å‹
        if self.model_config.runner_type != "generate":
            raise ValueError("LLM.generate() is only supported for generative models.")
        
        # 2. ä½¿ç”¨é»˜è®¤é‡‡æ ·å‚æ•°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if sampling_params is None:
            sampling_params = self.get_default_sampling_params()
        
        # 3. æ·»åŠ æ‰€æœ‰è¯·æ±‚åˆ°å¼•æ“
        self._validate_and_add_requests(
            prompts=prompts,
            params=sampling_params,
            lora_request=lora_request,
        )
        
        # 4. è¿è¡Œå¼•æ“ï¼Œå¾ªç¯è°ƒç”¨ step() ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ
        outputs = self._run_engine(use_tqdm=use_tqdm)
        
        return outputs

    def _run_engine(self, *, use_tqdm: bool = True) -> list[RequestOutput]:
        """Run the engine until all requests are completed."""
        outputs = []
        
        # å¾ªç¯ç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆ
        while self.llm_engine.has_unfinished_requests():
            step_outputs = self.llm_engine.step()
            for output in step_outputs:
                if output.finished:
                    outputs.append(output)
        
        # æŒ‰è¯·æ±‚ ID æ’åº
        return sorted(outputs, key=lambda x: int(x.request_id))

    def chat(
        self,
        messages: list[dict],
        sampling_params: SamplingParams | None = None,
        *,
        chat_template: str | None = None,
        add_generation_prompt: bool = True,
    ) -> list[RequestOutput]:
        """Generate responses for a chat conversation.
        
        Converts the chat conversation to a text prompt using the tokenizer
        and calls the generate() method.
        """
        # 1. é¢„å¤„ç†èŠå¤©æ¶ˆæ¯ï¼Œåº”ç”¨èŠå¤©æ¨¡æ¿
        prompts = self.preprocess_chat(
            messages=messages,
            chat_template=chat_template,
            add_generation_prompt=add_generation_prompt,
        )
        
        # 2. è°ƒç”¨ generate
        return self.generate(prompts, sampling_params=sampling_params)

    def embed(self, prompts: PromptType | Sequence[PromptType], ...) -> list[EmbeddingRequestOutput]:
        """Generate embedding vectors for each prompt."""
        # ç”¨äº embedding æ¨¡å‹
        ...
    
    def classify(self, prompts: ...) -> list[ClassificationRequestOutput]:
        """Generate class logits for each prompt."""
        # ç”¨äºåˆ†ç±»æ¨¡å‹
        ...
```

#### API æœåŠ¡å™¨ (`vllm/entrypoints/openai/api_server.py`)

æä¾› OpenAI å…¼å®¹çš„ HTTP APIï¼š

```python
# ç®€åŒ–çš„ API æœåŠ¡å™¨ç»“æ„

app = FastAPI()

@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    """å¤„ç† Chat Completion è¯·æ±‚"""
    # 1. éªŒè¯è¯·æ±‚
    # 2. è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼
    # 3. è°ƒç”¨ AsyncLLMEngine
    # 4. è¿”å›å“åº”ï¼ˆæ”¯æŒæµå¼ï¼‰
    ...

@app.post("/v1/completions")
async def create_completion(request: CompletionRequest):
    """å¤„ç† Text Completion è¯·æ±‚"""
    ...

@app.post("/v1/embeddings")
async def create_embedding(request: EmbeddingRequest):
    """å¤„ç† Embedding è¯·æ±‚"""
    ...
```

### 2.2 `engine/` - æ¨ç†å¼•æ“ï¼ˆLegacyï¼‰

V0 ç‰ˆæœ¬çš„å¼•æ“å®ç°ï¼ˆç°å·²é‡å®šå‘åˆ° V1ï¼‰ï¼š

```
engine/
â”œâ”€â”€ __init__.py             # å¯¼å‡º LLMEngine
â”œâ”€â”€ llm_engine.py           # âš ï¸ ç°åœ¨å¯¼å…¥è‡ª v1
â”œâ”€â”€ async_llm_engine.py     # å¼‚æ­¥å¼•æ“åŒ…è£…å™¨
â”œâ”€â”€ arg_utils.py            # EngineArgs å‚æ•°è§£æ
â””â”€â”€ protocol.py             # åè®®å®šä¹‰
```

**å½“å‰çŠ¶æ€**ï¼š`engine/llm_engine.py` å®é™…ä¸Šä» V1 å¯¼å…¥ï¼š
```python
# vllm/engine/llm_engine.py (å½“å‰)
from vllm.v1.engine.llm_engine import LLMEngine

# è¿™æ„å‘³ç€ from vllm.engine import LLMEngine 
# å®é™…è·å–çš„æ˜¯ V1 ç‰ˆæœ¬çš„å¼•æ“
```

### 2.3 `v1/` - V1 æ–°æ¶æ„ â­â­â­

vLLM çš„æ–°ä¸€ä»£æ¶æ„ï¼Œæ˜¯å½“å‰çš„é»˜è®¤å®ç°ï¼š

```
v1/
â”œâ”€â”€ engine/                      # V1 å¼•æ“
â”‚   â”œâ”€â”€ __init__.py              # å¯¼å‡º EngineCoreRequest ç­‰
â”‚   â”œâ”€â”€ llm_engine.py            # â­ LLMEngine ä¸»ç±»
â”‚   â”œâ”€â”€ core_client.py           # å¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ input_processor.py       # è¾“å…¥å¤„ç†å™¨
â”‚   â”œâ”€â”€ output_processor.py      # è¾“å‡ºå¤„ç†å™¨
â”‚   â”œâ”€â”€ parallel_sampling.py     # å¹¶è¡Œé‡‡æ ·æ”¯æŒ (n>1)
â”‚   â””â”€â”€ async_llm_engine.py      # å¼‚æ­¥å¼•æ“
â”‚
â”œâ”€â”€ worker/                      # Worker å®ç°
â”‚   â”œâ”€â”€ gpu_model_runner.py      # â­ GPU æ¨¡å‹è¿è¡Œå™¨ (æ ¸å¿ƒ)
â”‚   â”œâ”€â”€ gpu_worker.py            # GPU Worker
â”‚   â”œâ”€â”€ gpu_input_batch.py       # è¾“å…¥æ‰¹æ¬¡ç®¡ç†
â”‚   â”œâ”€â”€ cpu_model_runner.py      # CPU æ¨¡å‹è¿è¡Œå™¨
â”‚   â”œâ”€â”€ worker_base.py           # Worker åŸºç±»
â”‚   â”œâ”€â”€ lora_model_runner_mixin.py # LoRA æ”¯æŒ
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ core/                        # æ ¸å¿ƒè°ƒåº¦é€»è¾‘
â”‚   â”œâ”€â”€ sched/                   # è°ƒåº¦å™¨
â”‚   â”‚   â”œâ”€â”€ scheduler.py         # è°ƒåº¦å™¨å®ç°
â”‚   â”‚   â””â”€â”€ output.py            # è°ƒåº¦è¾“å‡º
â”‚   â””â”€â”€ kv_cache_manager.py      # KV Cache ç®¡ç†
â”‚
â”œâ”€â”€ attention/                   # V1 æ³¨æ„åŠ›
â”‚   â””â”€â”€ backends/                # æ³¨æ„åŠ›åç«¯
â”‚       â”œâ”€â”€ flash_attn.py        # FlashAttention
â”‚       â”œâ”€â”€ flashinfer.py        # FlashInfer
â”‚       â”œâ”€â”€ triton_attn.py       # Triton å®ç°
â”‚       â”œâ”€â”€ flex_attention.py    # Flex Attention
â”‚       â””â”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ sample/                      # é‡‡æ ·å™¨
â”‚   â”œâ”€â”€ sampler.py               # é‡‡æ ·å®ç°
â”‚   â”œâ”€â”€ metadata.py              # é‡‡æ ·å…ƒæ•°æ®
â”‚   â”œâ”€â”€ logits_processor/        # Logits å¤„ç†å™¨
â”‚   â””â”€â”€ rejection_sampler.py     # æ‹’ç»é‡‡æ ·ï¼ˆæŠ•æœºè§£ç ç”¨ï¼‰
â”‚
â”œâ”€â”€ spec_decode/                 # æŠ•æœºè§£ç 
â”‚   â”œâ”€â”€ eagle.py                 # EAGLE æŠ•æœºè§£ç 
â”‚   â”œâ”€â”€ medusa.py                # Medusa æŠ•æœºè§£ç 
â”‚   â”œâ”€â”€ ngram_proposer.py        # N-gram æè®®å™¨
â”‚   â””â”€â”€ suffix_decoding.py       # åç¼€è§£ç 
â”‚
â”œâ”€â”€ kv_cache_interface.py        # KV Cache æ¥å£
â”œâ”€â”€ kv_offload/                  # KV Cache å¸è½½
â”œâ”€â”€ outputs.py                   # è¾“å‡ºå®šä¹‰
â”œâ”€â”€ request.py                   # è¯·æ±‚å®šä¹‰
â””â”€â”€ metrics/                     # æŒ‡æ ‡æ”¶é›†
```

#### V1 LLMEngine è¯¦è§£ (`vllm/v1/engine/llm_engine.py`)

```python
# vllm/v1/engine/llm_engine.py (ç®€åŒ–ç‰ˆ)

class LLMEngine:
    """V1 LLMEngine - å½“å‰æ¨èçš„æ¨ç†å¼•æ“å®ç°ã€‚"""
    
    def __init__(
        self,
        vllm_config: VllmConfig,
        executor_class: type[Executor],
        log_stats: bool,
        ...
    ) -> None:
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        
        # 1. åˆå§‹åŒ– Tokenizer
        if not self.model_config.skip_tokenizer_init:
            tokenizer = cached_tokenizer_from_config(self.model_config)
        
        # 2. åˆ›å»ºè¾“å…¥å¤„ç†å™¨
        self.input_processor = InputProcessor(self.vllm_config, tokenizer)
        
        # 3. åˆ›å»ºè¾“å‡ºå¤„ç†å™¨ï¼ˆè´Ÿè´£ detokenizationï¼‰
        self.output_processor = OutputProcessor(
            self.tokenizer,
            log_stats=self.log_stats,
        )
        
        # 4. åˆ›å»ºå¼•æ“æ ¸å¿ƒå®¢æˆ·ç«¯
        self.engine_core = EngineCoreClient.make_client(
            multiprocess_mode=multiprocess_mode,
            asyncio_mode=False,
            vllm_config=vllm_config,
            executor_class=executor_class,
        )

    @classmethod
    def from_engine_args(cls, engine_args: EngineArgs, ...) -> "LLMEngine":
        """Creates an LLM engine from the engine arguments."""
        # 1. ä» engine_args åˆ›å»º VllmConfig
        vllm_config = engine_args.create_engine_config(usage_context)
        
        # 2. è·å–æ‰§è¡Œå™¨ç±»
        executor_class = Executor.get_class(vllm_config)
        
        # 3. åˆ›å»ºå¼•æ“
        return cls(vllm_config=vllm_config, executor_class=executor_class, ...)

    def add_request(
        self,
        request_id: str,
        prompt: EngineCoreRequest | PromptType,
        params: SamplingParams | PoolingParams,
        ...
    ) -> None:
        """Add a request to the engine."""
        # 1. å¤„ç†åŸå§‹è¾“å…¥
        if isinstance(prompt, EngineCoreRequest):
            request = prompt
        else:
            request = self.input_processor.process_inputs(
                request_id, prompt, params, ...
            )
        
        # 2. æ·»åŠ åˆ°è¾“å‡ºå¤„ç†å™¨ï¼ˆç”¨äºè·Ÿè¸ªï¼‰
        self.output_processor.add_request(request, ...)
        
        # 3. æ·»åŠ åˆ°å¼•æ“æ ¸å¿ƒ
        self.engine_core.add_request(request)

    def step(self) -> list[RequestOutput | PoolingRequestOutput]:
        """Perform one decoding iteration."""
        # 1. ä»å¼•æ“æ ¸å¿ƒè·å–è¾“å‡º
        outputs = self.engine_core.get_output()
        
        # 2. å¤„ç†è¾“å‡ºï¼ˆdetokenization ç­‰ï¼‰
        processed_outputs = self.output_processor.process_outputs(
            outputs.outputs,
            ...
        )
        
        # 3. ä¸­æ­¢å·²å®Œæˆçš„è¯·æ±‚
        self.engine_core.abort_requests(processed_outputs.reqs_to_abort)
        
        return processed_outputs.request_outputs
```

#### GPUModelRunner è¯¦è§£ (`vllm/v1/worker/gpu_model_runner.py`)

è¿™æ˜¯å®é™…æ‰§è¡Œæ¨¡å‹æ¨ç†çš„æ ¸å¿ƒç±»ï¼š

```python
# vllm/v1/worker/gpu_model_runner.py (ç®€åŒ–ç‰ˆ)

class GPUModelRunner:
    """GPU Model Runner - åœ¨ GPU ä¸Šæ‰§è¡Œæ¨¡å‹æ¨ç†"""
    
    def __init__(self, vllm_config: VllmConfig, device: torch.device):
        self.vllm_config = vllm_config
        self.model_config = vllm_config.model_config
        self.device = device
        
        # æ¨¡å‹ç›¸å…³
        self.model: nn.Module  # åœ¨ load_model() ä¸­è®¾ç½®
        
        # KV Cache
        self.kv_caches: list[torch.Tensor] = []
        
        # é‡‡æ ·å™¨
        self.sampler = Sampler(...)
        
        # æŠ•æœºè§£ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.speculative_config:
            self.drafter = ...  # EAGLE/Medusa/NGram
            self.rejection_sampler = RejectionSampler(self.sampler)
        
        # è¯·æ±‚çŠ¶æ€ç¼“å­˜
        self.requests: dict[str, CachedRequestState] = {}
        
        # è¾“å…¥æ‰¹æ¬¡ç®¡ç†
        self.input_batch = InputBatch(...)
        
        # é¢„åˆ†é…çš„ GPU ç¼“å†²åŒº
        self.input_ids = torch.zeros(max_num_tokens, dtype=torch.int32, device=device)
        self.positions = torch.zeros(max_num_tokens, dtype=torch.int64, device=device)
        ...

    def load_model(self) -> None:
        """Load the model onto the device."""
        loader = get_model_loader(self.load_config)
        self.model = loader.load_model(self.vllm_config)
        
        # è®¾ç½® LoRAï¼ˆå¦‚æœæœ‰ï¼‰
        if self.lora_config:
            self.set_lora_state(...)

    def execute_model(
        self,
        scheduler_output: SchedulerOutput,
    ) -> ModelRunnerOutput:
        """Execute model forward pass and sampling.
        
        è¿™æ˜¯æ¨ç†çš„æ ¸å¿ƒæ–¹æ³•ï¼Œæ¯ä¸ª step è°ƒç”¨ä¸€æ¬¡ã€‚
        """
        # 1. æ›´æ–°å†…éƒ¨çŠ¶æ€
        self._update_states(scheduler_output)
        
        # 2. å‡†å¤‡è¾“å…¥
        num_scheduled_tokens = np.array([
            scheduler_output.num_scheduled_tokens[req_id]
            for req_id in self.input_batch.req_id_to_index
        ])
        logits_indices, spec_decode_metadata = self._prepare_inputs(
            scheduler_output,
            num_scheduled_tokens,
        )
        
        # 3. æ„å»ºæ³¨æ„åŠ›å…ƒæ•°æ®
        attn_metadata = self._prepare_attention_metadata(...)
        
        # 4. æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­
        with set_forward_context(attn_metadata, self.vllm_config):
            hidden_states = self.model(
                input_ids=self.input_ids[:total_num_tokens],
                positions=self.positions[:total_num_tokens],
                intermediate_tensors=intermediate_tensors,
                ...
            )
        
        # 5. è®¡ç®— logits
        selected_hidden_states = hidden_states[logits_indices]
        logits = self.model.compute_logits(selected_hidden_states)
        
        # 6. é‡‡æ ·
        sampling_metadata = self._prepare_sampling_metadata(...)
        
        if spec_decode_metadata is not None:
            # æŠ•æœºè§£ç ï¼šä½¿ç”¨æ‹’ç»é‡‡æ ·
            sampler_output = self.rejection_sampler(
                spec_decode_metadata,
                draft_probs,
                logits,
                sampling_metadata,
            )
        else:
            # æ™®é€šé‡‡æ ·
            sampler_output = self.sampler(logits, sampling_metadata)
        
        # 7. è¿”å›ç»“æœ
        return ModelRunnerOutput(
            sampled_token_ids=sampler_output.sampled_token_ids,
            logprobs=sampler_output.logprobs,
            ...
        )

    def _prepare_inputs(self, scheduler_output, num_scheduled_tokens):
        """å‡†å¤‡æ¨¡å‹è¾“å…¥å¼ é‡"""
        # å¡«å…… input_ids, positions ç­‰
        ...

    def _prepare_attention_metadata(self, ...):
        """å‡†å¤‡æ³¨æ„åŠ›å…ƒæ•°æ®ï¼ˆç”¨äº PagedAttentionï¼‰"""
        # åŒ…æ‹¬ block table, sequence lengths ç­‰
        ...
```

### 2.4 `model_executor/` - æ¨¡å‹æ‰§è¡Œå™¨ â­â­â­

è¿™æ˜¯æ•´ä¸ªæ¨ç†æ¡†æ¶çš„æ ¸å¿ƒï¼ŒåŒ…å«æ¨¡å‹å®šä¹‰å’Œæ‰§è¡Œé€»è¾‘ï¼š

```
model_executor/
â”œâ”€â”€ models/                      # ğŸ”´ æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ llama.py                 # â­ Llama æ¨¡å‹
â”‚   â”œâ”€â”€ qwen2.py                 # â­ Qwen2 æ¨¡å‹
â”‚   â”œâ”€â”€ mixtral.py               # Mixtral MoE æ¨¡å‹
â”‚   â”œâ”€â”€ deepseek_v2.py           # DeepSeek V2
â”‚   â”œâ”€â”€ gpt2.py                  # GPT-2
â”‚   â”œâ”€â”€ phi3.py                  # Phi-3
â”‚   â”œâ”€â”€ gemma.py                 # Gemma
â”‚   â”œâ”€â”€ mamba.py                 # Mamba (çŠ¶æ€ç©ºé—´æ¨¡å‹)
â”‚   â”œâ”€â”€ qwen2_vl.py              # Qwen2-VL (è§†è§‰è¯­è¨€)
â”‚   â”œâ”€â”€ llava.py                 # LLaVA (è§†è§‰è¯­è¨€)
â”‚   â”œâ”€â”€ whisper.py               # Whisper (éŸ³é¢‘)
â”‚   â”œâ”€â”€ registry.py              # â­ æ¨¡å‹æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ interfaces.py            # æ¨¡å‹æ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ interfaces_base.py       # åŸºç¡€æ¥å£
â”‚   â””â”€â”€ utils.py                 # æ¨¡å‹å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...ï¼ˆ200+ æ¨¡å‹æ–‡ä»¶ï¼‰
â”‚
â”œâ”€â”€ layers/                      # ğŸ”´ æ¨¡å‹å±‚å®ç°
â”‚   â”œâ”€â”€ linear.py                # â­ çº¿æ€§å±‚ï¼ˆå«é‡åŒ–æ”¯æŒï¼‰
â”‚   â”œâ”€â”€ activation.py            # æ¿€æ´»å‡½æ•° (SiLU, GELU, etc.)
â”‚   â”œâ”€â”€ layernorm.py             # LayerNorm å®ç°
â”‚   â”œâ”€â”€ vocab_parallel_embedding.py  # è¯åµŒå…¥å±‚
â”‚   â”œâ”€â”€ logits_processor.py      # Logits å¤„ç†å™¨
â”‚   â”œâ”€â”€ sampler.py               # é‡‡æ ·å±‚
â”‚   â”œâ”€â”€ pooler.py                # æ± åŒ–å±‚
â”‚   â”‚
â”‚   â”œâ”€â”€ rotary_embedding/        # RoPE ä½ç½®ç¼–ç 
â”‚   â”‚   â”œâ”€â”€ __init__.py          # å¯¼å‡º get_rope()
â”‚   â”‚   â””â”€â”€ base.py              # RotaryEmbedding å®ç°
â”‚   â”‚
â”‚   â”œâ”€â”€ fused_moe/               # èåˆ MoE å±‚
â”‚   â”‚   â”œâ”€â”€ layer.py             # FusedMoE ä¸»ç±»
â”‚   â”‚   â”œâ”€â”€ fused_moe.py         # èåˆå†…æ ¸è°ƒç”¨
â”‚   â”‚   â””â”€â”€ config.py            # MoE é…ç½®
â”‚   â”‚
â”‚   â””â”€â”€ quantization/            # ğŸ”´ é‡åŒ–å®ç°
â”‚       â”œâ”€â”€ __init__.py          # å¯¼å‡ºé‡åŒ–æ–¹æ³•
â”‚       â”œâ”€â”€ base_config.py       # QuantizationConfig åŸºç±»
â”‚       â”œâ”€â”€ fp8.py               # â­ FP8 é‡åŒ–
â”‚       â”œâ”€â”€ awq.py               # AWQ é‡åŒ–
â”‚       â”œâ”€â”€ awq_marlin.py        # AWQ Marlin æ ¼å¼
â”‚       â”œâ”€â”€ gptq.py              # GPTQ é‡åŒ–
â”‚       â”œâ”€â”€ gptq_marlin.py       # GPTQ Marlin æ ¼å¼ï¼ˆé«˜æ•ˆ GPTQï¼‰
â”‚       â”œâ”€â”€ bitsandbytes.py      # BitsAndBytes é‡åŒ–
â”‚       â”œâ”€â”€ gguf.py              # GGUF æ ¼å¼æ”¯æŒ
â”‚       â”œâ”€â”€ compressed_tensors/  # CompressedTensors æ”¯æŒ
â”‚       â””â”€â”€ utils/               # é‡åŒ–å·¥å…·
â”‚           â”œâ”€â”€ fp8_utils.py     # FP8 å·¥å…·
â”‚           â”œâ”€â”€ w8a8_utils.py    # W8A8 å·¥å…·
â”‚           â””â”€â”€ marlin_utils.py  # Marlin å·¥å…·
â”‚
â”œâ”€â”€ model_loader/                # æ¨¡å‹åŠ è½½å™¨
â”‚   â”œâ”€â”€ loader.py                # ä¸»åŠ è½½å™¨
â”‚   â”œâ”€â”€ weight_utils.py          # æƒé‡å·¥å…·
â”‚   â””â”€â”€ tensorizer.py            # Tensorizer æ”¯æŒ
â”‚
â”œâ”€â”€ custom_op.py                 # CustomOp åŸºç±»
â”œâ”€â”€ parameter.py                 # å‚æ•°å®šä¹‰
â”œâ”€â”€ guided_decoding/             # å¼•å¯¼è§£ç 
â””â”€â”€ utils.py                     # å·¥å…·å‡½æ•°
```

#### æ¨¡å‹æ³¨å†Œè¡¨ (`registry.py`)

vLLM ä½¿ç”¨æ³¨å†Œè¡¨æ¨¡å¼ç®¡ç†æ”¯æŒçš„æ¨¡å‹ï¼š

```python
# vllm/model_executor/models/registry.py

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆéƒ¨åˆ†ï¼‰
_TEXT_GENERATION_MODELS = {
    # è¯­è¨€æ¨¡å‹
    "LlamaForCausalLM": ("llama", "LlamaForCausalLM"),
    "Qwen2ForCausalLM": ("qwen2", "Qwen2ForCausalLM"),
    "MistralForCausalLM": ("llama", "LlamaForCausalLM"),  # ä½¿ç”¨ Llama å®ç°
    "MixtralForCausalLM": ("mixtral", "MixtralForCausalLM"),
    "DeepseekV2ForCausalLM": ("deepseek_v2", "DeepseekV2ForCausalLM"),
    "Phi3ForCausalLM": ("phi3", "Phi3ForCausalLM"),
    "GemmaForCausalLM": ("gemma", "GemmaForCausalLM"),
    
    # è§†è§‰è¯­è¨€æ¨¡å‹
    "Qwen2VLForConditionalGeneration": ("qwen2_vl", "Qwen2VLForConditionalGeneration"),
    "LlavaForConditionalGeneration": ("llava", "LlavaForConditionalGeneration"),
    
    # åµŒå…¥æ¨¡å‹
    "BertModel": ("bert", "BertEmbeddingModel"),
    
    # çŠ¶æ€ç©ºé—´æ¨¡å‹
    "MambaForCausalLM": ("mamba", "MambaForCausalLM"),
    
    # ... 200+ å…¶ä»–æ¨¡å‹
}

def get_model_architecture(config) -> tuple[str, str]:
    """Get the module and class name for a model config."""
    architectures = getattr(config, "architectures", [])
    for arch in architectures:
        if arch in _TRANSFORMERS_MODELS:
            return _TRANSFORMERS_MODELS[arch]
    raise ValueError(f"Model architecture {architectures} not supported")
```

### 2.5 `attention/` - æ³¨æ„åŠ›æœºåˆ¶ â­â­â­

```
attention/
â”œâ”€â”€ __init__.py              # å¯¼å‡º Attention ç±»
â”œâ”€â”€ layer.py                 # â­ Attention å±‚å°è£…
â”œâ”€â”€ selector.py              # åç«¯è‡ªåŠ¨é€‰æ‹©å™¨
â”œâ”€â”€ ops/                     # æ³¨æ„åŠ›æ“ä½œ
â”‚   â”œâ”€â”€ paged_attn.py        # PagedAttention æ“ä½œ
â”‚   â””â”€â”€ prefix_prefill.py    # å‰ç¼€é¢„å¡«å……
â”‚
â”œâ”€â”€ backends/                # æ³¨æ„åŠ›åç«¯å®ç°
â”‚   â”œâ”€â”€ abstract.py          # æŠ½è±¡åŸºç±»
â”‚   â”œâ”€â”€ registry.py          # åç«¯æ³¨å†Œè¡¨
â”‚   â””â”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ layers/                  # ç‰¹æ®Šæ³¨æ„åŠ›å±‚
â”‚   â””â”€â”€ encoder_only_attention.py # ä»…ç¼–ç å™¨æ³¨æ„åŠ›
â”‚
â””â”€â”€ utils/                   # å·¥å…·
```

**æ³¨æ„**: V1 æ¶æ„çš„æ³¨æ„åŠ›åç«¯ä½äº `vllm/v1/attention/backends/`ï¼ŒåŒ…å«ï¼š
- `flash_attn.py` - FlashAttention
- `flashinfer.py` - FlashInfer  
- `triton_attn.py` - Triton å®ç°
- `flex_attention.py` - Flex Attention
- ä»¥åŠ ROCmã€TPU ç­‰å¹³å°ç‰¹å®šçš„å®ç°

#### Attention å±‚ (`layer.py`)

```python
# vllm/attention/layer.py (ç®€åŒ–ç‰ˆ)

class Attention(nn.Module):
    """Multi-head attention layer with paged attention support."""
    
    def __init__(
        self,
        num_heads: int,
        head_size: int,
        scale: float,
        num_kv_heads: int | None = None,
        cache_config: CacheConfig | None = None,
        quant_config: QuantizationConfig | None = None,
        attn_type: AttentionType = AttentionType.DECODER,
        prefix: str = "",
        **kwargs,
    ):
        super().__init__()
        self.num_heads = num_heads
        self.head_size = head_size
        self.scale = scale
        self.num_kv_heads = num_kv_heads or num_heads
        
        # æ ¹æ®é…ç½®é€‰æ‹©æœ€ä½³æ³¨æ„åŠ›åç«¯
        self.impl = get_attn_backend(
            num_heads=num_heads,
            head_size=head_size,
            num_kv_heads=self.num_kv_heads,
            dtype=cache_config.dtype if cache_config else torch.float16,
            **kwargs,
        )
    
    def forward(
        self,
        query: torch.Tensor,           # [num_tokens, num_heads * head_size]
        key: torch.Tensor,             # [num_tokens, num_kv_heads * head_size]
        value: torch.Tensor,           # [num_tokens, num_kv_heads * head_size]
        kv_cache: torch.Tensor | None, # KV Cache å¼ é‡
        attn_metadata: AttentionMetadata,  # æ³¨æ„åŠ›å…ƒæ•°æ®
    ) -> torch.Tensor:
        """Forward pass with paged attention."""
        return self.impl.forward(
            query, key, value, kv_cache, attn_metadata, self.k_scale, self.v_scale
        )
```

#### æ³¨æ„åŠ›åç«¯é€‰æ‹© (`selector.py`)

vLLM è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„æ³¨æ„åŠ›åç«¯ï¼š

```python
# åç«¯é€‰æ‹©ä¼˜å…ˆçº§ï¼ˆç®€åŒ–ï¼‰
def get_attn_backend(...) -> AttentionBackend:
    """Select the best attention backend for the current configuration."""
    
    # 1. FlashInfer (å¦‚æœå¯ç”¨ä¸”åˆé€‚)
    if is_flashinfer_available() and ...:
        return FlashInferBackend(...)
    
    # 2. FlashAttention (æœ€å¸¸ç”¨)
    if is_flash_attn_available() and head_size in [64, 80, 96, 128, 256]:
        return FlashAttentionBackend(...)
    
    # 3. xFormers (å¤‡é€‰)
    if is_xformers_available():
        return XFormersBackend(...)
    
    # 4. PyTorch SDPA (fallback)
    return TorchSDPABackend(...)
```

### 2.6 `config/` - é…ç½®ç±»

æ‰€æœ‰é…ç½®ç›¸å…³çš„å®šä¹‰ï¼š

```
config/
â”œâ”€â”€ __init__.py             # å¯¼å‡ºæ‰€æœ‰é…ç½®ç±»
â”œâ”€â”€ vllm.py                 # â­ VllmConfig ä¸»é…ç½®
â”œâ”€â”€ model.py                # ModelConfig æ¨¡å‹é…ç½®
â”œâ”€â”€ cache.py                # CacheConfig KV Cache é…ç½®
â”œâ”€â”€ parallel.py             # ParallelConfig å¹¶è¡Œé…ç½®
â”œâ”€â”€ scheduler.py            # SchedulerConfig è°ƒåº¦å™¨é…ç½®
â”œâ”€â”€ device.py               # DeviceConfig è®¾å¤‡é…ç½®
â”œâ”€â”€ lora.py                 # LoRAConfig LoRA é…ç½®
â”œâ”€â”€ speculative.py          # SpeculativeConfig æŠ•æœºè§£ç é…ç½®
â”œâ”€â”€ compilation.py          # CompilationConfig ç¼–è¯‘é…ç½®
â””â”€â”€ ...
```

#### VllmConfig (`config/vllm.py`)

```python
# vllm/config/vllm.py

@dataclass
class VllmConfig:
    """Top-level configuration for vLLM."""
    
    model_config: ModelConfig           # æ¨¡å‹ç›¸å…³é…ç½®
    cache_config: CacheConfig           # KV Cache é…ç½®
    parallel_config: ParallelConfig     # å¹¶è¡Œé…ç½®
    scheduler_config: SchedulerConfig   # è°ƒåº¦å™¨é…ç½®
    device_config: DeviceConfig         # è®¾å¤‡é…ç½®
    load_config: LoadConfig             # åŠ è½½é…ç½®
    lora_config: LoRAConfig | None      # LoRA é…ç½®
    multimodal_config: MultiModalConfig | None  # å¤šæ¨¡æ€é…ç½®
    speculative_config: SpeculativeConfig | None  # æŠ•æœºè§£ç é…ç½®
    observability_config: ObservabilityConfig     # å¯è§‚æµ‹æ€§é…ç½®
    compilation_config: CompilationConfig         # ç¼–è¯‘é…ç½®

# ModelConfig ç¤ºä¾‹
@dataclass
class ModelConfig:
    model: str                          # æ¨¡å‹è·¯å¾„æˆ– HuggingFace ID
    tokenizer: str | None               # Tokenizer è·¯å¾„
    dtype: torch.dtype                  # æ•°æ®ç±»å‹
    trust_remote_code: bool             # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
    max_model_len: int                  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
    quantization: str | None            # é‡åŒ–æ–¹æ³•
    revision: str | None                # æ¨¡å‹ç‰ˆæœ¬
    ...
```

### 2.7 `distributed/` - åˆ†å¸ƒå¼æ”¯æŒ

```
distributed/
â”œâ”€â”€ __init__.py              # å¯¼å‡ºåˆ†å¸ƒå¼å·¥å…·
â”œâ”€â”€ parallel_state.py        # â­ å¹¶è¡ŒçŠ¶æ€ç®¡ç†
â”œâ”€â”€ communication_op.py      # é€šä¿¡æ“ä½œ
â”œâ”€â”€ utils.py                 # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ kv_transfer/             # KV Cache ä¼ è¾“ï¼ˆç”¨äºåˆ†ç¦»å¼æ¨ç†ï¼‰
â”‚   â”œâ”€â”€ kv_connector/        # KV è¿æ¥å™¨
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ eplb/                    # ä¸“å®¶å¹¶è¡Œè´Ÿè½½å‡è¡¡
```

#### å¹¶è¡ŒçŠ¶æ€ç®¡ç† (`parallel_state.py`)

```python
# vllm/distributed/parallel_state.py

# å…¨å±€å¹¶è¡ŒçŠ¶æ€
_TENSOR_MODEL_PARALLEL_GROUP = None
_PIPELINE_MODEL_PARALLEL_GROUP = None
_DATA_PARALLEL_GROUP = None

def get_tensor_model_parallel_rank() -> int:
    """è·å–å½“å‰è¿›ç¨‹çš„å¼ é‡å¹¶è¡Œ rank"""
    if _TENSOR_MODEL_PARALLEL_GROUP is None:
        return 0
    return torch.distributed.get_rank(_TENSOR_MODEL_PARALLEL_GROUP)

def get_tensor_model_parallel_world_size() -> int:
    """è·å–å¼ é‡å¹¶è¡Œä¸–ç•Œå¤§å°"""
    if _TENSOR_MODEL_PARALLEL_GROUP is None:
        return 1
    return torch.distributed.get_world_size(_TENSOR_MODEL_PARALLEL_GROUP)

def tensor_model_parallel_all_reduce(tensor: torch.Tensor) -> torch.Tensor:
    """å¼ é‡å¹¶è¡Œ all-reduce æ“ä½œ"""
    if get_tensor_model_parallel_world_size() == 1:
        return tensor
    return all_reduce(tensor, group=_TENSOR_MODEL_PARALLEL_GROUP)
```

---

## 3. å…¸å‹è°ƒç”¨é“¾åˆ†æï¼ˆLlama/Qwen2ï¼‰

### 3.1 å®Œæ•´è°ƒç”¨é“¾å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·ä»£ç å…¥å£                                      â”‚
â”‚  llm = LLM(model="Qwen/Qwen2.5-7B-Instruct")                           â”‚
â”‚  outputs = llm.generate(prompts, sampling_params)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM ç±» (vllm/entrypoints/llm.py)                                       â”‚
â”‚                                                                         â”‚
â”‚  def __init__(self, model, ...):                                        â”‚
â”‚      engine_args = EngineArgs(model=model, ...)                         â”‚
â”‚      self.llm_engine = LLMEngine.from_engine_args(engine_args)         â”‚
â”‚                                                                         â”‚
â”‚  def generate(self, prompts, sampling_params):                          â”‚
â”‚      self._validate_and_add_requests(prompts, params)                   â”‚
â”‚      outputs = self._run_engine()  # å¾ªç¯è°ƒç”¨ engine.step()             â”‚
â”‚      return outputs                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLMEngine (vllm/v1/engine/llm_engine.py)                               â”‚
â”‚                                                                         â”‚
â”‚  def __init__(...):                                                     â”‚
â”‚      self.input_processor = InputProcessor(...)                         â”‚
â”‚      self.output_processor = OutputProcessor(...)                       â”‚
â”‚      self.engine_core = EngineCoreClient.make_client(...)              â”‚
â”‚                                                                         â”‚
â”‚  def step(self):                                                        â”‚
â”‚      engine_core_outputs = self.engine_core.step()  # è°ƒç”¨æ ¸å¿ƒå¼•æ“      â”‚
â”‚      return self.output_processor.process(...)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EngineCoreClient â†’ EngineCore (vllm/v1/engine/core_client.py)          â”‚
â”‚                                                                         â”‚
â”‚  å†…éƒ¨ç»´æŠ¤ model_executorï¼Œè´Ÿè´£è°ƒåº¦å’Œç®¡ç†è¯·æ±‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPUModelRunner (vllm/v1/worker/gpu_model_runner.py)                    â”‚
â”‚                                                                         â”‚
â”‚  def execute_model(self, scheduler_output):                             â”‚
â”‚      # 1. å‡†å¤‡è¾“å…¥                                                       â”‚
â”‚      model_input = self._prepare_inputs(...)                            â”‚
â”‚      # 2. å‡†å¤‡æ³¨æ„åŠ›å…ƒæ•°æ®                                                â”‚
â”‚      attn_metadata = self._prepare_attention_metadata(...)              â”‚
â”‚      # 3. æ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­                                                â”‚
â”‚      with set_forward_context(...):                                     â”‚
â”‚          hidden_states = self.model(                                    â”‚
â”‚              input_ids=model_input.input_ids,                           â”‚
â”‚              positions=model_input.positions,                           â”‚
â”‚              ...                                                        â”‚
â”‚          )                                                              â”‚
â”‚      # 4. è®¡ç®— logits å¹¶é‡‡æ ·                                             â”‚
â”‚      logits = self.model.compute_logits(hidden_states)                  â”‚
â”‚      sampler_output = self.sampler(logits, sampling_metadata)           â”‚
â”‚      return sampler_output                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Forward (ä»¥ Qwen2ForCausalLM ä¸ºä¾‹)                                â”‚
â”‚  vllm/model_executor/models/qwen2.py                                    â”‚
â”‚                                                                         â”‚
â”‚  class Qwen2ForCausalLM:                                                â”‚
â”‚      def forward(self, input_ids, positions, ...):                      â”‚
â”‚          hidden_states = self.model(input_ids, positions, ...)          â”‚
â”‚          return hidden_states                                           â”‚
â”‚                                                                         â”‚
â”‚  class Qwen2Model:                                                      â”‚
â”‚      def forward(self, input_ids, positions, ...):                      â”‚
â”‚          # 1. Embedding                                                 â”‚
â”‚          hidden_states = self.embed_tokens(input_ids)                   â”‚
â”‚          residual = None                                                â”‚
â”‚          # 2. å¾ªç¯æ‰€æœ‰ Decoder Layer                                     â”‚
â”‚          for layer in self.layers:                                      â”‚
â”‚              hidden_states, residual = layer(positions, hidden_states,  â”‚
â”‚                                              residual)                  â”‚
â”‚          # 3. æœ€ç»ˆ LayerNorm                                             â”‚
â”‚          hidden_states, _ = self.norm(hidden_states, residual)          â”‚
â”‚          return hidden_states                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qwen2DecoderLayer.forward()                                            â”‚
â”‚                                                                         â”‚
â”‚  def forward(self, positions, hidden_states, residual):                 â”‚
â”‚      # Self Attention                                                   â”‚
â”‚      if residual is None:                                               â”‚
â”‚          residual = hidden_states                                       â”‚
â”‚          hidden_states = self.input_layernorm(hidden_states)            â”‚
â”‚      else:                                                              â”‚
â”‚          hidden_states, residual = self.input_layernorm(hidden_states,  â”‚
â”‚                                                         residual)       â”‚
â”‚      hidden_states = self.self_attn(positions, hidden_states)           â”‚
â”‚                                                                         â”‚
â”‚      # MLP                                                              â”‚
â”‚      hidden_states, residual = self.post_attention_layernorm(           â”‚
â”‚          hidden_states, residual)                                       â”‚
â”‚      hidden_states = self.mlp(hidden_states)                            â”‚
â”‚      return hidden_states, residual                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                       â”‚
            â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qwen2Attention.forward()     â”‚   â”‚  Qwen2MLP.forward()                â”‚
â”‚                               â”‚   â”‚                                   â”‚
â”‚  # QKV æŠ•å½±                    â”‚   â”‚  # gate_up_proj (W13)             â”‚
â”‚  qkv, _ = self.qkv_proj(x)    â”‚   â”‚  gate_up, _ = self.gate_up_proj(x)â”‚
â”‚  q, k, v = qkv.split(...)     â”‚   â”‚  x = self.act_fn(gate_up)         â”‚
â”‚  # RoPE                        â”‚   â”‚  # down_proj (W2)                 â”‚
â”‚  q, k = self.rotary_emb(...)  â”‚   â”‚  x, _ = self.down_proj(x)         â”‚
â”‚  # Attention                   â”‚   â”‚  return x                         â”‚
â”‚  attn_output = self.attn(qkv) â”‚   â”‚                                   â”‚
â”‚  # O æŠ•å½±                      â”‚   â”‚                                   â”‚
â”‚  output, _ = self.o_proj(...)  â”‚   â”‚                                   â”‚
â”‚  return output                 â”‚   â”‚                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 å…³é”®æ–‡ä»¶åˆ—è¡¨

| å±‚çº§ | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|-----|---------|------|
| å…¥å£ | `vllm/entrypoints/llm.py` | LLM ç±»å®šä¹‰ |
| å¼•æ“ | `vllm/v1/engine/llm_engine.py` | V1 LLMEngine |
| è¿è¡Œå™¨ | `vllm/v1/worker/gpu_model_runner.py` | GPU æ¨¡å‹è¿è¡Œå™¨ |
| æ¨¡å‹ | `vllm/model_executor/models/qwen2.py` | Qwen2 æ¨¡å‹ |
| æ¨¡å‹ | `vllm/model_executor/models/llama.py` | Llama æ¨¡å‹ |
| çº¿æ€§å±‚ | `vllm/model_executor/layers/linear.py` | çº¿æ€§å±‚å®šä¹‰ |
| æ³¨æ„åŠ› | `vllm/attention/layer.py` | æ³¨æ„åŠ›å±‚ |
| é‡åŒ– | `vllm/model_executor/layers/quantization/fp8.py` | FP8 é‡åŒ– |

---

## 4. æ¨¡å‹å®šä¹‰è¯¦è§£ï¼ˆLlama/Qwen2ï¼‰

### 4.1 æ¨¡å‹ç±»å±‚æ¬¡ç»“æ„

```
nn.Module
    â”‚
    â”œâ”€â”€ LlamaForCausalLM / Qwen2ForCausalLM    # é¡¶å±‚æ¨¡å‹
    â”‚       â”‚
    â”‚       â”œâ”€â”€ LlamaModel / Qwen2Model        # ä¸»ä½“æ¨¡å‹
    â”‚       â”‚       â”‚
    â”‚       â”‚       â”œâ”€â”€ VocabParallelEmbedding  # è¯åµŒå…¥
    â”‚       â”‚       â”œâ”€â”€ LlamaDecoderLayer[]     # Decoder å±‚åˆ—è¡¨
    â”‚       â”‚       â”‚       â”‚
    â”‚       â”‚       â”‚       â”œâ”€â”€ LlamaAttention   # æ³¨æ„åŠ›
    â”‚       â”‚       â”‚       â”‚   â”œâ”€â”€ QKVParallelLinear  # Wqkv
    â”‚       â”‚       â”‚       â”‚   â”œâ”€â”€ RowParallelLinear  # Wo
    â”‚       â”‚       â”‚       â”‚   â””â”€â”€ Attention          # æ³¨æ„åŠ›è®¡ç®—
    â”‚       â”‚       â”‚       â”‚
    â”‚       â”‚       â”‚       â”œâ”€â”€ LlamaMLP         # MLP
    â”‚       â”‚       â”‚       â”‚   â”œâ”€â”€ MergedColumnParallelLinear  # W13
    â”‚       â”‚       â”‚       â”‚   â””â”€â”€ RowParallelLinear           # W2
    â”‚       â”‚       â”‚       â”‚
    â”‚       â”‚       â”‚       â”œâ”€â”€ RMSNorm (input)
    â”‚       â”‚       â”‚       â””â”€â”€ RMSNorm (post_attn)
    â”‚       â”‚       â”‚
    â”‚       â”‚       â””â”€â”€ RMSNorm (final)
    â”‚       â”‚
    â”‚       â”œâ”€â”€ ParallelLMHead                  # LM Head
    â”‚       â””â”€â”€ LogitsProcessor                 # Logits å¤„ç†
```

### 4.2 å››ä¸ªå…³é”®çº¿æ€§å±‚

åœ¨ Llama/Qwen2 è¿™ç±» Dense æ¨¡å‹ä¸­ï¼Œæ¯å±‚æœ‰ 4 ä¸ªå…³é”®çš„çº¿æ€§æŠ•å½±ï¼š

| å±‚å | ç±»å‹ | è¾“å…¥ç»´åº¦ | è¾“å‡ºç»´åº¦ | è¯´æ˜ |
|-----|------|---------|---------|------|
| `qkv_proj` | QKVParallelLinear | hidden_size | (q+k+v)_size | Q/K/V æŠ•å½±åˆå¹¶ |
| `o_proj` | RowParallelLinear | head_dim * num_heads | hidden_size | è¾“å‡ºæŠ•å½± |
| `gate_up_proj` | MergedColumnParallelLinear | hidden_size | intermediate_size * 2 | Gate + Up åˆå¹¶ |
| `down_proj` | RowParallelLinear | intermediate_size | hidden_size | Down æŠ•å½± |

### 4.3 ä»£ç ç¤ºä¾‹ï¼šQwen2MLP

```python
# vllm/model_executor/models/qwen2.py

class Qwen2MLP(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
        quant_config: QuantizationConfig | None = None,
        prefix: str = "",
    ) -> None:
        super().__init__()
        # gate_up_proj åˆå¹¶äº† gate_proj å’Œ up_proj
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,  # [gate_size, up_size]
            bias=False,
            quant_config=quant_config,
            prefix=f"{prefix}.gate_up_proj",
        )
        # down_proj
        self.down_proj = RowParallelLinear(
            intermediate_size,
            hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=f"{prefix}.down_proj",
        )
        self.act_fn = SiluAndMul()

    def forward(self, x):
        gate_up, _ = self.gate_up_proj(x)   # GEMM: W13
        x = self.act_fn(gate_up)            # SiLU æ¿€æ´»
        x, _ = self.down_proj(x)            # GEMM: W2
        return x
```

### 4.4 ä»£ç ç¤ºä¾‹ï¼šQwen2Attention

```python
# vllm/model_executor/models/qwen2.py

class Qwen2Attention(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # QKV åˆå¹¶æŠ•å½±
        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=True,
            quant_config=quant_config,
            prefix=f"{prefix}.qkv_proj",
        )
        # è¾“å‡ºæŠ•å½±
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=f"{prefix}.o_proj",
        )
        self.rotary_emb = get_rope(...)
        self.attn = Attention(...)

    def forward(self, positions, hidden_states):
        qkv, _ = self.qkv_proj(hidden_states)  # GEMM: Wqkv
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        q, k = self.rotary_emb(positions, q, k)  # RoPE
        attn_output = self.attn(q, k, v)          # Attention
        output, _ = self.o_proj(attn_output)     # GEMM: Wo
        return output
```

---

## 5. çº¿æ€§å±‚å®ç°ï¼ˆLinear Layersï¼‰

### 5.1 çº¿æ€§å±‚ç±»å±‚æ¬¡ç»“æ„

```
LinearBase (CustomOp)
    â”‚
    â”œâ”€â”€ ReplicatedLinear          # å¤åˆ¶çº¿æ€§å±‚
    â”œâ”€â”€ ColumnParallelLinear      # åˆ—å¹¶è¡Œçº¿æ€§å±‚
    â”‚   â”œâ”€â”€ MergedColumnParallelLinear  # åˆå¹¶åˆ—å¹¶è¡Œï¼ˆç”¨äº MLPï¼‰
    â”‚   â””â”€â”€ QKVParallelLinear           # QKV å¹¶è¡Œï¼ˆç”¨äº Attentionï¼‰
    â””â”€â”€ RowParallelLinear         # è¡Œå¹¶è¡Œçº¿æ€§å±‚
```

### 5.2 LinearBase åŸºç±»

```python
# vllm/model_executor/layers/linear.py

class LinearBase(CustomOp):
    def __init__(
        self,
        input_size: int,
        output_size: int,
        skip_bias_add: bool = False,
        params_dtype: torch.dtype | None = None,
        quant_config: QuantizationConfig | None = None,  # é‡åŒ–é…ç½®
        prefix: str = "",
        ...
    ):
        # æ ¹æ® quant_config é€‰æ‹©é‡åŒ–æ–¹æ³•
        if quant_config is None:
            self.quant_method = UnquantizedLinearMethod()
        else:
            self.quant_method = quant_config.get_quant_method(self, prefix=prefix)
```

### 5.3 Forward æµç¨‹

```python
# ColumnParallelLinear.forward()
def forward(self, input_):
    bias = self.bias if not self.skip_bias_add else None
    
    # Matrix multiply - æ ¸å¿ƒ GEMM è°ƒç”¨
    assert self.quant_method is not None
    output_parallel = self.quant_method.apply(self, input_, bias)
    
    if self.gather_output and self.tp_size > 1:
        output = tensor_model_parallel_all_gather(output_parallel)
    else:
        output = output_parallel
    
    return output, output_bias
```

---

## 6. å¼•æ“é…ç½®ä¸å‚æ•°ä¼ é€’

### 6.1 é…ç½®ç±»å±‚æ¬¡

```
VllmConfig                          # é¡¶å±‚é…ç½®
    â”œâ”€â”€ ModelConfig                 # æ¨¡å‹é…ç½®
    â”œâ”€â”€ CacheConfig                 # KV Cache é…ç½®
    â”œâ”€â”€ ParallelConfig              # å¹¶è¡Œé…ç½®
    â”œâ”€â”€ SchedulerConfig             # è°ƒåº¦å™¨é…ç½®
    â”œâ”€â”€ DeviceConfig                # è®¾å¤‡é…ç½®
    â”œâ”€â”€ LoRAConfig                  # LoRA é…ç½®ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ MultiModalConfig            # å¤šæ¨¡æ€é…ç½®ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ SpeculativeConfig           # æŠ•æœºè§£ç é…ç½®ï¼ˆå¯é€‰ï¼‰
    â””â”€â”€ ObservabilityConfig         # å¯è§‚æµ‹æ€§é…ç½®
```

### 6.2 å‚æ•°æµå‘

```
ç”¨æˆ·å‚æ•° (model, dtype, quantization, ...)
         â”‚
         â–¼
    EngineArgs                      # vllm/engine/arg_utils.py
         â”‚
         â–¼
    VllmConfig.from_engine_args()   # åˆ›å»ºå®Œæ•´é…ç½®
         â”‚
         â”œâ”€â”€â†’ ModelConfig           # ä¼ ç»™æ¨¡å‹åŠ è½½å™¨
         â”œâ”€â”€â†’ CacheConfig           # ä¼ ç»™ KV Cache ç®¡ç†
         â”œâ”€â”€â†’ ParallelConfig        # ä¼ ç»™åˆ†å¸ƒå¼ç®¡ç†
         â””â”€â”€â†’ quant_config          # ä¼ ç»™é‡åŒ–å±‚
```

---

## 7. å°ç»“ä¸å…³é”®è·¯å¾„

### 7.1 æ¶æ„å±‚æ¬¡æ€»ç»“

vLLM çš„æ ¸å¿ƒæ¶æ„å¯ä»¥æ¦‚æ‹¬ä¸ºå…­ä¸ªå±‚æ¬¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: å…¥å£å±‚ (entrypoints/)                                          â”‚
â”‚  - LLM ç±»ï¼šç¦»çº¿æ¨ç†                                                       â”‚
â”‚  - OpenAI API Serverï¼šåœ¨çº¿æœåŠ¡                                           â”‚
â”‚  - CLIï¼šå‘½ä»¤è¡Œå·¥å…·                                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: å¼•æ“å±‚ (v1/engine/)                                            â”‚
â”‚  - LLMEngineï¼šè¯·æ±‚ç®¡ç†å’Œç”Ÿå‘½å‘¨æœŸ                                          â”‚
â”‚  - InputProcessorï¼šè¾“å…¥é¢„å¤„ç†å’Œ tokenization                             â”‚
â”‚  - OutputProcessorï¼šè¾“å‡ºåå¤„ç†å’Œ detokenization                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: è°ƒåº¦å±‚ (v1/core/)                                              â”‚
â”‚  - Schedulerï¼šè¯·æ±‚è°ƒåº¦                                                    â”‚
â”‚  - KV Cache Managerï¼šKV Cache åˆ†é…å’Œç®¡ç†                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 4: æ‰§è¡Œå±‚ (v1/worker/)                                            â”‚
â”‚  - GPUModelRunnerï¼šGPU æ¨¡å‹è¿è¡Œ                                          â”‚
â”‚  - InputBatchï¼šæ‰¹æ¬¡ç®¡ç†                                                   â”‚
â”‚  - Samplerï¼šé‡‡æ ·                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 5: æ¨¡å‹å±‚ (model_executor/models/)                                â”‚
â”‚  - 200+ æ¨¡å‹å®ç°                                                          â”‚
â”‚  - Transformer å±‚ç»„è£…                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 6: ç®—å­å±‚ (model_executor/layers/, csrc/)                         â”‚
â”‚  - çº¿æ€§å±‚ï¼ˆå«é‡åŒ–ï¼‰                                                       â”‚
â”‚  - æ³¨æ„åŠ›å±‚                                                               â”‚
â”‚  - æ¿€æ´»å‡½æ•°ã€LayerNorm ç­‰                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 å…³é”®æ–‡ä»¶é€ŸæŸ¥

| ç›®çš„ | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|------|---------|------|
| **ç”¨æˆ·å…¥å£** | | |
| ç¦»çº¿æ¨ç† | `vllm/entrypoints/llm.py` | LLM ç±» |
| åœ¨çº¿æœåŠ¡ | `vllm/entrypoints/openai/api_server.py` | API æœåŠ¡å™¨ |
| CLI | `vllm/entrypoints/cli/main.py` | å‘½ä»¤è¡Œå…¥å£ |
| **å¼•æ“æ ¸å¿ƒ** | | |
| V1 å¼•æ“ | `vllm/v1/engine/llm_engine.py` | LLMEngine |
| æ ¸å¿ƒå®¢æˆ·ç«¯ | `vllm/v1/engine/core_client.py` | EngineCoreClient |
| **æ‰§è¡Œå™¨** | | |
| GPU æ‰§è¡Œ | `vllm/v1/worker/gpu_model_runner.py` | GPUModelRunner |
| é‡‡æ ·å™¨ | `vllm/v1/sample/sampler.py` | Sampler |
| **æ¨¡å‹å®šä¹‰** | | |
| Llama | `vllm/model_executor/models/llama.py` | LlamaForCausalLM |
| Qwen2 | `vllm/model_executor/models/qwen2.py` | Qwen2ForCausalLM |
| æ¨¡å‹æ³¨å†Œ | `vllm/model_executor/models/registry.py` | æ¨¡å‹æ³¨å†Œè¡¨ |
| **å±‚å®ç°** | | |
| çº¿æ€§å±‚ | `vllm/model_executor/layers/linear.py` | Linear å±‚ |
| FP8 é‡åŒ– | `vllm/model_executor/layers/quantization/fp8.py` | FP8 å®ç° |
| æ³¨æ„åŠ› | `vllm/attention/layer.py` | Attention å±‚ |
| **é…ç½®** | | |
| ä¸»é…ç½® | `vllm/config/vllm.py` | VllmConfig |
| å‚æ•°è§£æ | `vllm/engine/arg_utils.py` | EngineArgs |

### 7.3 æ ¸å¿ƒæ•°æ®æµ

```
ç”¨æˆ·è¾“å…¥ (prompts)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenization (InputProcessor)       â”‚
â”‚ "Hello" â†’ [15496, 995]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduling (Scheduler)              â”‚
â”‚ - è¯·æ±‚æ’é˜Ÿ                           â”‚
â”‚ - KV Cache åˆ†é…                     â”‚
â”‚ - æ‰¹æ¬¡ç»„ç»‡                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Forward (GPUModelRunner)      â”‚
â”‚ 1. Embedding                        â”‚
â”‚ 2. N Ã— Decoder Layer               â”‚
â”‚    - Attention (qkv â†’ attn â†’ o)    â”‚
â”‚    - MLP (gate_up â†’ act â†’ down)    â”‚
â”‚ 3. Final Norm                       â”‚
â”‚ 4. LM Head â†’ Logits                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sampling (Sampler)                  â”‚
â”‚ Logits â†’ Token IDs                  â”‚
â”‚ [3.2, 1.5, ...] â†’ [15496]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Detokenization (OutputProcessor)    â”‚
â”‚ [15496, 995, ...] â†’ "Hello world"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.4 æ€§èƒ½ä¼˜åŒ–å…³é”®ç‚¹

| ä¼˜åŒ–æŠ€æœ¯ | ä½ç½® | è¯´æ˜ |
|---------|------|------|
| **PagedAttention** | `vllm/attention/` | KV Cache åˆ†é¡µç®¡ç† |
| **è¿ç»­æ‰¹å¤„ç†** | `vllm/v1/core/sched/` | åŠ¨æ€è¯·æ±‚è°ƒåº¦ |
| **CUDA Graph** | `vllm/compilation/` | å‡å°‘å†…æ ¸å¯åŠ¨å¼€é”€ |
| **é‡åŒ–æ¨ç†** | `vllm/model_executor/layers/quantization/` | FP8/AWQ/GPTQ |
| **å¼ é‡å¹¶è¡Œ** | `vllm/distributed/` | å¤š GPU æ¨ç† |
| **FlashAttention** | `vllm/attention/backends/` | é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®— |
| **æŠ•æœºè§£ç ** | `vllm/v1/spec_decode/` | åŠ é€Ÿç”Ÿæˆ |
| **å‰ç¼€ç¼“å­˜** | `vllm/v1/core/` | å…±äº«å‰ç¼€ KV Cache |

### 7.5 äºŒæ¬¡å¼€å‘æŒ‡å—

**æ·»åŠ æ–°æ¨¡å‹**ï¼š
1. åœ¨ `vllm/model_executor/models/` åˆ›å»ºæ¨¡å‹æ–‡ä»¶
2. ç»§æ‰¿é€‚å½“çš„åŸºç±»ï¼ˆå¦‚ `nn.Module`ï¼‰
3. åœ¨ `registry.py` æ³¨å†Œæ¨¡å‹
4. å®ç° `forward()` å’Œ `compute_logits()` æ–¹æ³•

**æ·»åŠ æ–°é‡åŒ–æ–¹æ³•**ï¼š
1. åœ¨ `vllm/model_executor/layers/quantization/` åˆ›å»ºæ–‡ä»¶
2. ç»§æ‰¿ `QuantizationConfig` å’Œ `QuantizeMethodBase`
3. å®ç° `create_weights()` å’Œ `apply()` æ–¹æ³•
4. åœ¨ `__init__.py` æ³¨å†Œ

**ä¿®æ”¹çº¿æ€§å±‚ GEMM**ï¼š
1. æŸ¥çœ‹ `vllm/model_executor/layers/linear.py`
2. ä¿®æ”¹ `UnquantizedLinearMethod.apply()` æˆ–åˆ›å»ºæ–°çš„ LinearMethod
3. å¯¹äº CUDA kernelï¼Œä¿®æ”¹ `csrc/` ä¸‹çš„ç›¸å…³æ–‡ä»¶

---

## 8. æ‰©å±•é˜…è¯»

- **çº¿æ€§å±‚ä¸ GEMM è¯¦è§£** â†’ [framework_lineargemm.md](./framework_lineargemm.md)
- **é¡¹ç›®æ•´ä½“ç»“æ„** â†’ [framework_overview.md](./framework_overview.md)
- **å®˜æ–¹æ–‡æ¡£** â†’ https://docs.vllm.ai/en/stable/
- **PagedAttention è®ºæ–‡** â†’ https://arxiv.org/abs/2309.06180
